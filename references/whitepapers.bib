@techreport{anthropic2025claude41,
  abstract = {This system card documents Claude Opus 4.1, a drop-in replacement for Opus 4 that delivers superior performance and precision for real-world coding and agentic tasks. The model achieves 74.5% software engineering accuracy, representing significant improvements over previous versions. Claude Opus 4.1 demonstrates up to 50% faster task completion with 45% fewer tool uses. This system card details the safety evaluations, capability assessments, and responsible deployment considerations for this advanced AI system, which represents Anthropic's most capable model to date.},
  address = {San Francisco, CA},
  author = {Anthropic},
  file = {:/home/b/documents/techreport/anthropic2025claude41.pdf:pdf},
  institution = {Anthropic},
  month = {8},
  pdf = {https://assets.anthropic.com/m/4c024b86c698d3d4/original/Claude-4-1-System-Card.pdf},
  title = {Claude Opus 4.1 System Card},
  type = {System Card},
  url = {https://www.anthropic.com/claude/opus},
  year = {2025}
}

@techreport{openai2025gpt45,
  abstract = {GPT-4.5 is OpenAI's largest and most knowledgeable model yet, released as a research preview on February 27, 2025. Building on GPT-4o, GPT-4.5 scales pre-training further and is designed to be more general-purpose than OpenAI's powerful STEM-focused reasoning models. The model is trained using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Unlike previous iterations, GPT-4.5 focuses on improving natural conversational abilities by scaling unsupervised learning rather than pure chain-of-thought reasoning. Early testing shows that interacting with GPT-4.5 feels more natural, with its broader knowledge base, stronger alignment with user intent, and improved emotional intelligence making it well-suited for tasks like writing, programming, and solving practical problems—with fewer hallucinations. OpenAI conducted extensive safety evaluations prior to deployment and did not find any significant increase in safety risk compared to existing models.},
  author = {OpenAI},
  file = {:/home/b/documents/techreport/openai2025gpt45.pdf:pdf},
  institution = {OpenAI},
  month = {2},
  pdf = {https://cdn.openai.com/gpt-4-5-system-card-2272025.pdf},
  title = {GPT-4.5 System Card},
  type = {System Card},
  url = {https://openai.com/index/gpt-4-5-system-card/},
  year = {2025}
}

@techreport{openai2025gpt5,
  author = {OpenAI},
  institution = {OpenAI},
  month = {8},
  note = {Note: As of August 2025, while GPT-5 was announced by OpenAI on August 7, 2025, a formal system card document could not be verified. This entry may refer to a hypothetical or forthcoming document.},
  title = {GPT-5 System Card},
  type = {System Card},
  year = {2025}
}

@techreport{google2025gemini25,
  abstract = {In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. With thinking capabilities, strong performance and improvements across the board on speed and efficiency, Gemini 2.5 is uniquely well suited for real world deployment and use in production applications, reaching a new level of capabilities at the Pareto frontier. Our 2.5 model family shows strong multimodal capabilities with up to 3 hours of video understanding and a context window of up to 2M tokens. The models also push the frontier of next generation agentic capabilities with native multimodal tool use, improved coding capabilities, compositional function calling, and native Python code execution. Gemini 2.5 leads SoTA on the MRCR (Multi Round Coreference Resolution) benchmark. It can rapidly process multimodal inputs including video and quickly produce coherent text and image outputs.},
  author = {Gemini Team, Google},
  file = {:/home/b/documents/techreport/google2025gemini25.pdf:pdf},
  institution = {Google DeepMind},
  month = {3},
  note = {arXiv:2507.06261},
  pdf = {https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf},
  title = {Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2507.06261},
  year = {2025}
}

@techreport{google2025gemini25pro,
  abstract = {Model card for Gemini 2.5 Pro, Google DeepMind's advanced multimodal AI model. Gemini 2.5 Pro features a context window of up to 1 million tokens (with plans to expand to 2 million), can generate up to 65,000 tokens in a single response, and includes multimodal capabilities with vision support and function calling. The model achieves world-leading performance across WebDev Arena and LMArena leaderboards. Gemini 2.5 Pro includes Deep Think, an experimental enhanced reasoning mode that uses new research techniques to consider multiple hypotheses before responding, achieving impressive scores on 2025 USAMO and 84.0% on MMMU benchmark. The model represents Google's most secure model family to date with improved protections against security threats and enhanced content safety compared to previous versions.},
  author = {Google DeepMind},
  file = {:/home/b/documents/techreport/google2025gemini25pro.pdf:pdf},
  institution = {Google DeepMind},
  month = {3},
  note = {Additional technical report available at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf},
  pdf = {https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro.pdf},
  title = {Gemini 2.5 Pro Model Card},
  type = {Model Card},
  url = {https://blog.google/products/gemini/gemini-2-5-pro-updates/},
  year = {2025}
}

@techreport{google2025gemini25flash,
  abstract = {Gemini 2.5 Flash is our powerful and most efficient workhorse model designed for speed and low-cost. It features enhanced reasoning capabilities, multimodal understanding across text, audio, images and video, a 1-million token context window, and thinking budgets for balancing latency and cost. The model is ideal for tasks like summarization, chat applications, data extraction, and captioning, while achieving 20-30% efficiency improvements in token usage compared to previous versions.},
  author = {Google DeepMind},
  file = {:/home/b/documents/techreport/google2025gemini25flash.pdf:pdf},
  institution = {Google DeepMind},
  month = {5},
  pdf = {https://storage.googleapis.com/model-cards/documents/gemini-2.5-flash.pdf},
  title = {Gemini 2.5 Flash Model Card},
  type = {Model Card},
  url = {https://deepmind.google/models/gemini/flash/},
  year = {2025}
}

@techreport{meta2025llama4scout,
  abstract = {The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. Llama 4 Scout is a 17 billion parameter model with 16 experts that delivers state-of-the-art performance for its class with an industry-leading 10 million token context length. The model uses a mixture-of-experts (MoE) architecture and incorporates early fusion for native multimodality, enabling both text and image understanding. Trained on approximately 40 trillion tokens of multimodal data with a knowledge cutoff of August 2024, the model supports 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. The model is released as BF16 weights and can fit within a single H100 GPU with on-the-fly int4 quantization.},
  author = {Meta AI},
  institution = {Meta},
  month = {4},
  note = {Released April 5, 2025. Available at r̆lhttps://huggingface.co/meta-llama/Llama-4-Scout-17B-16E},
  title = {Llama 4 Scout Model Card},
  type = {Model Card},
  url = {https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md},
  year = {2025}
}

@techreport{meta2025llama4maverick,
  abstract = {The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Maverick is a 17 billion active parameter model with 128 experts, and 400 billion total parameters. It is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks. Maverick is purpose-built for developers building sophisticated AI products—combining multilingual fluency, precise image understanding, and safe assistant behavior.},
  author = {Meta AI},
  day = {5},
  institution = {Meta},
  month = {4},
  note = {Supports 12 languages. Mixture-of-experts architecture with 17B active parameters across 128 experts, 400B total parameters. Pre-trained on ~22 trillion tokens of multimodal data.},
  pdf = {https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct},
  title = {Llama 4 Maverick Model Card},
  type = {Model Card},
  url = {https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct},
  year = {2025}
}

@techreport{mistral2025magistral,
  abstract = {We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.},
  archiveprefix = {arXiv},
  author = {Abhinav Rastogi and Albert Q. Jiang and Andy Lo and Gabrielle Berrada and Guillaume Lample and Jason Rute and Joep Barmentlo and Karmesh Yadav and Kartik Khandelwal and Khyathi Raghavi Chandu and Léonard Blier and Lucile Saulnier and Matthieu Dinot and Maxime Darrin and Neha Gupta and Roman Soletskyi and Sagar Vaze and Teven Le Scao and Yihan Wang and Adam Yang and Alexander H. Liu and Alexandre Sablayrolles and Amélie Héliou and Amélie Martin and Andy Ehrenberg and Anmol Agarwal and Antoine Roux and Arthur Darcet and Arthur Mensch and Baptiste Bout and Baptiste Rozière and Baudouin De Monicault and Chris Bamford and Christian Wallenwein and Christophe Renaudin and Clémence Lanfranchi and Darius Dabert and Devon Mizelle and Diego de las Casas and Elliot Chane-Sane and Emilien Fugier and Emma Bou Hanna and Gauthier Delerce and Gauthier Guinet and Georgii Novikov and Guillaume Martin and Himanshu Jaju and Jan Ludziejewski and Jean-Hadrien Chabran and Jean-Malo Delignon and Joachim Studnia and Jonas Amar and Josselin Somerville Roberts and Julien Denize and Karan Saxena and Kush Jain and Lingxiao Zhao and Louis Martin and Luyu Gao and Lélio Renard Lavaud and Marie Pellat and Mathilde Guillaumin and Mathis Felardos and Maximilian Augustin and Mickaël Seznec and Nikhil Raghuraman and Olivier Duchenne and Patricia Wang and Patrick von Platen and Patryk Saffer and Paul Jacob and Paul Wambergue and Paula Kurylowicz and Pavank and others},
  eprint = {2506.10910},
  file = {:/home/b/documents/techreport/mistral2025magistral.pdf:pdf},
  institution = {Mistral AI},
  month = {6},
  pdf = {https://arxiv.org/pdf/2506.10910},
  primaryclass = {cs.LG},
  title = {Magistral},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2506.10910},
  year = {2025}
}

@misc{mistral2025medium3,
  abstract = {Mistral AI has introduced Mistral Medium 3, a new class of language model that delivers state-of-the-art performance with remarkable efficiency. The model is designed to balance SOTA performance with 8X lower cost and simplified enterprise deployment. Key highlights include leading performance in professional use cases like coding and multimodal understanding, enterprise capabilities such as hybrid or on-premises/in-VPC deployment, and competitive benchmarking, performing at or above 90% of Claude Sonnet 3.7 while being significantly less expensive. The model represents a breakthrough in making advanced AI more accessible and cost-effective for enterprise applications, with the ability to be deployed across various cloud environments and customized for specific organizational needs.},
  author = {Mistral AI},
  howpublished = {r̆lhttps://mistral.ai/news/mistral-medium-3},
  month = {5},
  note = {Blog post announcement},
  title = {Medium is the new large: Mistral Medium 3},
  url = {https://mistral.ai/news/mistral-medium-3},
  year = {2025}
}

@techreport{openai2025o3,
  abstract = {OpenAI o3 and OpenAI o4-mini combine state-of-the-art reasoning with full tool capabilities---web browsing, Python, image and file analysis, image generation, canvas, automations, file search, and memory. This is the first launch and system card to be released under Version 2 of our Preparedness Framework. OpenAI's Safety Advisory Group (SAG) reviewed the results of our Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of our three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasks---especially excelling in areas like programming, business/consulting, and creative ideation.},
  author = {OpenAI},
  day = {16},
  file = {:/home/b/documents/techreport/openai2025o3.pdf:pdf},
  institution = {OpenAI},
  month = {4},
  pdf = {https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf},
  title = {OpenAI o3 and o4-mini System Card},
  type = {System Card},
  url = {https://openai.com/index/o3-o4-mini-system-card/},
  year = {2025}
}

@techreport{deepseek2025v3,
  abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},
  author = {DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and others},
  file = {:/home/b/documents/techreport/deepseek2025v3.pdf:pdf},
  institution = {DeepSeek},
  month = {2},
  number = {arXiv:2412.19437},
  openalex = {W4405903187},
  pdf = {https://arxiv.org/pdf/2412.19437},
  title = {DeepSeek-V3 Technical Report},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2412.19437},
  year = {2025}
}

@techreport{anthropic2024claude3,
  abstract = {This model card provides detailed information about the Claude 3 model family, including Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. The document outlines model capabilities, safety evaluations, performance benchmarks, and responsible deployment considerations. Claude 3 represents a significant advancement in AI systems, with Opus achieving near-human levels of comprehension and fluency on complex tasks while maintaining safety standards according to Anthropic's Responsible Scaling Policy.},
  address = {San Francisco, CA},
  author = {Anthropic},
  file = {:/home/b/documents/techreport/anthropic2024claude3.pdf:pdf},
  institution = {Anthropic},
  month = {3},
  pdf = {https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf},
  title = {Claude 3 Model Card},
  type = {Model Card},
  url = {https://www.anthropic.com/claude-3-model-card},
  year = {2024}
}

@techreport{anthropic2024claude35sonnet,
  abstract = {This system card documents Claude 3.5 Sonnet, Anthropic's most intelligent model in the Claude 3.5 family at the time of release. The document details safety evaluations, capability assessments, and responsible deployment practices. Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning, undergraduate-level knowledge, and coding proficiency while operating at twice the speed of Claude 3 Opus. The model underwent extensive safety testing and remains classified at ASL-2 according to Anthropic's Responsible Scaling Policy.},
  address = {San Francisco, CA},
  author = {Anthropic},
  file = {:/home/b/documents/techreport/anthropic2024claude35sonnet.pdf:pdf},
  institution = {Anthropic},
  month = {6},
  pdf = {https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf},
  title = {Claude 3.5 Sonnet System Card},
  type = {System Card},
  url = {https://www.anthropic.com/claude-3-7-sonnet-system-card},
  year = {2024}
}

@techreport{anthropic2024claude35haiku,
  abstract = {This system card documents Claude 3.5 Haiku, which matches the performance of Claude 3 Opus on many evaluations while maintaining the speed characteristics of the previous Haiku generation. The model shows particular strength in coding tasks, scoring 40.6% on SWE-bench Verified, outperforming many agents using publicly available state-of-the-art models. Claude 3.5 Haiku supports a context window of up to 200,000 tokens and underwent extensive safety evaluations across multiple languages and policy domains, maintaining rigorous safety standards while delivering substantial capability advances.},
  address = {San Francisco, CA},
  author = {Anthropic},
  file = {:/home/b/documents/techreport/anthropic2024claude35haiku.pdf:pdf},
  institution = {Anthropic},
  month = {10},
  pdf = {https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf},
  title = {Claude 3.5 Haiku System Card},
  type = {System Card},
  url = {https://www.anthropic.com/claude/haiku},
  year = {2024}
}

@techreport{openai2024gpt4o,
  abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network.},
  author = {OpenAI},
  file = {:/home/b/documents/techreport/openai2024gpt4o.pdf:pdf},
  institution = {OpenAI},
  month = {10},
  note = {arXiv:2410.21276 [cs.CL]},
  openalex = {W4404330874},
  pdf = {https://cdn.openai.com/gpt-4o-system-card.pdf},
  title = {GPT-4o System Card},
  type = {System Card},
  url = {https://openai.com/index/gpt-4o-system-card/},
  year = {2024}
}

@techreport{meta2024llama3,
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  author = {Llama Team and Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and others},
  institution = {Meta},
  month = {7},
  note = {arXiv:2407.21783 [cs.AI]},
  pdf = {https://scontent.fmaa8-1.fna.fbcdn.net/v/t39.2365-6/468347782_9231729823505907_4580471254289036098_n.pdf},
  title = {The Llama 3 Herd of Models},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2407.21783},
  year = {2024}
}

@techreport{jiang2024mixtral,
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bou Hanna, Emma and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Le Scao, Teven and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  file = {:/home/b/documents/techreport/jiang2024mixtral.pdf:pdf},
  institution = {Mistral AI},
  month = {1},
  number = {arXiv:2401.04088},
  openalex = {W4390723197},
  pdf = {https://arxiv.org/pdf/2401.04088},
  title = {Mixtral of Experts},
  type = {Technical Report},
  year = {2024}
}

@techreport{gemini2024gemini15,
  abstract = {In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
  author = {Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang and Soroosh Mariooryad and Yifan Ding and Xinyang Geng and Fred Alcober and Roy Frostig and Mark Omernick and others},
  file = {:/home/b/documents/techreport/gemini2024gemini15.pdf:pdf},
  institution = {Google DeepMind},
  month = {3},
  openalex = {W4392679398},
  pdf = {https://arxiv.org/pdf/2403.05530},
  title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2403.05530},
  year = {2024}
}

@techreport{anthropic2024claude3,
  author = {Anthropic},
  institution = {Anthropic},
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  type = {Model Card},
  year = {2024}
}

@techreport{openai2024gpt4,
  abstract = {This system card analyzes GPT-4, the latest large language model in the GPT family of models. It highlights safety challenges presented by the model's limitations (e.g., producing convincing text that is subtly false) and capabilities (e.g., increased adeptness at providing illicit advice, performance in dual-use capabilities, and risky emergent behaviors). The document provides a high-level overview of the safety processes OpenAI adopted to prepare GPT-4 for deployment, spanning work across measurements, model-level changes, product- and system-level interventions (such as monitoring and policies), and external expert engagement. It demonstrates that while mitigations and processes alter GPT-4's behavior and prevent certain kinds of misuses, they are limited and remain brittle in some cases. OpenAI brought in over 50 external red-teamers across a range of domains to test the model. Since completing its training in August 2022, the AI model has undergone thorough evaluation, adversarial testing, and iterative improvement, along with implementing system-level mitigations.},
  author = {OpenAI},
  day = {15},
  file = {:/home/b/documents/techreport/openai2024gpt4.pdf:pdf},
  institution = {OpenAI},
  month = {3},
  note = {Published March 15, 2023. Contains 103 references. Addresses GPT-4's tendency to hallucinate and produce content that is nonsensical or untruthful.},
  pdf = {https://cdn.openai.com/papers/gpt-4-system-card.pdf},
  title = {GPT-4 System Card},
  type = {System Card},
  url = {https://cdn.openai.com/papers/gpt-4-system-card.pdf},
  year = {2023}
}

@techreport{openai2024gpt4vision,
  abstract = {GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. This system card outlines how OpenAI prepared the vision capabilities of GPT-4 for deployment and describes the early access period of the model for small scale users and safety learnings OpenAI gained from this period, multimodal evaluations built to study the model's fitness for deployment, key findings of expert red teamers, and the mitigations OpenAI implemented prior to broad release. Beginning in March 2023, Be My Eyes and OpenAI collaborated to develop Be My AI, a new tool to describe the visual world for people who are blind or have low vision.},
  author = {OpenAI},
  file = {:/home/b/documents/techreport/openai2024gpt4vision.pdf:pdf},
  institution = {OpenAI},
  month = {9},
  pdf = {https://cdn.openai.com/papers/GPTV_System_Card.pdf},
  title = {GPT-4V(ision) System Card},
  type = {System Card},
  url = {https://openai.com/research/gpt-4v-system-card},
  year = {2023}
}

@techreport{openai2023gpt4,
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  author = {OpenAI},
  doi = {10.48550/arxiv.2303.08774},
  eprint = {2303.08774},
  eprintclass = {cs.CL},
  eprinttype = {arXiv},
  file = {:/home/b/documents/techreport/openai2023gpt4.pdf:pdf},
  institution = {OpenAI},
  month = {3},
  openalex = {W4327810158},
  pdf = {https://arxiv.org/pdf/2303.08774},
  title = {GPT-4 Technical Report},
  type = {Technical Report},
  year = {2023}
}

@techreport{touvron2023llama,
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  doi = {10.48550/arXiv.2302.13971},
  file = {:/home/b/documents/techreport/touvron2023llama.pdf:pdf},
  institution = {Meta AI},
  month = {2},
  note = {arXiv:2302.13971},
  openalex = {W4322718191},
  pdf = {https://arxiv.org/pdf/2302.13971.pdf},
  title = {LLaMA: Open and Efficient Foundation Language Models},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2302.13971},
  year = {2023}
}

@techreport{touvron2023llama2,
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Canton Ferrer, Cristian and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  eprint = {2307.09288},
  eprinttype = {arXiv},
  file = {:/home/b/documents/techreport/touvron2023llama2.pdf:pdf},
  institution = {Meta AI},
  month = {7},
  number = {arXiv:2307.09288},
  openalex = {W4384918448},
  pdf = {https://arxiv.org/pdf/2307.09288.pdf},
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2307.09288},
  year = {2023}
}

@techreport{jiang2023mistral,
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation.},
  archiveprefix = {arXiv},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  eprint = {2310.06825},
  file = {:/home/b/documents/techreport/jiang2023mistral.pdf:pdf},
  institution = {Mistral AI},
  month = {10},
  openalex = {W4387561528},
  pdf = {https://arxiv.org/pdf/2310.06825},
  primaryclass = {cs.CL},
  title = {Mistral 7B},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2310.06825},
  year = {2023}
}

@techreport{gemini2023gemini,
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  author = {Gemini Team, Google},
  file = {:/home/b/documents/techreport/gemini2023gemini.pdf:pdf},
  institution = {Google DeepMind},
  month = {12},
  openalex = {W4390041933},
  pdf = {https://arxiv.org/pdf/2312.11805},
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2312.11805},
  year = {2023}
}

@inproceedings{openai2022whisper,
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/openai2022whisper.pdf:pdf},
  month = {7},
  openalex = {W4311000453},
  pages = {28492--28518},
  pdf = {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  volume = {202},
  year = {2023}
}

@techreport{betker2023dalle3,
  abstract = {We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this is due to noisy and inaccurate image captions in the training dataset. We address this issue by training a bespoke image captioner to generate detailed, accurate captions for the training images, and then training text-to-image models on these synthetic captions. This approach leads to significant improvements in prompt following ability. We train a state-of-the-art image captioner to produce highly descriptive captions, using a blend of 95% synthetic captions and 5% ground truth captions. The resulting model, DALL-E 3, demonstrates superior performance compared to existing text-to-image systems, with human evaluators consistently preferring images generated by DALL-E 3 across benchmarks measuring prompt following, coherence, and aesthetics.},
  author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
  file = {:/home/b/documents/techreport/betker2023dalle3.pdf:pdf},
  institution = {OpenAI},
  month = {9},
  note = {Introduces DALL-E 3, which improves text-to-image generation through training on highly descriptive synthetic captions generated by a custom image captioner},
  pdf = {https://cdn.openai.com/papers/dall-e-3.pdf},
  title = {Improving Image Generation with Better Captions},
  type = {Technical Report},
  url = {https://cdn.openai.com/papers/dall-e-3.pdf},
  year = {2023}
}

@article{chowdhery2022palm,
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  file = {:/home/b/documents/article/chowdhery2022palm.pdf:pdf},
  journal = {Journal of Machine Learning Research},
  number = {240},
  openalex = {W4224308101},
  pages = {1--113},
  pdf = {https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf},
  title = {PaLM: Scaling Language Modeling with Pathways},
  volume = {24},
  year = {2023}
}

@techreport{anil2023palm2,
  abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. PaLM 2 is able to decompose a complex task into simpler subtasks and is better at understanding nuances of the human language, such as understanding riddles, idioms and implicit inferences. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between the models themselves and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
  author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and El Shafey, Laurent and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
  file = {:/home/b/documents/techreport/anil2023palm2.pdf:pdf},
  institution = {Google},
  month = {5},
  openalex = {W4377121468},
  pdf = {https://arxiv.org/pdf/2305.10403},
  title = {PaLM 2 Technical Report},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2305.10403},
  year = {2023}
}

@techreport{openai2022chatgpt,
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.},
  author = {OpenAI},
  day = {30},
  institution = {OpenAI},
  month = {11},
  note = {ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022. The model was trained using Reinforcement Learning from Human Feedback (RLHF).},
  title = {Introducing ChatGPT},
  type = {Blog Post},
  url = {https://openai.com/index/chatgpt/},
  year = {2022}
}

@techreport{ramesh2022dalle2,
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  doi = {10.48550/arXiv.2204.06125},
  file = {:/home/b/documents/techreport/ramesh2022dalle2.pdf:pdf},
  institution = {OpenAI},
  month = {4},
  note = {DALL-E 2},
  openalex = {W4283159404},
  pdf = {https://arxiv.org/pdf/2204.06125.pdf},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2204.06125},
  year = {2022}
}

@inproceedings{deepmind2022chinchilla,
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4× more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, a 7% improvement over Gopher.},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS 2022)},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  file = {:/home/b/documents/inproceedings/deepmind2022chinchilla.pdf:pdf},
  month = {12},
  openalex = {W4220764020},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {Advances in Neural Information Processing Systems},
  title = {Training Compute-Optimal Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@techreport{bai2022constitutional,
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and El Showk, Sheer and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  file = {:/home/b/documents/techreport/bai2022constitutional.pdf:pdf},
  institution = {Anthropic},
  month = {12},
  openalex = {W4311991106},
  pdf = {https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf},
  title = {Constitutional AI: Harmlessness from AI Feedback},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2212.08073},
  year = {2022}
}

@misc{openai2021dalle,
  abstract = {We've trained a neural network called DALL·E that creates images from text captions for a wide range of concepts expressible in natural language. DALL·E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs. We've found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.},
  author = {OpenAI},
  day = {5},
  howpublished = {OpenAI Blog},
  month = {1},
  note = {Introduces DALL·E, a 12-billion parameter version of GPT-3 trained to generate images from text descriptions. See also the related paper: Ramesh et al., "Zero-Shot Text-to-Image Generation", ICML 2021},
  title = {DALL·E: Creating Images from Text},
  url = {https://openai.com/index/dall-e/},
  year = {2021}
}

@techreport{deepmind2021gopher,
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and de Masson d'Autume, Cyprien and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  file = {:/home/b/documents/techreport/deepmind2021gopher.pdf:pdf},
  institution = {DeepMind},
  month = {12},
  openalex = {W4226146865},
  pdf = {https://arxiv.org/pdf/2112.11446},
  title = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2112.11446},
  year = {2021}
}

@techreport{buterin2014ethereum,
  abstract = {The Ethereum protocol was originally conceived as an upgraded version of a cryptocurrency, providing advanced features such as on-blockchain escrow, withdrawal limits, financial contracts, gambling markets and the like via a highly generalized programming language. The Ethereum protocol would not support any of the applications directly, but the existence of a Turing-complete programming language means that arbitrary contracts can theoretically be created for any transaction type or application. What is more interesting about Ethereum, however, is that the Ethereum protocol moves far beyond just currency. Protocols around decentralized file storage, decentralized computation and decentralized prediction markets, among dozens of other such concepts, have the potential to substantially increase the efficiency of the computational industry, and provide a massive boost to other peer-to-peer protocols by adding for the first time an economic layer. Finally, there is also a substantial array of applications that have nothing to do with money at all.},
  author = {Buterin, Vitalik},
  file = {:/home/b/documents/techreport/buterin2014ethereum.pdf:pdf},
  institution = {Ethereum Foundation},
  month = {12},
  pdf = {https://ethereum.org/content/whitepaper/whitepaper-pdf/Ethereum_Whitepaper_-_Buterin_2014.pdf},
  title = {Ethereum: A Next-Generation Smart Contract and Decentralized Application Platform},
  type = {White Paper},
  url = {https://ethereum.org/en/whitepaper/},
  year = {2014}
}

@inproceedings{ongaro2014raft,
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
  address = {Philadelphia, PA},
  author = {Ongaro, Diego and Ousterhout, John},
  booktitle = {2014 USENIX Annual Technical Conference (USENIX ATC 14)},
  file = {:/home/b/documents/inproceedings/ongaro2014raft.pdf:pdf},
  month = {6},
  note = {Awarded Best Paper},
  openalex = {W2156580773},
  pages = {305--319},
  pdf = {https://www.usenix.org/system/files/conference/atc14/atc14-paper-ongaro.pdf},
  publisher = {USENIX Association},
  title = {In Search of an Understandable Consensus Algorithm},
  url = {https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro},
  year = {2014}
}

@inproceedings{corbett2012spanner,
  abstract = {Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.},
  address = {Hollywood, CA},
  author = {Corbett, James C. and Dean, Jeffrey and Epstein, Michael and Fikes, Andrew and Frost, Christopher and Furman, J. J. and Ghemawat, Sanjay and Gubarev, Andrey and Heiser, Christopher and Hochschild, Peter and Hsieh, Wilson and Kanthak, Sebastian and Kogan, Eugene and Li, Hongyi and Lloyd, Alexander and Melnik, Sergey and Mwaura, David and Nagle, David and Quinlan, Sean and Rao, Rajesh and Rolig, Lindsay and Saito, Yasushi and Szymaniak, Michal and Taylor, Christopher and Wang, Ruth and Woodford, Dale},
  booktitle = {10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12)},
  file = {:/home/b/documents/inproceedings/corbett2012spanner.pdf:pdf},
  month = {10},
  openalex = {W2013409485},
  pages = {251--264},
  pdf = {https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf},
  publisher = {USENIX Association},
  title = {Spanner: Google's Globally-Distributed Database},
  year = {2012}
}

@inproceedings{kreps2011kafka,
  abstract = {Log data has long been a component of analytics infrastructure, where it refers to various types of activity data including user activity events, operational metrics, and system logs. Log data was traditionally used for offline analysis through data warehousing or batch processing systems. We present Kafka, a distributed messaging system for collecting and delivering high volumes of log data with low latency. Our system incorporates ideas from existing log aggregators and messaging systems, and is suitable for both offline and online message consumption. We show that Kafka has superior performance when compared to two popular messaging systems. We have been using Kafka in production for some time and it is processing over 60 billion messages per day at LinkedIn.},
  author = {Kreps, Jay and Narkhede, Neha and Rao, Jun},
  booktitle = {Proceedings of the 6th International Workshop on Networking Meets Databases (NetDB 2011)},
  file = {:/home/b/documents/inproceedings/kreps2011kafka.pdf:pdf},
  location = {Athens, Greece},
  note = {Co-located with SIGMOD 2011},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/09/Kafka.pdf},
  publisher = {ACM},
  title = {Kafka: a Distributed Messaging System for Log Processing},
  year = {2011}
}

@article{lakshman2010cassandra,
  abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
  author = {Lakshman, Avinash and Malik, Prashant},
  doi = {10.1145/1773912.1773922},
  file = {:/home/b/documents/article/lakshman2010cassandra.pdf:pdf},
  journal = {ACM SIGOPS Operating Systems Review},
  month = {4},
  number = {2},
  openalex = {W2132693424},
  pages = {35--40},
  pdf = {https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf},
  publisher = {ACM},
  title = {Cassandra: A Decentralized Structured Storage System},
  volume = {44},
  year = {2010}
}

@inproceedings{hunt2010zookeeper,
  abstract = {In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. The interface exposed by ZooKeeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability for all requests that change the ZooKeeper state. These design decisions enable the implementation of a high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2:1 to 100:1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications.},
  address = {Boston, MA},
  author = {Hunt, Patrick and Konar, Mahadev and Junqueira, Flavio P. and Reed, Benjamin},
  booktitle = {Proceedings of the 2010 USENIX Annual Technical Conference (USENIX ATC 10)},
  file = {:/home/b/documents/inproceedings/hunt2010zookeeper.pdf:pdf},
  month = {6},
  openalex = {W192446467},
  pages = {11--11},
  pdf = {https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf},
  publisher = {USENIX Association},
  title = {ZooKeeper: Wait-free Coordination for Internet-scale Systems},
  url = {https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems},
  year = {2010}
}

@techreport{nakamoto2008bitcoin,
  abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
  author = {Nakamoto, Satoshi},
  day = {31},
  file = {:/home/b/documents/techreport/nakamoto2008bitcoin.pdf:pdf},
  institution = {bitcoin.org},
  month = {10},
  openalex = {W4248175462},
  pages = {1--9},
  pdf = {https://bitcoin.org/bitcoin.pdf},
  title = {Bitcoin: A Peer-to-Peer Electronic Cash System},
  type = {Whitepaper},
  url = {https://bitcoin.org/bitcoin.pdf},
  year = {2008}
}

@inproceedings{decandia2007dynamo,
  abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an "always-on" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.},
  address = {Stevenson, Washington, USA},
  author = {DeCandia, Giuseppe and Hastorun, Deniz and Jampani, Madan and Kakulapati, Gunavardhan and Lakshman, Avinash and Pilchin, Alex and Sivasubramanian, Swaminathan and Vosshall, Peter and Vogels, Werner},
  booktitle = {Proceedings of the 21st ACM SIGOPS Symposium on Operating Systems Principles},
  doi = {10.1145/1294261.1294281},
  file = {:/home/b/documents/inproceedings/decandia2007dynamo.pdf:pdf},
  note = {Also published in ACM SIGOPS Operating Systems Review, Volume 41, Issue 6},
  openalex = {W3215028506},
  pages = {205--220},
  pdf = {https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf},
  publisher = {ACM},
  title = {Dynamo: Amazon's Highly Available Key-value Store},
  year = {2007}
}

@inproceedings{chang2006bigtable,
  abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
  address = {Seattle, WA},
  author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C. and Wallach, Deborah A. and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E.},
  booktitle = {Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI '06)},
  file = {:/home/b/documents/inproceedings/chang2006bigtable.pdf:pdf},
  month = {11},
  note = {Awarded Best Paper},
  openalex = {W2624304035},
  pages = {205--218},
  pdf = {https://research.google.com/archive/bigtable-osdi06.pdf},
  publisher = {USENIX Association},
  title = {Bigtable: A Distributed Storage System for Structured Data},
  year = {2006}
}

@inproceedings{dwork2006differential,
  abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers, culminating in those described in this paper, can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
  address = {Berlin, Heidelberg},
  author = {Dwork, Cynthia},
  booktitle = {Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II},
  doi = {10.1007/11787006_1},
  editor = {Bugliesi, Michele and Preneel, Bart and Sassone, Vladimiro and Wegener, Ingo},
  file = {:/home/b/documents/inproceedings/dwork2006differential.pdf:pdf},
  pages = {1--12},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/dwork.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Differential Privacy},
  volume = {4052},
  year = {2006}
}

@inproceedings{burrows2006chubby,
  abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences.},
  address = {Seattle, WA},
  author = {Burrows, Mike},
  booktitle = {Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI '06)},
  file = {:/home/b/documents/inproceedings/burrows2006chubby.pdf:pdf},
  month = {11},
  openalex = {W1992479210},
  pages = {335--350},
  pdf = {https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf},
  publisher = {USENIX Association},
  title = {The Chubby Lock Service for Loosely-Coupled Distributed Systems},
  year = {2006}
}

@inproceedings{dean2004mapreduce,
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
  address = {San Francisco, CA},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  booktitle = {Proceedings of the 6th Symposium on Operating Systems Design and Implementation (OSDI '04)},
  file = {:/home/b/documents/inproceedings/dean2004mapreduce.pdf:pdf},
  month = {12},
  pages = {137--150},
  pdf = {https://www.usenix.org/legacy/event/osdi04/tech/full_papers/dean/dean.pdf},
  publisher = {USENIX Association},
  title = {MapReduce: Simplified Data Processing on Large Clusters},
  year = {2004}
}

@inproceedings{ghemawat2003gfs,
  abstract = {We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.},
  address = {Bolton Landing, NY, USA},
  author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
  booktitle = {Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP '03)},
  doi = {10.1145/945445.945450},
  file = {:/home/b/documents/inproceedings/ghemawat2003gfs.pdf:pdf},
  month = {10},
  openalex = {W2119565742},
  pages = {29--43},
  pdf = {https://research.google.com/archive/gfs-sosp2003.pdf},
  publisher = {ACM},
  title = {The Google File System},
  year = {2003}
}

@techreport{lamport2001paxos,
  abstract = {At the PODC 2001 conference, I got tired of everyone saying how difficult it was to understand the Paxos algorithm, which I had presented in my paper ``The Part-Time Parliament'' published in ACM Transactions on Computer Systems in 1998. I therefore cornered a couple of people at the conference and explained the algorithm to them orally, with no paper. When I got home, I wrote down the explanation as a short note, which I later revised based on comments from Fred Schneider and Butler Lampson. The current version is 13 pages long, and contains no formula more complicated than $n1 > n2$. Although people got so hung up in the pseudo-Greek names that they found the paper hard to understand, the algorithm itself is very simple.},
  author = {Lamport, Leslie},
  file = {:/home/b/documents/techreport/lamport2001paxos.pdf:pdf},
  institution = {Microsoft Research},
  journal = {ACM SIGACT News (Distributed Computing Column)},
  month = {12},
  number = {4},
  openalex = {W2182688186},
  pages = {51--58},
  pdf = {https://courses.cs.washington.edu/courses/cse490h/11wi/CSE490H_files/CSE550.paxos-simple.pdf},
  title = {Paxos Made Simple},
  type = {Technical Report},
  volume = {32},
  year = {2001}
}

@phdthesis{fielding2000rest,
  abstract = {The World Wide Web has succeeded in large part because its software architecture has been designed to meet the needs of an Internet-scale distributed hypermedia system. The Web has been iteratively developed over the past ten years through a series of modifications to the standards that define its architecture. In order to identify those aspects of the Web that needed improvement and avoid undesirable modifications, a model for the modern Web architecture was needed to guide its design, definition, and deployment. Software architecture research investigates methods for determining how best to partition a system, how components identify and communicate with each other, how information is communicated, how elements of a system can evolve independently, and how all of the above can be described using formal and informal notations. My work is motivated by the desire to understand and evaluate the architectural design of network-based application software through principled use of architectural constraints, thereby obtaining the functional, performance, and social properties desired of an architecture. An architectural style is a named, coordinated set of architectural constraints. This dissertation defines a framework for understanding software architecture via architectural styles and demonstrates how styles can be used to guide the architectural design of network-based application software. A survey of architectural styles for network-based applications is used to classify styles according to the architectural properties they induce on an architecture for distributed hypermedia. I then introduce the Representational State Transfer (REST) architectural style and describe how REST has been used to guide the design and development of the architecture for the modern Web. REST emphasizes scalability of component interactions, generality of interfaces, independent deployment of components, and intermediary components to reduce interaction latency, enforce security, and encapsulate legacy systems. I describe the software engineering principles guiding REST and the interaction constraints chosen to retain those principles, contrasting them to the constraints of other architectural styles. Finally, I describe the lessons learned from applying REST to the design of the Hypertext Transfer Protocol and Uniform Resource Identifier standards, and from their subsequent deployment in Web client and server software.},
  author = {Fielding, Roy Thomas},
  file = {:/home/b/documents/phdthesis/fielding2000rest.pdf:pdf},
  month = {1},
  note = {Chair: Richard N. Taylor; Committee: Mark S. Ackerman, David S. Rosenblum},
  pdf = {https://ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation.pdf},
  school = {University of California, Irvine},
  title = {Architectural Styles and the Design of Network-based Software Architectures},
  type = {Doctoral dissertation},
  url = {https://ics.uci.edu/~fielding/pubs/dissertation/top.htm},
  year = {2000}
}
