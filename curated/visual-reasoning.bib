@inproceedings{wei2022chainofthought,
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning.},
  archiveprefix = {arXiv},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {2201.11903},
  file = {:/home/b/documents/inproceedings/wei2022chainofthought.pdf:pdf},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{wang2023selfconsistency,
  abstract = {Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).},
  archiveprefix = {arXiv},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle = {International Conference on Learning Representations},
  eprint = {2203.11171},
  file = {:/home/b/documents/inproceedings/wang2023selfconsistency.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=1PL1NIMMrw},
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  url = {https://openreview.net/forum?id=1PL1NIMMrw},
  year = {2023}
}

@article{zhang2023multimodal,
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.},
  archiveprefix = {arXiv},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  eprint = {2302.00923},
  file = {:/home/b/documents/article/zhang2023multimodal.pdf:pdf},
  journal = {Transactions on Machine Learning Research},
  pdf = {https://openreview.net/pdf?id=y1pPWFVfvR},
  title = {Multimodal Chain-of-Thought Reasoning in Language Models},
  url = {https://openreview.net/forum?id=y1pPWFVfvR},
  year = {2023}
}

@inproceedings{chen2024visual,
  abstract = {Knowledge-based visual reasoning remains a daunting task since it not only requires machines to interpret the concepts and relationships from visual scenes but also associate them with external world knowledge to conduct a chain of reasoning on open-world questions. We propose Visual Chain-of-thought Prompting (VCTP) for knowledge-based reasoning, which involves the interaction between visual content and natural language in an iterative step-by-step reasoning manner. VCTP contains three stages: see, think, and confirm. The confirm stage further uses the LLM to generate the supporting rationale for the answer, which is then passed through a cross-modality classifier to verify that it's consistent with the visual context.},
  archiveprefix = {arXiv},
  author = {Chen, Zhenfang and Zhou, Qinhong and Shen, Yikang and Hong, Yining and Sun, Zhiqing and Gutfreund, Dan and Gan, Chuang},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  eprint = {2301.06687},
  file = {:/home/b/documents/inproceedings/chen2024visual.pdf:pdf},
  number = {2},
  pdf = {https://zfchenunique.github.io/files/aaai24_vcot_arxiv.pdf},
  title = {Visual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/27888},
  volume = {38},
  year = {2024}
}

@inproceedings{mondal2024kamcot,
  abstract = {Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. We propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers.},
  archiveprefix = {arXiv},
  author = {Mondal, Debjyoti and Modi, Suraj and Panda, Subhadarshi and Singh, Rituraj and Rao, Godawari Sudhakar},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v38i17.29844},
  eprint = {2401.12863},
  file = {:/home/b/documents/inproceedings/mondal2024kamcot.pdf:pdf},
  number = {17},
  pages = {18798--18806},
  pdf = {https://arxiv.org/pdf/2401.12863.pdf},
  title = {KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/29844},
  volume = {38},
  year = {2024}
}

@inproceedings{chen2024spatialvlm,
  abstract = {Understanding and reasoning about spatial relationships is crucial for Visual Question Answering (VQA) and robotics. Vision Language Models (VLMs) have shown impressive performance in some VQA benchmarks but struggle with 3D spatial reasoning such as recognizing distances or size differences between physical objects. We propose training VLMs with extensive spatial reasoning data from the internet, developing an automatic 3D spatial VQA data generation framework that creates 2 billion VQA examples from 10 million real-world images. This enhanced VLM enables new applications in chain-of-thought spatial reasoning and robotics.},
  archiveprefix = {arXiv},
  author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  eprint = {2401.12168},
  file = {:/home/b/documents/inproceedings/chen2024spatialvlm.pdf:pdf},
  pages = {14455--14465},
  pdf = {https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf},
  title = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.html},
  year = {2024}
}

@inproceedings{shao2024visual,
  abstract = {Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. We address these challenges by collecting a large-scale Visual CoT dataset with 438k question-answer pairs, annotating intermediate bounding boxes highlighting key regions, and proposing a multi-turn processing pipeline that dynamically focuses on visual inputs.},
  archiveprefix = {arXiv},
  author = {Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and Zong, Zhuofan and Wang, Letian and Liu, Yu and Li, Hongsheng},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {2403.16999},
  file = {:/home/b/documents/inproceedings/shao2024visual.pdf:pdf},
  note = {NeurIPS 2024 Datasets and Benchmarks Track (Spotlight)},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0ff38d72a2e0aa6dbe42de83a17b2223-Paper-Datasets_and_Benchmarks_Track.pdf},
  title = {Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/0ff38d72a2e0aa6dbe42de83a17b2223-Abstract-Datasets_and_Benchmarks_Track.html},
  volume = {37},
  year = {2024}
}

@inproceedings{chen2018iterative,
  abstract = {We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs with parallel updates; and a global graph-reasoning module. The graph module has three components: a knowledge graph where classes are represented as nodes with edges encoding different types of semantic relationships; a region graph of the current image where regions are nodes and spatial relationships between regions are edges; and an assignment graph that assigns regions to classes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates.},
  archiveprefix = {arXiv},
  author = {Chen, Xinlei and Li, Li-Jia and Fei-Fei, Li and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  eprint = {1803.11189},
  file = {:/home/b/documents/inproceedings/chen2018iterative.pdf:pdf},
  pages = {7239--7248},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.pdf},
  title = {Iterative Visual Reasoning Beyond Convolutions},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.html},
  year = {2018}
}

@inproceedings{liu2017referring,
  abstract = {Referring expression is a kind of language expression that used for referring to particular objects. To make the expression without ambiguation, people often use attributes to describe the particular object. In this paper, we explore the role of attributes by incorporating them into both referring expression generation and comprehension. We first train an attribute learning model from visual objects and their paired descriptions, then in the generation task, we use the learned attributes as input into the generation model.},
  author = {Liu, Jingyu and Wang, Liang and Yang, Ming-Hsuan},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  doi = {10.1109/ICCV.2017.520},
  file = {:/home/b/documents/inproceedings/liu2017referring.pdf:pdf},
  pages = {4866--4874},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Referring_Expression_Generation_ICCV_2017_paper.pdf},
  title = {Referring Expression Generation and Comprehension via Attributes},
  url = {https://ieeexplore.ieee.org/document/8237782/},
  year = {2017}
}

@inproceedings{peng2024kosmos2,
  abstract = {We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown, i.e., [text span](bounding boxes), where object descriptions are sequences of location tokens. To train the model, we construct a large-scale dataset about grounded image-text pairs (GrIT) together with multimodal corpora. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability to downstream applications, while maintaining the conventional capabilities of MLLMs. Kosmos-2 is evaluated on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This study sheds a light on the big convergence of language, multimodal perception, and world modeling, which is a key step toward artificial general intelligence.},
  archiveprefix = {arXiv},
  author = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Ye, Qixiang and Wei, Furu},
  booktitle = {International Conference on Learning Representations},
  eprint = {2306.14824},
  file = {:/home/b/documents/inproceedings/peng2024kosmos2.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=lLmqxkfSIw},
  title = {Kosmos-2: Grounding Multimodal Large Language Models to the World},
  url = {https://openreview.net/forum?id=lLmqxkfSIw},
  year = {2024}
}

@inproceedings{jin2023refclip,
  abstract = {Referring Expression Comprehension (REC) is a task of grounding the referent based on an expression, and its development is greatly limited by expensive instance-level annotations. Most existing weakly supervised methods are built based on two-stage detection networks, which are computationally expensive. We resort to the efficient one-stage detector and propose a novel weakly supervised model called RefCLIP. Specifically, RefCLIP redefines weakly supervised REC as an anchor-text matching problem, which can avoid the complex post-processing in existing methods. To achieve weakly supervised learning, we introduce anchor-based contrastive loss to optimize RefCLIP via numerous anchor-text pairs. Based on RefCLIP, we further propose the first model-agnostic weakly supervised training scheme for existing REC models, where RefCLIP acts as a mature teacher to generate pseudo-labels for teaching common REC models. With our careful designs, this scheme can even help existing REC models achieve better weakly supervised performance than RefCLIP, e.g., TransVG and SimREC. To validate our approaches, we conduct extensive experiments on four REC benchmarks, i.e., RefCOCO, RefCOCO+, RefCOCOg and ReferItGame. Experimental results not only report our significant performance gains over existing weakly supervised models, e.g., +24.87% on RefCOCO, but also show the 5x faster inference speed.},
  address = {Vancouver, BC, Canada},
  author = {Lei Jin and Gen Luo and Yiyi Zhou and Xiaoshuai Sun and Guannan Jiang and Annan Shu and Rongrong Ji},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52729.2023.00263},
  file = {:/home/b/documents/inproceedings/jin2023refclip.pdf:pdf},
  month = {6},
  pages = {2681--2690},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.html},
  year = {2023}
}

@inproceedings{yan2024vigor,
  abstract = {By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce ViGoR (Visual Grounding Through Fine-Grained Reward Modeling), a framework that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. Our approach employs a comprehensive reward model that evaluates both the accuracy of visual grounding and the overall quality of generated text, enabling more effective training through reinforcement learning from human feedback. We conduct extensive experiments across multiple datasets and demonstrate substantial improvements in visual grounding performance while maintaining text generation quality.},
  address = {Milan, Italy},
  archiveprefix = {arXiv},
  author = {Siming Yan and Min Bai and Weifeng Chen and Xiong Zhou and Qixing Huang and Erran Li},
  booktitle = {Computer Vision -- ECCV 2024: 18th European Conference on Computer Vision},
  doi = {10.1007/978-3-031-73030-6_3},
  eprint = {2402.06118},
  file = {:/home/b/documents/inproceedings/yan2024vigor.pdf:pdf},
  month = {9},
  pdf = {https://arxiv.org/pdf/2402.06118.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling},
  url = {https://link.springer.com/chapter/10.1007/978-3-031-73030-6_3},
  volume = {15098},
  year = {2024}
}

@inproceedings{wan2024contrastive,
  abstract = {Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer. CRG achieves substantial improvements across a wide variety of VL tasks: when region annotations are provided, CRG increases absolute accuracy by up to 11.1% on ViP-Bench, up to 10% improvement on spatial reasoning, and 7.5% improvement on compositional generalization. For cases without reference regions, CRG can re-rank regions proposed by object detection models with an average 3.2% accuracy improvement when multiple proposals are available.},
  address = {Milan, Italy},
  archiveprefix = {arXiv},
  author = {David Wan and Jaemin Cho and Elias Stengel-Eskin and Mohit Bansal},
  booktitle = {Computer Vision -- ECCV 2024: 18th European Conference on Computer Vision},
  doi = {10.1007/978-3-031-72986-7_12},
  eprint = {2403.02325},
  file = {:/home/b/documents/inproceedings/wan2024contrastive.pdf:pdf},
  month = {9},
  pdf = {https://arxiv.org/pdf/2403.02325.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Contrastive Region Guidance: Improving Grounding in Vision-Language Models Without Training},
  url = {https://dl.acm.org/doi/10.1007/978-3-031-72986-7_12},
  volume = {15137},
  year = {2024}
}

@inproceedings{zhang2023grounding,
  abstract = {Classical planning systems have shown great advances in utilizing rule-based human knowledge to compute accurate plans for service robots, but they face challenges due to the strong assumptions of perfect perception and action executions. To tackle these challenges, one solution is to connect the symbolic states and actions generated by classical planners to the robot's sensory observations, thus closing the perception-action loop. This work proposes a visually-grounded planning framework, named TPVQA, which leverages Vision-Language Models (VLMs) to detect action failures and verify action affordances towards enabling successful plan execution. The framework bridges the gap between symbolic planning and real-world perception using pre-trained vision-language models. Results from quantitative experiments show that TPVQA surpasses competitive baselines from previous studies in task completion rate.},
  address = {London, UK},
  archiveprefix = {arXiv},
  author = {Xiaohan Zhang and Yan Ding and Saeid Amiri and Hao Yang and Andy Kaminski and Chad Esselink and Shiqi Zhang},
  booktitle = {ICRA 2023 Workshop on Robot Execution Failures and Failure Management Strategies},
  eprint = {2304.08587},
  file = {:/home/b/documents/inproceedings/zhang2023grounding.pdf:pdf},
  month = {6},
  organization = {IEEE},
  pdf = {https://arxiv.org/pdf/2304.08587.pdf},
  title = {Grounding Classical Task Planners via Vision-Language Models},
  url = {https://robot-failures.github.io/icra2023/},
  year = {2023}
}

@inproceedings{prasad2024rephrase,
  abstract = {An increasing number of vision-language tasks can now be handled in zero and few-shot settings by combining large language models with vision encoders, creating large vision-language models (LVLMs). However, how an input is presented can significantly impact zero-shot model performance. Underspecified inputs can result in incorrect answers due to missing visual information, complex implicit reasoning, or linguistic ambiguity. We propose Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient image details using the LVLM as a captioner and reasoner, and proposes modifications to the original question. We use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the best modification. Our experiments demonstrate substantial improvements in zero-shot accuracy: 3.85% increase on VQAv2, 6.41% on A-OKVQA, and 7.94% on VizWiz. In oracle settings using gold answers, we achieved up to 14.41% accuracy increase, demonstrating that adding visually-grounded information can improve model performance by reducing underspecification.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.05861},
  file = {:/home/b/documents/inproceedings/prasad2024rephrase.pdf:pdf},
  month = {5},
  pdf = {https://arxiv.org/pdf/2310.05861.pdf},
  publisher = {OpenReview.net},
  title = {Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
  url = {https://openreview.net/forum?id=L4nOxziGf9},
  year = {2024}
}

@inproceedings{zhao2025cotvla,
  abstract = {Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input-output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. To address this limitation, we introduce CoT-VLA, which incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. Our approach leverages visual reasoning to provide intermediate planning steps, enabling more sophisticated manipulation capabilities. Experimental results demonstrate that CoT-VLA achieves strong performance, outperforming state-of-the-art VLA models by 17% in real-world manipulation tasks and 6% in simulation benchmarks.},
  archiveprefix = {arXiv},
  author = {Qingqing Zhao and Yao Lu and Moo Jin Kim and Zipeng Fu and Zhuoyang Zhang and Yecheng Wu and Zhaoshuo Li and Qianli Ma and Song Han and Chelsea Finn and Ankur Handa and Ming-Yu Liu and Donglai Xiang and Gordon Wetzstein and Tsung-Yi Lin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  eprint = {2503.22020},
  file = {:/home/b/documents/inproceedings/zhao2025cotvla.pdf:pdf},
  month = {6},
  pages = {1702--1713},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.pdf},
  publisher = {IEEE},
  title = {CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.html},
  year = {2025}
}

@article{arnab2025temporal,
  abstract = {Despite recent advances in Vision-Language Models (VLMs), long-video understanding remains a challenging problem. Although state-of-the-art long-context VLMs can process around 1000 input frames, they still struggle to effectively leverage this sequence length, and succumb to irrelevant distractors within the context window. We present Temporal Chain of Thought, an inference strategy for video question-answering that curates the model's input context. We use the VLM itself to iteratively identify and extract the most relevant frames from the video, which are then used for answering. We demonstrate how leveraging more computation at inference-time to select the most relevant context leads to improvements in accuracy, in agreement with recent work on inference-time scaling of LLMs. Moreover, we achieve state-of-the-art results on 4 diverse video question-answering datasets, showing consistent improvements with 3 different VLMs. A standout result is on LVBench videos averaging 68 minutes long, where our approach using a 32K context window outperforms the same VLM using standard inference with a 700K context window by 2.8 points.},
  archiveprefix = {arXiv},
  author = {Anurag Arnab and Ahmet Iscen and Mathilde Caron and Alireza Fathi and Cordelia Schmid},
  eprint = {2507.02001},
  file = {:/home/b/documents/article/arnab2025temporal.pdf:pdf},
  journal = {arXiv preprint arXiv:2507.02001},
  month = {7},
  pdf = {https://arxiv.org/pdf/2507.02001.pdf},
  title = {Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames},
  url = {https://arxiv.org/abs/2507.02001},
  year = {2025}
}

@article{jin2025cotvid,
  abstract = {System2 reasoning is developing rapidly these days with the emergence of Deep-Thinking Models and chain-of-thought technology, which has become a centralized discussion point in the AI community. However, there is a relative gap in the research on complex video reasoning at present. We propose CoT-Vid, a novel training-free paradigm for the video domain with a multistage complex reasoning design. Distinguishing from existing video LLMs, which rely heavily on perceptual abilities, our paradigm achieved surprising performance gain with explicit reasoning mechanism. The paradigm consists of three main components: dynamic inference path routing, problem decoupling strategy, and video self-consistency verification. Experimental results demonstrate that CoT-Vid outperforms the base model by 9.3% on Egochema and 5.6% on VideoEspresso, and rivals models like GPT-4V, GPT-4o, and Gemini-1.5-flash. Our findings suggest that explicit reasoning mechanisms can significantly enhance video understanding performance without requiring additional training.},
  archiveprefix = {arXiv},
  author = {Hongbo Jin and Ruyang Liu and Wenhao Zhang and Guibo Luo and Ge Li},
  eprint = {2505.11830},
  file = {:/home/b/documents/article/jin2025cotvid.pdf:pdf},
  journal = {arXiv preprint arXiv:2505.11830},
  month = {5},
  pdf = {https://arxiv.org/pdf/2505.11830.pdf},
  title = {CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning},
  url = {https://arxiv.org/abs/2505.11830},
  year = {2025}
}

@article{corbire2025retrievalbased,
  abstract = {While chain-of-thought (CoT) prompting improves reasoning in large language models, its effectiveness in vision-language models (VLMs) remains limited due to over-reliance on textual cues and memorized knowledge. To address this limitation, we present DrivingVQA, a visual question answering dataset derived from driving theory exams, containing 3,931 multiple-choice problems with expert-written explanations and grounded entities relevant to the reasoning process. We also introduce RIV-CoT (Retrieval-Based Interleaved Visual Chain-of-Thought), a framework that enhances VLMs' visual reasoning by explicitly grounding their chain-of-thought process in visual evidence. Our experiments demonstrate that RIV-CoT improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over vanilla CoT prompting, highlighting the importance of visual grounding in complex real-world scenarios.},
  archiveprefix = {arXiv},
  author = {Charles Corbière and Simon Roburin and Syrielle Montariol and Antoine Bosselut and Alexandre Alahi},
  eprint = {2501.04671},
  file = {:/home/b/documents/article/corbire2025retrievalbased.pdf:pdf},
  journal = {arXiv preprint arXiv:2501.04671},
  month = {1},
  note = {Project website: r̆lhttps://vita-epfl.github.io/DrivingVQA},
  pdf = {https://arxiv.org/pdf/2501.04671.pdf},
  title = {Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios},
  url = {https://arxiv.org/abs/2501.04671},
  year = {2025}
}

@inproceedings{wang2025mllmtool,
  abstract = {Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. To eliminate that, this paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.},
  address = {Tucson, AZ, USA},
  archiveprefix = {arXiv},
  author = {Wang, Chenyu and Luo, Weixin and Dong, Sixun and Xuan, Xiaohua and Li, Zhengxin and Ma, Lin and Gao, Shenghua},
  booktitle = {2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  doi = {10.1109/WACV61041.2025.00650},
  eprint = {2401.10727},
  file = {:/home/b/documents/inproceedings/wang2025mllmtool.pdf:pdf},
  pages = {6678--6687},
  pdf = {https://openaccess.thecvf.com/content/WACV2025/papers/Wang_MLLM-Tool_A_Multimodal_Large_Language_Model_for_Tool_Agent_Learning_WACV_2025_paper.pdf},
  publisher = {IEEE},
  title = {MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning},
  url = {https://openaccess.thecvf.com/content/WACV2025/html/Wang_MLLM-Tool_A_Multimodal_Large_Language_Model_for_Tool_Agent_Learning_WACV_2025_paper.html},
  year = {2025}
}

@inproceedings{gao2025multimodal,
  abstract = {The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via Trajectory Tuning on VLMs for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and Qwen2-VL-7B, which outperforms untrained VLMs by 20%, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities.},
  archiveprefix = {arXiv},
  author = {Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Ma, Xiaojian and Yuan, Tao and Fan, Yue and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
  booktitle = {International Conference on Learning Representations (ICLR)},
  eprint = {2412.15606},
  file = {:/home/b/documents/inproceedings/gao2025multimodal.pdf:pdf},
  note = {Spotlight},
  pdf = {https://openreview.net/pdf?id=0bmGL4q7vJ},
  title = {Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage},
  url = {https://openreview.net/forum?id=0bmGL4q7vJ},
  year = {2025}
}

@article{kumar2025reinforcing,
  abstract = {Despite tremendous recent advances in large model reasoning ability, vision-language models (VLMs) still struggle with detailed visual reasoning, especially when compute resources are limited. To address this challenge, we draw inspiration from methods like DeepSeek-R1 for VLMs and train smaller-scale models with Group Relative Policy Optimization (GRPO) to use external tools such as zoom. The greatest benefit is obtained with a combination of GRPO learning, a simple reward structure, a simplified tool-calling interface, allocating additional tokens to the result of the tool call, and a training data mix that over-represents visually difficult examples.},
  archiveprefix = {arXiv},
  author = {Kumar, Sunil and Zhao, Bowen and Dirac, Leo and Varshavskaya, Paulina},
  eprint = {2506.14821},
  file = {:/home/b/documents/article/kumar2025reinforcing.pdf:pdf},
  journal = {arXiv preprint arXiv:2506.14821},
  note = {Submitted: 10 Jun 2025, Last revised: 5 Aug 2025},
  pdf = {https://arxiv.org/pdf/2506.14821},
  title = {Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints},
  url = {https://arxiv.org/abs/2506.14821},
  year = {2025}
}

@article{davis2025augmented,
  abstract = {Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.},
  archiveprefix = {arXiv},
  author = {Davis, Anthony C. and Sadiq, Burhan and Shu, Tianmin and Huang, Chien-Ming},
  doi = {10.48550/arXiv.2507.22933},
  eprint = {2507.22933},
  file = {:/home/b/documents/article/davis2025augmented.pdf:pdf},
  institution = {Johns Hopkins University},
  journal = {arXiv preprint arXiv:2507.22933},
  note = {Submitted: 24 Jul 2025},
  pdf = {https://arxiv.org/pdf/2507.22933},
  title = {Augmented Vision-Language Models: A Systematic Review},
  url = {https://arxiv.org/abs/2507.22933},
  year = {2025}
}

@inproceedings{wang2025learning,
  abstract = {Visual grounding tasks aim to localize image regions based on natural language references. In this work, we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We find that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We propose to prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. We propose attribute modeling to explicitly capture the important object attributes, and spatial relation modeling to capture inter-object relationship, both of which are common linguistic pattern in referring expression. Our constructed dataset contains 500K images, 1M objects, and 16M referring expressions, making it one of the largest grounding datasets to date, and the first grounding dataset with purely model-generated queries and human-annotated objects.},
  archiveprefix = {arXiv},
  author = {Wang, Shijie and Kim, Dahun and Taalimi, Ali and Sun, Chen and Kuo, Weicheng},
  booktitle = {2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  eprint = {2407.14563},
  file = {:/home/b/documents/inproceedings/wang2025learning.pdf:pdf},
  pdf = {https://openaccess.thecvf.com/content/WACV2025/papers/Wang_Learning_Visual_Grounding_from_Generative_Vision_and_Language_Model_WACV_2025_paper.pdf},
  publisher = {IEEE},
  title = {Learning Visual Grounding from Generative Vision and Language Model},
  url = {https://openaccess.thecvf.com/content/WACV2025/html/Wang_Learning_Visual_Grounding_from_Generative_Vision_and_Language_Model_WACV_2025_paper.html},
  year = {2025}
}

@inproceedings{xu2024vlmgrounder,
  abstract = {3D visual grounding is crucial for robots, requiring integration of natural language and 3D scene understanding. Traditional methods depending on supervised learning with 3D point clouds are limited by scarce datasets. Recently zero-shot methods leveraging LLMs have been proposed to address the data issue. While effective, these methods only use object-centric information, limiting their ability to handle complex queries. We present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder dynamically stitches image sequences, employs a grounding and feedback scheme to find the target object, and uses a multi-view ensemble projection to accurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D datasets show VLM-Grounder outperforms previous zero-shot methods, achieving 51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D geometry or object priors.},
  archiveprefix = {arXiv},
  author = {Xu, Runsen and Huang, Zhiwei and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  booktitle = {8th Annual Conference on Robot Learning (CoRL)},
  eprint = {2410.13860},
  file = {:/home/b/documents/inproceedings/xu2024vlmgrounder.pdf:pdf},
  note = {Student Paper},
  pdf = {https://openreview.net/pdf?id=IcOrwlXzMi},
  title = {VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding},
  url = {https://openreview.net/forum?id=IcOrwlXzMi},
  year = {2024}
}

@inproceedings{le2025progressive,
  abstract = {Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular Vision-Language alignments (PromViL), a novel framework to enhance LVLMs' ability in performing grounded compositional visual reasoning tasks. The approach constructs a hierarchical structure of multi-modal alignments, ranging from simple to complex concepts. By progressively aligning textual descriptions with corresponding visual regions, the model learns to leverage contextual information from lower levels to inform higher-level reasoning. To facilitate this learning process, we introduce a data generation process that creates a novel dataset derived from Visual Genome, providing a wide range of nested compositional vision-language pairs. PromViL, with only 4.9% tunable parameters and 60K fine-tuning data samples, shows significant improvements: an approximately 9.0 point increase on our benchmark; up to 5.5 on zero-shot grounding tasks; and nearly 5 point and 10 point increases in accuracy and validity on zero-shot compositional reasoning tasks.},
  archiveprefix = {arXiv},
  author = {Le, Quang-Hung and Dang, Long Hoang and Le, Ngan and Tran, Truyen and Le, Thao Minh},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  eprint = {2412.08125},
  file = {:/home/b/documents/inproceedings/le2025progressive.pdf:pdf},
  note = {Code available at r̆lhttps://github.com/lqh52/PromViL},
  pdf = {https://arxiv.org/pdf/2412.08125.pdf},
  title = {Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/32471},
  year = {2025}
}

@article{kao2025think,
  abstract = {Reasoning segmentation is a challenging vision-language task that aims to output the segmentation mask with respect to a complex, implicit, and even non-visual query text. Previous works incorporated multimodal Large Language Models (MLLMs) with segmentation models to approach the difficult problem. However, their segmentation quality often falls short in complex cases, particularly when dealing with out-of-domain objects with intricate structures, blurry boundaries, occlusions, or high similarity with surroundings. In this paper, we introduce ThinkFirst, a training-free reasoning segmentation framework that leverages GPT's chain of thought to improve segmentation quality. Our approach allows GPT-4o or other powerful MLLMs to generate a detailed, chain-of-thought description of an image, which is then passed to a language-instructed segmentation assistant to aid the segmentation process.},
  archiveprefix = {arXiv},
  author = {Kao, Shiu-hong and Tai, Yu-Wing and Tang, Chi-Keung},
  eprint = {2503.07503},
  file = {:/home/b/documents/article/kao2025think.pdf:pdf},
  journal = {arXiv preprint arXiv:2503.07503},
  note = {Submitted: 10 Mar 2025, Last revised: 28 Jun 2025. Project page: r̆lhttps://danielshkao.github.io/thinkfirst.html},
  pdf = {https://arxiv.org/pdf/2503.07503},
  title = {Think Before You Segment: High-Quality Reasoning Segmentation with GPT Chain of Thoughts},
  url = {https://arxiv.org/abs/2503.07503},
  year = {2025}
}

@article{liu2025mmc,
  abstract = {Visual language models (VLMs) face challenges like hallucinations in reasoning tasks. This paper proposes a multimodal actor-critic framework to enhance VLM reasoning, where an actor model generates step-by-step reasoning paths based on image and text inputs, while a critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets using Monte Carlo Tree Search (MCTS) to explore reasoning paths and generate diverse reasoning trajectories. We develop the MMC dataset to train and refine VLM reasoning capabilities.},
  archiveprefix = {arXiv},
  author = {Liu, Shuhang and Zhang, Zhenrong and Hu, Pengfei and Ma, Jiefeng and Du, Jun and Wang, Qing and Zhang, Jianshu and Liu, Quan and Gao, Jianqing and Ma, Feng},
  doi = {10.48550/arXiv.2504.11009},
  eprint = {2504.11009},
  file = {:/home/b/documents/article/liu2025mmc.pdf:pdf},
  institution = {NERC-SLIP, University of Science and Technology of China and IFLYTEK Research},
  journal = {arXiv preprint arXiv:2504.11009},
  note = {Submitted: 15 Apr 2025},
  pdf = {https://arxiv.org/pdf/2504.11009},
  title = {MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique},
  url = {https://arxiv.org/abs/2504.11009},
  year = {2025}
}

@inproceedings{soo2024recurrent,
  abstract = {In neuroscience, recurrent neural networks (RNNs) are modeled as continuous-time dynamical systems to more accurately reflect the dynamics inherent in biological circuits. However, convolutional neural networks (CNNs) remain the preferred architecture in vision neuroscience due to their ability to efficiently process visual information, which comes at the cost of the biological realism provided by RNNs. Here, we introduce a hybrid architecture that integrates the continuous-time recurrent dynamics of RNNs with the spatial processing capabilities of CNNs. Our models preserve the dynamical characteristics typical of RNNs while having comparable performance with their conventional CNN counterparts on benchmarks like ImageNet. Compared to conventional CNNs, our models demonstrate increased robustness to noise due to noise-suppressing mechanisms inherent in recurrent dynamical systems.},
  author = {Soo, Wayne WM and Battista, Aldo and Radmard, Puria and Wang, Xiao-Jing},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  file = {:/home/b/documents/inproceedings/soo2024recurrent.pdf:pdf},
  note = {Spotlight Paper},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f536d5697b79a9b3b3debbb7a552a7da-Paper-Conference.pdf},
  title = {Recurrent neural network dynamical systems for biological vision},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/f536d5697b79a9b3b3debbb7a552a7da-Abstract-Conference.html},
  year = {2024}
}

@article{chemnitz2025dynamical,
  abstract = {In this chapter, we utilize dynamical systems to analyze several aspects of machine learning algorithms. As an expository contribution we demonstrate how to re-formulate a wide variety of challenges from deep neural networks, (stochastic) gradient descent, and related topics into dynamical statements. We tackle three concrete challenges: first, we consider the process of information propagation through a neural network, i.e., we study the input-output map for different architectures. We explain the universal embedding property for augmented neural ODEs representing arbitrary functions of given regularity, the classification of multilayer perceptrons and neural ODEs in terms of suitable function classes, and the memory-dependence in neural delay equations. Second, we consider the training aspect of neural networks dynamically, i.e., we describe a dynamical systems perspective on gradient descent and study stability for overdetermined problems. For stochastic gradient descent, we present stability results for the overparameterized setting via Lyapunov exponents of interpolation solutions. Third, we explain several results regarding mean-field limits of neural networks. We describe a result that extends existing techniques to heterogeneous neural networks involving graph limits via digraph measures. We show how large classes of neural networks naturally fall within the framework of Kuramoto-type models on graphs and their large-graph limits.},
  archiveprefix = {arXiv},
  author = {Dennis Chemnitz and Maximilian Engel and Christian Kuehn and Sara-Viola Kuntz},
  eprint = {2507.05164},
  file = {:/home/b/documents/article/chemnitz2025dynamical.pdf:pdf},
  journal = {arXiv preprint arXiv:2507.05164},
  note = {Book Chapter Preprint},
  pdf = {https://arxiv.org/pdf/2507.05164},
  title = {A Dynamical Systems Perspective on the Analysis of Neural Networks},
  url = {https://arxiv.org/abs/2507.05164},
  year = {2025}
}

@inproceedings{thawakar2025llamavo1,
  abstract = {Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning chain benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMs' abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favourably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8% across six benchmarks while being 5x faster during inference scaling. Our benchmark, model, and code are publicly available.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Omkar Thawakar and Dinura Dissanayake and Ketan More and Ritesh Thawkar and Ahmed Heakl and Noor Ahsan and Yuhao Li and Mohammed Zumri and Jean Lahoud and Rao Muhammad Anwer and Hisham Cholakkal and Ivan Laptev and Mubarak Shah and Fahad Shahbaz Khan and Salman Khan},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
  doi = {10.18653/v1/2025.findings-acl.1247},
  eprint = {2501.06186},
  file = {:/home/b/documents/inproceedings/thawakar2025llamavo1.pdf:pdf},
  pdf = {https://arxiv.org/pdf/2501.06186},
  publisher = {Association for Computational Linguistics},
  title = {LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs},
  url = {https://aclanthology.org/2025.findings-acl.1247/},
  year = {2025}
}
