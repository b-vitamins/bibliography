@misc{alamu2025pinnsclimate,
  abstract = {Physics-Informed Neural Networks (PINNs) represent a transformative paradigm at the intersection of machine learning and physical sciences, offering a powerful framework to address the complex challenges inherent in climate modeling. The article provides a comprehensive overview of PINNs, detailing their foundational principles, unique advantages over conventional numerical and purely data-driven machine learning methods, and their diverse applications across atmospheric, oceanic, and broader Earth system domains.},
  author = {Raphael Alamu and Sudarshana Karkala and Sazzad Hossain and Mahendra Krishnapatnam and Ankur Aggarwal and Zarif Zahir and Harshad Vijay Pandhare and Varun Shah},
  day = {12},
  howpublished = {ResearchGate},
  keywords = {physics-informed neural networks, climate modeling, machine learning, physical sciences, atmospheric modeling, oceanic modeling},
  month = {6},
  note = {Comprehensive review article on the application of Physics-Informed Neural Networks to climate modeling challenges},
  title = {Physics-Informed Neural Networks for Climate Modeling: Bridging Machine Learning and Physical Laws},
  url = {https://www.researchgate.net/publication/392622112_Physics-Informed_Neural_Networks_for_Climate_Modeling_Bridging_Machine_Learning_and_Physical_Laws},
  year = {2025}
}

@article{kim2025pint,
  abstract = {The paper introduces PINT, a framework that integrates physical constraints into neural time series models to improve their ability to capture complex dynamics. It applies this approach to the ERA5 WeatherBench dataset, focusing on long-term temperature forecasting by incorporating the Simple Harmonic Oscillator Equation into neural network architectures.},
  archiveprefix = {arXiv},
  author = {Keonvin Park and Jisu Kim and Jaemin Seo},
  eprint = {2502.04018},
  file = {:/home/b/documents/article/kim2025pint.pdf:pdf},
  journal = {arXiv preprint arXiv:2502.04018},
  month = {2},
  note = {Accepted at ICLR 2025 Workshop World Models},
  openalex = {W4407245174},
  pdf = {https://arxiv.org/pdf/2502.04018.pdf},
  primaryclass = {cs.LG},
  title = {PINT: Physics-Informed Neural Time Series Models with Applications to Long-term Inference on WeatherBench 2m-Temperature Data},
  url = {https://arxiv.org/abs/2502.04018},
  year = {2025}
}

@article{park2025foundation,
  abstract = {Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws into neural network training. However, traditional PINNs are typically designed for single PDEs, requiring problem-specific architectures. This paper explores the potential of a foundation PINN model capable of solving multiple PDEs within a unified architecture. We investigate the efficacy of a single PINN framework trained on four distinct PDEs: the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave Equation, and the 2D Laplace Equation, demonstrating its ability to learn diverse physical dynamics. To enhance sample efficiency, we integrate active learning strategies into the training process, selecting data points based on uncertainty estimation using Monte Carlo Dropout. We evaluate models trained on varying percentages of the full dataset (10%, 20%, 30%, 40%, and 50%) to assess the impact of active sampling on performance.},
  author = {Keon Vin Park},
  eprint = {2502.07425},
  eprinttype = {arXiv},
  journal = {arXiv preprint arXiv:2502.07425},
  month = {2},
  note = {Withdrawn},
  primaryclass = {cs.LG},
  title = {Towards a Foundation Model for Physics-Informed Neural Networks: Multi-PDE Learning with Active Sampling},
  url = {https://arxiv.org/abs/2502.07425},
  year = {2025}
}

@article{nguyen2024aurora,
  abstract = {Reliable forecasting of the Earth system is essential for mitigating natural disasters and supporting human progress. Traditional numerical models, although powerful, are extremely computationally expensive. Recent advances in artificial intelligence (AI) have shown promise in improving both predictive performance and efficiency, yet their potential remains underexplored in many Earth system domains. Here we introduce Aurora, a large-scale foundation model trained on over one million hours of diverse geophysical data that can outperform operational forecasts in predicting various Earth system phenomena at significantly lower computational cost. Aurora demonstrates state-of-the-art performance across multiple domains including 5-day global air pollution forecasts, 10-day global ocean wave forecasts, 5-day tropical cyclone track forecasts, and 10-day global weather forecasts at high resolution while reducing computational requirements by orders of magnitude compared to traditional numerical models.},
  author = {Cristian Bodnar and Wessel P. Bruinsma and Ana Lucic and Megan Stanley and Anna Vaughan and Johannes Brandstetter and Patrick Garvan and Maik Riechert and Jonathan A. Weyn and Haiyu Dong and Jayesh K. Gupta and Kit Thambiratnam and Alexander T. Archibald and Chun-Chieh Wu and Elizabeth Heider and Max Welling and Richard E. Turner and Paris Perdikaris},
  doi = {10.1038/s41586-025-09005-y},
  file = {:/home/b/documents/article/nguyen2024aurora.pdf:pdf},
  journal = {Nature},
  month = {5},
  pages = {1180--1187},
  pdf = {https://arxiv.org/pdf/2405.13063.pdf},
  title = {Aurora: A Foundation Model of the Atmosphere},
  url = {https://www.nature.com/articles/s41586-025-09005-y},
  volume = {641},
  year = {2025}
}

@article{jha2025practicalintro,
  abstract = {This paper explores neural operator architectures for approximating solutions to parametric partial differential equations (PDEs). The review covers foundational neural operator models including Deep Operator Networks (DeepONet), Principal Component Analysis-based Neural Networks (PCANet), and Fourier Neural Operators (FNO). These architectures are demonstrated on two classical linear parametric PDEs: the Poisson equation and linear elastic deformation. The introduction is designed to be self-contained, hands-on, and transparent regarding algorithmic details, focusing on foundational ideas and practical implementation aspects rather than providing an exhaustive review. The paper includes algorithms and Python implementations for all critical computations, including random function sampling and Markov Chain Monte Carlo (MCMC) for Bayesian inference, as well as a detailed exploration of neural operators' application to Bayesian inverse problems.},
  archiveprefix = {arXiv},
  author = {Prashant K. Jha},
  eprint = {2503.05598},
  file = {:/home/b/documents/article/jha2025practicalintro.pdf:pdf},
  journal = {arXiv preprint arXiv:2503.05598},
  month = {3},
  note = {53 pages, 17 figures},
  pdf = {https://arxiv.org/pdf/2503.05598.pdf},
  primaryclass = {cs.CE},
  title = {From Theory to Application: A Practical Introduction to Neural Operators in Scientific Computing},
  url = {https://arxiv.org/abs/2503.05598},
  year = {2025}
}

@article{li2025fromthermodynamics,
  abstract = {Protein design with desirable properties has been a significant challenge for many decades. Generative artificial intelligence is a promising approach and has achieved great success in various protein generation tasks. Notably, diffusion models stand out for their robust mathematical foundations and impressive generative capabilities, offering unique advantages in certain applications such as protein design. In this review, we first give the definition and characteristics of diffusion models and then focus on two strategies: Denoising Diffusion Probabilistic Models and Score-based Generative Models, where DDPM is the discrete form of SGM. Furthermore, we discuss their applications in protein design, peptide generation, drug discovery, and protein-ligand interaction. Finally, we outline the future perspectives of diffusion models to advance autonomous protein design and engineering. The E(3) group consists of all rotations, reflections, and translations in three-dimensions. The equivariance on the E(3) group can keep the physical stability of the frame of each amino acid as much as possible, and we reflect on how to keep the diffusion model E(3) equivariant for protein generation.},
  archiveprefix = {arXiv},
  author = {Wen-ran Li and Xavier F. Cadet and David Medina-Ortiz and Mehdi D. Davari and Ramanathan Sowdhamini and Cedric Damour and Yu Li and Alain Miranville and Frederic Cadet},
  eprint = {2501.02680},
  file = {:/home/b/documents/article/li2025fromthermodynamics.pdf:pdf},
  journal = {arXiv preprint arXiv:2501.02680},
  month = {1},
  openalex = {W4406122403},
  pdf = {https://arxiv.org/pdf/2501.02680.pdf},
  primaryclass = {cs.LG},
  title = {From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering},
  year = {2025}
}

@article{ni2025agentic,
  abstract = {Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.},
  archiveprefix = {arXiv},
  author = {Bo Ni and Markus J. Buehler},
  doi = {10.48550/arxiv.2502.10173},
  eprint = {2502.10173},
  file = {:/home/b/documents/article/ni2025agentic.pdf:pdf},
  journal = {arXiv preprint},
  month = {2},
  openalex = {W4407632734},
  pdf = {https://arxiv.org/pdf/2502.10173.pdf},
  primaryclass = {q-bio.BM},
  title = {Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model},
  year = {2025}
}

@article{zeng2024mattergen,
  abstract = {The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.},
  author = {Claudio Zeni and Robert Pinsler and Daniel Zügner and Andrew Fowler and Matthew Horton and Xiang Fu and Zilong Wang and Aliaksandra Shysheya and Jonathan Crabbé and Shoko Ueda and Roberto Sordillo and Lixin Sun and Jake Smith and Bichlien Nguyen and Hannes Schulz and Sarah Lewis and Chin-Wei Huang and Ziheng Lu and Yichi Zhou and Han Yang and Hongxia Hao and Jielan Li and Chunlei Yang and Wenjie Li and Ryota Tomioka and Tian Xie},
  doi = {10.1038/s41586-025-08628-5},
  journal = {Nature},
  month = {3},
  number = {8055},
  openalex = {W4389471480},
  pages = {624--632},
  pdf = {https://arxiv.org/abs/2312.03687},
  title = {MatterGen: A Generative Model for Inorganic Materials Design},
  url = {https://www.nature.com/articles/s41586-025-08628-5},
  volume = {639},
  year = {2025}
}

@inproceedings{kim2025ldmol,
  abstract = {With the emergence of diffusion models as a frontline generative model, many researchers have proposed molecule generation techniques with conditional diffusion models. However, the unavoidable discreteness of a molecule makes it difficult for a diffusion model to connect raw data with highly complex conditions like natural language. To address this challenge, we propose LDMol, a novel latent diffusion model for text-conditioned molecule generation. LDMol is composed of three building blocks: a molecule encoder that produces a chemically informative feature space, a natural language-conditioned latent diffusion model using a Diffusion Transformer (DiT), and an autoregressive decoder for molecule reconstruction. Recognizing that multiple SMILES notations can represent the same molecule, we employ a contrastive learning strategy to extract the chemical informative feature space. To construct a feature space that is chemically meaningful and easily learnable, we suggest a novel contrastive learning strategy of training the encoder to encode a structural similarity of the molecule.},
  address = {Vancouver, Canada},
  author = {Jinho Chang and Jong Chul Ye},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2025ldmol.pdf:pdf},
  month = {7},
  pdf = {https://arxiv.org/pdf/2405.17829.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LDMol: A Text-to-Molecule Diffusion Model with Structurally Informative Latent Space Surpasses AR Models},
  url = {https://openreview.net/forum?id=l6mkb1LBVP},
  year = {2025}
}

@article{liu2025confgan,
  abstract = {The accurate determination of a molecule's accessible conformations is key to studying its properties. Traditional computational methods like molecular dynamics simulations require substantial computational resources and time. Recently, deep generative models have made significant progress in learning complex data distributions, making them highly applicable in molecular conformation generation. In this study, the authors developed ConfGAN, a conformation generation model based on conditional generative adversarial networks. Key innovations include: An efficient molecular-motif graph representation that treats molecules as composed of functional groups, captures interactions between groups, and provides rich chemical prior knowledge for conformation generation. A generator network that takes molecular graphs as input and attempts to generate stable conformations with minimal potential energy. A discriminator that provides feedback based on energy differences and guides generation of conformations complying with chemical rules. The model explicitly encodes molecular knowledge to ensure the physical plausibility of generated conformations. Through extensive evaluation, ConfGAN demonstrated superior performance compared to existing deep learning-based models. The authors also showed that conformations generated by ConfGAN have potential applications in molecular docking and electronic property calculations.},
  author = {Congsheng Xu and Xiaomei Deng and Yi Lu and Peiyuan Yu},
  doi = {10.1039/D4DD00179F},
  file = {:/home/b/documents/article/liu2025confgan.pdf:pdf},
  journal = {Digital Discovery},
  month = {1},
  number = {1},
  openalex = {W4404496394},
  pages = {161--171},
  pdf = {https://www.rsc.org/suppdata/d4/dd/d4dd00179f/d4dd00179f1.pdf},
  publisher = {Royal Society of Chemistry},
  title = {Generation of molecular conformations using generative adversarial neural networks},
  url = {https://doi.org/10.1039/D4DD00179F},
  volume = {4},
  year = {2025}
}

@article{lin2025goedelprover,
  abstract = {We introduce Goedel-Prover, an open-source large language model (LLM) that achieves state-of-the-art performance in automated formal proof generation for mathematical problems. Through training statement formalizers to translate natural language math problems from the Numina dataset into formal language (Lean 4), we create a dataset of 1.64 million formal statements. We then iteratively build a large dataset of formal proofs by training a series of provers, where each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. On the miniF2F benchmark, Goedel-Prover achieves a Pass@32 score of 57.6%, outperforming DeepSeek-Prover-V1.5-RL by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Additionally, we generate 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by prior work.},
  author = {Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},
  doi = {10.48550/arxiv.2502.07640},
  file = {:/home/b/documents/article/lin2025goedelprover.pdf:pdf},
  journal = {arXiv preprint arXiv:2502.07640},
  month = {2},
  openalex = {W4407425807},
  pdf = {http://arxiv.org/pdf/2502.07640},
  title = {Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},
  url = {https://arxiv.org/abs/2502.07640},
  year = {2025}
}

@inproceedings{gridach2025agenticai,
  abstract = {The integration of Agentic AI into scientific discovery marks a new frontier in research automation. These AI systems, capable of reasoning, planning, and autonomous decision-making, are transforming how scientists perform literature review, generate hypotheses, conduct experiments, and analyze results. This paper provides a comprehensive overview of Agentic AI for scientific discovery, categorizing existing systems and tools, and highlighting recent progress across fields such as chemistry, biology, and materials science.},
  archiveprefix = {arXiv},
  author = {Mourad Gridach and Jay Nanavati and Khaldoun Zine El Abidine and Lenon Mendes and Christina Mack},
  booktitle = {ICLR 2025 Workshop on Agentic AI},
  eprint = {2503.08979},
  file = {:/home/b/documents/inproceedings/gridach2025agenticai.pdf:pdf},
  month = {3},
  pdf = {https://openreview.net/pdf?id=TyCYakX9BD},
  primaryclass = {cs.CL},
  title = {Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions},
  url = {https://openreview.net/forum?id=TyCYakX9BD},
  year = {2025}
}

@article{zheng2025researcherbench,
  abstract = {The emergence of deep research systems presents significant capabilities in problem-solving, extending from basic queries to sophisticated research tasks. However, existing benchmarks primarily evaluate these systems as agents for web retrieval and report generation, overlooking their potential to discover novel insights on the frontiers of scientific research. To address this gap, we introduce ResearcherBench, the first benchmark focused on evaluating the capabilities of these advanced, agentic systems - which we refer to as Deep AI Research Systems (DARS) - on frontier AI scientific questions. We compiled a dataset of 65 research questions expertly selected from real-world scientific scenarios such as laboratory discussions and interviews, spanning 35 different AI subjects and categorized into three types: technical details, literature review, and open consulting. Our dual evaluation framework combines rubric assessment, which uses expert-designed criteria to evaluate insight quality, with factual assessment, which measures citation accuracy (faithfulness) and coverage (groundedness). We evaluated several leading commercial DARS and baseline systems. Results show that OpenAI Deep Research and Gemini Deep Research significantly outperform other systems, with particular strength in open-ended consulting questions. Such capabilities represent a meaningful step toward AI self-improvement, aligning with the vision of ASI for AI. We open-source ResearcherBench to provide a standardized platform for promoting the development of next-generation AI research assistants, hoping to foster a new perspective in AI research evaluation for a novel pattern of scientific collaboration.},
  archiveprefix = {arXiv},
  author = {Tianze Xu and Pengrui Lu and Lyumanshan Ye and Xiangkun Hu and Pengfei Liu},
  eprint = {2507.16280},
  file = {:/home/b/documents/article/zheng2025researcherbench.pdf:pdf},
  journal = {arXiv preprint arXiv:2507.16280},
  month = {7},
  pdf = {https://arxiv.org/pdf/2507.16280.pdf},
  primaryclass = {cs.AI},
  title = {ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry},
  url = {https://arxiv.org/abs/2507.16280},
  year = {2025}
}

@article{wang2025threedimensional,
  abstract = {Physics-informed neural networks (PINNs) have emerged as a transformative methodology integrating deep learning with scientific computing. This review establishes a three-dimensional analytical framework to systematically decode PINNs' development through methodological innovation, theoretical breakthroughs, and cross-disciplinary convergence. From a methodological perspective, we reveal the co-evolutionary path of adaptive optimization → domain decomposition → hybrid numerical-deep learning, spanning from foundational residual weighting strategy to neural tangent kernel-guided dynamic optimization, ultimately achieving multi-scale coupling with traditional finite element methods. From a theoretical perspective, we establish a dual-pillar framework combining convergence proofs and generalization guarantees, employing operator approximation theory to rigorously analyze approximation error bounds, while using a Bayesian-physics hybrid framework for uncertainty quantification. From an application perspective, we pioneer cross-domain knowledge transfer through specialized architectures: Fourier Neural Operators for spectral analysis, graph neural networks for irregular geometries, and DeepONets for operator learning. Performance improvements demonstrate 15--30% accuracy gains in turbulent flows, 20--40% enhancement in medical imaging, and 10--25% optimization in seismic systems. This review proposes a roadmap for PINN 2.0, focusing on neuro-symbolic approaches, federated physics learning, and quantum-accelerated optimization, establishing PINNs as the cornerstone of next-generation scientific computing paradigms.},
  author = {Zhiyuan Ren and Shijie Zhou and Dong Liu and Qihe Liu},
  doi = {10.3390/app15148092},
  journal = {Applied Sciences},
  month = {7},
  number = {14},
  openalex = {W4412519765},
  pages = {8092},
  pdf = {https://www.mdpi.com/2076-3417/15/14/8092/pdf},
  title = {Physics-Informed Neural Networks: A Review of Methodological Evolution, Theoretical Foundations, and Interdisciplinary Frontiers Toward Next-Generation Scientific Computing},
  url = {https://www.mdpi.com/2076-3417/15/14/8092},
  volume = {15},
  year = {2025}
}

@inproceedings{deac2025learninggeneration,
  abstract = {Autoregressive models (ARMs) have become the workhorse for sequence generation tasks, since many problems can be modeled as next-token prediction. While there appears to be a natural ordering for text (i.e., left-to-right), for many data types, such as graphs, the canonical ordering is less obvious. To address this problem, we introduce a variant of ARM that generates high-dimensional data using a probabilistic ordering that is sequentially inferred from data. Our model incorporates a trainable probability distribution called an "order-policy" that dynamically decides the autoregressive order in a state-dependent manner. To train the model, we introduce a variational lower bound on the exact log-likelihood, which we optimize with stochastic gradient estimation. Experimentally, we demonstrate that our approach can learn meaningful autoregressive orderings in image and graph generation. On the challenging domain of molecular graph generation, we achieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated using the Fréchet ChemNet Distance (FCD), Synthetic Accessibility Score (SAS), and Quantitative Estimate of Drug-likeness (QED).},
  author = {Zhe Wang and Jiaxin Shi and Nicolas Heess and Arthur Gretton and Michalis K. Titsias},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  pages = {50589--50620},
  pdf = {https://proceedings.mlr.press/v235/wang25z/wang25z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning-Order Autoregressive Models with Application to Molecular Graph Generation},
  url = {https://proceedings.mlr.press/v235/wang25z.html},
  volume = {235},
  year = {2025}
}

@misc{scholkopf2025future,
  address = {Cambridge, MA, USA},
  author = {Schölkopf, Bernhard},
  howpublished = {Invited talk},
  institution = {Max Planck Institute for Intelligent Systems},
  month = {6},
  note = {Presentation at Broad Institute of MIT and Harvard, March 28, 2024},
  title = {The Future of Scientific Discovery: From Tools to Partners},
  year = {2025}
}

@article{shuaibi2024aqpinn,
  abstract = {The growing computational demands of artificial intelligence (AI) in addressing climate change raise significant concerns about inefficiencies and environmental impact, as highlighted by the Jevons paradox. We propose an attention-enhanced quantum physics-informed neural networks model (AQ-PINNs) to tackle these challenges. This approach integrates quantum computing techniques into physics-informed neural networks (PINNs) for climate modeling, aiming to enhance predictive accuracy in fluid dynamics governed by the Navier-Stokes equations while reducing the computational burden and carbon footprint. By harnessing variational quantum multi-head self-attention mechanisms, our AQ-PINNs achieve a 51.51% reduction in model parameters compared to classical multi-head self-attention methods while maintaining comparable convergence and loss. It also employs quantum tensor networks to enhance representational capacity, which can lead to more efficient gradient computations and reduced susceptibility to barren plateaus. Our AQ-PINNs represent a crucial step towards more sustainable and effective climate modeling solutions.},
  archiveprefix = {arXiv},
  author = {Siddhant Dutta and Nouhaila Innan and Sadok Ben Yahia and Muhammad Shafique},
  eprint = {2409.01626},
  file = {:/home/b/documents/article/shuaibi2024aqpinn.pdf:pdf},
  journal = {arXiv preprint arXiv:2409.01626},
  month = {September},
  openalex = {W4402955241},
  pages = {1--6},
  pdf = {https://arxiv.org/pdf/2409.01626.pdf},
  primaryclass = {quant-ph},
  title = {AQ-PINNs: Attention-Enhanced Quantum Physics-Informed Neural Networks for Carbon-Efficient Climate Modeling},
  year = {2024}
}

@article{yang2025arbitraryresolution,
  abstract = {Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling factors. Furthermore, we show that our method also outperforms state-of-the-art data-driven partial differential equation solvers on Navier-Stokes equations. Overall, our work bridges the gap between simulation of a physical process and interpolation of low-resolution output, showing that it is possible to combine both approaches and significantly improve upon each other.},
  author = {Qidong Yang and Álex Hernández-García and Paula Harder and Venkatesh Ramesh and Prasanna Sattegeri and Daniela Szwarcman and Campbell D. Watson and David Rolnick},
  journal = {Journal of Machine Learning Research},
  month = {1},
  number = {420},
  openalex = {W4378469105},
  pages = {1--30},
  pdf = {https://www.jmlr.org/papers/volume25/23-0420/23-0420.pdf},
  title = {Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling},
  url = {https://www.jmlr.org/papers/v25/23-0420.html},
  volume = {25},
  year = {2024}
}

@inproceedings{zhu2024globaltomo,
  abstract = {Global seismic tomography, taking advantage of seismic waves from natural earthquakes, provides essential insights into the earth's internal dynamics. Advanced Full-waveform Inversion (FWI) techniques, whose aim is to meticulously interpret every detail in seismograms, confront formidable computational demands in forward modeling and adjoint simulations on a global scale. Recent advancements in Machine Learning (ML) offer a transformative potential for accelerating the computational efficiency of FWI and extending its applicability to larger scales. This work presents the first 3D global synthetic dataset tailored for seismic wavefield modeling and full-waveform tomography, referred to as the GlobalTomo dataset. This dataset is uniquely comprehensive, incorporating explicit wave physics and robust geophysical parameterization at realistic global scales, generated through state-of-the-art forward simulations optimized for 3D global wavefield calculations. Through extensive analysis and the establishment of ML baselines, we illustrate that ML approaches are particularly suitable for global FWI, overcoming its limitations with rapid forward modeling and flexible inversion strategies. This work represents a cross-disciplinary effort to enhance our understanding of the earth's interior through physics-ML modeling.},
  author = {Shiqian Li and Zhi Li and Zhancun Mu and Shiji Xin and Zhixiang Dai and Kuangdai Leng and Ruihua Zhang and Xiaodong Song and Yixin Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  doi = {10.48550/arxiv.2406.18202},
  file = {:/home/b/documents/inproceedings/zhu2024globaltomo.pdf:pdf},
  month = {5},
  openalex = {W4400104539},
  pdf = {https://openreview.net/pdf?id=jUxzh1bi3i},
  title = {GlobalTomo: A Global Dataset for Physics-ML Seismic Wavefield Modeling and FWI},
  url = {https://openreview.net/forum?id=jUxzh1bi3i},
  year = {2024}
}

@inproceedings{shen2024ups,
  abstract = {We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developing unified neural operators for diverse families of spatiotemporal PDEs from various domains, dimensions, and resolutions. UPS embeds different PDEs into a shared representation space and processes them using a FNO-transformer architecture. Rather than training from scratch, which is data-demanding and computationally expensive, UPS warm-starts the transformer from pretrained LLMs and performs explicit alignment to reduce the modality gap while improving data and compute efficiency. The cross-modal UPS achieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench, outperforming existing unified models using 4 times less data and 26 times less compute. Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.},
  archiveprefix = {arXiv},
  author = {Junhong Shen and Tanya Marwah and Ameet Talwalkar},
  booktitle = {International Conference on Machine Learning, Workshop on AI for Science},
  eprint = {2403.07187},
  file = {:/home/b/documents/inproceedings/shen2024ups.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=0r9mhjRv1E},
  primaryclass = {cs.LG},
  title = {UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation},
  url = {https://openreview.net/forum?id=0r9mhjRv1E},
  year = {2024}
}

@article{li2023surveygenerative,
  abstract = {Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at https://github.com/gersteinlab/GenAI4Drug.},
  author = {Xiangru Tang and Howard Dai and Elizabeth Knight and Fang Wu and Yunyang Li and Tianxiao Li and Mark Gerstein},
  doi = {10.1093/bib/bbae338},
  journal = {Briefings in Bioinformatics},
  month = {7},
  number = {4},
  openalex = {W4391835351},
  pages = {bbae338},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11247410/pdf/bbae338.pdf},
  pmcid = {PMC11247410},
  pmid = {39007594},
  title = {A Survey of Generative AI for de novo Drug Design: New Frontiers in Molecule and Protein Generation},
  url = {https://doi.org/10.1093/bib/bbae338},
  volume = {25},
  year = {2024}
}

@article{corso2024protpardelle,
  abstract = {Many protein design methods ignore sidechain atoms, yet the sidechains drive most protein function. Here, we develop a generative model that can jointly generate all-atom protein structure and sequence. Our model produces sidechains along with the backbone and directly samples sequence by collapsing a superposition over amino acid identities. To create this generative model, we develop a generative diffusion model that acts on a superposition of possible protein structures, where each structure corresponds to a different sequence. We call this approach Protpardelle. We show that our approach can generate full-atom structures with chemical fidelity and sequence consistency. Protpardelle enables direct functional site conditioning, and we demonstrate its utility for scaffold design around enzyme active sites and other functional motifs.},
  author = {Alexander E. Chu and Jinho Kim and Lucy Cheng and Gina El Nesr and Minkai Xu and Richard W. Shuai and Po-Ssu Huang},
  doi = {10.1073/pnas.2311500121},
  journal = {Proceedings of the National Academy of Sciences},
  month = {6},
  number = {27},
  openalex = {W4393440055},
  pages = {e2311500121},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11228509/pdf/},
  pmcid = {PMC11228509},
  pmid = {38916999},
  title = {An all-atom protein generative model},
  volume = {121},
  year = {2024}
}

@inproceedings{komorowska2024dynamicsinformed,
  abstract = {Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein's dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties. We introduce a method for conditioning diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding SDE theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into a conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible.},
  address = {Vienna, Austria},
  author = {Komorowska, Urszula Julia and Mathis, Simon V. and Didi, Kieran and Vargas, Francisco and Liò, Pietro and Jamnik, Mateja},
  booktitle = {The Twelfth International Conference on Learning Representations},
  doi = {10.17863/CAM.107995},
  file = {:/home/b/documents/inproceedings/komorowska2024dynamicsinformed.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=jZPqf2G9Sw},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Dynamics-Informed Protein Design with Structure Conditioning},
  url = {https://openreview.net/forum?id=jZPqf2G9Sw},
  year = {2024}
}

@inproceedings{vargas2024transitionpath,
  abstract = {Monte Carlo methods, Variational Inference, and their combinations play a pivotal role in sampling from intractable probability distributions. However, current studies lack a unified evaluation framework, relying on disparate performance measures and limited method comparisons across diverse tasks. In response, we introduce a benchmark that evaluates sampling methods using a standardized task suite and a broad range of performance criteria, including sample quality, model fit, and computational efficiency. We study existing metrics for quantifying mode collapse and introduce novel metrics for this purpose. Our findings provide insights into the strengths and weaknesses of existing sampling methods, serving as a valuable reference for future developments in the field.},
  author = {Denis Blessing and Xiaogang Jia and Johannes Esslinger and Francisco Vargas and Gerhard Neumann},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vargas2024transitionpath.pdf:pdf},
  openalex = {W4399655877},
  pages = {4205--4229},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/blessing24a/blessing24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling},
  url = {https://proceedings.mlr.press/v235/blessing24a.html},
  volume = {235},
  year = {2024}
}

@misc{lu2024mattersim,
  abstract = {Accurate and fast prediction of materials properties is central to the digital transformation of materials design. However, the vast design space and diverse operating conditions pose significant challenges for accurately modeling arbitrary material candidates and forecasting their properties. We present MatterSim, a deep learning model actively learned from large-scale first-principles computations, for efficient atomistic simulations at first-principles level and accurate prediction of broad material properties across the periodic table, spanning temperatures from 0 to 5000 K and pressures up to 1000 GPa. Out-of-the-box, the model serves as a machine learning force field, and shows remarkable capabilities not only in predicting ground-state material structures and energetics, but also in simulating their behavior under realistic temperatures and pressures, signifying an up to ten-fold enhancement in precision compared to the prior best-in-class. This enables MatterSim to compute materials' lattice dynamics, mechanical and thermodynamic properties, and beyond, to an accuracy comparable with first-principles methods. Specifically, MatterSim predicts Gibbs free energies for a wide range of inorganic solids with near-first-principles accuracy and achieves a 15 meV/atom resolution for temperatures up to 1000 K compared with experiments. This opens an opportunity to predict experimental phase diagrams of materials at minimal computational cost. Moreover, MatterSim also serves as a platform for continuous learning and customization by integrating domain-specific data. The model can be fine-tuned for atomistic simulations at a desired level of theory or for direct structure-to-property predictions, achieving high data efficiency with a reduction in data requirements by up to 97%.},
  archiveprefix = {arXiv},
  author = {Han Yang and Chenxi Hu and Yichi Zhou and Xixian Liu and Yu Shi and Jielan Li and Guanzhi Li and Zekun Chen and Shuizhou Chen and Claudio Zeni and Matthew Horton and Robert Pinsler and Andrew Fowler and Daniel Zügner and Tian Xie and Jake Smith and Lixin Sun and Qian Wang and Lingyu Kong and Chang Liu and Hongxia Hao and Ziheng Lu},
  eprint = {2405.04967},
  howpublished = {arXiv preprint arXiv:2405.04967},
  month = {5},
  openalex = {W4396816788},
  primaryclass = {cond-mat.mtrl-sci},
  title = {MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures},
  url = {https://arxiv.org/abs/2405.04967},
  year = {2024}
}

@article{ross2025gpmolformer,
  abstract = {Transformer-based models trained on large and general purpose datasets consisting of molecular strings have recently emerged as a powerful tool for successfully modeling various structure-property relations. Inspired by this success, we extend the paradigm of training chemical language transformers on large-scale chemical datasets to generative tasks in this work. Specifically, we propose GP-MoLFormer, an autoregressive molecular string generator that is trained on more than 1.1B (billion) chemical SMILES. GP-MoLFormer uses a 46.8M parameter transformer decoder model with linear attention and rotary positional encodings as the base architecture. GP-MoLFormer's utility is evaluated and compared with that of existing baselines on three different tasks: de novo generation, scaffold-constrained molecular decoration, and unconstrained property-guided optimization. While the first two are handled with no additional training, we propose a parameter-efficient fine-tuning method for the last task, which uses property-ordered molecular pairs as input. We call this new approach pair-tuning. Our results show GP-MoLFormer performs better or comparable with baselines across all three tasks, demonstrating its general utility for a variety of molecular generation tasks. We further report strong memorization of training data in GP-MoLFormer generations, which has so far remained unexplored for chemical language models. Our analyses reveal that training data memorization and novelty in generations are impacted by the quality and scale of the training data; duplication bias in training data can enhance memorization at the cost of lowering novelty. We further establish a scaling law relating inference compute and novelty in generations.},
  author = {Jerret Ross and Brian Belgodere and Samuel C. Hoffman and Vijil Chenthamarakshan and Jiri Navratil and Youssef Mroueh and Payel Das},
  file = {:/home/b/documents/article/ross2025gpmolformer.pdf:pdf},
  journal = {arXiv preprint arXiv:2405.04912},
  month = {4},
  openalex = {W4396816591},
  pdf = {https://arxiv.org/pdf/2405.04912.pdf},
  title = {GP-MoLFormer: A Foundation Model For Molecular Generation},
  url = {https://arxiv.org/abs/2405.04912},
  year = {2024}
}

@article{trinh2024solving,
  abstract = {Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning, owing to their reputed difficulty among the mathematics competition community. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges, resulting in severe scarcity of training data. We propose AlphaGeometry, a neuro-symbolic system that trains a neural language model on a large corpus of synthetic data to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015, and discovers a more general version of a translated IMO theorem from 2004.},
  author = {Trinh, Trieu H. and Wu, Yuhuai and Le, Quoc V. and He, He and Luong, Thang},
  doi = {10.1038/s41586-023-06747-5},
  file = {:/home/b/documents/article/trinh2024solving.pdf:pdf},
  journal = {Nature},
  month = {1},
  number = {7995},
  openalex = {W4390940921},
  pages = {476--482},
  pdf = {https://www.nature.com/articles/s41586-023-06747-5.pdf},
  title = {Solving olympiad geometry without human demonstrations},
  url = {https://doi.org/10.1038/s41586-023-06747-5},
  volume = {625},
  year = {2024}
}

@misc{song2024leancopilot,
  abstract = {Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop), confirming the significance of having LLMs integrated into the theorem proving workflow in Lean. When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%).},
  archiveprefix = {arXiv},
  author = {Song, Peiyang and Yang, Kaiyu and Anandkumar, Anima},
  eprint = {2404.12534},
  file = {:/home/b/documents/misc/song2024leancopilot.pdf:pdf},
  month = {4},
  note = {Open source under MIT license at r̆lhttps://github.com/lean-dojo/LeanCopilot},
  pdf = {https://arxiv.org/pdf/2404.12534.pdf},
  primaryclass = {cs.LG},
  title = {Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean},
  url = {https://arxiv.org/abs/2404.12534},
  year = {2024}
}

@inproceedings{zhang2024consistent,
  abstract = {Autoformalization is the task of automatically translating mathematical content written in natural language to a formal language expression. The growing language interpretation capabilities of Large Language Models (LLMs), including in formal languages, are lowering the barriers for autoformalization. However, LLMs alone are not capable of consistently and reliably delivering autoformalization, in particular as the complexity and specialization of the target domain grows. As the field evolves into the direction of systematically applying autoformalization towards large mathematical libraries, the need to improve syntactic, terminological and semantic control increases. This paper proposes the coordinated use of three mechanisms, most-similar retrieval augmented generation (MS-RAG), denoising steps, and auto-correction with syntax error feedback (Auto-SEF) to improve autoformalization quality. The empirical analysis, across different models, demonstrates that these mechanisms can deliver autoformalizaton results which are syntactically, terminologically and semantically more consistent. These mechanisms can be applied across different LLMs and have shown to deliver improve results across different model types.},
  address = {Miami, Florida, USA},
  author = {Zhang, Lan and Quan, Xin and Freitas, Andre},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2024.emnlp-main.233},
  file = {:/home/b/documents/inproceedings/zhang2024consistent.pdf:pdf},
  month = {11},
  openalex = {W4404783316},
  pages = {4020--4033},
  pdf = {https://aclanthology.org/2024.emnlp-main.233.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Consistent Autoformalization for Constructing Mathematical Libraries},
  url = {https://aclanthology.org/2024.emnlp-main.233},
  year = {2024}
}

@article{wang2023expertsguide,
  abstract = {Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparison purposes. To this end, we also release a highly optimized library in JAX that can be used to reproduce all results reported in this paper, enable future research studies, as well as facilitate easy adaptation to new use-case scenarios.},
  archiveprefix = {arXiv},
  author = {Wang, Sifan and Sankaran, Shyam and Wang, Hanwen and Perdikaris, Paris},
  eprint = {2308.08468},
  file = {:/home/b/documents/article/wang2023expertsguide.pdf:pdf},
  journal = {arXiv preprint arXiv:2308.08468},
  month = {8},
  openalex = {W4385965970},
  pages = {1--36},
  pdf = {http://arxiv.org/pdf/2308.08468},
  primaryclass = {cs.LG},
  title = {An Expert's Guide to Training Physics-informed Neural Networks},
  url = {https://arxiv.org/abs/2308.08468},
  year = {2023}
}

@article{kovachki2023neuraloperator,
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.},
  author = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  file = {:/home/b/documents/article/kovachki2023neuraloperator.pdf:pdf},
  journal = {Journal of Machine Learning Research},
  number = {89},
  openalex = {W4287023463},
  pages = {1--97},
  pdf = {https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf},
  title = {Neural Operator: Learning Maps Between Function Spaces},
  url = {https://jmlr.org/papers/v24/21-1524.html},
  volume = {24},
  year = {2023}
}

@article{bora2023biascorrections,
  abstract = {Numerical simulation for climate modeling resolving all important scales is a computationally taxing process. Therefore, to circumvent this issue a low resolution simulation is performed, which is subsequently corrected for bias using reanalyzed data (ERA5), known as nudging correction. The existing implementation for nudging correction uses a relaxation based method for the algebraic difference between low resolution and ERA5 data. In this study, we replace the bias correction process with a surrogate model based on the Deep Operator Network (DeepONet). DeepONet (Deep Operator Neural Network) learns the mapping from the state before nudging (a functional) to the nudging tendency (another functional). The nudging tendency is a very high dimensional data albeit having many low energy modes. Therefore, the DeepoNet is combined with a convolution based auto-encoder-decoder (AED) architecture in order to learn the nudging tendency in a lower dimensional latent space efficiently. The accuracy of the DeepONet model is tested against the nudging tendency obtained from the E3SMv2 (Energy Exascale Earth System Model) and shows good agreement. The overarching goal of this work is to deploy the DeepONet model in an online setting and replace the nudging module in the E3SM loop for better efficiency and accuracy.},
  author = {Aniruddha Bora and Khemraj Shukla and Shixuan Zhang and Bryce E. Harrop and L. Ruby Leung and George Em Karniadakis},
  file = {:/home/b/documents/article/bora2023biascorrections.pdf:pdf},
  journal = {arXiv preprint arXiv:2302.03173},
  month = {2},
  openalex = {W4319653593},
  pdf = {https://arxiv.org/pdf/2302.03173.pdf},
  title = {Learning bias corrections for climate models using deep neural operators},
  url = {https://arxiv.org/abs/2302.03173},
  year = {2023}
}

@inproceedings{bartolucci2023reno,
  abstract = {Recently, operator learning, or learning mappings between infinite-dimensional function spaces, has garnered significant attention, notably in relation to learning partial differential equations from data. Conceptually clear when outlined on paper, neural operators necessitate discretization in the transition to computer implementations. This step can compromise their integrity, often causing them to deviate from the underlying operators. This research offers a fresh take on neural operators with a framework Representation equivalent Neural Operators (ReNO) designed to address these issues. At its core is the concept of operator aliasing, which measures inconsistency between neural operators and their discrete representations. We explore this for widely-used operator learning techniques. Our findings detail how aliasing introduces errors when handling different discretizations and grids and loss of crucial continuous structures. More generally, this framework not only sheds light on existing challenges but, given its constructive and broad nature, also potentially offers tools for developing new neural operators.},
  author = {Francesca Bartolucci and Emmanuel de Bézenac and Bogdan Raonic and Roberto Molinaro and Siddhartha Mishra and Rima Alaifari},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  file = {:/home/b/documents/inproceedings/bartolucci2023reno.pdf:pdf},
  openalex = {W4379089545},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/dc35c593e61f6df62db541b976d09dcf-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/dc35c593e61f6df62db541b976d09dcf-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@article{arodz2023scattering,
  abstract = {Recent advances in machine learning establish the ability of certain neural-network architectures called neural operators to approximate maps between function spaces. Motivated by a prospect of employing them in fundamental physics, we examine applications to scattering processes in quantum mechanics. We use an iterated variant of Fourier neural operators to learn the physics of Schrödinger operators, which map from the space of initial wave functions and potentials to the final wave functions. These deep operator learning ideas are put to test in two concrete problems: a neural operator predicting the time evolution of a wave packet scattering off a central potential in $1+1$ dimensions, and the double-slit experiment in $2+1$ dimensions. At inference, neural operators can become orders of magnitude more efficient compared to traditional finite-difference solvers.},
  archiveprefix = {arXiv},
  author = {Sebastian Mizera},
  doi = {10.1103/PhysRevD.108.L101701},
  eprint = {2308.14789},
  file = {:/home/b/documents/article/arodz2023scattering.pdf:pdf},
  journal = {Physical Review D},
  month = {11},
  openalex = {W4388730298},
  pages = {L101701},
  pdf = {https://arxiv.org/pdf/2308.14789},
  primaryclass = {hep-th},
  title = {Scattering with Neural Operators},
  volume = {108},
  year = {2023}
}

@inproceedings{lee2023hyperdeeponet,
  abstract = {Fast and accurate predictions for complex physical dynamics are a big challenge across various applications. Real-time prediction on resource-constrained hardware is even more crucial in the real-world problems. DeepONet and its variants have gained popularity for learning operators that map between function spaces. However, they require many parameters and have a high computational cost when learning operators, especially those with complex (discontinuous or non-smooth) target functions. In this work, we propose HyperDeepONet which uses the expressive power of the hypernetwork to enable learning of a complex operator with smaller set of parameters. In this architecture, a hypernetwork generates the weights for the trunk net of a DeepONet. We show that the DeepONet and its variant models can be thought of as a method of injecting the input function information into the target function. From this perspective, these models can be viewed as a special case of HyperDeepONet. We analyze the complexity of DeepONet and conclude that HyperDeepONet needs relatively lower complexity to obtain the desired accuracy for operator learning. We successfully apply HyperDeepONet to various operator learning problems using low computational resources compared to other benchmarks.},
  author = {Jae Yong Lee and Sung Woong Cho and Hyung Ju Hwang},
  booktitle = {The Eleventh International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2023hyperdeeponet.pdf:pdf},
  openalex = {W4390306946},
  pdf = {https://openreview.net/pdf?id=OAw6V3ZAhSd},
  title = {HyperDeepONet: learning operator with complex target function space using the limited resources via hypernetwork},
  url = {https://openreview.net/forum?id=OAw6V3ZAhSd},
  year = {2023}
}

@article{watson2023rfdiffusion,
  abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods. Despite this progress, a general framework for protein design that enables solution of a wide range of challenges, including de novo binder and higher-order symmetric architectures, yet to be described. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on a wide range of protein design problems. We validate the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by solving a 2.07 Å cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model.},
  author = {Joseph L. Watson and David Juergens and Nathaniel R. Bennett and Brian L. Trippe and Jason Yim and Helen E. Eisenach and Woody Ahern and Andrew J. Borst and Robert J. Ragotte and Lukas F. Milles and Basile I. M. Wicky and Nikita Hanikel and Samuel J. Pellock and Alexis Courbet and William Sheffler and Jue Wang and Preetham Venkatesh and Isaac Sappington and Susana Vázquez Torres and Anna Lauko and Valentin De Bortoli and Émile Mathieu and Sergey Ovchinnikov and Regina Barzilay and Tommi Jaakkola and Frank DiMaio and Minkyung Baek and David Baker},
  doi = {10.1038/s41586-023-06415-8},
  journal = {Nature},
  month = {8},
  number = {7976},
  openalex = {W4383957026},
  pages = {1089--1100},
  pmcid = {PMC10468394},
  pmid = {37433327},
  title = {De novo design of protein structure and function with RFdiffusion},
  url = {https://www.nature.com/articles/s41586-023-06415-8},
  volume = {620},
  year = {2023}
}

@article{ingraham2023chroma,
  abstract = {Three billion years of evolution has produced a tremendous diversity of protein molecules, but the full potential of proteins is likely to be much greater. Accessing this potential has been challenging for both computation and experiments because the space of possible protein molecules is much larger than the space of those likely to have functions. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences, and that can be conditioned to steer the generative process towards desired properties and functions. We introduce technical innovations for generative modeling of proteins: a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling, layers for efficiently synthesizing three-dimensional structures of proteins from predicted inter-residue geometries and a general low-temperature sampling algorithm for diffusion models. Chroma achieves protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics and even natural-language prompts. Experimental characterization of 310 proteins shows that sampling from Chroma results in proteins that are highly expressed, fold and have favourable biophysical properties.},
  author = {John B. Ingraham and Max Baranov and Zak Costello and Karl W. Barber and Wujie Wang and Ahmed Ismail and Vincent Frappier and Dana M. Lord and Christopher Ng-Thow-Hing and Erik R. Van Vlack and Shan Tie and Vincent Xue and Sarah C. Cowles and Alan Leung and João V. Rodrigues and Claudio L. Morales-Perez and Alex M. Ayoub and Robin Green and Katherine Puentes and Frank Oplinger and Nishant V. Panwar and Fritz Obermeyer and Adam R. Root and Andrew L. Beam and Frank J. Poelwijk and Gevorg Grigoryan},
  doi = {10.1038/s41586-023-06728-8},
  journal = {Nature},
  month = {11},
  number = {7989},
  openalex = {W4388694364},
  pages = {1070--1078},
  pmcid = {PMC10686827},
  pmid = {37968394},
  title = {Illuminating protein space with a programmable generative model},
  url = {https://doi.org/10.1038/s41586-023-06728-8},
  volume = {623},
  year = {2023}
}

@inproceedings{xia2022surveypretrained,
  abstract = {Deep learning has achieved remarkable success in learning representations for molecules, which is crucial for various biochemical applications, ranging from property prediction to drug design. However, training Deep Neural Networks (DNNs) from scratch often requires abundant labeled molecules, which are expensive to acquire in the real world. To alleviate this issue, tremendous efforts have been devoted to Molecular Pre-trained Models (CPMs), where DNNs are pre-trained using large-scale unlabeled molecular databases and then fine-tuned over specific downstream tasks. Despite the prosperity, there lacks a systematic review of this fast-growing field. In this paper, we present the first survey that summarizes the current progress of CPMs. We first highlight the limitations of training molecular representation models from scratch to motivate CPM studies. Next, we systematically review recent advances on this topic from several key perspectives, including molecular descriptors, encoder architectures, pre-training strategies, and applications. We also highlight the challenges and promising avenues for future research, providing a useful resource for both machine learning and scientific communities.},
  author = {Jun Xia and Yanqiao Zhu and Yuanqi Du and Stan Z. Li},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023},
  doi = {10.24963/ijcai.2023/760},
  file = {:/home/b/documents/inproceedings/xia2022surveypretrained.pdf:pdf},
  openalex = {W4385767952},
  pages = {6787--6795},
  pdf = {https://www.ijcai.org/proceedings/2023/0760.pdf},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  title = {A Systematic Survey of Chemical Pre-trained Models},
  url = {https://doi.org/10.24963/ijcai.2023/760},
  year = {2023}
}

@article{korotcov2023benchmark,
  abstract = {We propose a new benchmark for generative molecular design that is based on docking, a popular computational method for predicting the binding of a molecule to a protein. The goal is to generate 250 molecules that achieve the maximum possible docking score using SMINA, a popular docking software. When trained using a few thousands compounds, a typical training set size, the tested methods fail to generate highly active structures according to the docking software. The highest scoring molecules in most cases did not outperform the top 10 percent molecules found in either the ZINC database or the training set. This suggests a limitation of the current incarnation of models for de novo drug design. Our benchmark is released as an easy to use package and is available at https://github.com/cieplinski-tobiasz/smina-docking-benchmark.},
  author = {Tobiasz Ciepliński and Tomasz Danel and Sabina Podlewska and Stanisław Jastrzębski},
  doi = {10.1021/acs.jcim.2c01355},
  journal = {Journal of Chemical Information and Modeling},
  month = {5},
  number = {11},
  openalex = {W4378071155},
  pages = {3238--3247},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC10268949/pdf/},
  title = {Generative Models Should at Least Be Able to Design Molecules That Dock Well: A New Benchmark},
  url = {https://doi.org/10.1021/acs.jcim.2c01355},
  volume = {63},
  year = {2023}
}

@article{wu2023surveygnn,
  abstract = {Material molecular representation (MMR) plays an important role in material property or chemical reaction prediction. However, traditional expert-designed MMR methods face challenges in dealing with high dimensionality and heterogeneity of material data, leading to limited generalization capabilities and insufficient information representation. In recent years, graph neural networks (GNNs), a deep learning algorithm specifically designed for graph structures, have made inroads into the field of MMR. It will be instructive and inspiring to conduct a survey on various GNNs used for MMR.},
  author = {Desheng Wu and Hongye Wang and Yifei Gong and Dong Fan and Peng Ding and Qian Li and Quan Qian},
  doi = {10.20517/jmi.2023.10},
  file = {:/home/b/documents/article/wu2023surveygnn.pdf:pdf},
  journal = {Journal of Materials Informatics},
  openalex = {W4380368491},
  pages = {12},
  pdf = {https://f.oaes.cc/xmlpdf/f05d8de1-30e3-4eca-afc1-e963c440cd81/5815_down.pdf?v=17},
  title = {Graph neural networks for molecular and materials representation},
  url = {https://www.oaepublish.com/articles/jmi.2023.10},
  volume = {3},
  year = {2023}
}

@inproceedings{yang2023leandojo,
  abstract = {Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. LeanDojo is an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. We introduce ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. We create a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. Experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4.},
  author = {Kaiyu Yang and Aidan M. Swope and Alex Gu and Rahul Chalamala and Peiyang Song and Shixing Yu and Saad Godil and Ryan Prenger and Anima Anandkumar},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/yang2023leandojo.pdf:pdf},
  openalex = {W4382498654},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4441469427094f8873d0fecb0c4e1cee-Paper-Datasets_and_Benchmarks.pdf},
  series = {Advances in Neural Information Processing Systems},
  title = {LeanDojo: Theorem Proving with Retrieval-Augmented Language Models},
  volume = {36},
  year = {2023}
}

@inproceedings{jiang2023draftsketchprove,
  abstract = {The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.},
  author = {Albert Qiaochu Jiang and Sean Welleck and Jin Peng Zhou and Timothée Lacroix and Jiacheng Liu and Wenda Li and Mateja Jamnik and Guillaume Lample and Yuhuai Wu},
  booktitle = {The Eleventh International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2023draftsketchprove.pdf:pdf},
  month = {5},
  openalex = {W4307308174},
  pdf = {https://openreview.net/pdf?id=SMa9EAovKMC},
  title = {Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  url = {https://openreview.net/forum?id=SMa9EAovKMC},
  year = {2023}
}

@article{gomes2024closedloop,
  abstract = {Transformer-based large language models are making significant strides in various fields, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. We showcase Coscientist's potential for accelerating research across six diverse tasks, including reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for autonomous experimental design and execution.},
  author = {Daniil A. Boiko and Robert MacKnight and Ben Kline and Gabe Gomes},
  doi = {10.1038/s41586-023-06792-0},
  file = {:/home/b/documents/article/gomes2024closedloop.pdf:pdf},
  journal = {Nature},
  month = {12},
  number = {7992},
  openalex = {W4389991792},
  pages = {570--578},
  pdf = {https://arxiv.org/pdf/2304.05332.pdf},
  publisher = {Nature Publishing Group},
  title = {Autonomous Chemical Research with Large Language Models},
  volume = {624},
  year = {2023}
}

@inproceedings{desai2022oneshot,
  abstract = {Solving differential equations efficiently and accurately sits at the heart of progress in many areas of scientific research, from classical dynamical systems to quantum mechanics. There is a surge of interest in using Physics-Informed Neural Networks (PINNs) to tackle such problems as they provide numerous benefits over traditional numerical approaches. Despite their potential benefits for solving differential equations, transfer learning has been under explored. In this study, we present a general framework for transfer learning PINNs that results in one-shot inference for linear systems of both ordinary and partial differential equations. This means that highly accurate solutions to many unknown differential equations can be obtained instantaneously without retraining an entire network. We demonstrate the efficacy of the proposed deep learning approach by solving several real-world problems, such as first- and second-order linear ordinary equations, the Poisson equation, and the time-dependent Schrodinger complex-value partial differential equation.},
  address = {Baltimore, Maryland, USA},
  author = {Shaan Desai and Marios Mattheakis and Hayden Joy and Pavlos Protopapas and Stephen Roberts},
  booktitle = {Proceedings of the 2nd ICML Workshop on AI for Science},
  file = {:/home/b/documents/inproceedings/desai2022oneshot.pdf:pdf},
  month = {7},
  openalex = {W4286898080},
  pdf = {https://arxiv.org/pdf/2110.11286},
  publisher = {OpenReview.net},
  title = {One-Shot Transfer Learning of Physics-Informed Neural Networks},
  url = {https://openreview.net/forum?id=1AspJ4zzeqq},
  year = {2022}
}

@article{cuomo2022survey,
  abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which starts from the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN).},
  author = {Salvatore Cuomo and Vincenzo Schiano Di Cola and Fabio Giampaolo and Gianluigi Rozza and Maziar Raissi and Francesco Piccialli},
  doi = {10.1007/s10915-022-01939-z},
  file = {:/home/b/documents/article/cuomo2022survey.pdf:pdf},
  journal = {Journal of Scientific Computing},
  month = {7},
  number = {88},
  openalex = {W4221161478},
  pdf = {https://arxiv.org/pdf/2201.05624.pdf},
  title = {Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next},
  url = {https://link.springer.com/article/10.1007/s10915-022-01939-z},
  volume = {92},
  year = {2022}
}

@article{dauparas2022proteinmpnn,
  abstract = {Although deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo designs have been generated using physically based approaches such as Rosetta. Here, we describe a deep learning-based protein sequence design method, ProteinMPNN, that has outstanding performance in both silico and experimental tests. On native backbones, ProteinMPNN has a sequence recovery of 52.4% compared with 32.9% for Rosetta. The amino acid at different positions can be coupled between single or multiple chains, enabling application to a wide range of current challenges. We demonstrate the broad utility and high accuracy using x-ray crystallography, cryo-electron microscopy, functional studies by rescuing previously failed designs, which were made using Rosetta and AlphaFold, for monomers, cyclic homo-oligomers, tetrahedral nanoparticles, target-binding proteins.},
  author = {Justas Dauparas and Ivan Anishchenko and Nathaniel Bennett and Hua Bai and Robert J. Ragotte and Lukas F. Milles and Basile I. M. Wicky and Alexis Courbet and Robbert J. de Haas and Neville Bethel and Pei J. Y. Leung and Timothy F. Huddy and Sam Pellock and Daniel Tischer and Frederick Chan and Brian Koepnick and Hannah Nguyen and Alex Kang and Banumathi Sankaran and Asim K. Bera and Neil P. King and David Baker},
  doi = {10.1126/science.add2187},
  journal = {Science},
  number = {6615},
  openalex = {W4296032638},
  pages = {49--56},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC9997061/pdf/},
  title = {Robust deep learning--based protein sequence design using ProteinMPNN},
  volume = {378},
  year = {2022}
}

@article{wicky2022denovodesign,
  abstract = {Deep learning has revolutionized protein structure prediction, but almost all experimentally characterized de novo designs have been generated using physically based approaches like Rosetta. Here, we describe a deep learning-based protein sequence design method, ProteinMPNN, with outstanding performance in both in silico and experimental tests. On native protein backbones, ProteinMPNN achieves a sequence recovery of 52.4%, compared to 32.9% for Rosetta. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. We experimentally characterize ProteinMPNN designs for six different design challenges and find that the designs are highly thermostable and express at high levels in Escherichia coli. We use X-ray crystallography, cryoelectron microscopy, and functional studies to show that ProteinMPNN can be used to design well-folded proteins and protein complexes, improving the success rate of de novo protein design.},
  author = {Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Courbet, Alexis and de Haas, Robbert J. and Bethel, Neville and Leung, Philip J. Y. and Huddy, Timothy F. and Pellock, Samuel and Tischer, Doug and Chan, Frederick and Koepnick, Brian and Nguyen, Hannah and Kang, Alex and Sankaran, Banumathi and Bera, Asim K. and King, Neil P. and Baker, David},
  doi = {10.1126/science.add2187},
  journal = {Science},
  month = {10},
  number = {6615},
  openalex = {W4296032638},
  pages = {49--56},
  pdf = {https://www.science.org/doi/epdf/10.1126/science.add2187},
  title = {Robust deep learning–based protein sequence design using ProteinMPNN},
  volume = {378},
  year = {2022}
}

@inproceedings{lewkowycz2022minerva,
  abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering questions at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves strong performance in a variety of evaluations, including state-of-the-art performance on the MATH dataset. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a quarter of them.},
  author = {Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/lewkowycz2022minerva.pdf:pdf},
  openalex = {W4283768109},
  pages = {3843--3857},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Solving Quantitative Reasoning Problems with Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{wu2022autoformalization,
  abstract = {Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (25.3%) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from 29.6% to 35.2%.},
  address = {Red Hook, NY, USA},
  author = {Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and Markus Norman Rabe and Charles E. Staats and Mateja Jamnik and Christian Szegedy},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/wu2022autoformalization.pdf:pdf},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/d0c6bc641a56bebee9d985b937307367-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  title = {Autoformalization with Large Language Models},
  volume = {35},
  year = {2022}
}

@inproceedings{mishra2022lila,
  abstract = {Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), while the best performing model only obtains 60.40%, indicating the room for improvement in general mathematical reasoning and understanding.},
  address = {Abu Dhabi, United Arab Emirates},
  author = {Swaroop Mishra and Matthew Finlayson and Pan Lu and Leonard Tang and Sean Welleck and Chitta Baral and Tanmay Rajpurohit and Oyvind Tafjord and Ashish Sabharwal and Peter Clark and Ashwin Kalyan},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  doi = {10.18653/v1/2022.emnlp-main.392},
  file = {:/home/b/documents/inproceedings/mishra2022lila.pdf:pdf},
  openalex = {W4385572906},
  pages = {5807--5832},
  pdf = {https://aclanthology.org/2022.emnlp-main.392.pdf},
  publisher = {Association for Computational Linguistics},
  title = {LILA: A Unified Benchmark for Mathematical Reasoning},
  url = {https://aclanthology.org/2022.emnlp-main.392},
  year = {2022}
}

@article{paul2023deepgenerative,
  abstract = {Recent advances and accomplishments of artificial intelligence (AI) and deep generative models have established their usefulness in medicinal applications, especially in drug discovery and development. To correctly apply AI, the developer and user face questions such as which protocols to consider, which factors to scrutinize, and how the deep generative models can integrate the relevant disciplines. This review summarizes classical and newly developed AI approaches, providing an updated and accessible guide to the broad computational drug discovery and development community. We introduce deep generative models from different standpoints and describe the theoretical frameworks for representing chemical and biological structures and their applications. We discuss the data and technical challenges and highlight future directions of multimodal deep generative models for accelerating drug discovery.},
  author = {Xiangxiang Zeng and Fei Wang and Yuan Luo and Seung-Gu Kang and Jian Tang and Felice C. Lightstone and Evandro F. Fang and Wendy Cornell and Ruth Nussinov and Feixiong Cheng},
  doi = {10.1016/j.xcrm.2022.100794},
  journal = {Cell Reports Medicine},
  month = {12},
  number = {12},
  openalex = {W4307447012},
  pages = {100794},
  pdf = {https://www.cell.com/article/S2666379122003494/pdf},
  title = {Deep generative molecular design reshapes drug discovery},
  url = {https://doi.org/10.1016/j.xcrm.2022.100794},
  volume = {3},
  year = {2022}
}

@article{jagtap2021conservative,
  abstract = {Physics-informed neural networks (PINNs) have emerged as a new paradigm for solving partial differential equations (PDEs) in forward and inverse problems. However, PINNs still face several challenges, especially in handling multi-scale and multi-physics problems. Here, we develop a distributed framework for PINNs based on two recent extensions: conservative PINNs (cPINNs) and extended PINNs (XPINNs), which employ domain decomposition in space and in time-space, respectively. This domain decomposition provides several advantages over vanilla PINNs, including: parallelization capacity, large representation capacity, efficient hyperparameter tuning, and it is particularly effective for multi-scale and multi-physics problems. We present a parallel algorithm for cPINNs and XPINNs constructed with a hybrid programming model described by MPI + X, where X ∈ \CPUs, GPUs ̇The main advantage of cPINN and XPINN over classical data and model parallel approaches is the flexibility of optimizing all hyperparameters of each neural network separately in each subdomain.},
  author = {Shukla, Khemraj and Jagtap, Ameya D. and Karniadakis, George Em},
  doi = {10.1016/j.jcp.2021.110683},
  journal = {Journal of Computational Physics},
  keywords = {Physics-informed neural networks, Domain decomposition, Parallel computing, Partial differential equations, Deep learning},
  pages = {110683},
  title = {Parallel physics-informed neural networks via domain decomposition},
  volume = {447},
  year = {2021}
}

@inproceedings{lutjens2021pcepinns,
  abstract = {Climate models project an uncertainty range of possible warming scenarios from 1.5 to 5 degree Celsius global temperature increase until 2100, according to the CMIP6 model ensemble. Climate risk management and infrastructure adaptation requires the accurate quantification of the uncertainties at the local level. Ensembles of high-resolution climate models could accurately quantify the uncertainties, but most physics-based climate models are computationally too expensive to run as ensemble. Recent works in physics-informed neural networks (PINNs) have combined deep learning and the physical sciences to learn up to 15k faster copies of climate submodels. However, the application of PINNs in climate modeling has so far been mostly limited to deterministic models. We leverage a novel method that combines polynomial chaos expansion (PCE), a classic technique for uncertainty propagation, with PINNs. The PCE-PINNs learn a fast surrogate model that is demonstrated for uncertainty propagation of known parameter uncertainties. We showcase the effectiveness in ocean modeling by using the local advection-diffusion equation.},
  archiveprefix = {arXiv},
  author = {Björn Lütjens and Catherine H. Crawford and Mark Veillette and Dava Newman},
  booktitle = {Workshop on Deep Learning for Simulation at the International Conference on Learning Representations},
  eprint = {2105.02939},
  month = {5},
  primaryclass = {cs.LG},
  title = {PCE-PINNs: Physics-Informed Neural Networks for Uncertainty Propagation in Ocean Modeling},
  url = {https://arxiv.org/abs/2105.02939},
  year = {2021}
}

@article{lu2019deeponet,
  abstract = {It is widely known that neural networks are universal approximators of continuous functions. However, a less known but powerful result is that a neural network with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to deep neural networks. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a deep neural network for encoding the discrete input function space (branch net) and another deep neural network for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
  author = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  doi = {10.1038/s42256-021-00302-5},
  file = {:/home/b/documents/article/lu2019deeponet.pdf:pdf},
  journal = {Nature Machine Intelligence},
  month = {3},
  number = {3},
  openalex = {W3137474564},
  pages = {218--229},
  pdf = {https://arxiv.org/pdf/1910.03193},
  title = {Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  url = {https://doi.org/10.1038/s42256-021-00302-5},
  volume = {3},
  year = {2021}
}

@inproceedings{li2021fourier,
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
  author = {Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew M. Stuart and Anima Anandkumar},
  booktitle = {9th International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2021fourier.pdf:pdf},
  openalex = {W3118310857},
  pdf = {https://openreview.net/pdf?id=c8P9NQVtmnO},
  title = {Fourier Neural Operator for Parametric Partial Differential Equations},
  url = {https://openreview.net/forum?id=c8P9NQVtmnO},
  year = {2021}
}

@article{jumper2021alphafold,
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. There are tens of thousands of protein families, and researchers have experimentally determined the structure of proteins from only a fraction of these. Structural genomics consortia have dedicated decades of effort to this problem, but the barriers remain too high. Here we provide an in-depth description of our deep learning system, AlphaFold, that can predict protein structures with atomic accuracy even in cases where no similar structure is known. We validated these predictions in the 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experiment for a majority of targets. At the scale enabled by AlphaFold, it is now possible to predict the structure of hundreds of thousands of proteins, providing structural coverage for essentially every protein in 21 reference proteomes.},
  author = {John Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Žídek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A. A. Kohl and Andrew J. Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  doi = {10.1038/s41586-021-03819-2},
  journal = {Nature},
  month = {8},
  number = {7873},
  openalex = {W3177828909},
  pages = {583--589},
  title = {Highly accurate protein structure prediction with AlphaFold},
  volume = {596},
  year = {2021}
}

@article{baek2021rosettafold,
  abstract = {DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo-electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure.},
  author = {Minkyung Baek and Frank DiMaio and Ivan Anishchenko and Justas Dauparas and Sergey Ovchinnikov and Gyu Rie Lee and Jue Wang and Qian Cong and Lisa N. Kinch and R. Dustin Schaeffer and Claudia Millàn and Hahnbeom Park and Carson Adams and Caleb R. Glassman and Andy DeGiovanni and Jose H. Pereira and Andria V. Rodrigues and Alberdina A. van Dijk and Ana C. Ebrecht and Diederik J. Opperman and Theo Sagmeister and Christoph Buhlheller and Tea Pavkov-Keller and Manoj K. Rathinaswamy and Udit Dalwadi and Calvin K. Yip and John E. Burke and K. Christopher Garcia and Nick V. Grishin and Paul D. Adams and Randy J. Read and David Baker},
  doi = {10.1126/science.abj8754},
  file = {:/home/b/documents/article/baek2021rosettafold.pdf:pdf},
  journal = {Science},
  month = {8},
  number = {6557},
  openalex = {W3186179742},
  pages = {871--876},
  pdf = {https://escholarship.org/content/qt3gz3w9v7/qt3gz3w9v7.pdf},
  title = {Accurate prediction of protein structures and interactions using a three-track neural network},
  volume = {373},
  year = {2021}
}

@article{merk2022graphinvent,
  abstract = {Deep learning methods applied to chemistry can be used to accelerate the discovery of new molecules. This work introduces GraphINVENT, a platform developed for graph-based molecular design using graph neural networks (GNNs). GraphINVENT uses a tiered deep neural network architecture to probabilistically generate new molecules a single bond at a time. All models implemented in GraphINVENT can quickly learn to build molecules resembling the training set molecules without any explicit programming of chemical rules. The models have been benchmarked using the MOSES distribution-based metrics, showing how GraphINVENT models compare well with state-of-the-art generative models. This work compares six different GNN-based generative models in GraphINVENT, and shows that ultimately the gated-graph neural network performs best against the metrics considered here.},
  author = {Rocío Mercado and Tobias Rastemo and Edvard Lindelöf and Günter Klambauer and Ola Engkvist and Hongming Chen and Esben Jannik Bjerrum},
  doi = {10.1088/2632-2153/abcf91},
  journal = {Machine Learning: Science and Technology},
  number = {2},
  openalex = {W3107551345},
  pages = {025023},
  pdf = {https://iopscience.iop.org/article/10.1088/2632-2153/abcf91},
  title = {Graph networks for molecular design},
  volume = {2},
  year = {2021}
}

@article{jagtap2020xpinns,
  author = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
  doi = {10.1016/j.cma.2020.113028},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  openalex = {W3015865829},
  pages = {113028},
  publisher = {Elsevier},
  title = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  volume = {365},
  year = {2020}
}

@article{udrescu2020aifeynman,
  abstract = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to remain intractable in general, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, finding that it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.},
  author = {Udrescu, Silviu-Marian and Tegmark, Max},
  doi = {10.1126/sciadv.aay2631},
  journal = {Science Advances},
  month = {4},
  number = {16},
  openalex = {W3016401366},
  pages = {eaay2631},
  pdf = {https://cdn.ncbi.nlm.nih.gov/pmc/articles/PMC7159912/pdf/aay2631.pdf},
  title = {AI Feynman: A physics-inspired method for symbolic regression},
  volume = {6},
  year = {2020}
}

@article{raissi2019physicsinformed,
  abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. We present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In the first type of problems, we are given scattered and potentially noisy observations in space and time, and we are interested in learning the solution everywhere. In the second type of problems, we are provided with data only at the initial and boundary conditions, and we are interested in obtaining the solution in the entire computational domain with the undertaking of respecting the underlying physical laws.},
  author = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
  doi = {10.1016/j.jcp.2018.10.045},
  file = {:/home/b/documents/article/raissi2019physicsinformed.pdf:pdf},
  journal = {Journal of Computational Physics},
  keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge-Kutta methods, Nonlinear dynamics},
  month = {2},
  pages = {686--707},
  pdf = {https://arxiv.org/pdf/1711.10561.pdf},
  publisher = {Elsevier},
  title = {Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  volume = {378},
  year = {2019}
}

@article{raissi2018hiddenphysics,
  abstract = {While there is currently a lot of enthusiasm about ``big data'', useful data is usually ``small'' and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems spanning a number of scientific domains, including the Navier-Stokes, Schrödinger, Kuramoto-Sivashinsky, and nonlinear Schrödinger equations. The methodology provides a promising new direction for harnessing the power of data together with the underlying laws of physics for enhancing predictive models.},
  author = {Maziar Raissi and George Em Karniadakis},
  doi = {10.1016/j.jcp.2017.11.039},
  file = {:/home/b/documents/article/raissi2018hiddenphysics.pdf:pdf},
  journal = {Journal of Computational Physics},
  openalex = {W2745110207},
  pages = {125--141},
  pdf = {https://arxiv.org/pdf/1708.00588},
  title = {Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations},
  volume = {357},
  year = {2018}
}

@article{gomezbombarelli2018automatic,
  abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. We also demonstrate that we can use powerful gradient-based optimization to efficiently guide the search for optimized functional molecules.},
  author = {Rafael Gómez-Bombarelli and Jennifer N. Wei and David Duvenaud and José Miguel Hernández-Lobato and Benjamı́n Sánchez-Lengeling and Dennis Sheberla and Jorge Aguilera-Iparraguirre and Timothy D. Hirzel and Ryan P. Adams and Alán Aspuru-Guzik},
  doi = {10.1021/acscentsci.7b00572},
  journal = {ACS Central Science},
  month = {2},
  number = {2},
  openalex = {W3098269892},
  pages = {268--276},
  pdf = {https://pubs.acs.org/doi/pdf/10.1021/acscentsci.7b00572},
  title = {Automatic chemical design using a data-driven continuous representation of molecules},
  volume = {4},
  year = {2018}
}

@inproceedings{jin2018junctiontree,
  abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
  author = {Wengong Jin and Regina Barzilay and Tommi Jaakkola},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jin2018junctiontree.pdf:pdf},
  pages = {2323--2332},
  pdf = {http://proceedings.mlr.press/v80/jin18a/jin18a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Junction Tree Variational Autoencoder for Molecular Graph Generation},
  url = {https://proceedings.mlr.press/v80/jin18a.html},
  volume = {80},
  year = {2018}
}

@article{raissi2017physicsinformedparti,
  abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations.},
  author = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
  doi = {10.48550/arxiv.1711.10561},
  file = {:/home/b/documents/article/raissi2017physicsinformedparti.pdf:pdf},
  journal = {arXiv preprint arXiv:1711.10561},
  month = {11},
  openalex = {W2772097715},
  pdf = {http://arxiv.org/pdf/1711.10561},
  title = {Physics-Informed Deep Learning (Part I): Data-Driven Solutions of Nonlinear Partial Differential Equations},
  url = {https://arxiv.org/abs/1711.10561},
  year = {2017}
}
