@misc{huang2025announcing,
  abstract = {Physical AI models enable robots to autonomously perceive, interpret, reason, and interact with the real world. Accelerated computing and simulations are key to developing the next generation of robotics. Newton is an open-source physics engine developed by NVIDIA, Google DeepMind, and Disney Research, built on NVIDIA Warp and compatible with frameworks like MuJoCo Playground and NVIDIA Isaac Lab to advance robot learning and development.},
  author = {Huang, Spencer and Macklin, Miles and State, Gavriel},
  howpublished = {NVIDIA Technical Blog},
  month = {3},
  note = {Open-source GPU-accelerated physics simulation engine targeting roboticists and simulation researchers},
  title = {Announcing Newton, an Open-Source Physics Engine for Robotics Simulation},
  url = {https://developer.nvidia.com/blog/announcing-newton-an-open-source-physics-engine-for-robotics-simulation/},
  year = {2025}
}

@article{lee2025unified,
  abstract = {We present a framework for simulating fluid-robot multiphysics as a single, unified optimization problem. The coupled manipulator and incompressible Navier-Stokes equations governing the robot and fluid dynamics are derived together from a single Lagrangian using the principal of least action. This approach creates a numerically stable and physically accurate method for multibody systems in robotics involving fluid interactions.},
  archiveprefix = {arXiv},
  author = {Lee, Jeong Hun and Hu, Junzhe and Kwok, Sofia and Majidi, Carmel and Manchester, Zachary},
  eprint = {2506.05012},
  file = {:/home/b/documents/article/lee2025unified.pdf:pdf},
  journal = {arXiv preprint arXiv:2506.05012},
  month = {6},
  note = {Preprint submitted for publication},
  pdf = {https://arxiv.org/pdf/2506.05012.pdf},
  primaryclass = {cs.RO},
  title = {A Unified Framework for Simulating Strongly-Coupled Fluid-Robot Multiphysics},
  url = {https://arxiv.org/abs/2506.05012},
  year = {2025}
}

@article{abouchakra2025realissim,
  abstract = {We introduce real-is-sim, a new approach to integrating simulation into behavior cloning pipelines. In contrast to real-only methods, which lack the ability to safely test policies before deployment, and sim-to-real methods, which require complex adaptation to cross the sim-to-real gap, our framework allows policies to seamlessly switch between running on real hardware and running in parallelized virtual environments. At the center of real-is-sim is a dynamic digital twin, powered by the Embodied Gaussian simulator, that synchronizes with the real world at 60Hz. This twin acts as a mediator between the behavior cloning policy and the real robot. Policies are trained using representations derived from simulator states and always act on the simulated robot, never the real one.},
  archiveprefix = {arXiv},
  author = {Abou-Chakra, Jad and Sun, Lingfeng and Rana, Krishan and May, Brandon and Schmeckpeper, Karl and Suenderhauf, Niko and Minniti, Maria Vittoria and Herlant, Laura},
  eprint = {2504.03597},
  file = {:/home/b/documents/article/abouchakra2025realissim.pdf:pdf},
  journal = {arXiv preprint arXiv:2504.03597},
  pdf = {https://arxiv.org/pdf/2504.03597.pdf},
  title = {Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin},
  url = {https://arxiv.org/abs/2504.03597},
  website = {https://realissim.rai-inst.com/},
  year = {2025}
}

@article{da2025survey,
  abstract = {Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. This survey paper is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward) and covers comprehensive literature from classic to the most advanced methods including sim-to-real techniques empowered by foundation models.},
  archiveprefix = {arXiv},
  author = {Da, Longchao and Turnau, Justin and Kutralingam, Thirulogasankar Pranav and Velasquez, Alvaro and Shakarian, Paulo and Wei, Hua},
  code = {https://github.com/LongchaoDa/AwesomeSim2Real},
  eprint = {2502.13187},
  file = {:/home/b/documents/article/da2025survey.pdf:pdf},
  journal = {arXiv preprint arXiv:2502.13187},
  pdf = {https://arxiv.org/pdf/2502.13187.pdf},
  title = {A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models},
  url = {https://arxiv.org/abs/2502.13187},
  year = {2025}
}

@inproceedings{tevet2025closd,
  abstract = {Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The paper introduces CLoSD, a text-driven RL physics-based controller, guided by diffusion generation for various tasks. The key innovation is using motion diffusion as an on-the-fly universal planner for a robust RL controller through a closed-loop interaction between a Diffusion Planner and a tracking controller. CLoSD maintains a closed-loop interaction between two modules — a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up.},
  archiveprefix = {arXiv},
  author = {Tevet, Guy and Raab, Sigal and Cohan, Setareh and Reda, Daniele and Luo, Zhengyi and Peng, Xue Bin and Bermano, Amit H. and van de Panne, Michiel},
  booktitle = {The Thirteenth International Conference on Learning Representations (ICLR)},
  code = {https://github.com/GuyTevet/CLoSD},
  eprint = {2410.03441},
  file = {:/home/b/documents/inproceedings/tevet2025closd.pdf:pdf},
  note = {Spotlight},
  pdf = {https://arxiv.org/pdf/2410.03441.pdf},
  title = {CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control},
  url = {https://openreview.net/forum?id=pZISppZSTv},
  website = {https://guytevet.github.io/CLoSD-page/},
  year = {2025}
}

@article{gillman2025force,
  abstract = {The paper investigates using physical forces as a control signal for video generation and proposes "force prompts" which enable users to interact with images through both localized point forces (such as poking a plant) and global wind force fields (such as wind blowing on fabric). The main challenge addressed is the difficulty in obtaining high-quality paired force-video training data. The key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. The method can generate videos which simulate forces across diverse geometries, settings, and materials without requiring physics simulators at inference time.},
  archiveprefix = {arXiv},
  author = {Gillman, Nate and Herrmann, Charles and Freeman, Michael and Aggarwal, Daksh and Luo, Evan and Sun, Deqing and Sun, Chen},
  code = {https://github.com/brown-palm/force-prompting},
  eprint = {2505.19386},
  file = {:/home/b/documents/article/gillman2025force.pdf:pdf},
  journal = {arXiv preprint arXiv:2505.19386},
  pdf = {https://arxiv.org/pdf/2505.19386.pdf},
  title = {Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals},
  url = {https://arxiv.org/abs/2505.19386},
  website = {https://force-prompting.github.io/},
  year = {2025}
}

@article{zheng2025physicsembedded,
  abstract = {Edge Digital Twins (EDTs) are crucial for monitoring power electronics systems. This paper proposes Physics-Embedded Neural ODEs (PENODE) to address challenges in capturing hybrid dynamics, with experimental results showing significantly higher accuracy and 75% reduction in neuron count compared to conventional approaches. The method bridges the gap between simulated environments and real-world applications by incorporating physical constraints directly into the neural network architecture.},
  archiveprefix = {arXiv},
  author = {Zheng, Jialin and Wang, Haoyu and Zeng, Yangbin and Mou, Di and Zhang, Xin and Li, Hong and Vazquez, Sergio and Franquelo, Leopoldo G.},
  eprint = {2508.02887},
  file = {:/home/b/documents/article/zheng2025physicsembedded.pdf:pdf},
  journal = {arXiv preprint arXiv:2508.02887},
  month = {8},
  note = {Submitted 4 Aug 2024. Original entry had incorrect author attribution},
  pdf = {https://arxiv.org/pdf/2508.02887},
  title = {Physics-Embedded Neural ODEs for Sim2Real Edge Digital Twins of Hybrid Power Electronics Systems},
  url = {https://arxiv.org/abs/2508.02887},
  year = {2025}
}

@misc{wong2025survey,
  abstract = {Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. The authors also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, the paper offers a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods---such as world models and geometric equivariance---to help researchers select suitable tools.},
  archiveprefix = {arXiv},
  author = {Wong, Lik Hang Kenny and Kang, Xueyang and Bai, Kaixin and Zhang, Jianwei},
  eprint = {2505.01458},
  file = {:/home/b/documents/misc/wong2025survey.pdf:pdf},
  pdf = {https://arxiv.org/pdf/2505.01458.pdf},
  primaryclass = {cs.RO},
  title = {A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI},
  url = {https://arxiv.org/abs/2505.01458},
  year = {2025}
}

@misc{chen2025neural,
  archiveprefix = {arXiv},
  author = {Chen, Zhimeng and Zhang, Tianyu and Jin, Lizhou and Gao, Cheng and Wu, Wei and Li, Yinzhou},
  eprint = {2508.15755},
  note = {WARNING: This arXiv paper number appears to be incorrect or non-existent. Could not verify paper existence through comprehensive searches.},
  primaryclass = {cs.RO},
  title = {Neural Robot Dynamics: Learning Stable and Safe Robot Dynamics from Demonstrations and Sensory Inputs},
  year = {2025}
}

@misc{shang2025roboscape,
  abstract = {World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this work, we propose RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. Specifically, RoboScape introduces two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics). Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate the effectiveness of our approach through downstream applications, including robotic policy training with generated data and policy evaluation, showcasing SOTA performance on visual quality, geometric accuracy, and action controllability.},
  archiveprefix = {arXiv},
  author = {Shang, Yu and Zhang, Xin and Tang, Yinzhou and Jin, Lei and Gao, Chen and Wu, Wei and Li, Yong},
  eprint = {2506.23135},
  file = {:/home/b/documents/misc/shang2025roboscape.pdf:pdf},
  month = {June},
  note = {Submitted to arXiv},
  pdf = {https://arxiv.org/pdf/2506.23135.pdf},
  primaryclass = {cs.CV},
  title = {{RoboScape}: Physics-informed Embodied World Model},
  url = {https://arxiv.org/abs/2506.23135},
  year = {2025}
}

@article{lelidec2024contact,
  abstract = {Physics simulation is ubiquitous in robotics. Whether in model-based approaches (e.g., trajectory optimization), or model-free algorithms (e.g., reinforcement learning), physics simulators are a central component of modern control pipelines in robotics. Over the past decades, several robotic simulators have been developed, each with dedicated contact modeling assumptions and algorithmic solutions. In this article, we survey the main contact models and the associated numerical methods commonly used in robotics for simulating advanced robot motions involving contact interactions. In particular, we recall the physical laws underlying contacts and friction (i.e., Signorini condition, Coulomb's law, and the maximum dissipation principle), and how they are transcribed in current simulators. For each physics engine, we expose their inherent physical relaxations along with their limitations due to the numerical techniques employed. Based on our study, we propose theoretically grounded quantitative criteria on which we build benchmarks assessing both the physical and computational aspects of simulation. We support our work with an open-source and efficient C++ implementation of the existing algorithmic variations. Our results demonstrate that some approximations or algorithms commonly used in robotics can severely widen the reality gap and impact target applications.},
  archiveprefix = {arXiv},
  author = {Le Lidec, Quentin and Jallet, Wilson and Montaut, Louis and Laptev, Ivan and Schmid, Cordelia and Carpentier, Justin},
  doi = {10.1109/TRO.2024.3434208},
  eprint = {2304.06372},
  file = {:/home/b/documents/article/lelidec2024contact.pdf:pdf},
  journal = {IEEE Transactions on Robotics},
  number = {4},
  pages = {3716--3733},
  pdf = {https://arxiv.org/pdf/2304.06372},
  primaryclass = {cs.RO},
  publisher = {IEEE},
  title = {Contact Models in Robotics: a Comparative Analysis},
  url = {https://ieeexplore.ieee.org/document/10612225/},
  volume = {40},
  year = {2024}
}

@misc{team2024genesis,
  abstract = {Genesis is a physics platform for robotics and embodied AI, designed as a universal physics engine that integrates multiple physics solvers into a unified framework. It supports rigid body, MPM, SPH, FEM, and PBD solvers for simulating diverse materials including rigid bodies, liquids, gases, and deformable objects. Genesis delivers ultra-fast simulation speeds up to 43 million FPS and includes native ray-tracing rendering, cross-platform support, and is designed to be fully differentiable.},
  author = {The Genesis Embodied AI Team},
  howpublished = {GitHub Open Source Release},
  license = {Apache 2.0},
  month = {12},
  note = {Ultra-fast physics simulation platform achieving 10-80x speedup over existing GPU-accelerated robotics simulators},
  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},
  url = {https://github.com/Genesis-Embodied-AI/Genesis},
  urldate = {https://genesis-embodied-ai.github.io/},
  year = {2024}
}

@article{bai2024energybased,
  abstract = {Numerical methods for contact mechanics are of great importance in engineering applications, enabling the prediction and analysis of complex surface interactions under various conditions. We propose an energy-based physics-informed neural network (PINNs) framework for solving frictionless contact problems under large deformation, using a surface contact energy inspired by the Lennard-Jones potential. The framework introduces relaxation, gradual loading, and output scaling techniques and demonstrates competitive computational efficiency compared to FEM software.},
  archiveprefix = {arXiv},
  author = {Bai, Jinshuai and Lin, Zhongya and Wang, Yizheng and Wen, Jiancong and Liu, Yinghua and Rabczuk, Timon and Gu, YuanTong and Feng, Xi-Qiao},
  eprint = {2411.03671},
  file = {:/home/b/documents/article/bai2024energybased.pdf:pdf},
  journal = {arXiv preprint arXiv:2411.03671},
  month = {11},
  note = {Last revised 30 Jan 2025},
  pdf = {https://arxiv.org/pdf/2411.03671.pdf},
  primaryclass = {cs.CE},
  title = {Energy-based physics-informed neural network for frictionless contact problems under large deformation},
  url = {https://arxiv.org/abs/2411.03671},
  year = {2024}
}

@article{sahin2024physicsinformed,
  abstract = {This paper explores the application of physics-informed neural networks (PINNs) to tackle forward problems in 3D contact mechanics, focusing on small deformation elasticity. The authors utilize a mixed-variable formulation, enhanced with output transformations, to enforce Dirichlet and Neumann boundary conditions as hard constraints. The inherent inequality constraints in contact mechanics, particularly the Karush-Kuhn-Tucker (KKT) conditions, are addressed as soft constraints by integrating them into the network's loss function.},
  archiveprefix = {arXiv},
  author = {Sahin, Tarik and Wolff, Daniel and Popp, Alexander},
  eprint = {2412.09022},
  file = {:/home/b/documents/article/sahin2024physicsinformed.pdf:pdf},
  journal = {arXiv preprint arXiv:2412.09022},
  keywords = {Physics-informed neural networks, Contact mechanics, Three-dimensional problems, Mixed-variable formulation},
  month = {12},
  note = {University of the Bundeswehr Munich},
  pdf = {https://arxiv.org/pdf/2412.09022.pdf},
  primaryclass = {cs.CE},
  title = {Physics-Informed Neural Networks for Solving Contact Problems in Three Dimensions},
  url = {https://arxiv.org/abs/2412.09022},
  year = {2024}
}

@inproceedings{si2024difftactile,
  abstract = {We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. The system emphasizes high-fidelity contact modeling, supporting simulations of diverse contact modes and material interactions. Key components include a FEM-based soft body model, multi-material simulator, and penalty-based contact model. The differentiable nature facilitates gradient-based optimization for refining physical properties and efficient learning of tactile-assisted grasping and contact-rich manipulation skills.},
  archiveprefix = {arXiv},
  author = {Si, Zilin and Zhang, Gu and Ben, Qingwei and Romero, Branden and Xian, Zhou and Liu, Chao and Gan, Chuang},
  booktitle = {The Twelfth International Conference on Learning Representations (ICLR)},
  eprint = {2403.08716},
  file = {:/home/b/documents/inproceedings/si2024difftactile.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=eJHnSg783t},
  title = {DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation},
  url = {https://openreview.net/forum?id=eJHnSg783t},
  website = {https://difftactile.github.io/},
  year = {2024}
}

@inproceedings{liu2024differentiable,
  abstract = {Vision foundation models trained on massive amounts of visual data have shown unprecedented reasoning and planning skills in open-world settings. A key challenge in applying them to robotic tasks is the modality gap between visual data and action data. The paper introduces differentiable robot rendering, a method allowing the visual appearance of a robot body to be directly differentiable with respect to its control parameters. Their model integrates a kinematics-aware deformable model and Gaussian Splatting and is compatible with any robot form factors and degrees of freedom. The authors demonstrate its capability in applications including reconstruction of robot poses from images and controlling robots through vision language models.},
  archiveprefix = {arXiv},
  author = {Liu, Ruoshi and Canberk, Alper and Song, Shuran and Vondrick, Carl},
  booktitle = {Conference on Robot Learning (CoRL)},
  code = {https://github.com/cvlab-columbia/drrobot},
  eprint = {2410.13851},
  file = {:/home/b/documents/inproceedings/liu2024differentiable.pdf:pdf},
  pdf = {https://arxiv.org/pdf/2410.13851.pdf},
  title = {Differentiable Robot Rendering},
  url = {https://openreview.net/forum?id=lt0Yf8Wh5O},
  website = {https://drrobot.cs.columbia.edu/},
  year = {2024}
}

@inproceedings{zhao2024neural,
  abstract = {Acquiring new skills through Imitation Learning (IL) is crucial for handling diverse complex tasks in robotics. However, model-free IL faces challenges of data inefficiency and prolonged training time, whereas model-based methods struggle to obtain accurate nonlinear models. To address these challenges, we develop Neural ODE-based Imitation Learning (NODE-IL), a novel model-based imitation learning framework that employs Neural Ordinary Differential Equations (Neural ODEs) for learning task dynamics and control policies. NODE-IL comprises Dynamic-NODE for learning continuous differentiable task transition dynamics and Control-NODE for learning long-horizon control policy in an MPC fashion. Extensively evaluated on challenging manipulation tasks, NODE-IL demonstrates significant advantages in data efficiency, requiring less than 70 samples to achieve robust performance, outperforming BCO and GP-IL methods with 70% higher average success rate.},
  address = {Abu Dhabi, United Arab Emirates},
  author = {Zhao, Shiyao and Xu, Yucheng and Kasaei, Mohammadreza and Khadem, Mohsen and Li, Zhibin},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS58592.2024.10802736},
  month = {10},
  pages = {8524--8530},
  publisher = {IEEE},
  title = {Neural ODE-based Imitation Learning (NODE-IL): Data-efficient imitation learning for long-horizon multi-skill robot manipulation},
  url = {https://ieeexplore.ieee.org/document/10802736/},
  year = {2024}
}

@inproceedings{ingebrand2024zeroshot,
  abstract = {Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.},
  archiveprefix = {arXiv},
  author = {Ingebrand, Tyler and Thorpe, Adam J. and Topcu, Ufuk},
  booktitle = {Advances in Neural Information Processing Systems 37},
  eprint = {2405.08954},
  file = {:/home/b/documents/inproceedings/ingebrand2024zeroshot.pdf:pdf},
  note = {Original entry had incorrect author attribution},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/7ce9df1d14adc2806df18c8e168d7ef9-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Zero-Shot Transfer of Neural ODEs},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/7ce9df1d14adc2806df18c8e168d7ef9-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@misc{tao2024maniskill3,
  abstract = {Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization for simulation, rendering, and visual inputs, enabling up to 30,000+ FPS in benchmarked environments with 2-3x less GPU memory usage than other platforms. The comprehensive range of environments includes 12 distinct domains spanning mobile manipulation, humanoids, dexterous manipulation, and classic control. We provide millions of demonstration frames from motion planning, RL, and teleoperation, along with baselines spanning popular RL and learning-from-demonstrations algorithms.},
  archiveprefix = {arXiv},
  author = {Tao, Stone and Xiang, Fanbo and Shukla, Arth and Qin, Yuzhe and Hinrichsen, Xander and Yuan, Xiaodi and Bao, Chen and Lin, Xuelang and Chen, Dahua and Jia, Zhengmao and Gu, Jiayuan and Chen, Zhiao and Jia, Conghui and Zhang, Yixin and Bisk, Yonathan and Singh, Gaurav and Su, Huan},
  eprint = {2410.00425},
  file = {:/home/b/documents/misc/tao2024maniskill3.pdf:pdf},
  note = {Website: \url{http://maniskill.ai/}},
  pdf = {https://arxiv.org/pdf/2410.00425.pdf},
  primaryclass = {cs.RO},
  title = {{ManiSkill3}: {GPU} Parallelized Robotics Simulation and Rendering for Generalizable Embodied {AI}},
  url = {https://arxiv.org/abs/2410.00425},
  year = {2024}
}

@misc{yi2024generating,
  abstract = {We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. Our method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses.},
  archiveprefix = {arXiv},
  author = {Yi, Hongwei and Thies, Justus and Black, Michael J. and Peng, Xue Bin and Rempe, Davis},
  eprint = {2404.10685},
  file = {:/home/b/documents/misc/yi2024generating.pdf:pdf},
  note = {Project website: \url{https://research.nvidia.com/labs/toronto-ai/tesmo/}},
  pdf = {https://arxiv.org/pdf/2404.10685.pdf},
  primaryclass = {cs.CV},
  title = {Generating Human Interaction Motions in Scenes with Text Control},
  url = {https://arxiv.org/abs/2404.10685},
  year = {2024}
}

@inproceedings{lee2023aquarium,
  abstract = {We present Aquarium, a differentiable fluid-structure interaction solver for robotics that offers stable simulation, accurately coupled fluid-robot physics in two dimensions, and full differentiability with respect to fluid and robot states and parameters. Aquarium achieves stable simulation with accurate flow physics by directly integrating over the incompressible Navier-Stokes equations using a fully implicit Crank-Nicolson scheme with a second-order finite-volume spatial discretization.},
  author = {Lee, Jeong Hun and Michelis, Mike Y. and Katzschmann, Robert and Manchester, Zachary},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA48891.2023.10161494},
  file = {:/home/b/documents/inproceedings/lee2023aquarium.pdf:pdf},
  note = {Code available at r̆lhttps://github.com/RoboticExplorationLab/Aquarium.jl},
  pages = {11272--11279},
  pdf = {https://arxiv.org/pdf/2301.07028.pdf},
  title = {Aquarium: A Fully Differentiable Fluid-Structure Interaction Solver for Robotics Applications},
  url = {https://ieeexplore.ieee.org/document/10161494/},
  year = {2023}
}

@inproceedings{yang2023unisim,
  abstract = {UniSim is a neural sensor simulator that takes a single recorded log captured by a sensor-equipped vehicle and converts it into a realistic closed-loop multi-sensor simulation. UniSim builds neural feature grids to reconstruct both the static background and dynamic actors in the scene, and composites them together to simulate LiDAR and camera data at new viewpoints, with actors added or removed and at new placements. To better handle extrapolated views, it incorporates learnable priors for dynamic objects, and leverages a convolutional network to complete unseen regions. Experiments show UniSim can simulate realistic sensor data with small domain gap on downstream tasks. With UniSim, the authors demonstrate, for the first time, closed-loop evaluation of an autonomy system on safety-critical scenarios as if it were in the real world.},
  archiveprefix = {arXiv},
  author = {Yang, Ze and Chen, Yun and Wang, Jingkang and Manivasagam, Sivabalan and Ma, Wei-Chiu and Yang, Anqi Joyce and Urtasun, Raquel},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {2308.01898},
  file = {:/home/b/documents/inproceedings/yang2023unisim.pdf:pdf},
  pages = {1389--1399},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf},
  title = {UniSim: A Neural Closed-Loop Sensor Simulator},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.html},
  website = {https://waabi.ai/unisim/},
  year = {2023}
}

@inproceedings{jiang2023motiongpt,
  abstract = {MotionGPT proposes a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. The core insight is that human motion displays a semantic coupling akin to human language, often perceived as a form of body language. The authors employ discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. MotionGPT consists of a motion tokenizer responsible for converting raw motion data into discrete motion tokens, as well as a motion-aware language model that learns to understand the motion tokens from large language pre-training models. Building upon this "motion vocabulary", they perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between.},
  archiveprefix = {arXiv},
  author = {Jiang, Biao and Chen, Xin and Liu, Wen and Yu, Jingyi and Yu, Gang and Chen, Tao},
  booktitle = {Advances in Neural Information Processing Systems},
  code = {https://github.com/OpenMotionLab/MotionGPT},
  eprint = {2306.14795},
  file = {:/home/b/documents/inproceedings/jiang2023motiongpt.pdf:pdf},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3fbf0c1ea0716c03dea93bb6be78dd6f-Paper-Conference.pdf},
  title = {MotionGPT: Human Motion as a Foreign Language},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3fbf0c1ea0716c03dea93bb6be78dd6f-Abstract-Conference.html},
  volume = {36},
  website = {https://motion-gpt.github.io/},
  year = {2023}
}

@misc{liu2023softmac,
  abstract = {Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. However, current differentiable physics simulators typically handle either rigid or soft body dynamics but rarely both. This limitation restricts their applicability in manipulation scenarios involving diverse materials. We present SoftMAC, a simulation framework that couples soft bodies with articulated rigid bodies and clothes. Our approach enables comprehensive dynamic modeling and provides an explicit and differentiable coupling mechanism that allows for more general interactions, such as soft bodies serving as manipulators and engaging with underactuated systems. SoftMAC provides a novel forecast-based contact model for MPM which effectively reduces penetration without introducing artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we propose a penetration tracing algorithm that reconstructs the signed distance field in local area. We validate our framework through comprehensive experiments on robotic manipulation applications with various soft, rigid, and cloth components.},
  archiveprefix = {arXiv},
  author = {Liu, Min and Yang, Gang and Luo, Siyuan and Shao, Lin},
  eprint = {2312.03297},
  file = {:/home/b/documents/misc/liu2023softmac.pdf:pdf},
  note = {Project website: \url{https://minliu01.github.io/SoftMAC}},
  pdf = {https://arxiv.org/pdf/2312.03297.pdf},
  primaryclass = {cs.RO},
  title = {{SoftMAC}: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes},
  url = {https://arxiv.org/abs/2312.03297},
  year = {2023}
}

@misc{xian2023fluidlab,
  abstract = {Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. The previous research in robotic fluid manipulation mostly considered fluids as governed by an ideal, Newtonian model in simple task settings, while real-world fluid systems manifest complexities in terms of complex material behaviors and multi-component interactions. We present FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. Tasks in FluidLab address interactions between solid and fluid as well as among multiple fluids. At the heart of the platform is a fully differentiable physics simulator called FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings. We identify several challenges for fluid manipulation learning by evaluating reinforcement learning and trajectory optimization methods on our platform and propose domain-specific optimization schemes coupled with differentiable physics, which were shown to be effective in tackling optimization problems featured by fluid systems' non-convex and non-smooth properties.},
  archiveprefix = {arXiv},
  author = {Xian, Zhou and Zhu, Bo and Xu, Zhenjia and Tung, Hsiao-Yu and Torralba, Antonio and Fragkiadaki, Katerina and Gan, Chuang},
  eprint = {2303.02346},
  file = {:/home/b/documents/misc/xian2023fluidlab.pdf:pdf},
  note = {ICLR 2023. Project website: \url{https://fluidlab2023.github.io/}},
  pdf = {https://arxiv.org/pdf/2303.02346.pdf},
  primaryclass = {cs.RO},
  title = {{FluidLab}: A Differentiable Environment for Benchmarking Complex Fluid Manipulation},
  url = {https://arxiv.org/abs/2303.02346},
  year = {2023}
}

@inproceedings{wang2023dexgraspnet,
  abstract = {Robotic dexterous grasping is the first step to enable human-like dexterous object manipulation and thus a crucial robotic technology. However, dexterous grasping is much more under-explored than object grasping with parallel grippers, partially due to the lack of a large-scale dataset. In this work, we present DexGraspNet, a large-scale robotic dexterous grasp dataset generated by our highly efficient synthesis method that can be generally applied to any dexterous hand. DexGraspNet contains 1.32 million grasps for 5355 objects, covering more than 133 object categories and containing more than 200 diverse grasps for each object on average. Our synthesis method leverages a deeply accelerated differentiable force closure estimator and thus can efficiently and robustly synthesize stable and diverse grasps on a large scale. All grasps have been validated by the Isaac Gym simulator. Compared to the previous dataset generated by GraspIt!, our dataset has not only more objects and grasps, but also higher diversity and quality. Via performing cross-dataset experiments, we show that training several algorithms of dexterous grasp synthesis on our dataset significantly outperforms training on the previous one.},
  address = {London, UK},
  archiveprefix = {arXiv},
  author = {Wang, Ruicheng and Zhang, Jialiang and Chen, Jiayi and Xu, Yinzhen and Li, Puhao and Liu, Tengyu and Wang, He},
  booktitle = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA48891.2023.10160982},
  eprint = {2210.02697},
  month = {May},
  pages = {6150--6156},
  pdf = {https://pku-epic.github.io/DexGraspNet/assets/dexgraspnet.pdf},
  primaryclass = {cs.RO},
  title = {{DexGraspNet}: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation},
  url = {https://ieeexplore.ieee.org/document/10160982},
  year = {2023}
}

@article{collins2021review,
  abstract = {The use of simulators in robotics research is widespread, underpinning the majority of recent advances in the field. There are now more options available to researchers than ever before, however navigating through the plethora of choices in search of the right simulator is often non-trivial. Depending on the field of research and the scenario to be simulated there will often be a range of suitable physics simulators from which it is difficult to ascertain the most relevant one. We have compiled a broad review of physics simulators for use within the major fields of robotics research. This review provides an extensive index of the leading physics simulators applicable to robotics researchers and aims to assist them in choosing the best simulator for their use case.},
  author = {Collins, Jack and Chand, Shelvin and Vanderkop, Anthony and Howard, David},
  doi = {10.1109/ACCESS.2021.3068769},
  journal = {IEEE Access},
  pages = {51416--51431},
  pdf = {https://ieeexplore.ieee.org/iel7/6287639/9312710/09386154.pdf},
  title = {A Review of Physics Simulators for Robotic Applications},
  url = {https://ieeexplore.ieee.org/document/9386154/},
  volume = {9},
  year = {2021}
}

@misc{garg2021semantics,
  abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world ``mean'' to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. This survey provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world.},
  archiveprefix = {arXiv},
  author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
  eprint = {2101.00443},
  file = {:/home/b/documents/misc/garg2021semantics.pdf:pdf},
  note = {Also published in Foundations and Trends® in Robotics: Vol. 8: No. 1--2, pp 1--224},
  pdf = {https://arxiv.org/pdf/2101.00443.pdf},
  primaryclass = {cs.RO},
  title = {Semantics for Robotic Mapping, Perception and Interaction: A Survey},
  url = {https://arxiv.org/abs/2101.00443},
  year = {2021}
}

@inproceedings{freeman2021brax,
  abstract = {We present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common MuJoCo-like tasks in minutes.},
  archiveprefix = {arXiv},
  author = {Freeman, C. Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  booktitle = {Neural Information Processing Systems Track on Datasets and Benchmarks},
  eprint = {2106.13281},
  file = {:/home/b/documents/inproceedings/freeman2021brax.pdf:pdf},
  month = {June},
  note = {NeurIPS 2021 Datasets and Benchmarks Track},
  pdf = {https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/d1f491a404d6854880943e5c3cd9ca25-Paper-round1.pdf},
  primaryclass = {cs.LG},
  title = {{Brax} - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract-round1.html},
  year = {2021}
}

@article{mller2020detailed,
  abstract = {We present a rigid body simulation method that can resolve small temporal and spatial details by using a quasi explicit integration scheme that is unconditionally stable. Traditional rigid body simulators linearize constraints because they operate on the velocity level or solve the equations of motion implicitly thereby freezing the constraint directions for multiple iterations. Our method always works with the most recent constraint directions. This allows us to trace high speed motion of objects colliding against curved geometry, to reduce the number of constraints, to increase the robustness of the simulation, and to simplify the formulation of the solver. In this paper we provide all the details to implement a fully fledged rigid body solver that handles contacts, a variety of joint types and the interaction with soft objects.},
  author = {Müller, Matthias and Macklin, Miles and Chentanez, Nuttapong and Jeschke, Stefan and Kim, Tae-Yong},
  doi = {10.1111/cgf.14105},
  file = {:/home/b/documents/article/mller2020detailed.pdf:pdf},
  journal = {Computer Graphics Forum},
  note = {Proc. ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
  number = {8},
  pages = {101--112},
  pdf = {https://matthias-research.github.io/pages/publications/PBDBodies.pdf},
  publisher = {John Wiley \& Sons},
  title = {Detailed Rigid Body Simulation with Extended Position Based Dynamics},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14105},
  volume = {39},
  year = {2020}
}

@inproceedings{zhao2020simtoreal,
  abstract = {Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This survey explores sim-to-real transfer methods in deep reinforcement learning for robotics, covering fundamental background and main approaches including domain randomization, domain adaptation, imitation learning, meta-learning, and knowledge distillation. The authors categorize recent relevant works, outline main application scenarios, and discuss opportunities and challenges of different approaches while pointing to promising research directions.},
  author = {Zhao, Wenshuai and Queralta, Jorge Peña and Westerlund, Tomi},
  booktitle = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  doi = {10.1109/SSCI47803.2020.9308468},
  location = {Canberra, Australia},
  pages = {737--744},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9308468},
  title = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},
  url = {https://ieeexplore.ieee.org/document/9308468},
  year = {2020}
}

@inproceedings{tan2018simtoreal,
  abstract = {We present a system for learning quadruped locomotion from scratch using deep reinforcement learning with simple reward signals. The system automates the design process that often requires extensive expertise and tedious manual tuning. Control policies are learned in a physics simulator and then deployed on real robots. We narrow the reality gap by improving the physics simulator, developing accurate actuator models, simulating latency, and randomizing physical environments. The system was evaluated on two agile locomotion gaits: trotting and galloping, with successful transfer from simulation to real-world performance.},
  address = {Pittsburgh, Pennsylvania},
  author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
  booktitle = {Robotics: Science and Systems XIV},
  doi = {10.15607/RSS.2018.XIV.010},
  file = {:/home/b/documents/inproceedings/tan2018simtoreal.pdf:pdf},
  month = {6},
  note = {Google AI Research. Video available on YouTube},
  pdf = {https://arxiv.org/pdf/1804.10332.pdf},
  title = {Sim-to-Real: Learning Agile Locomotion For Quadruped Robots},
  url = {https://roboticsproceedings.org/rss14/p10.html},
  year = {2018}
}

@misc{li2018learning,
  abstract = {Real-life control tasks involve matters of various substances---rigid or soft bodies, liquid, gas---each with distinct physical behaviors. This poses challenges to traditional rigid-body physics engines. Particle-based simulators have been developed to model the dynamics of these complex scenes; however, relying on approximation techniques, their simulation often deviates from real-world physics, especially in the long term. We propose to learn a particle-based simulator for complex control tasks. Specifically, we present Dynamic Particle Interaction Networks (DPI-Nets) for learning particle dynamics, focusing on capturing the dynamic, hierarchical, and long-range interactions of particles. The learned simulator, just like other particle-based systems, acts widely on objects of different materials; the particle-based representation poses strong inductive bias for learning. DPI-Nets have successfully captured the complex behaviors of deformable objects, fluids, and rigid-bodies. With learned DPI-Nets, robots have achieved success in manipulation tasks that involve deformable objects of complex physical properties.},
  archiveprefix = {arXiv},
  author = {Li, Yunzhu and Wu, Jiajun and Tedrake, Russ and Tenenbaum, Joshua B. and Torralba, Antonio},
  eprint = {1810.01566},
  file = {:/home/b/documents/misc/li2018learning.pdf:pdf},
  note = {ICLR 2019. Project website: \url{http://dpi.csail.mit.edu/}},
  pdf = {https://arxiv.org/pdf/1810.01566.pdf},
  primaryclass = {cs.LG},
  title = {Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids},
  url = {https://arxiv.org/abs/1810.01566},
  year = {2018}
}

@inproceedings{tobin2017domain,
  abstract = {We explore domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We trained a real-world object detector accurate to 1.5 cm that is robust to distractors and partial occlusions using only simulated data with non-realistic random textures, and demonstrated grasping capabilities in cluttered environments.},
  address = {Vancouver, BC, Canada},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2017.8202133},
  file = {:/home/b/documents/inproceedings/tobin2017domain.pdf:pdf},
  isbn = {978-1-5386-2682-5},
  note = {First successful transfer of deep neural network trained only on simulated RGB images to real world for robotic control},
  pages = {23--30},
  pdf = {https://arxiv.org/pdf/1703.06907.pdf},
  title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
  url = {https://ieeexplore.ieee.org/document/8202133/},
  year = {2017}
}

@inproceedings{schenck2017reasoning,
  abstract = {Simulators are powerful tools for reasoning about a robot's interactions with its environment. However, when simulations diverge from reality, that reasoning becomes less useful. In this paper, we show how to close the loop between liquid simulation and real-time perception. Our results show that closed-loop simulation is an effective way to prevent large divergence between the simulated and real liquid states, enabling reasoning about liquids that would otherwise be infeasible due to large divergences, such as reasoning about occluded liquid.},
  address = {Cambridge, Massachusetts},
  author = {Schenck, Connor and Fox, Dieter},
  booktitle = {Robotics: Science and Systems XIII},
  doi = {10.15607/RSS.2017.XIII.014},
  file = {:/home/b/documents/inproceedings/schenck2017reasoning.pdf:pdf},
  month = {7},
  note = {University of Washington. Demonstrates real-time liquid tracking with simulation correction},
  pdf = {https://arxiv.org/pdf/1703.01656.pdf},
  title = {Reasoning About Liquids via Closed-Loop Simulation},
  url = {https://www.roboticsproceedings.org/rss13/p14.html},
  year = {2017}
}

@article{tan2012soft,
  abstract = {We present a physically-based system to simulate and control the locomotion of soft body characters without skeletons. We use the finite element method to simulate the deformation of the soft body, and we instrument a character with muscle fibers to allow it to actively control its shape. To perform locomotion, we use a variety of intuitive controls such as moving a point on the character, specifying the center of mass or the angular momentum, and maintaining balance. To control the locomotion, we formulate and solve a quadratic program with complementary conditions (QPCC) to plan the muscle contraction and the contact forces simultaneously.},
  articleno = {26},
  author = {Tan, Jie and Turk, Greg and Liu, C. Karen},
  doi = {10.1145/2185520.2185522},
  file = {:/home/b/documents/article/tan2012soft.pdf:pdf},
  journal = {ACM Transactions on Graphics},
  month = {7},
  note = {Proc. ACM SIGGRAPH},
  number = {4},
  numpages = {11},
  pdf = {https://faculty.cc.gatech.edu/~turk/my_papers/soft_body_locomotion.pdf},
  publisher = {ACM},
  title = {Soft Body Locomotion},
  url = {http://www.jie-tan.net/project/softBodyLocomotion.html},
  volume = {31},
  year = {2012}
}

@book{featherstone2008rigid,
  abstract = {Rigid Body Dynamics Algorithms presents the subject of computational rigid-body dynamics through the medium of spatial 6D vector notation. It explains how to model a rigid-body system and how to analyze it, and it presents the most comprehensive collection of the best rigid-body dynamics algorithms to be found in a single source. The use of spatial vector notation greatly reduces the volume of algebra which allows systems to be described using fewer equations and fewer quantities.},
  address = {New York, NY},
  author = {Featherstone, Roy},
  doi = {10.1007/978-1-4899-7560-7},
  isbn = {978-0-387-74314-1},
  pages = {IX, 272},
  publisher = {Springer New York},
  title = {Rigid Body Dynamics Algorithms},
  url = {https://link.springer.com/book/10.1007/978-0-387-74314-1},
  year = {2008}
}

@article{mller2007position,
  abstract = {The most popular approaches for the simulation of dynamic systems in computer graphics are force based. Internal and external forces are accumulated from which accelerations are computed based on Newton's second law of motion. A time integration method is then used to update the velocities and finally the positions of the object. A few simulation methods (most rigid body simulators) use impulse based dynamics and directly manipulate velocities. In this paper we present an approach which omits the velocity layer as well and immediately works on the positions. The main advantage of a position based approach is its controllability. Overshooting problems of explicit integration schemes in force based systems can be avoided. In addition, collision constraints can be handled easily and penetrations can be resolved completely by projecting points to valid locations.},
  author = {Müller, Matthias and Heidelberger, Bruno and Hennix, Marcus and Ratcliff, John},
  doi = {10.1016/j.jvcir.2007.01.005},
  file = {:/home/b/documents/article/mller2007position.pdf:pdf},
  journal = {Journal of Visual Communication and Image Representation},
  month = {4},
  number = {2},
  pages = {109--118},
  pdf = {https://matthias-research.github.io/pages/publications/posBasedDyn.pdf},
  publisher = {Elsevier},
  title = {Position Based Dynamics},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S1047320307000065},
  volume = {18},
  year = {2007}
}

@article{matyka2004pressure,
  abstract = {Motivated by existing models used for soft body simulation which are rather complex to implement, we present a novel technique which is based on simple laws of physics and gives high quality results in real-time. We base the implementation on simple thermodynamics laws and use the Clausius-Clapeyron state equation for pressure calculation. In addition, this provides us with a pressure force that is accumulated into a force accumulator of a 3D mesh object by using an existing spring-mass engine. Finally after integration of Newtons second law we obtain the behavior of a soft body with fixed or non-fixed air pressure inside of it.},
  archiveprefix = {arXiv},
  author = {Matyka, Maciej and Ollila, Mark},
  eprint = {physics/0407003},
  file = {:/home/b/documents/article/matyka2004pressure.pdf:pdf},
  journal = {arXiv preprint physics/0407003},
  month = {7},
  note = {Presented at SIGRAD 2003, Umeå, Sweden},
  pages = {5},
  pdf = {https://arxiv.org/pdf/physics/0407003},
  primaryclass = {physics.comp-ph},
  title = {Pressure Model of Soft Body Simulation},
  url = {https://arxiv.org/abs/physics/0407003},
  year = {2004}
}

@inproceedings{mller2003particlebased,
  abstract = {Realistically animated fluids can add substantial realism to interactive applications such as virtual surgery simulators or computer games. We propose an interactive method based on Smoothed Particle Hydrodynamics (SPH) to simulate fluids with free surfaces. The method extends SPH-based techniques by Desbrun for deformable bodies, gearing it toward fluid simulation by deriving the force density fields directly from the Navier-Stokes equation and by adding a term to model surface tension effects. In contrast to Eulerian grid-based approaches, the particle-based approach makes mass conservation equations and convection terms dispensable which reduces the complexity of the simulation.},
  author = {Müller, Matthias and Charypar, David and Gross, Markus},
  booktitle = {Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation},
  doi = {10.2312/SCA03/154-159},
  file = {:/home/b/documents/inproceedings/mller2003particlebased.pdf:pdf},
  month = {7},
  pages = {154--159},
  pdf = {https://matthias-research.github.io/pages/publications/sca03.pdf},
  publisher = {ACM},
  title = {Particle-Based Fluid Simulation for Interactive Applications},
  url = {https://dl.acm.org/doi/10.5555/846276.846298},
  year = {2003}
}

@inproceedings{mller2002stable,
  abstract = {The linear strain measures that are commonly used in real-time animations of deformable objects yield fast and stable simulations. However, they are not suitable for large deformations. Recently, more realistic results have been achieved in computer graphics by using Green's non-linear strain tensor, but the non-linearity makes the simulation more costly and introduces numerical problems. In this paper, we present a new simulation technique that is stable and fast like linear models, but without the disturbing artifacts that occur with large deformations.},
  address = {San Antonio, TX, USA},
  author = {Müller, Matthias and Dorsey, Julie and McMillan, Leonard and Jagnow, Robert and Cutler, Barbara},
  booktitle = {Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation},
  doi = {10.1145/545261.545269},
  file = {:/home/b/documents/inproceedings/mller2002stable.pdf:pdf},
  pages = {49--54},
  pdf = {https://matthias-research.github.io/pages/publications/warp.pdf},
  publisher = {ACM},
  title = {Stable real-time deformations},
  url = {https://dl.acm.org/doi/10.1145/545261.545269},
  year = {2002}
}

@techreport{baraff1997introduction,
  abstract = {An introduction to physically based modeling for rigid body simulation focusing on unconstrained rigid body dynamics. The solution is computed by numerically timestepping the ordinary differential equations of rigid body motion, derived from Newton's 2nd law, and conservation of linear momentum and angular momentum.},
  author = {Baraff, David},
  file = {:/home/b/documents/techreport/baraff1997introduction.pdf:pdf},
  institution = {SIGGRAPH '97 Course Notes},
  pages = {D1--D31},
  pdf = {https://www.cs.cmu.edu/~baraff/sigcourse/notesd1.pdf},
  publisher = {ACM},
  title = {An Introduction to Physically Based Modeling: Rigid Body Simulation I---Unconstrained Rigid Body Dynamics},
  url = {https://www.cs.cmu.edu/~baraff/sigcourse/notesd1.pdf},
  year = {1997}
}

@inproceedings{baraff1993nonpenetrating,
  abstract = {This paper surveys recent work on dynamic simulation of rigid bodies with non-interpenetration constraints. The problems of collision detection and contact force determination are discussed, along with both frictionless and frictional collision and contact. For the collision-detection problem, methods which deal with objects' motion as a single continuous function of time are compared and contrasted.},
  address = {Barcelona, Spain},
  author = {Baraff, David},
  booktitle = {Proceedings of Eurographics '93 State of the Art Reports},
  file = {:/home/b/documents/inproceedings/baraff1993nonpenetrating.pdf:pdf},
  month = {9},
  pages = {24},
  pdf = {https://www.cs.cmu.edu/~baraff/papers/eg93.pdf},
  publisher = {Eurographics Association},
  title = {Non-penetrating rigid body simulation},
  url = {https://www.ri.cmu.edu/publications/non-penetrating-rigid-body-simulation/},
  year = {1993}
}
