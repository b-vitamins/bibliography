@inproceedings{wang2025data,
  author = {Wang, Jiachen T. and Mittal, Prateek and Song, Dawn and Jia, Ruoxi},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Honorable Mention. Introduces "In-Run Data Shapley," a method to calculate the Shapley value for data attribution during a single model training run. This approach dramatically reduces the computational cost of data valuation, making it feasible for foundation models.},
  openalex = {W4399794768},
  title = {Data Shapley in One Training Run},
  url = {https://openreview.net/pdf?id=2o2N3kP299},
  year = {2025}
}

@inproceedings{fang2025alphaedit,
  author = {Fang, Junfeng and Jiang, Houcheng and Wang, Kun and Ma, Yunshan and Shi, Jie and Wang, Xiang and He, Xiangnan and Chua, Tat-Seng},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Outstanding Paper. Addresses the problem of model editing in LLMs by proposing to project parameter perturbations onto the null space of preserved knowledge. This approach, AlphaEdit, mitigates the disruption of existing knowledge during sequential edits.},
  openalex = {W4403882889},
  title = {AlphaEdit: Null-Space Constrained Model Editing for Language Models},
  url = {https://openreview.net/pdf?id=P4wYdCHJgN},
  year = {2025}
}

@inproceedings{narasimhan2025faster,
  author = {Narasimhan, Harikrishna and Jitkrittum, Wittawat and Rawat, Ankit Singh and Kim, Seungyeon and Gupta, Neha and Menon, Aditya Krishna and Kumar, Sanjiv},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Honorable Mention. Combines the inference efficiency techniques of cascades and speculative decoding. The proposed speculative cascading method implements a deferral rule through speculative execution, yielding better cost-quality trade-offs than either baseline approach.},
  openalex = {W4399198692},
  title = {Faster Cascades via Speculative Decoding},
  url = {https://openreview.net/pdf?id=J2sN6fT7D7},
  year = {2025}
}

@inproceedings{qi2025safety,
  author = {Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Outstanding Paper. Identifies "shallow safety alignment"—where safety behaviors are only learned for the first few output tokens—as a root cause for multiple LLM vulnerabilities. Demonstrates that deepening this alignment improves robustness against common exploits.},
  openalex = {W4399553916},
  title = {Safety Alignment Should be Made More Than Just a Few Tokens Deep},
  url = {https://openreview.net/pdf?id=iKjZ2fQY5K},
  year = {2025}
}

@inproceedings{ravi2025sam,
  author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and Rädle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Dollar, Piotr and Feichtenhofer, Christoph},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Honorable Mention. Presents the Segment Anything Model 2 (SAM 2), a foundation model for promptable visual segmentation in both images and videos. The work includes a new large-scale video segmentation dataset and a streaming transformer architecture for real-time processing.},
  openalex = {W4401307635},
  title = {SAM 2: Segment Anything in Images and Videos},
  url = {https://openreview.net/pdf?id=3aB7j4yZ3f},
  year = {2025}
}

@inproceedings{ren2025learning,
  author = {Ren, Yi and Sutherland, Danica J.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Outstanding Paper. Provides a theoretical framework to analyze the learning dynamics of LLM finetuning. The work offers a unified interpretation for observed phenomena like hallucination and the "squeezing effect" in direct preference optimization (DPO).},
  openalex = {W4400714246},
  title = {Learning Dynamics of LLM Finetuning},
  url = {https://openreview.net/pdf?id=tPNHOoZFl9},
  year = {2025}
}

@inproceedings{yang2024learning,
  author = {Yang, Sherry and Du, Yilun and Ghasemipour, Seyed Kamyar Seyed and Tompson, Jonathan and Kaelbling, Leslie Pack and Schuurmans, Dale and Abbeel, Pieter},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Outstanding Paper. Introduces UniSim, a universal simulator trained on diverse datasets through a unified interface based on visual perceptions and text descriptions. UniSim can simulate realistic experiences, enabling the training of robotic agents purely in simulation for zero-shot real-world transfer.},
  openalex = {W4387560742},
  title = {Learning Interactive Real-World Simulators},
  url = {https://openreview.net/pdf?id=sFyTZEqmUY},
  year = {2024}
}

@inproceedings{zhang2024beyond,
  author = {Zhang, Bohang and Gai, Jingchu and Du, Yiheng and Ye, Qiwei and He, Di and Wang, Liwei},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Honorable Mention. Introduces a quantitative framework to measure the expressive power of Graph Neural Networks (GNNs) beyond the traditional Weisfeiler-Lehman test. This allows for a finer-grained comparison of different GNN architectures.},
  openalex = {W4390963127},
  title = {Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness},
  url = {https://openreview.net/pdf?id=kL8c2aQ12b},
  year = {2024}
}

@inproceedings{zhao2024position,
  author = {Zhao, Dora and Andrews, Jerone and Papakyriakopoulos, Orestis and Xiang, Alice},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Analyzes the use of the term "diversity" across 135 ML datasets and argues for a more rigorous, measurement-theoretic approach to conceptualizing and evaluating such value-laden properties in dataset construction.},
  openalex = {W4400611527},
  title = {Position: Measure Dataset Diversity, Don't Just Claim It},
  url = {https://proceedings.mlr.press/v235/zhao24a/zhao24a.pdf},
  year = {2024}
}

@inproceedings{zhao2024probabilistic,
  author = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Casts many LLM capability and safety techniques as sampling from an unnormalized distribution. The paper leverages Sequential Monte Carlo (SMC) with learned twist functions to focus computation on promising sequences, improving inference for tasks like red-teaming and infilling.},
  openalex = {W4396244053},
  title = {Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo},
  url = {https://proceedings.mlr.press/v235/zhao24b/zhao24b.pdf},
  year = {2024}
}

@inproceedings{bruce2024genie,
  author = {Bruce, Jake and Dennis, Michael and Edwards, Ashley and Parker-Holder, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris and Aytar, Yusuf and Bechtle, Sarah and Behbahani, Feryal and Chan, Stephanie and Heess, Nicolas and Gonzalez, Lucy and Osindero, Simon and Ozair, Sherjil and Reed, Scott and Zhang, Jingwei and Zolna, Konrad and Clune, Jeff and de Freitas, Nando and Singh, Satinder and Rocktäschel, Tim},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Introduces Genie, the first generative interactive environment trained unsupervised from unlabeled internet videos. This 11B-parameter foundation world model can generate an endless variety of action-controllable virtual worlds from text or image prompts.},
  openalex = {W4392182423},
  title = {Genie: Generative Interactive Environments},
  url = {https://proceedings.mlr.press/v235/bruce24a/bruce24a.pdf},
  year = {2024}
}

@inproceedings{carlini2024stealing,
  author = {Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy and Steinke, Thomas and Hayase, Jonathan and Cooper, A. Feder and Lee, Katherine and Jagielski, Matthew and Nasr, Milad and Conmy, Arthur and Wallace, Eric and Rolnick, David and Tramer, Florian},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Introduces the first model-stealing attack that extracts precise, non-trivial information (the embedding projection layer) from black-box production language models like ChatGPT and PaLM-2, given only typical API access.},
  openalex = {W4392736117},
  title = {Stealing part of a production language model},
  url = {https://proceedings.mlr.press/v235/carlini24a/carlini24a.pdf},
  year = {2024}
}

@inproceedings{chen2024flow,
  author = {Chen, Ricky T. Q. and Lipman, Yaron},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Honorable Mention. Generalizes the concept of flow matching for generative modeling to arbitrary geometries and manifolds. This provides a more principled and flexible way to construct continuous normalizing flows on non-Euclidean data.},
  openalex = {W4319653963},
  title = {Flow Matching on General Geometries},
  url = {https://openreview.net/pdf?id=x2a2aT8z52},
  year = {2024}
}

@inproceedings{darcet2024vision,
  author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Outstanding Paper. Identifies artifacts in Vision Transformer feature maps, where high-norm tokens appear in low-information background areas. Proposes a simple solution of adding extra "register" tokens to the input sequence, which absorb this unwanted information and improve model performance across various tasks.},
  openalex = {W4387225729},
  title = {Vision Transformers Need Registers},
  url = {https://openreview.net/pdf?id=2dnO3LLiJ1},
  year = {2024}
}

@inproceedings{esser2024scaling,
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Müller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Presents a large-scale study of Rectified Flow, a generative model formulation, demonstrating its superior performance for high-resolution text-to-image synthesis. Also introduces a novel Transformer architecture that improves text comprehension and image quality.},
  openalex = {W4392538976},
  title = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  url = {https://proceedings.mlr.press/v235/esser24a/esser24a.pdf},
  year = {2024}
}

@inproceedings{frey2024protein,
  author = {Frey, Nathan C. and Berenberg, Dan and Zadorozhny, Karina and Kleinhenz, Joseph and Lafrance-Vanasse, Julien and Hotzel, Isidro and Wu, Yan and Ra, Stephen and Bonneau, Richard and Cho, Kyunghyun and Loukas, Andreas and Gligorijevic, Vladimir and Saremi, Saeed},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Outstanding Paper. Addresses sequence-based antibody design with an innovative generative model tailored for discrete protein sequence data. The work includes extensive wet lab experiments that validate the effectiveness of the generative method by measuring antibody binding affinity.},
  openalex = {W4381714643},
  title = {Protein Discovery with Discrete Walk-Jump Sampling},
  url = {https://openreview.net/pdf?id=zMPHKOmQNb},
  year = {2024}
}

@inproceedings{hu2024amortizing,
  author = {Hu, Edward J and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Honorable Mention. Proposes a method to amortize the cost of intractable inference in LLMs by training a smaller, auxiliary model to approximate the posterior distribution. This allows for more efficient sampling and inference in complex, structured scenarios.},
  openalex = {W4392866283},
  title = {Amortizing intractable inference in large language models},
  url = {https://openreview.net/pdf?id=fSivCvT2f1},
  year = {2024}
}

@inproceedings{kadkhodaie2024generalization,
  author = {Kadkhodaie, Zahra and Guth, Florentin and Simoncelli, Eero P and Mallat, Stéphane},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Outstanding Paper. Provides an in-depth analysis of generalization and memorization in image diffusion models. The work explains the transition from memorization to generalization by linking architectural inductive biases to harmonic analysis via "geometry-adaptive harmonic representations."},
  openalex = {W4387389908},
  title = {Generalization in diffusion models arises from geometry-adaptive harmonic representations},
  url = {https://openreview.net/pdf?id=ANvmVS2Yr0},
  year = {2024}
}

@inproceedings{karras2024guiding,
  author = {Karras, Tero and Aittala, Miika and Kynkäänniemi, Tuomas and Lehtinen, Jaakko and Aila, Timo and Laine, Samuli},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2024 Best Paper Runner-up. Introduces a novel guidance technique for diffusion models that uses a smaller, less-trained version of the model itself for guidance, rather than a separate unconditional model. This approach provides disentangled control over image quality and diversity, setting new state-of-the-art FID scores on ImageNet.},
  openalex = {W4399424703},
  title = {Guiding a Diffusion Model with a Bad Version of Itself},
  url = {https://papers.nips.cc/paper_files/paper/2024/file/831c1c73a83084364132b00424fb8d4b-Paper-Conference.pdf},
  year = {2024}
}

@inproceedings{khan2024debating,
  author = {Khan, Akbir and Hughes, John and Valentine, Dan and Ruis, Laura and Sachan, Kshitij and Radhakrishnan, Ansh and Grefenstette, Edward and Bowman, Samuel R. and Rocktäschel, Tim and Perez, Ethan},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Investigates whether weaker models can assess the correctness of stronger models through a debate framework. The study shows that debate consistently helps both non-expert models and humans identify correct answers, and optimizing debaters for persuasiveness further improves truthfulness.},
  openalex = {W4391800466},
  title = {Debating with More Persuasive LLMs Leads to More Truthful Answers},
  url = {https://proceedings.mlr.press/v235/khan24a/khan24a.pdf},
  year = {2024}
}

@inproceedings{kirk2024the,
  author = {Kirk, Hannah Rose and Whitefield, Alexander and Röttger, Paul and Bean, Andrew and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2024 Datasets & Benchmarks Best Paper. Introduces PRISM, a dataset mapping the demographics and preferences of 1,500 diverse participants from 75 countries to their feedback in conversations with 21 LLMs. The dataset enables research into subjective and multicultural alignment, highlighting the need for careful consideration of who provides human feedback.},
  openalex = {W4395484108},
  title = {The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/be2e1b68b44f2419e19f6c35a1b8cf35-Paper-Datasets_and_Benchmarks_Track.pdf},
  year = {2024}
}

@inproceedings{kondratyuk2024videopoet,
  author = {Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jose and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh N and Yan, Jimmy and Chiu, Ming-Chang and Somandepalli, Krishna and Akbari, Hassan and Alon, Yair and Cheng, Yong and Dillon, Joshua V and Gupta, Agrim and Hahn, Meera and Hauth, Anja and Hendon, David and Martinez, Alonso and Minnen, David and Sirotenko, Mikhail and Sohn, Kihyuk and Yang, Xuan and Adam, Hartwig and Yang, Ming-Hsuan and Essa, Irfan and Wang, Huisheng and Ross, David and Seybold, Bryan and Jiang, Lu},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Presents VideoPoet, a large language model capable of synthesizing high-quality video from a wide variety of conditioning signals, including text, images, and audio. The model uses a decoder-only transformer architecture and demonstrates state-of-the-art capabilities in zero-shot video generation.},
  openalex = {W4390137271},
  title = {VideoPoet: A Large Language Model for Zero-Shot Video Generation},
  url = {https://proceedings.mlr.press/v235/kondratyuk24a/kondratyuk24a.pdf},
  year = {2024}
}

@inproceedings{lin2024not,
  author = {Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and shen, yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2024 Best Paper Runner-up. Introduces Selective Language Modeling (SLM) which selectively trains on useful tokens rather than applying uniform loss to all tokens. The Rho-1 model achieves up to 30% improvement on math tasks and matches DeepSeekMath with only 3% of pretraining tokens.},
  openalex = {W4394782324},
  title = {Not All Tokens Are What You Need for Pretraining},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3322a9a72a1707de14badd5e552ff466-Paper-Conference.pdf},
  year = {2024}
}

@inproceedings{lou2024discrete,
  author = {Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Extends diffusion models to discrete data like natural language by proposing score entropy, a novel loss that generalizes score matching. The resulting Score Entropy Discrete Diffusion (SEDD) models outperform existing language diffusion paradigms and are competitive with autoregressive models.},
  openalex = {W4387964173},
  title = {Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  url = {https://proceedings.mlr.press/v235/lou24a/lou24a.pdf},
  year = {2024}
}

@inproceedings{amos2024never,
  author = {Amos, Ido and Berant, Jonathan and Gupta, Ankit},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2024 Outstanding Paper. Demonstrates that training Transformer models from scratch underestimates their performance on long-sequence tasks. Shows that a pre-training and fine-tuning setup leads to dramatic gains, questioning the conclusions of many prior comparisons between Transformers and newer architectures.},
  openalex = {W4387390330},
  title = {Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors},
  url = {https://openreview.net/pdf?id=PdaPky8MUn},
  year = {2024}
}

@inproceedings{attias2024information,
  author = {Attias, Idan and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Livni, Roi and Roy, Daniel M.},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Provides a precise characterization of the trade-off between a learning algorithm's accuracy and its information leakage (memorization) in stochastic convex optimization. The work establishes lower bounds on the conditional mutual information for any learner with a given excess error.},
  openalex = {W4391871165},
  pages = {2035--2068},
  title = {Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing},
  url = {https://proceedings.mlr.press/v235/attias24a/attias24a.pdf},
  year = {2024}
}

@inproceedings{shi2024stochastic,
  author = {Shi, Zekun and Hu, Zheyuan and Lin, Min and Kawaguchi, Kenji},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2024 Best Paper. Introduces a method to efficiently perform arbitrary contractions of high-order derivative tensors. The approach constructs input tangents to univariate high-order automatic differentiation, enabling the efficient randomization of any differential operator for use in stochastic optimization and other applications.},
  openalex = {W4405032514},
  title = {Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators},
  url = {https://papers.nips.cc/paper_files/paper/2024/file/b575135c322b724febf1a141398ca948-Paper-Conference.pdf},
  year = {2024}
}

@inproceedings{tian2024visual,
  author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2024 Best Paper. Proposes Visual Autoregressive (VAR) modeling, a new paradigm for image generation that predicts the next image scale in a coarse-to-fine manner. VAR significantly improves upon standard autoregressive models in efficiency and quality, achieving results competitive with diffusion models.},
  openalex = {W4393969561},
  title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  url = {https://papers.nips.cc/paper_files/paper/2024/file/8211242e88d3e3a09289254c73c34a87-Paper-Conference.pdf},
  year = {2024}
}

@inproceedings{tramer2024position,
  author = {Tramer, Florian and Kamath, Gautam and Carlini, Nicholas},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2024 Best Paper. Critically reviews the practice of using non-private models pretrained on large public datasets to boost differentially private learning performance. The paper questions whether this approach should be considered privacy-preserving and scrutinizes the appropriateness of existing benchmarks.},
  openalex = {W4311555033},
  pages = {48453--48467},
  title = {Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining},
  url = {https://proceedings.mlr.press/v235/tramer24a/tramer24a.pdf},
  year = {2024}
}

@inproceedings{wang2023decodingtrust,
  author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang T. and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Datasets and Benchmarks Paper. Presents a comprehensive evaluation framework and benchmark for assessing the trustworthiness of GPT-style models. The work evaluates models across eight perspectives, including toxicity, bias, robustness, and privacy, revealing nuanced vulnerabilities and performance trade-offs.},
  openalex = {W4381586841},
  title = {DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/32cbf649e3b58691b69320872322a6b2-Paper-Datasets_and_Benchmarks.pdf},
  year = {2023}
}

@inproceedings{whittington2023disentanglement,
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Honorable Mention. Mathematically proves that biological constraints on neuronal activity, such as non-negativity and energy efficiency, promote disentangled representations where neurons become selective for single factors of variation. Provides a theory for why the brain partitions cells into functional types.},
  openalex = {W4302307803},
  title = {Disentanglement with Biological Constraints: A Theory of Functional Cell Types},
  url = {https://openreview.net/pdf?id=u9g9b0a_O_},
  year = {2023}
}

@inproceedings{wijmans2023emergence,
  author = {Wijmans, Erik and Savva, Manolis and Essa, Irfan and Lee, Stefan and Morcos, Ari and Batra, Dhruv},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Outstanding Paper. Investigates whether AI navigation agents build implicit spatial representations ("mental maps"). The study finds that "blind" agents trained via reinforcement learning develop effective navigation skills by utilizing memory over long horizons, suggesting mapping is a fundamental mechanism for intelligent navigation.},
  openalex = {W4387476016},
  title = {Emergence of Maps in the Memories of Blind Navigation Agents},
  url = {https://openreview.net/pdf?id=TUb2G85222},
  year = {2023}
}

@inproceedings{yu2023climsim,
  author = {Yu, Sungduk and Hannah, Walter M. and Peng, Liran and Lin, Jerry and Bhouri, Mohamed Aziz and Gupta, Ritwik and Lütjens, Björn and Will, Justus C. and Behrens, Gunnar and Busecke, Julius J. M. and Loose, Nora and Stern, Charles I. and Beucler, Tom and Harrop, Bryce and Heuer, Helge and Hillman, Benjamin R. and Jenney, Andrea and Liu, Nana and White, Alistair and Zheng, Tian and Kuang, Zhiming and others},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Datasets and Benchmarks Paper. Introduces ClimSim, the largest and most comprehensive dataset designed for training hybrid machine learning-physics climate simulators. The dataset contains multi-scale climate simulations aimed at improving the accuracy and resolution of climate projections by emulating compute-intensive processes.},
  openalex = {W4380993852},
  title = {ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8f291d972997f7e3415ff13b33379169-Paper-Datasets_and_Benchmarks.pdf},
  year = {2023}
}

@inproceedings{zhang2023rethinking,
  author = {Zhang, Bohang and Luo, Shengjie and Wang, Liwei and He, Di},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Outstanding Paper. Introduces a new class of expressivity metrics based on graph biconnectivity to study the power of GNNs beyond the standard Weisfeiler-Lehman test. The authors find most prior GNNs fail these metrics and propose a more expressive and efficient Transformer-like architecture.},
  openalex = {W4317951161},
  title = {Rethinking the Expressive Power of GNNs via Graph Biconnectivity},
  url = {https://openreview.net/pdf?id=PAbz4p4d2D},
  year = {2023}
}

@inproceedings{abbe2023generalization,
  author = {Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2023 Outstanding Paper. Investigates how different network architectures generalize on unseen logical (Boolean) functions. The work provides theoretical and experimental evidence that certain models learn a min-degree-interpolator, explaining the length generalization problem and motivating a new curriculum learning algorithm.},
  openalex = {W4318719501},
  title = {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  url = {https://proceedings.mlr.press/v202/abbe23a/abbe23a.pdf},
  year = {2023}
}

@inproceedings{defazio2023learningrate,
  author = {Defazio, Aaron and Mishchenko, Konstantin},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2023 Outstanding Paper. Describes a single-loop gradient-based method that does not require knowledge of the learning rate and asymptotically achieves the optimal rate of convergence for convex Lipschitz functions. The method automatically matches hand-tuned learning rates across a wide range of ML problems.},
  openalex = {W4317668739},
  pages = {7449--7479},
  title = {Learning-Rate-Free Learning by D-Adaptation},
  url = {https://proceedings.mlr.press/v202/defazio23a/defazio23a.pdf},
  year = {2023}
}

@inproceedings{fiegel2023adapting,
  author = {Fiegel, Côme and Menard, Pierre and Kozuno, Tadashi and Munos, Remi and Perchet, Vianney and Valko, Michal},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2023 Outstanding Paper. Establishes a problem-independent lower bound for learning optimal strategies in zero-sum imperfect information games. Proposes two Follow the Regularized Leader (FTRL) algorithms, one of which matches the lower bound with prior knowledge and another that adapts without it.},
  openalex = {W4316590263},
  pages = {10093--10135},
  title = {Adapting to game trees in zero-sum imperfect information games},
  url = {https://proceedings.mlr.press/v202/fiegel23a/fiegel23a.pdf},
  year = {2023}
}

@inproceedings{garrido2023duality,
  author = {Garrido, Quentin and Chen, Yubei and Bardes, Adrien and Najman, Laurent and LeCun, Yann},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Honorable Mention. Explores the theoretical connections between contrastive (e.g., SimCLR) and non-contrastive (e.g., VICReg) self-supervised learning methods. The paper shows how these families can be closely related and challenges common assumptions, such as the need for large output dimensions in non-contrastive methods.},
  openalex = {W4281621864},
  title = {On the duality between contrastive and non-contrastive self-supervised learning},
  url = {https://openreview.net/pdf?id=kXu0g66JIH},
  year = {2023}
}

@inproceedings{kim2023universal,
  author = {Kim, Donggyun and Kim, Jinwoo and Cho, Seongwoong and Luo, Chong and Hong, Seunghoon},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Outstanding Paper. Presents Visual Token Matching (VTM), a universal few-shot learning framework for dense prediction tasks like segmentation and depth estimation. VTM uses non-parametric matching on patch-level tokens, allowing it to robustly learn new tasks with very few labeled examples.},
  openalex = {W4361193754},
  title = {Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching},
  url = {https://openreview.net/pdf?id=Qk2g86n4oz},
  year = {2023}
}

@inproceedings{kirchenbauer2023watermark,
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2023 Outstanding Paper. Proposes a watermarking framework to embed algorithmically detectable signals into text generated by large language models. The watermark is imperceptible to humans and can be detected without access to the model API or parameters, helping to mitigate potential harms.},
  openalex = {W4318149317},
  title = {A Watermark for Large Language Models},
  url = {https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf},
  year = {2023}
}

@inproceedings{kong2023conditional,
  author = {Kong, Xiangzhe and Huang, Wenbing and Liu, Yang},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Honorable Mention. Frames antibody design as a 3D equivariant graph translation problem. Proposes a Multi-channel Equivariant Attention Network (MEAN) to co-design the 1D sequences and 3D structures of antibody CDRs, significantly outperforming prior models.},
  openalex = {W4291962725},
  title = {Conditional Antibody Design as 3D Equivariant Graph Translation},
  url = {https://openreview.net/pdf?id=jK2E3t2w7E},
  year = {2023}
}

@inproceedings{allenzhu2023towards,
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Honorable Mention. Provides a formal theory explaining how ensembles of deep learning models improve test accuracy and how this superior performance can be transferred to a single model via knowledge distillation. The theory relies on a "multi-view" data structure assumption.},
  openalex = {W3113303810},
  title = {Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
  url = {https://openreview.net/pdf?id=T9Npj1Ygxj},
  year = {2023}
}

@inproceedings{muennighoff2023scaling,
  author = {Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Scao, Teven Le and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Paper Runner-up. Investigates the training of language models in data-constrained regimes. The paper proposes and validates a scaling law for compute optimality that accounts for the decreasing value of repeated tokens, finding that training on repeated data for up to 4 epochs is effective.},
  openalex = {W4378505278},
  title = {Scaling Data-Constrained Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a8a082d36307e53f03767cb886f3760a-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{poole2023dreamfusion,
  author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Outstanding Paper. Proposes a method for generating 3D models from text descriptions by using a pretrained 2D text-to-image diffusion model as a prior. The technique optimizes a 3D model (NeRF) via gradient descent so its 2D renderings achieve a low loss, requiring no 3D training data.},
  openalex = {W4298187450},
  title = {DreamFusion: Text-to-3D using 2D Diffusion},
  url = {https://openreview.net/pdf?id=Fp7I--2a5_},
  year = {2023}
}

@inproceedings{rafailov2023direct,
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Paper Runner-up. Presents Direct Preference Optimization (DPO), a stable and lightweight alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning language models. DPO reframes the constrained reward maximization problem as a simple classification loss on human preference data, eliminating the need to fit a separate reward model.},
  openalex = {W4378771755},
  pages = {28481--28506},
  title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/2dcf359b13d7164243a37825b4845a76-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{schaeffer2023are,
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Paper. Challenges the concept of "emergent abilities" in large language models, where capabilities appear unpredictably at certain scales. The paper argues that these abilities may be a mirage created by the choice of metrics, and that with different metrics, performance scales predictably.},
  openalex = {W4367628401},
  pages = {1016--1030},
  title = {Are Emergent Abilities of Large Language Models a Mirage?},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/211464b35b3787df0a7a43477172c833-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{steinke2023privacy,
  author = {Steinke, Thomas and Nasr, Milad and Jagielski, Matthew},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2023 Outstanding Paper. Proposes a novel and efficient scheme for auditing the differential privacy guarantees of a machine learning system. The method exploits the parallelism of being able to add or remove multiple training examples independently, allowing for a privacy audit with only a single training run.},
  openalex = {W4376654471},
  pages = {1066--1080},
  title = {Privacy Auditing with One (1) Training Run},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/01b7573c426c4e11527a20627ba3fe61-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{bakhtin2023mastering,
  author = {Bakhtin, Anton and Wu, David J and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H and Brown, Noam},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2023 Honorable Mention. Develops an AI agent, Diplodocus, that achieves high performance in the complex human-cooperative game of No-Press Diplomacy. The agent uses a planning algorithm that regularizes a reward-maximizing policy toward a human imitation-learned policy, achieving a higher average score than most human participants in a tournament.},
  openalex = {W4305033123},
  title = {Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning},
  url = {https://openreview.net/pdf?id=z862t02J95},
  year = {2023}
}

@inproceedings{wang2022pico,
  author = {Wang, Haobo and Xiao, Ruixuan and Li, Yixuan and Feng, Lei and Niu, Gang and Chen, Gang and Zhao, Junbo},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper Honorable Mention. Addresses partial label learning (PLL) by combining contrastive learning for representation quality with a novel prototype-based algorithm for label disambiguation. The framework, PiCO, significantly outperforms prior PLL methods and achieves results comparable to full supervision.},
  openalex = {W4221158619},
  title = {PiCO: Contrastive Label Disambiguation for Partial Label Learning},
  url = {https://openreview.net/pdf?id=EhYjZy6e1gJ},
  year = {2022}
}

@inproceedings{wang2022solving,
  author = {Wang, Jiali and Huang, Wen and Jiang, Rujun and Li, Xudong and Wang, Alex},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Reformulates the Stackelberg Prediction Game with Least Squares loss (SPG-LS) as a spherically constrained least squares (SCLS) problem. This novel reformulation leads to factorization-free algorithms that are orders of magnitude faster for large-scale problems.},
  openalex = {W4281644644},
  pages = {22665--22679},
  title = {Solving Stackelberg Prediction Game with Least Squares Loss via Spherically Constrained Least Squares Reformulation},
  url = {https://proceedings.mlr.press/v162/wang22g/wang22g.pdf},
  year = {2022}
}

@inproceedings{bao2022analyticdpm,
  author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Presents the theoretical discovery that the optimal reverse variance in a Diffusion Probabilistic Model (DPM) has an analytic form dependent on the score function. Proposes a training-free inference framework to estimate this variance, improving log-likelihood and enabling 20-80x speedups.},
  openalex = {W4221139906},
  title = {Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},
  url = {https://openreview.net/pdf?id=0xiJLKH-ufZ},
  year = {2022}
}

@inproceedings{yan2022active,
  author = {Yan, Tom and Zhang, Chicheng},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper Runner-up. Initiates the study of query-based auditing algorithms that can efficiently estimate the demographic parity of ML models. Proposes an optimal deterministic algorithm and a practical randomized algorithm with comparable guarantees to put AI governance on firmer theoretical foundations.},
  openalex = {W4283214557},
  pages = {24929--24962},
  title = {Active fairness auditing},
  url = {https://proceedings.mlr.press/v162/yan22a/yan22a.pdf},
  year = {2022}
}

@inproceedings{zhao2022comparing,
  author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Introduces H-divergence, a new class of discrepancies for comparing probability distributions based on the optimal loss for a decision task. This framework generalizes Jensen-Shannon divergence and MMD, achieving superior test power in two-sample tests.},
  openalex = {W4213399638},
  title = {Comparing Distributions by Measuring Differences that Affect Decision Making},
  url = {https://openreview.net/pdf?id=KB5onONJIAU},
  year = {2022}
}

@inproceedings{chandra2022gradient,
  author = {Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Presents a method to automatically compute "hypergradients" with a simple modification to backpropagation. This allows gradient-based optimizers like SGD and Adam to tune their own hyperparameters (e.g., learning rate) during training, reducing manual effort.},
  openalex = {W2975301014},
  title = {Gradient Descent: The Ultimate Optimizer},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/08c5953d186f3432746e35d172346766-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{chen2022learning,
  author = {Chen, Yanxi and Poor, H. Vincent},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Develops a two-stage meta-algorithm to learn a mixture of multiple linear dynamical systems (LDSs) from unlabeled, short sample trajectories. The method provides end-to-end performance guarantees for recovering the ground-truth LDS models.},
  openalex = {W4226474978},
  pages = {3507--3557},
  title = {Learning Mixtures of Linear Dynamical Systems},
  url = {https://proceedings.mlr.press/v162/chen22t/chen22t.pdf},
  year = {2022}
}

@inproceedings{cheng2022adversarially,
  author = {Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper Runner-up. Proposes Adversarially Trained Actor Critic (ATAC), a model-free offline RL algorithm based on relative pessimism. ATAC frames offline RL as a Stackelberg game, leading to a policy that provably outperforms the behavior policy over a wide range of hyperparameters.},
  openalex = {W4225623389},
  pages = {3852--3878},
  title = {Adversarially Trained Actor Critic for Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v162/cheng22b/cheng22b.pdf},
  year = {2022}
}

@inproceedings{dao2022monarch,
  author = {Dao, Tri and Chen, Beidi and Sohoni, Nimit S. and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and Re, Christopher},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper Runner-up. Proposes Monarch matrices, a class of hardware-efficient and expressive structured matrices parameterized as products of two block-diagonal matrices. This structure enables faster training and fine-tuning for models like ViT and GPT-2 with comparable quality.},
  openalex = {W4226426852},
  pages = {4690--4721},
  title = {Monarch: Expressive Structured Matrices for Efficient and Accurate Training},
  url = {https://proceedings.mlr.press/v162/dao22a/dao22a.pdf},
  year = {2022}
}

@inproceedings{deitke2022procthor,
  author = {Deitke, Matt and VanderBilt, Eli and Herrasti, Alvaro and Weihs, Luca and Salvador, Jordi and Ehsani, Kiana and Han, Winson and Kolve, Eric and Farhadi, Ali and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Introduces ProcTHOR, a framework for procedurally generating thousands of diverse, interactive 3D environments. This enables large-scale training of embodied AI agents that can generalize better to unseen environments, achieving state-of-the-art results on a range of embodied AI tasks.},
  openalex = {W4282981845},
  title = {ProcTHOR: Large-Scale Embodied AI Using Procedural Generation},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a7143c4be8c8a77a4a39559c29ac583a-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{dong2022privacy,
  author = {Dong, Tian and Zhao, Bo and Lyu, Lingjuan},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Identifies that dataset condensation (DC), originally designed for training efficiency, also provides privacy benefits. The work connects DC to differential privacy, theoretically and empirically demonstrating that it limits information leakage about individual training samples.},
  openalex = {W4320167258},
  pages = {5378--5396},
  title = {Privacy for Free: How does Dataset Condensation Help Privacy?},
  url = {https://proceedings.mlr.press/v162/dong22c/dong22c.pdf},
  year = {2022}
}

@inproceedings{ethayarajh2022understanding,
  author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Frames dataset difficulty with respect to a model V as the lack of V-usable information. This information-theoretic framework allows for comparing the difficulty of different datasets and instances for a given model and interpreting input attributes.},
  openalex = {W4286902671},
  pages = {5988--6008},
  title = {Understanding Dataset Difficulty with V-Usable Information},
  url = {https://proceedings.mlr.press/v162/ethayarajh22a/ethayarajh22a.pdf},
  year = {2022}
}

@inproceedings{fan2022minedojo,
  author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Datasets and Benchmarks Paper. Introduces MineDojo, a new benchmark suite built on Minecraft that features thousands of open-ended tasks specified in natural language. It is accompanied by a massive database of YouTube videos and Wiki pages, enabling the development of generalist agents that learn from internet-scale knowledge.},
  openalex = {W4283218993},
  title = {MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/317201c977925e648496459f3eb52192-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{flennerhag2022bootstrapped,
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and van Hasselt, Hado and Silver, David and Singh, Satinder},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Proposes a meta-learning algorithm where the meta-learner teaches itself by bootstrapping a target from its own parameters. This approach extends the effective meta-learning horizon without backpropagating through all updates, achieving state-of-the-art results on Atari and multi-task meta-learning.},
  openalex = {W4286981945},
  title = {Bootstrapped Meta-Learning},
  url = {https://openreview.net/pdf?id=b-ny3x071E5},
  year = {2022}
}

@inproceedings{geerts2022expressiveness,
  author = {Geerts, Floris and Reutter, Juan L},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Provides a general framework based on tensor language to analyze the separation power and approximation properties of GNN architectures. This approach connects GNN expressiveness to the Weisfeiler-Leman (WL) tests, offering a unified toolbox for GNN designers.},
  openalex = {W4223655873},
  title = {Expressiveness and Approximation Properties of Graph Neural Networks},
  url = {https://fgeerts.github.io/pdf/GeertsR22.pdf},
  year = {2022}
}

@inproceedings{gu2022efficiently,
  author = {Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper Honorable Mention. Introduces the Structured State Space sequence model (S4), which uses a novel parameterization to make fundamental state space models (SSMs) computationally efficient. S4 excels at modeling long-range dependencies and achieved state-of-the-art on the Long Range Arena benchmark.},
  openalex = {W3209374680},
  title = {Efficiently Modeling Long Sequences with Structured State Spaces},
  url = {https://openreview.net/pdf?id=uYLFoz1vlAC},
  year = {2022}
}

@inproceedings{han2022gmixup,
  author = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Proposes a method to apply Mixup data augmentation to graph data. Instead of interpolating graphs directly, G-Mixup interpolates their underlying generators (graphons) in Euclidean space and then samples new graphs, improving GNN generalization and robustness.},
  openalex = {W4221150126},
  title = {G-Mixup: Graph Data Augmentation for Graph Classification},
  url = {https://proceedings.mlr.press/v162/han22c/han22c.pdf},
  year = {2022}
}

@inproceedings{han2022neural,
  author = {Han, X.Y. and Papyan, Vardan and Donoho, David L.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Provides a theoretical investigation of the Neural Collapse phenomenon using the analytically tractable Mean Squared Error (MSE) loss. Introduces the "central path" construct to derive closed-form dynamics that predict the emergence of Neural Collapse.},
  openalex = {W3169280296},
  title = {Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path},
  url = {https://openreview.net/pdf?id=w1UbdvWH_R3},
  year = {2022}
}

@inproceedings{hoffmann2022training,
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Presents a detailed study on the optimal scaling of large language models. By training over 400 models, the paper finds that for a given compute budget, model size and the number of training tokens should be scaled equally, leading to the 70B parameter Chinchilla model which outperforms much larger models.},
  openalex = {W4225591000},
  title = {Training Compute-Optimal Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{hsu2022learning,
  author = {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper Runner-up. Augments training data for protein inverse folding by using 12 million structures predicted by AlphaFold2. A sequence-to-sequence transformer trained on this data achieves a nearly 10 percentage point improvement in native sequence recovery over existing methods.},
  openalex = {W4223581484},
  title = {Learning inverse folding from millions of predicted structures},
  url = {https://proceedings.mlr.press/v162/hsu22a/hsu22a.pdf},
  year = {2022}
}

@inproceedings{karras2022elucidating,
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Provides a unified framework for understanding and designing diffusion-based generative models. The authors analyze design choices such as preconditioning, noise scheduling, and sampling, leading to new state-of-the-art models that generate high-fidelity images in as few as 5 sampling steps.},
  openalex = {W4281969232},
  title = {Elucidating the Design Space of Diffusion-Based Generative Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5a66b382f5467946e55395a04a1aaf5b-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{akbari2022minimum,
  author = {Akbari, Sina and Etesami, Jalal and Kiyavash, Negar},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper Runner-up. Considers the problem of designing a minimum-cost set of interventions to identify a causal effect when it is not identifiable from observational data alone. The paper proves the problem is NP-complete and proposes algorithms with approximation guarantees.},
  openalex = {W4229075193},
  pages = {258--289},
  title = {Minimum Cost Intervention Design for Causal Effect Identification},
  url = {https://proceedings.mlr.press/v162/akbari22a/akbari22a.pdf},
  year = {2022}
}

@inproceedings{lotfi2022bayesian,
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Investigates the limitations of using the marginal likelihood (Bayesian evidence) as a proxy for generalization. The paper shows that it can be negatively correlated with generalization and lead to both underfitting and overfitting, proposing a conditional marginal likelihood as a partial remedy.},
  openalex = {W4221142090},
  title = {Bayesian Model Selection, the Marginal Likelihood, and Generalization},
  url = {https://proceedings.mlr.press/v162/lotfi22a/lotfi22a.pdf},
  year = {2022}
}

@inproceedings{mutti2022the,
  author = {Mutti, Mirco and De Santi, Riccardo and Restelli, Marcello},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Argues that non-Markovian policies are crucial for maximum state entropy exploration in a finite-sample regime. The paper recasts the objective to target the entropy of state visitations within a single trial, showing that Markovian policies suffer non-zero regret.},
  openalex = {W4221164780},
  title = {The Importance of Non-Markovianity in Maximum State Entropy Exploration},
  url = {https://proceedings.mlr.press/v162/mutti22a/mutti22a.pdf},
  year = {2022}
}

@inproceedings{ndiaye2022stable,
  author = {Ndiaye, Eugene},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Combines Conformal Prediction (CP) techniques with algorithmic stability bounds to derive a prediction set that is computable with a single model fit. This approach avoids the need for data splitting while maintaining guaranteed coverage.},
  openalex = {W4226028869},
  pages = {16462--16479},
  title = {Stable Conformal Prediction Sets},
  url = {https://proceedings.mlr.press/v162/ndiaye22a/ndiaye22a.pdf},
  year = {2022}
}

@inproceedings{nilforoshan2022causal,
  author = {Nilforoshan, Hamed and Gaebler, Johann and Shroff, Ravi and Goel, Sharad},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Categorizes and analyzes popular causal definitions of algorithmic fairness, showing that they almost always result in strongly Pareto dominated decision policies. The work highlights formal limitations and potential adverse consequences of common mathematical notions of causal fairness.},
  openalex = {W4285428798},
  pages = {16848--16887},
  title = {Causal Conceptions of Fairness and their Consequences},
  url = {https://proceedings.mlr.press/v162/nilforoshan22a/nilforoshan22a.pdf},
  year = {2022}
}

@inproceedings{papernot2022hyperparameter,
  author = {Papernot, Nicolas and Steinke, Thomas},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Analyzes the privacy leakage that occurs during hyperparameter tuning in differentially private learning. The work provides sharp privacy guarantees for hyperparameter search procedures using the framework of Rényi Differential Privacy, showing that leakage is modest under specific conditions.},
  openalex = {W4286908675},
  title = {Hyperparameter Tuning with Renyi Differential Privacy},
  url = {https://openreview.net/pdf?id=-70L8lpp9DF},
  year = {2022}
}

@inproceedings{riad2022learning,
  author = {Riad, Rachid and Teboul, Olivier and Grangier, David and Zeghidour, Neil},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper. Introduces DiffStride, the first downsampling layer with learnable strides. This method learns the size of a cropping mask in the Fourier domain, allowing stride values to be optimized via gradient descent and removing the need for manual cross-validation.},
  openalex = {W4220727198},
  title = {Learning Strides in Convolutional Neural Networks},
  url = {https://openreview.net/pdf?id=M752z9FKJP},
  year = {2022}
}

@inproceedings{saharia2022photorealistic,
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 paper. Presents Imagen, a text-to-image diffusion model that achieves an unprecedented degree of photorealism and deep language understanding. The work shows that leveraging large, pretrained language models (like T5) as text encoders is more effective than scaling the image model itself.},
  openalex = {W4281485151},
  pages = {36593--36608},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/60c353aee697ac5454b591739c391374-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{schuhmann2022laion5b,
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Datasets and Benchmarks Paper. Presents LAION-5B, a publicly available dataset of 5.85 billion CLIP-filtered image-text pairs. This dataset, 14 times larger than its predecessor, aims to democratize research on large-scale multi-modal models by providing the community with the data needed to replicate and build upon models like CLIP and DALL-E.},
  openalex = {W4306820534},
  pages = {25278--25294},
  title = {LAION-5B: An open large-scale dataset for training next generation image-text models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/021bbc7ee2a3e14359a3d45c8430a3a3-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{shi2022gradient,
  author = {Shi, Jiaxin and Zhou, Yuhao and Hwang, Jessica and Titsias, Michalis and Mackey, Lester},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Develops a general framework for defining Stein operators for discrete distributions. This enables the construction of low-variance, unbiased gradient estimators for challenging discrete settings, with applications in variational inference and generative modeling.},
  openalex = {W4226143667},
  pages = {25829--25841},
  title = {Gradient Estimation with Discrete Stein Operators},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/22b95857e56d72491b014878a8767353-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{sorscher2022beyond,
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2022 Outstanding Paper. Challenges the notion that model performance improvement is strictly governed by power-law scaling. The paper demonstrates that by intelligently pruning the training data, it is possible to achieve better-than-power-law scaling, improving both performance and data efficiency.},
  openalex = {W4283761305},
  pages = {27798--27810},
  title = {Beyond neural scaling laws: beating power law scaling via data pruning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5c5329459c629833c1b544649f80243d-Paper-Conference.pdf},
  year = {2022}
}

@inproceedings{suh2022differentiable,
  author = {Suh, Hyung Ju and Simchowitz, Max and Zhang, Kaiqing and Tedrake, Russ},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2022 Outstanding Paper. Analyzes the performance of first-order gradient estimators from differentiable simulators, showing that physical system properties like stiffness or discontinuities can compromise their efficacy. Proposes an α-order estimator that combines the efficiency of first-order methods with the robustness of zeroth-order methods.},
  openalex = {W4226505391},
  pages = {20668--20696},
  title = {Do Differentiable Simulators Give Better Policy Gradients?},
  url = {https://proceedings.mlr.press/v162/suh22b/suh22b.pdf},
  year = {2022}
}

@inproceedings{topping2022understanding,
  author = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2022 Outstanding Paper Honorable Mention. Provides a precise mathematical description of the "over-squashing" phenomenon in GNNs, where information from distant nodes is distorted. The work introduces an edge-based combinatorial curvature and proves that negatively curved edges are responsible for the issue, proposing a rewiring method to alleviate it.},
  openalex = {W4225751401},
  title = {Understanding over-squashing and bottlenecks on graphs via curvature},
  url = {https://openreview.net/pdf?id=Kmc_2V42iC},
  year = {2022}
}

@inproceedings{vicol2021unbiased,
  author = {Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2021 Outstanding Paper. Introduces Persistent Evolution Strategies (PES), a method for unbiased, low-variance gradient estimation in unrolled computation graphs. PES allows for frequent parameter updates by correcting for truncation bias, making it applicable to training learned optimizers and hyperparameter tuning.},
  openalex = {W4310001898},
  pages = {10553--10563},
  title = {Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies},
  url = {https://proceedings.mlr.press/v139/vicol21a/vicol21a.pdf},
  year = {2021}
}

@inproceedings{wang2021rethinking,
  author = {Wang, Ruochen and Cheng, Minhao and Chen, Xiangning and Tang, Xiaocheng and Hsieh, Cho-Jui},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Critiques the standard architecture selection method in Differentiable Neural Architecture Search (DARTS), demonstrating that the magnitude of architecture parameters is not a reliable indicator of an operation's strength. Proposes a perturbation-based selection method that directly measures an operation's contribution, significantly improving the discovered architectures.},
  openalex = {W3190627196},
  title = {Rethinking Architecture Selection in Differentiable NAS},
  url = {https://openreview.net/pdf?id=PKubaeJkw3},
  year = {2021}
}

@inproceedings{zhang2021beyond,
  author = {Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin and Luu, Anh Tuan and Hui, Siu and Fu, Jie},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Generalizes hypercomplex multiplications, such as the Hamilton product used in quaternion networks, to arbitrary dimensions. The proposed method learns the multiplication rules from data, enabling the creation of parameter-efficient layers with a flexible 1/n reduction in parameters.},
  openalex = {W3120074043},
  title = {Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with $1/n$ Parameters},
  url = {https://openreview.net/pdf?id=y3_j_6kRC8O},
  year = {2021}
}

@inproceedings{bubeck2021universal,
  author = {Bubeck, Sébastien and Sellke, Mark},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Proposes a theoretical model to explain why robust deep networks require significant overparameterization. The work shows that for a function to smoothly interpolate training data below the noise level, the number of parameters must scale with both the number of samples (n) and the data dimensionality (d).},
  openalex = {W4287167969},
  title = {A Universal Law of Robustness via Isoperimetry},
  url = {https://proceedings.neurips.cc/paper/2021/file/14851AAS-Paper.pdf},
  year = {2021}
}

@inproceedings{abel2021expressivity,
  author = {Abel, David and Dabney, Will and Harutyunyan, Anna and Ho, Mark and Littman, Michael and Precup, Doina and Singh, Satinder},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Provides a careful theoretical analysis of the expressive power of Markov reward functions. The paper proves that while reward can express many tasks, there exist desirable tasks (defined as sets of behaviors or orderings over trajectories) that no Markov reward function can capture.},
  openalex = {W4286882963},
  title = {On the Expressivity of Markov Reward},
  url = {https://proceedings.neurips.cc/paper/2021/file/1f36c15d6a3d1e8a355144f2b5e26f8d-Paper.pdf},
  year = {2021}
}

@inproceedings{even2021continuized,
  author = {Even, Mathieu and Berthier, Raphaël and Bach, Francis and Flammarion, Nicolas and Gaillard, Pierre and Hendrikx, Hadrien and Massoulié, Laurent and Taylor, Adrien},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Describes a "continuized" version of Nesterov's accelerated gradient method based on a continuous-time differential equation with random gradient updates. This new perspective provides a unified framework for understanding acceleration in deterministic, stochastic, and decentralized settings.},
  openalex = {W3214183576},
  title = {Continuized Accelerations of Deterministic and Stochastic Gradient Descents, and of Gossip Algorithms},
  url = {https://proceedings.neurips.cc/paper/2021/file/4036080e8293244b0373809450403333-Paper.pdf},
  year = {2021}
}

@inproceedings{gemp2021eigengame,
  author = {Gemp, Ian and McWilliams, Brian and Vernade, Claire and Graepel, Thore},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Reformulates Principal Component Analysis (PCA) as a competitive game where each player controls an approximate eigenvector and seeks to maximize a local utility function. This novel perspective leads to a decentralized, parallelizable algorithm that scales to massive datasets.},
  openalex = {W3122828699},
  title = {EigenGame: PCA as a Nash Equilibrium},
  url = {https://openreview.net/pdf?id=NzTU59SYbNq},
  year = {2021}
}

@inproceedings{agarwal2021deep,
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Argues for more rigorous statistical evaluation in deep reinforcement learning. The paper highlights that point estimates can be misleading and advocates for the use of stratified bootstrap confidence intervals, performance profiles, and interquartile means to reliably compare algorithms.},
  openalex = {W4287018575},
  title = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  url = {https://proceedings.neurips.cc/paper/2021/file/212a1d7f04dd6495b8a3ff744c8cae6b-Paper.pdf},
  year = {2021}
}

@inproceedings{grathwohl2021oops,
  author = {Grathwohl, Will and Swersky, Kevin and Hashemi, Milad and Duvenaud, David and Maddison, Chris},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2021 Honorable Mention. Proposes a general and scalable approximate sampling strategy for probabilistic models with discrete variables. The method uses gradients of the likelihood with respect to its discrete inputs to propose updates in a Metropolis-Hastings sampler, outperforming generic samplers in challenging settings like Ising models.},
  openalex = {W3169733862},
  title = {Oops I Took A Gradient: Scalable Sampling for Discrete Distributions},
  url = {https://proceedings.mlr.press/v139/grathwohl21a/grathwohl21a.pdf},
  year = {2021}
}

@inproceedings{koch2021reduced,
  author = {Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob Gates},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Datasets & Benchmarks Best Paper. Analyzes thousands of papers to study the evolution of dataset usage within different machine learning subcommunities. The work finds a trend towards using fewer datasets over time, with these datasets often originating from a handful of elite institutions, and calls for more critical dataset selection.},
  openalex = {W4308481882},
  title = {Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research},
  url = {https://proceedings.neurips.cc/paper/2021/file/a7f1d7729183584b9e84826b1296b02a-Paper.pdf},
  year = {2021}
}

@inproceedings{lu2021optimal,
  author = {Lu, Yucheng and De Sa, Christopher},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2021 Honorable Mention. Provides a tight lower bound on the iteration complexity for decentralized training methods in a stochastic non-convex setting. The paper reveals a theoretical gap in the convergence rates of many existing algorithms and proposes a new algorithm, DeTAG, that achieves the optimal rate.},
  openalex = {W3166004119},
  title = {Optimal Complexity in Decentralized Training},
  url = {https://proceedings.mlr.press/v139/lu21a/lu21a.pdf},
  year = {2021}
}

@inproceedings{nitanda2021optimal,
  author = {Nitanda, Atsushi and Suzuki, Taiji},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Analyzes the convergence of averaged Stochastic Gradient Descent (SGD) for overparameterized two-layer neural networks. The study demonstrates that under the Neural Tangent Kernel (NTK) regime, averaged SGD can achieve the minimax optimal convergence rate, bridging a theoretical gap between overparameterized networks and kernel methods.},
  openalex = {W3124880721},
  title = {Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime},
  url = {https://openreview.net/pdf?id=PULSD5qI2N1},
  year = {2021}
}

@inproceedings{pfaff2021learning,
  author = {Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Introduces MeshGraphNets, a graph neural network framework designed to learn mesh-based physical simulations. The model learns to pass messages on a mesh graph and adaptively adjusts the mesh resolution, enabling accurate and efficient prediction of complex dynamics in fields like aerodynamics and structural mechanics.},
  openalex = {W3118902573},
  title = {Learning Mesh-Based Simulation with Graph Networks},
  url = {https://openreview.net/pdf?id=roNqYL0_XP},
  year = {2021}
}

@inproceedings{pillutla2021mauve,
  author = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Introduces MAUVE, a novel divergence measure to compare the distribution of model-generated text with that of human-written text. The method uses a continuous family of soft KL divergence measures on quantized text embeddings, providing a more reliable metric for open-ended text generation.},
  openalex = {W4287332927},
  pages = {4816--4828},
  title = {MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
  url = {https://proceedings.neurips.cc/paper/2021/file/32501ba244031f54b3979559092523f3-Paper.pdf},
  year = {2021}
}

@inproceedings{arakelyan2021complex,
  author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Introduces a framework to answer complex logical queries on knowledge graphs by translating them into differentiable objectives. These objectives are then solved using a pre-trained 1-hop link predictor, achieving superior accuracy with orders of magnitude less training data than methods trained directly on complex queries.},
  openalex = {W3124660976},
  title = {Complex Query Answering with Neural Link Predictors},
  url = {https://openreview.net/pdf?id=Mos9F9kDwkz},
  year = {2021}
}

@inproceedings{richard2021neural,
  author = {Richard, Alexander and Markovic, Dejan and Gebru, Israel D. and Krenn, Steven and Butler, Gladstone Alexander and Torre, Fernando and Sheikh, Yaser},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Presents a neural rendering approach for generating real-time, spatially accurate binaural audio from a single-channel source. The method uses a neural time-warping module and a temporal convolutional network, outperforming previous methods in both quantitative and perceptual studies.},
  openalex = {W3138953166},
  title = {Neural Synthesis of Binaural Speech from Mono Audio},
  url = {https://openreview.net/pdf?id=uAX8q61EVRu},
  year = {2021}
}

@inproceedings{richter2021solving,
  author = {Richter, Lorenz and Sallandt, Leon and Nüsken, Nikolas},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2021 Honorable Mention. Develops a novel method for solving high-dimensional parabolic partial differential equations (PDEs) using the tensor train format. The approach combines backward stochastic differential equations with regression-type methods in the tensor format, achieving a favorable trade-off between accuracy and efficiency.},
  openalex = {W3168548366},
  pages = {8998--9009},
  title = {Solving high-dimensional parabolic PDEs using the tensor train format},
  url = {https://proceedings.mlr.press/v139/richter21a/richter21a.pdf},
  year = {2021}
}

@inproceedings{rozen2021moser,
  author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Outstanding Paper. Proposes a new family of continuous normalizing flow generative models over Riemannian manifolds based on a classic result from differential geometry. Moser Flow circumvents the need to solve an ODE during training, enabling faster training and superior performance on manifold-valued data.},
  openalex = {W4287023565},
  pages = {17669--17680},
  title = {Moser Flow: Divergence-based Generative Modeling on Manifolds},
  url = {https://proceedings.neurips.cc/paper/2021/file/7a6a4323223e91129937346db157353f-Paper.pdf},
  year = {2021}
}

@inproceedings{song2021scorebased,
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2021 Outstanding Paper. Unifies score-based generative modeling and diffusion probabilistic models into a single framework using stochastic differential equations (SDEs). This approach enables new sampling procedures, exact likelihood computation, and controllable generation, achieving record-breaking performance on image generation benchmarks.},
  openalex = {W3121345697},
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  url = {https://openreview.net/pdf?id=AR-8-T-H-hP},
  year = {2021}
}

@inproceedings{tian2021understanding,
  author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2021 Honorable Mention. Provides a theoretical study of non-contrastive self-supervised learning (SSL) methods like BYOL and SimSiam. The work explains how these methods avoid representational collapse without negative pairs and proposes a simple alternative, DirectPred, that performs comparably.},
  openalex = {W3169212847},
  pages = {10268--10278},
  title = {Understanding self-supervised learning dynamics without contrastive pairs},
  url = {https://proceedings.mlr.press/v139/tian21a/tian21a.pdf},
  year = {2021}
}

@inproceedings{townshend2021atom3d,
  author = {Townshend, Raphael J. L. and Vögele, Martin and Suriana, Patricia and Derry, Alexander and Powers, Alexander and Laloudakis, Yianni and Balachandar, Sidhika and Jing, Bowen and Anderson, Brandon and Eismann, Stephan and Kondor, Risi and Altman, Russ B. and Dror, Ron O.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2021 Datasets & Benchmarks Best Paper. Introduces a collection of benchmark datasets with 3D representations of small molecules and biopolymers. ATOM3D is designed to spur the development of machine learning models for a wide range of problems, from molecular structure prediction to design and engineering tasks.},
  openalex = {W3196791236},
  pages = {1--14},
  title = {ATOM3D: Tasks on Molecules in Three Dimensions},
  url = {https://proceedings.neurips.cc/paper/2021/file/b9e15a1c402759ee4937583a543534a2-Paper.pdf},
  year = {2021}
}

@inproceedings{wei2020tuning,
  author = {Wei, Kaixuan and Aviles-Rivero, Angelica I and Liang, Jingwei and Fu, Ying and Schönlieb, Carola-Bibiane and Huang, Hua},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2020 Outstanding Paper. Addresses the challenge of manual parameter tuning in Plug-and-Play (PnP) algorithms for inverse imaging problems. The paper proposes using deep reinforcement learning to create a policy network that automatically determines internal parameters like penalty strength and denoising level.},
  openalex = {W3034925446},
  pages = {10158--10169},
  title = {Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems},
  url = {https://proceedings.mlr.press/v119/wei20b/wei20b.pdf},
  year = {2020}
}

@inproceedings{wilson2020efficiently,
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2020 Honorable Mention. Proposes a fast, general-purpose method for sampling from Gaussian Process (GP) posteriors that scales linearly with the number of test points. The approach is based on a pathwise decomposition of GPs that separates the prior from the data-dependent update, combining the strengths of different sampling strategies.},
  openalex = {W3034299904},
  pages = {10292--10302},
  title = {Efficiently sampling functions from Gaussian process posteriors},
  url = {https://proceedings.mlr.press/v119/wilson20a/wilson20a.pdf},
  year = {2020}
}

@inproceedings{yin2020metalearning,
  author = {Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Investigates the problem of meta-learning algorithms memorizing task-specific information from a limited number of training tasks. Proposes an information-theoretic framework, Amortized Bayesian Meta-Learning, to prevent this memorization, leading to improved generalization to new tasks.},
  openalex = {W2994871715},
  title = {Meta-Learning without Memorization},
  url = {https://openreview.net/pdf?id=B1lJzyStvB},
  year = {2020}
}

@inproceedings{yu2020playing,
  author = {Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Spotlight Paper. Extends the lottery ticket hypothesis to reinforcement learning and multilingual translation. The work shows that sparse, winning subnetworks can be found for RL agents and multilingual Transformers, achieving performance comparable to the dense models at a fraction of the parameter cost. Note: While this is an important contribution that was accepted at ICLR 2020, it was not designated as an Outstanding Paper.},
  openalex = {W2994914025},
  title = {Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP},
  url = {https://openreview.net/pdf?id=S1ez50EtDr},
  year = {2020}
}

@inproceedings{brown2020language,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and others},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2020 Best Paper. Introduces GPT-3, a 175-billion parameter language model which demonstrates that scaling up language models greatly improves task-agnostic, few-shot performance. The model achieves strong results on many NLP datasets without any gradient updates or fine-tuning, using only text-based interaction.},
  openalex = {W4292779060},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac14c469e-Paper.pdf},
  year = {2020}
}

@inproceedings{brunner2020identifiability,
  author = {Brunner, Gino and Liu, Yang and Pascual, Damián and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Provides a theoretical analysis of the identifiability of attention weights in Transformer models. The paper proves that under certain conditions, attention weights are not identifiable, which has significant implications for the interpretability of attention mechanisms as explanations for model behavior.},
  openalex = {W2995446988},
  title = {On Identifiability in Transformers},
  url = {https://openreview.net/pdf?id=BJxABpEYwB},
  year = {2020}
}

@inproceedings{celli2020noregret,
  author = {Celli, Andrea and Marchesi, Alberto and Farina, Gabriele and Gatti, Nicola},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2020 Best Paper. Presents the first uncoupled no-regret learning dynamics that are guaranteed to converge to the set of Extensive-Form Correlated Equilibria (EFCEs) in n-player general-sum games. This resolves a long-standing open question in the theory of multi-agent systems.},
  openalex = {W3036687207},
  title = {No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium},
  url = {https://proceedings.neurips.cc/paper/2020/file/51cce8902274a5585098157616117688-Paper.pdf},
  year = {2020}
}

@inproceedings{chen2020generative,
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2020 Honorable Mention. Demonstrates that a large-scale sequence Transformer trained to auto-regressively predict pixels can learn powerful image representations without supervised labels. The model, iGPT, achieves state-of-the-art results on low-resolution classification tasks and is competitive with top self-supervised methods on ImageNet.},
  openalex = {W3034445277},
  pages = {1691--1703},
  title = {Generative Pretraining From Pixels},
  url = {https://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  year = {2020}
}

@inproceedings{dathathri2020plug,
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Presents a method for steering large, unconditional language models to generate text with specific attributes without retraining. The approach combines a pretrained language model with one or more attribute models, using gradient-based updates to guide generation towards a desired style or topic.},
  openalex = {W2997195635},
  title = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  url = {https://openreview.net/pdf?id=H1e2IeAQPS},
  year = {2020}
}

@inproceedings{derezinski2020improved,
  author = {Derezinski, Michal and Khanna, Rajiv and Mahoney, Michael},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2020 Best Paper. Provides stronger theoretical guarantees for randomized algorithms for column subset selection and the Nyström method. The analysis reveals a "multiple-descent" risk curve, offering new insights into the behavior of these fundamental matrix approximation techniques.},
  openalex = {W3103423762},
  title = {Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystrom method},
  url = {https://proceedings.neurips.cc/paper/2020/file/1b201f757098edf13de098132912643a-Paper.pdf},
  year = {2020}
}

@inproceedings{grathwohl2020your,
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, Jörn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Establishes a formal connection between standard discriminative classifiers and implicit energy-based models (EBMs). Demonstrates that a standard softmax classifier can be re-interpreted as an EBM, enabling improvements in calibration, robustness, and out-of-distribution detection through joint training.},
  openalex = {W2995085126},
  title = {Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One},
  url = {https://openreview.net/pdf?id=Hkxzx0NtDr},
  year = {2020}
}

@inproceedings{khandelwal2020generalization,
  author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Proposes k-nearest neighbor language models (kNN-LM), which enhance a pretrained language model by interpolating its predictions with those from a k-nearest neighbor search over a large datastore of text. This approach improves performance on language modeling and related downstream tasks.},
  openalex = {W2995154514},
  title = {Generalization through Memorization: Nearest Neighbor Language Models},
  url = {https://openreview.net/pdf?id=HklBqANtDB},
  year = {2020}
}

@inproceedings{kitaev2020reformer,
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Introduces two techniques to improve the efficiency of Transformer models: locality-sensitive hashing (LSH) attention to replace dot-product attention for long sequences, and reversible residual layers to reduce memory consumption. Enables Transformers to process much longer context windows than previously feasible.},
  openalex = {W4295838474},
  title = {Reformer: The Efficient Transformer},
  url = {https://openreview.net/pdf?id=rkgNKkHtvB},
  year = {2020}
}

@inproceedings{lan2020albert,
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Outstanding Paper. Introduces two parameter-reduction techniques for BERT-style models: factorized embedding parameterization and cross-layer parameter sharing. These methods significantly lower memory consumption and increase training speed while achieving state-of-the-art results on major NLP benchmarks.},
  openalex = {W2996428491},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  url = {https://openreview.net/pdf?id=H1eA7AEtvS},
  year = {2020}
}

@inproceedings{maron2020learning,
  author = {Maron, Haggai and Litany, Or and Chechik, Gal and Fetaya, Ethan},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {ICML 2020 Outstanding Paper. Presents a principled approach for learning on sets where the elements themselves have inherent symmetries (e.g., sets of images). The work characterizes the space of linear layers that are equivariant to both element reordering and these internal symmetries, leading to a universal approximator called Deep Sets for Symmetric Elements (DSS).},
  openalex = {W3008654699},
  title = {On Learning Sets of Symmetric Elements},
  url = {https://proceedings.mlr.press/v119/maron20a/maron20a.pdf},
  year = {2020}
}

@inproceedings{novak2020neural,
  author = {Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A. and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 Spotlight Paper. Introduces the Neural Tangents library, a high-level API for defining, training, and evaluating infinite-width neural networks. The library simplifies the application of Neural Tangent Kernel (NTK) theory, enabling researchers to easily explore the properties of infinitely wide networks. Note: ICLR 2020 did not award Outstanding Paper prizes.},
  openalex = {W2993805018},
  title = {Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
  url = {https://openreview.net/pdf?id=SklD9yrFwB},
  year = {2020}
}

@inproceedings{raghu2020rapid,
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2020 paper. Analyzes the behavior of Model-Agnostic Meta-Learning (MAML), finding that its effectiveness stems more from feature reuse than rapid learning of new representations. The study shows that MAML primarily fine-tunes the final layers of a network, acting as a powerful feature extractor. Note: ICLR 2020 did not award Outstanding Paper prizes.},
  openalex = {W2995049146},
  title = {Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},
  url = {https://openreview.net/pdf?id=rkg-pTEFwH},
  year = {2020}
}
