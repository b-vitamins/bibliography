@inproceedings{wei2022chainofthought,
  author    = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  year      = {2022},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  pdf       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  abstract  = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning.}
}

@inproceedings{wang2023selfconsistency,
  author    = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  title     = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  booktitle = {International Conference on Learning Representations},
  year      = {2023},
  url       = {https://openreview.net/forum?id=1PL1NIMMrw},
  pdf       = {https://openreview.net/pdf?id=1PL1NIMMrw},
  abstract  = {Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting.}
}

@article{zhang2023multimodal,
  author    = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  title     = {Multimodal Chain-of-Thought Reasoning in Language Models},
  journal   = {Transactions on Machine Learning Research},
  year      = {2024},
  url       = {https://openreview.net/forum?id=y1pPWFVfvR},
  pdf       = {https://openreview.net/pdf?id=y1pPWFVfvR},
  abstract  = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.}
}

@inproceedings{chen2024visual,
  author    = {Chen, Zhenfang and Zhou, Qinhong and Shen, Yikang and Hong, Yining and Sun, Zhiqing and Gutfreund, Dan and Gan, Chuang},
  title     = {Visual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning},
  booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
  volume    = {38},
  number    = {2},
  pages     = {1254--1262},
  year      = {2024},
  isbn      = {978-1-57735-887-9},
  publisher = {AAAI Press},
  url       = {https://doi.org/10.1609/aaai.v38i2.27888},
  doi       = {10.1609/aaai.v38i2.27888},
  pdf       = {https://ojs.aaai.org/index.php/AAAI/article/download/27888/27801},
  articleno = {140},
  numpages  = {9},
  series    = {AAAI'24/IAAI'24/EAAI'24},
  abstract  = {Knowledge-based visual reasoning remains a daunting task since it not only requires machines to interpret the concepts and relationships from visual scenes but also associate them with external world knowledge to conduct a chain of reasoning on open-world questions. Previous works, however, treat visual perception and language-based reasoning as two independent modules, failing to attend to both modules throughout all stages of reasoning. To this end, we propose Visual Chain-of-thought Prompting (VCTP) for knowledge-based reasoning, which involves the interaction between visual content and natural language in an iterative step-by-step reasoning manner. VCTP contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to key visual concepts from natural language questions adaptively. It then transforms key visual context into text context for prompting with a visual captioning model, and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale for the answer, which is then passed through a cross-modality classifier to verify that it's consistent with the visual context. We iterate through the think-confirm stages to ensure the verified rationale is consistent with the answer. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our VCTP enjoys several benefits, 1), it achieves better performance than the previous few-shot learning baselines; 2), it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3), it is computation-efficient compared with other fine-tuning baselines. Our code is available at https://github.com/UMass-Foundation-Model/VisualCoT.git}
}

@inproceedings{mondal2024kamcot,
  author    = {Mondal, Debjyoti and Modi, Suraj and Panda, Subhadarshi and Singh, Rituraj and Rao, Godawari Sudhakar},
  title     = {KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {38},
  number    = {17},
  pages     = {18798--18806},
  year      = {2024},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/29844},
  pdf       = {https://ojs.aaai.org/index.php/AAAI/article/download/29844/29845},
  abstract  = {We propose KAM-CoT, a framework integrating chain of thought reasoning, Knowledge Graphs, and multiple modalities for comprehensive understanding of multimodal tasks. The framework uses a two-stage training process with knowledge graph grounding to generate effective rationales and answers, reducing hallucinations and enhancing answer quality.}
}

@inproceedings{chen2024spatialvlm,
  author    = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  title     = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {14455--14465},
  year      = {2024},
  url       = {https://openaccess.thecvf.com/content/CVPR2024/html/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.html},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf},
  abstract  = {Vision Language Models (VLMs) have demonstrated remarkable performance in certain VQA benchmarks, but still lack capabilities in 3D spatial reasoning. We develop an automatic 3D spatial VQA data generation framework capable of creating 2 billion VQA examples from real-world images, introducing the first Internet-scale 3D spatial reasoning dataset in metric space.}
}

@inproceedings{shao2024visual,
  author    = {Shao, Hao and Qian, Shengju and Xiao, Han and Song, Guanglu and Zong, Zhuofan and Wang, Letian and Liu, Yu and Li, Hongsheng},
  title     = {Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {37},
  year      = {2024},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/0ff38d72a2e0aa6dbe42de83a17b2223-Abstract-Datasets_and_Benchmarks_Track.html},
  pdf       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0ff38d72a2e0aa6dbe42de83a17b2223-Paper-Datasets_and_Benchmarks_Track.pdf},
  abstract  = {Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs. We introduce the Visual CoT dataset with 438k question-answer pairs annotated with intermediate bounding boxes and detailed reasoning steps to address these challenges.}
}

@inproceedings{peng2024kosmos2,
  author    = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Ye, Qixiang and Wei, Furu},
  title     = {Kosmos-2: Grounding Multimodal Large Language Models to the World},
  booktitle = {International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=lLmqxkfSIw},
  pdf       = {https://openreview.net/pdf?id=lLmqxkfSIw},
  abstract  = {We introduce Kosmos-2, a Multimodal Large Language Model (MLLM) that enables new capabilities of perceiving object descriptions and grounding text to the visual world. The model represents referring expressions as links in Markdown format and integrates grounding capability into downstream applications, shedding light on the convergence of language, multimodal perception, and world modeling.}
}

@inproceedings{jin2023refclip,
  author    = {Jin, Lei and Luo, Gen and Zhou, Yiyi and Sun, Xiaoshuai and Jiang, Guannan and Shu, Annan and Ji, Rongrong},
  title     = {RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2681--2690},
  year      = {2023},
  url       = {https://openaccess.thecvf.com/content/CVPR2023/html/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.html},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf},
  abstract  = {We propose RefCLIP, a novel weakly supervised model for Referring Expression Comprehension (REC) that redefines the task as an anchor-text matching problem. The approach introduces an anchor-based contrastive loss and acts as a universal teacher for generating pseudo-labels for other REC models, achieving significant performance gains.}
}

@inproceedings{yan2024vigor,
  author    = {Yan, Siming and Bai, Min and Chen, Weifeng and Zhou, Xiong and Huang, Qixing and Li, Erran},
  title     = {ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling},
  booktitle = {Computer Vision -- ECCV 2024: 18th European Conference on Computer Vision},
  pages     = {36--53},
  year      = {2024},
  url       = {https://link.springer.com/chapter/10.1007/978-3-031-73030-6_3},
  pdf       = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07792.pdf},
  abstract  = {Large vision language models (LVLMs) have shown unprecedented reasoning capabilities, but the generated text often suffers from inaccurate grounding in visual input, resulting in hallucination and other errors. We introduce ViGoR, a framework using fine-grained reward modeling to significantly enhance the visual grounding of LVLMs through cheaper human evaluations and automated methods.}
}

@inproceedings{wan2024contrastive,
  author    = {Wan, David and Cho, Jaemin and Stengel-Eskin, Elias and Bansal, Mohit},
  title     = {Contrastive Region Guidance: Improving Grounding in Vision-Language Models Without Training},
  booktitle = {Computer Vision -- ECCV 2024: 18th European Conference on Computer Vision},
  year      = {2024},
  pages     = {198--215},
  publisher = {Springer},
  doi       = {10.1007/978-3-031-72986-7_12},
  pdf       = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10172.pdf},
  url       = {https://eccv.ecva.net/virtual/2024/poster/754},
  abstract  = {Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts by contrasting model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer. CRG achieves substantial improvements across various tasks including up to 11.1\% accuracy increase on ViP-Bench, 10\% improvement on spatial reasoning, 11.5\% and 7.5\% gains on compositional generalization, and up to 8.4 AUROC improvement on image-text alignment.}
}

@inproceedings{zhang2023grounding,
  author    = {Zhang, Xiaohan and Ding, Yan and Amiri, Saeid and Yang, Hao and Kaminski, Andy and Esselink, Chad and Zhang, Shiqi},
  title     = {Grounding Classical Task Planners via Vision-Language Models},
  booktitle = {ICRA 2023 Workshop on Robot Execution Failures and Failure Management Strategies},
  year      = {2023},
  pdf       = {https://robot-failures.github.io/icra2023/papers/RobotFailuresICRA2023_paper_8.pdf},
  url       = {https://arxiv.org/abs/2304.08587},
  abstract  = {Classical planning systems have shown great advances in utilizing rule-based human knowledge to compute accurate plans for service robots, but they face challenges due to the strong assumptions of perfect perception and action executions. To tackle these challenges, one solution is to connect the symbolic states and actions generated by classical planners to the robot's sensory observations, thus closing the perception-action loop. In this work, we propose TPVQA, a visually-grounded planning framework that leverages Vision-Language Models (VLMs) to detect action failures and verify action affordances towards enabling successful plan execution. Results from quantitative experiments show that TPVQA surpasses competitive baselines from previous studies in task completion rate.}
}

@inproceedings{prasad2024rephrase,
  author    = {Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
  title     = {Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  pdf       = {https://openreview.net/pdf?id=L4nOxziGf9},
  url       = {https://openreview.net/forum?id=L4nOxziGf9},
  abstract  = {An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. We present RepARe (Rephrase, Augment and Reason), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We achieve accuracy improvements of 3.85\% on VQAv2, 6.41\% on A-OKVQA, and 7.94\% on VizWiz over strong zero-shot baselines.}
}

@inproceedings{zhao2025cotvla,
  author    = {Zhao, Qingqing and Lu, Yao and Kim, Moo Jin and Fu, Zipeng and Zhang, Zhuoyang and Wu, Yecheng and Li, Zhaoshuo and Ma, Qianli and Han, Song and Finn, Chelsea and Handa, Ankur and Liu, Ming-Yu and Xiang, Donglai and Wetzstein, Gordon and Lin, Tsung-Yi},
  title     = {CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2025},
  pages     = {1702--1713},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.pdf},
  url       = {https://cot-vla.github.io/},
  abstract  = {Current vision-language-action models (VLAs) primarily focus on direct input-output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. We introduce CoT-VLA, a method that incorporates explicit visual chain-of-thought reasoning by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. Our approach introduces a state-of-the-art 7B VLA that can understand and generate visual and action tokens. CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17\% in real-world manipulation tasks and 6\% in simulation benchmarks, demonstrating the effectiveness of visual chain-of-thought reasoning in robotic control.}
}

@article{arnab2025temporal,
  author    = {Arnab, Anurag and Iscen, Ahmet and Caron, Mathilde and Fathi, Alireza and Schmid, Cordelia},
  title     = {Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames},
  journal   = {arXiv preprint arXiv:2507.02001},
  year      = {2025},
  pdf       = {https://arxiv.org/pdf/2507.02001.pdf},
  url       = {https://arxiv.org/abs/2507.02001},
  abstract  = {Despite recent advances in Vision-Language Models (VLMs), long-video understanding remains a challenging problem. Although state-of-the-art long-context VLMs can process around 1000 input frames, they still struggle to effectively leverage this sequence length, and succumb to irrelevant distractors within the context window. We introduce Temporal Chain of Thought, an inference strategy for video question-answering that curates the model's input context. Our approach uses the VLM itself to iteratively identify and extract the most relevant frames from the video, which are then used for answering. We demonstrate how leveraging more computation at inference-time to select the most relevant context leads to improvements in accuracy, achieving state-of-the-art results on 4 diverse video question-answering datasets with consistent improvements across 3 different VLMs.}
}

@article{jin2025cotvid,
  author    = {Jin, Hongbo and Liu, Ruyang and Zhang, Wenhao and Luo, Guibo and Li, Ge},
  title     = {CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning},
  journal   = {arXiv preprint arXiv:2505.11830},
  year      = {2025},
  pdf       = {https://arxiv.org/pdf/2505.11830.pdf},
  url       = {https://arxiv.org/abs/2505.11830},
  abstract  = {With the emergence of System2 reasoning with Deep-Thinking Models and chain-of-thought technology, there is a gap in complex video reasoning research. We propose CoT-Vid, a novel training-free paradigm for the video domain with a multistage complex reasoning design that achieves surprising performance gains through explicit reasoning mechanisms, distinguishing it from existing video LLMs that rely heavily on perceptual abilities. The paradigm consists of three main components: dynamic inference path routing, problem decoupling strategy, and video self-consistency verification. Our method outperforms its base model by 9.3\% on Egochema and 5.6\% on VideoEspresso, rivaling or surpassing models like GPT-4V, GPT-4o, and Gemini-1.5-flash in video reasoning tasks.}
}

@article{corbire2025retrievalbased,
  author    = {Corbière, Charles and Roburin, Simon and Montariol, Syrielle and Bosselut, Antoine and Alahi, Alexandre},
  title     = {Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios},
  journal   = {arXiv preprint arXiv:2501.04671},
  year      = {2025},
  pdf       = {https://arxiv.org/pdf/2501.04671.pdf},
  url       = {https://arxiv.org/abs/2501.04671},
  abstract  = {While chain-of-thought prompting improves reasoning in large language models, its effectiveness in vision-language models (VLMs) remains limited due to over-reliance on textual cues and memorized knowledge. We introduce DrivingVQA, a visual question answering dataset derived from driving theory exams, containing 3,931 multiple-choice problems with expert-written explanations and grounded entities relevant to the reasoning process. We propose RIV-CoT (Retrieval-Based Interleaved Visual Chain-of-Thought), which uses visual crops corresponding to relevant entities to enable VLMs to reason more effectively in complex real-world scenarios. Our experimental results demonstrate improvements of 3.1\% answer accuracy and 4.6\% reasoning accuracy, showing the method's effectiveness in enhancing visual reasoning capabilities in challenging driving scenarios.}
}

@inproceedings{wang2025mllmtool,
  author    = {Wang, Chenyu and Luo, Weixin and Dong, Sixun and Xuan, Xiaohua and Li, Zhengxin and Ma, Lin and Gao, Shenghua},
  title     = {MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning},
  booktitle = {2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2025},
  pages     = {6678--6687},
  pdf       = {https://openaccess.thecvf.com/content/WACV2025/papers/Wang_MLLM-Tool_A_Multimodal_Large_Language_Model_for_Tool_Agent_Learning_WACV_2025_paper.pdf},
  url       = {https://github.com/MLLM-Tool/MLLM-Tool},
  abstract  = {Large language models have shown impressive performance in natural language tasks and are being used as central controllers in agent systems. While multiple studies focus on bridging LLMs to external tools, current LLMs' ability to perceive tool use is limited to single text queries, which may result in ambiguity in understanding users' real intentions. LLMs are expected to eliminate this by perceiving information in visual- or auditory-grounded instructions. We propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate evaluation of the model's capability, we collected a dataset featuring multi-modal input tools from HuggingFace with multiple potential choices for the same instruction, providing more potential solutions for the same query.}
}

@inproceedings{gao2025multimodal,
  author    = {Gao, Zhi and Zhang, Bofei and Li, Pengxiang and Ma, Xiaojian and Yuan, Tao and Fan, Yue and Wu, Yuwei and Jia, Yunde and Zhu, Song-Chun and Li, Qing},
  title     = {Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2025},
  pdf       = {https://openreview.net/pdf?id=0bmGL4q7vJ},
  url       = {https://openreview.net/forum?id=0bmGL4q7vJ},
  abstract  = {Multi-modal agents that use large language models as controllers to call external tools for solving practical tasks have shown significant potential. However, existing methods often rely on few-shot prompting approaches, which result in poor tool-usage reasoning and are dependent on specific prompts. We propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. We use GPT-4o mini to generate queries, files, and trajectories, followed by query-file and trajectory verifiers to preserve data quality. Based on our data synthesis pipeline, we collect the MM-Traj dataset containing 20K tasks with trajectories of tool usage. We develop T3-Agent via Trajectory Tuning on VLMs for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that T3-Agent achieves consistent improvements on two popular VLMs: MiniCPM-V-8.5B and Qwen2-VL-7B, outperforming untrained VLMs by 20\%, demonstrating the effectiveness of our data synthesis pipeline.}
}

@article{kumar2025reinforcing,
  author    = {Kumar, Sunil and Zhao, Bowen and Dirac, Leo and Varshavskaya, Paulina},
  title     = {Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints},
  journal   = {arXiv preprint arXiv:2506.14821},
  year      = {2025},
  pdf       = {https://arxiv.org/pdf/2506.14821.pdf},
  url       = {https://arxiv.org/abs/2506.14821},
  abstract  = {Despite tremendous recent advances in large model reasoning ability, vision-language models (VLMs) still struggle with detailed visual reasoning, especially when compute resources are limited. To address this challenge, we draw inspiration from methods like DeepSeek-r1 for VLMs and train smaller-scale models with Group Relative Policy Optimization (GRPO) to use external tools such as zoom. The greatest benefit is obtained with a combination of GRPO learning, a simple reward structure, a simplified tool-calling interface, allocating additional tokens to the result of the tool call, and a training data mix that over-represents visually difficult examples. Compared to similarly-sized baseline models, our method achieves better performance on some visual question-answering (VQA) tasks, thanks to the detailed visual information gathered from the external tool.}
}

@article{davis2025augmented,
  author    = {Davis, Anthony C. and Sadiq, Burhan and Shu, Tianmin and Huang, Chien-Ming},
  title     = {Augmented Vision-Language Models: A Systematic Review},
  journal   = {arXiv preprint arXiv:2507.22933},
  year      = {2025},
  url       = {https://arxiv.org/abs/2507.22933},
  pdf       = {https://arxiv.org/pdf/2507.22933},
  abstract  = {Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration.},
  month     = {7}
}

@inproceedings{wang2025learning,
  author    = {Wang, Shijie and Kim, Dahun and Taalimi, Ali and Sun, Chen and Kuo, Weicheng},
  title     = {Learning Visual Grounding from Generative Vision and Language Model},
  booktitle = {Proceedings of the {IEEE/CVF} Winter Conference on Applications of Computer Vision ({WACV})},
  year      = {2025},
  url       = {https://arxiv.org/abs/2407.14563},
  pdf       = {https://openaccess.thecvf.com/content/WACV2025/papers/Wang_Learning_Visual_Grounding_from_Generative_Vision_and_Language_Model_WACV_2025_paper.pdf},
  abstract  = {Visual grounding tasks aim to localize image regions based on natural language references. In this work, we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We hypothesize that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We propose to prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. To capture important object attributes and spatial relationships common in referring expressions, we propose attribute modeling and spatial relation modeling. Our constructed dataset contains 500K images, 1M objects, and 16M referring expressions, making it one of the largest grounding datasets to date. We conduct zero-shot transfer experiments on RefCOCO benchmarks and demonstrate that our approach significantly outperforms prior state-of-the-art methods without requiring human-annotated data.}
}

@inproceedings{xu2024vlmgrounder,
  author    = {Xu, Runsen and Huang, Zhiwei and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  title     = {{VLM}-Grounder: A {VLM} Agent for Zero-Shot {3D} Visual Grounding},
  booktitle = {Proceedings of the 8th Conference on Robot Learning},
  pages     = {3961--3985},
  year      = {2025},
  volume    = {270},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v270/xu25c.html},
  pdf       = {https://raw.githubusercontent.com/mlresearch/v270/main/assets/xu25c/xu25c.pdf},
  abstract  = {3D visual grounding is crucial for robots, requiring integration of natural language and 3D scene understanding. Traditional methods depending on supervised learning with 3D point clouds are limited by scarce datasets. Recently zero-shot methods leveraging LLMs have been proposed to address the data issue. While effective, these methods only use object-centric information, limiting their ability to handle complex queries. We present VLM-Grounder, a novel framework using vision-language models (VLMs) for zero-shot 3D visual grounding based solely on 2D images. VLM-Grounder dynamically stitches image sequences, employs a grounding and feedback scheme to find the target object, and uses multi-view ensemble projection to accurately estimate 3D bounding boxes. Without relying on 3D geometric information or object priors, VLM-Grounder achieves 51.6\% Acc@0.25 on ScanRefer and 48.0\% Acc on Nr3D, outperforming previous zero-shot methods and achieving comparable performance to supervised learning baselines.}
}

@inproceedings{le2025progressive,
  author    = {Le, Quang-Hung and Dang, Long Hoang and Le, Ngan and Tran, Truyen and Le, Thao Minh},
  title     = {Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models},
  booktitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  year      = {2025},
  url       = {https://arxiv.org/abs/2412.08125},
  pdf       = {https://arxiv.org/pdf/2412.08125},
  abstract  = {Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular Vision-Language alignments (PromViL), a novel framework to enhance LVLMs' ability in performing grounded compositional visual reasoning tasks. We construct a hierarchical structure of multi-modal alignments, ranging from simple to complex concepts. By progressively aligning textual descriptions with corresponding visual regions, the model learns to leverage contextual information from lower levels to inform higher-level reasoning. Our approach addresses limitations in current vision-language models where coarse-grained alignment processes ignore crucial object-level correspondences that are essential for tasks like visual reasoning and image captioning. Experiments demonstrate that PromViL significantly improves performance on compositional visual reasoning benchmarks while maintaining competitive results on standard vision-language tasks.}
}

@article{kao2025think,
  author    = {Kao, Shiu-hong and Tai, Yu-Wing and Tang, Chi-Keung},
  title     = {Think Before You Segment: High-Quality Reasoning Segmentation with {GPT} Chain of Thoughts},
  journal   = {arXiv preprint arXiv:2503.07503},
  year      = {2025},
  url       = {https://arxiv.org/abs/2503.07503},
  pdf       = {https://arxiv.org/pdf/2503.07503},
  abstract  = {Reasoning segmentation is a challenging vision-language task that aims to output the segmentation mask with respect to a complex, implicit, and even non-visual query text. Previous works incorporated multimodal Large Language Models (MLLMs) with segmentation models to approach the difficult problem. However, their segmentation quality often falls short in complex cases, particularly when dealing with out-of-domain objects with intricate structures, blurry boundaries, occlusions, or high similarity with surroundings. In this paper, we introduce ThinkFirst, a training-free reasoning segmentation framework that leverages GPT's chain of thought to address these challenging cases. Our approach allows GPT-4o or other powerful MLLMs to generate a detailed, chain-of-thought description of an image. This summarized description is then passed to a language-instructed segmentation assistant to aid the segmentation process. Our framework allows users to easily interact with the segmentation agent using multimodal inputs, such as easy text and image scribbles, for successive refinement or communication. We evaluate the performance of ThinkFirst on diverse objects. Extensive experiments show that, this zero-shot-CoT approach significantly improves the vanilla reasoning segmentation agent, both qualitatively and quantitatively, while being less sensitive or critical to user-supplied prompts after Thinking First.},
  month     = {3}
}

@article{liu2025mmc,
  author    = {Liu, Shuhang and Zhang, Zhenrong and Hu, Pengfei and Ma, Jiefeng and Du, Jun and Wang, Qing and Zhang, Jianshu and Liu, Quan and Gao, Jianqing and Ma, Feng},
  title     = {{MMC}: Iterative Refinement of {VLM} Reasoning via {MCTS}-based Multimodal Critique},
  journal   = {arXiv preprint arXiv:2504.11009},
  year      = {2025},
  url       = {https://arxiv.org/abs/2504.11009},
  pdf       = {https://arxiv.org/pdf/2504.11009},
  abstract  = {Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline.},
  month     = {4}
}

@inproceedings{thawakar2025llamavo1,
  author    = {Thawakar, Omkar and Dissanayake, Dinura and More, Ketan and Thawkar, Ritesh and Heakl, Ahmed and Ahsan, Noor and Li, Yuhao and Zumri, Mohammed and Lahoud, Jean and Anwer, Rao Muhammad and Cholakkal, Hisham and Laptev, Ivan and Shah, Mubarak and Khan, Fahad Shahbaz and Khan, Salman},
  title     = {{LlamaV-o1}: Rethinking Step-by-step Visual Reasoning in {LLM}s},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2025},
  pages     = {24290--24315},
  year      = {2025},
  month     = {7},
  address   = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.findings-acl.1247},
  pdf       = {https://aclanthology.org/2025.findings-acl.1247.pdf},
  doi       = {10.18653/v1/2025.findings-acl.1247},
  abstract  = {We propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LLMs) through three key contributions. First, we introduce a visual reasoning benchmark with eight different categories and over 4,000 reasoning steps, designed to evaluate multi-step reasoning tasks. Second, we present a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. Third, we present LlamaV-o1, a new multimodal visual reasoning model trained using a multi-step curriculum learning approach where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8\% across six benchmarks compared to Llava-CoT while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available.}
}

@article{wang2025multimodalsurvey,
  author    = {Wang, Yaoting and Wu, Shengqiong and Zhang, Yuecheng and Yan, Shuicheng and Liu, Ziwei and Luo, Jiebo and Fei, Hao},
  title     = {Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey},
  journal   = {arXiv preprint arXiv:2503.12605},
  year      = {2025},
  url       = {https://arxiv.org/abs/2503.12605},
  pdf       = {https://arxiv.org/pdf/2503.12605},
  abstract  = {By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address unique challenges across different modalities including image, video, speech, audio, 3D, and structured data, achieving success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus, and an up-to-date review of this domain has been lacking. To address this gap, we present the first systematic survey of MCoT reasoning, elucidating relevant foundational concepts and definitions. We provide a comprehensive taxonomy of current MCoT methodologies, analyze existing challenges, and suggest future research directions toward multimodal artificial general intelligence (AGI). Additionally, we compile publicly available MCoT resources including benchmarks and models to facilitate community research and development.},
  month     = {3}
}

@article{zhang2025unified,
  author    = {Cheng, Zihui and Chen, Qiguang and Xu, Xiao and Wang, Jiaqi and Wang, Weiyun and Fei, Hao and Wang, Yidong and Wang, Alex Jinpeng and Chen, Zhi and Che, Wanxiang and Qin, Libo},
  title     = {Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought},
  journal   = {arXiv preprint arXiv:2505.15510},
  year      = {2025},
  url       = {https://arxiv.org/abs/2505.15510},
  pdf       = {https://arxiv.org/pdf/2505.15510},
  abstract  = {Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing performance and interpretability. Recent MCoT methods fall into two categories: (i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved image-text outputs. Despite advances in both approaches, the mechanisms driving these improvements are not fully understood. To fill this gap, we reveal that MCoT boosts LVLMs by incorporating visual thoughts, which convey image information to the reasoning process regardless of the MCoT format, depending only on clarity and conciseness of expression. Based on this finding, we propose a unified framework that conceptualizes visual thoughts as information transmissions across transformer layers, enabling a comprehensive understanding of how visual information flows through reasoning processes in both T-MCoT and I-MCoT paradigms.},
  month     = {5}
}

@article{rose2023visual,
  author    = {Daniel Philip Rose and Vaishnavi Himakunthala and Andy Ouyang and Ryan He and Alex Mei and Yujie Lu and Michael Saxon and Chinmay Sonar and Diba Mirza and William Yang Wang},
  title     = {Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings},
  journal   = {arXiv preprint arXiv:2305.02317},
  year      = {2023},
  url       = {https://arxiv.org/abs/2305.02317},
  pdf       = {https://arxiv.org/pdf/2305.02317.pdf},
  abstract  = {Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.},
  note      = {ICLR 2024 Conference Withdrawn Submission}
}

@inproceedings{zhang2025improve,
  author       = {Ruohong Zhang and Bowen Zhang and Yanghao Li and Haotian Zhang and Zhiqing Sun and Zhe Gan and Yinfei Yang and Ruoming Pang and Yiming Yang},
  title        = {Improve Vision Language Model Chain-of-Thought Reasoning},
  booktitle    = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics},
  year         = {2025},
  url          = {https://machinelearning.apple.com/research/chain-of-thought},
  pdf          = {https://aclanthology.org/2025.acl-long.82.pdf},
  abstract     = {Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes often rely on datasets dominated by short annotations with minimal rationales. Training VLM on short answers leads to poor generalization on reasoning tasks that require more detailed explanations. To address this limitation, we propose a two-stage post-training strategy that extends the usage of short answer data for enhanced CoT reasoning. First, we augment short answers with CoT reasoning generated by GPT-4o, enhancing the VLM's CoT capabilities through fine-tuning. Second, we leverage short answers as outcome rewards for reinforcement learning. Specifically, short answers are used as correctness indicators to construct positive (correct) and negative (incorrect) pairs from model-generated reasoning chains. Using pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction.}
}

@inproceedings{zhu2025guiding,
  author    = {Kangyu Zhu and Ziyuan Qin and Huahui Yi and Zekun Jiang and Qicheng Lao and Shaoting Zhang and Kang Li},
  title     = {Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2025},
  pdf       = {https://aclanthology.org/2025.naacl-long.587.pdf},
  url       = {https://arxiv.org/abs/2501.02385},
  abstract  = {While mainstream vision-language models (VLMs) have advanced rapidly in understanding image-level information, they still lack the ability to focus on specific areas designated by humans. Rather, they typically rely on large volumes of high-quality image-text paired data to learn and generate posterior attention maps. To address this limitation, we propose using visual prompts: simple visual markers in various forms to guide and enhance the formation of region-specific attention. We introduce MedVP, a pioneering framework that integrates medical entity extraction, visual prompt generation, and dataset adaptation for visual prompt guided fine-tuning. Through comprehensive experiments on multiple medical VQA datasets, we successfully outperform recent state-of-the-art large models and conduct extensive experiments and human evaluation to analyze the impact of different visual prompt forms.}
}

@inproceedings{shtedritski2023what,
  author    = {Aleksandar Shtedritski and Christian Rupprecht and Andrea Vedaldi},
  title     = {What Does {CLIP} Know About a Red Circle? Visual Prompt Engineering for {VLM}s},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {11987--11997},
  year      = {2023},
  pdf       = {https://openaccess.thecvf.com/content/ICCV2023/papers/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.pdf},
  url       = {https://arxiv.org/abs/2304.06712},
  abstract  = {Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we explore the idea of visual prompt engineering for solving computer vision tasks beyond classification by editing in image space instead of text. In particular, we discover an emergent ability of CLIP, where, by simply drawing a red circle around an object, we can direct the model's attention to that region, while also maintaining global information. We show the power of this simple approach by achieving state-of-the-art in zero-shot referring expressions comprehension and strong performance in keypoint localization tasks.}
}

@article{zhong2025flowvla,
  author    = {Zhide Zhong and Haodong Yan and Junfeng Li and Xiangchen Liu and Xin Gong and Wenxuan Song and Jiayi Chen and Haoang Li},
  title     = {{FlowVLA}: Thinking in Motion with a Visual Chain of Thought},
  journal   = {arXiv preprint arXiv:2508.18269},
  year      = {2025},
  url       = {https://arxiv.org/abs/2508.18269},
  pdf       = {https://arxiv.org/pdf/2508.18269.pdf}
}

@article{li2025visualcog,
  author    = {Yaqi Li and Peng Chen and Mingyang Han and Pi Bu and Haoxiang Shi and Runzhou Zhao and Yang Yao and Xuan Zhang and Jun Song and Bo Zheng},
  title     = {Visual-{CoG}: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation},
  journal   = {arXiv preprint arXiv:2508.18032},
  year      = {2025},
  pdf       = {https://arxiv.org/pdf/2508.18032.pdf},
  url       = {https://arxiv.org/abs/2508.18032},
  abstract  = {Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. We propose the Visual-Chain of Guidance (Visual-CoG) paradigm, which employs a three-stage approach to enhance T2I generation performance. The first stage, semantic reasoning, extracts key semantic features from the prompt to guide the subsequent generation process. The second stage, process refining, leverages vision-language models to evaluate and refine intermediate generation steps. The third stage, outcome evaluation, employs a reward model to assess the final generated image quality. Through extensive experiments on multiple benchmarks, we demonstrate that Visual-CoG significantly improves the quality of text-to-image generation, particularly for complex and ambiguous prompts.}
}

@inproceedings{kang2025your,
  author    = {Seil Kang and Jinyeong Kim and Junhyeok Kim and Seong Jae Hwang},
  title     = {Your Large Vision-Language Model Only Needs A Few Attention Heads for Visual Grounding},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2025},
  note      = {CVPR 2025 Highlight},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2025/papers/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.pdf},
  url       = {https://arxiv.org/abs/2503.06287},
  abstract  = {Visual grounding seeks to localize the image region corresponding to a free-form text description. Current large vision-language models (LVLMs) demonstrate strong visual grounding capabilities, but their success requires costly fine-tuning. We discover that only a few attention heads in frozen LVLMs demonstrate strong visual grounding capabilities. These heads, which consistently capture object locations related to text semantics, are referred to as localization heads. Surprisingly, only three out of thousands of attention heads are sufficient to achieve competitive localization performance compared to existing LVLM-based visual grounding methods that require fine-tuning. Based on this finding, we introduce a straightforward and effective training-free visual grounding framework that utilizes text-to-image attention maps from localization heads to identify the target regions. The identification of localization heads is based on two explicit criteria: attention sum and attention variance. Through extensive experiments on RefCOCO, RefCOCO+, and RefCOCOg datasets, we demonstrate that our approach achieves competitive performance while being significantly more efficient than existing methods.}
}

@inproceedings{fu2025refocus,
  author    = {Xingyu Fu and Minqian Liu and Zhengyuan Yang and John Richard Corring and Yijuan Lu and Jianwei Yang and Dan Roth and Dinei Florencio and Cha Zhang},
  title     = {{ReFocus}: Visual Editing as a Chain of Thought for Structured Image Understanding},
  booktitle = {ICLR 2025 Workshop on Foundation Models in the Wild},
  year      = {2025},
  url       = {https://openreview.net/forum?id=rkvMUjeEcr},
  pdf       = {https://openreview.net/pdf?id=rkvMUjeEcr},
  abstract  = {Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate ``visual thoughts'' by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0\% on table tasks and 6.8\% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0\% average gain over the same model trained with QA pairs and 2.6\% over CoT.},
  keywords  = {Visual Chain of Thought Prompting and Training Data Collection, Table, Chart, Vision Language Model, Multimodal LLMs}
}

@inproceedings{li2025imagine,
  author    = {Chengzu Li and Wenshan Wu and Huanyu Zhang and Yan Xia and Shaoguang Mao and Li Dong and Ivan Vulić and Furu Wei},
  title     = {Imagine While Reasoning in Space: Multimodal Visualization-of-Thought},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  year      = {2025},
  url       = {https://proceedings.mlr.press/v242/li25imagine},
  pdf       = {https://proceedings.mlr.press/v242/li25imagine.pdf},
  abstract  = {Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.},
  keywords  = {Multimodal Large Language Models, Spatial Reasoning}
}

@inproceedings{hu2024sketchpad,
  author    = {Yushi Hu and Weijia Shi and Xingyu Fu and Dan Roth and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Ranjay Krishna},
  title     = {Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2024},
  volume    = {37},
  url       = {https://neurips.cc/virtual/2024/poster/94025},
  pdf       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/fb82011040977c7712409fbdb5456647-Paper-Conference.pdf},
  abstract  = {Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment on a wide range of math tasks (including geometry, functions, graph, chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7\% on math tasks, and 8.6\% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3\%), BLINK spatial reasoning (83.9\%), and visual correspondence (80.8\%). We will release all code and data.},
  keywords  = {multimodal, large language model, computer vision, reasoning}
}

@inproceedings{zhang2024deductive,
  author    = {Yizhe Zhang and He Bai and Ruixiang Zhang and Jiatao Gu and Shuangfei Zhai and Josh Susskind and Navdeep Jaitly},
  title     = {How Far Are We from Intelligent Visual Deductive Reasoning?},
  booktitle = {ICLR 2024 Workshop on How Far Are We from AGI?},
  year      = {2024},
  url       = {https://arxiv.org/abs/2403.04732},
  pdf       = {https://arxiv.org/pdf/2403.04732.pdf},
  abstract  = {Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. Moreover, a detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.}
}

@inproceedings{mitra2024compositional,
  author    = {Chancharik Mitra and Brandon Huang and Trevor Darrell and Roei Herzig},
  title     = {Compositional Chain-of-Thought Prompting for Large Multimodal Models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2024},
  url       = {https://github.com/chancharikmitra/CCoT},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2024/papers/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.pdf},
  abstract  = {The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)—a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language (VL) compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.}
}


@inproceedings{gunjal2024detecting,
  author    = {Anisha Gunjal and Jihan Yin and Erhan Bas},
  title     = {Detecting and Preventing Hallucinations in Large Vision Language Models},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {38},
  number    = {16},
  pages     = {18135--18143},
  year      = {2024},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/29771},
  doi       = {10.1609/aaai.v38i16.29771},
  abstract  = {Instruction-tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across diverse multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded remains challenging. Current state-of-the-art models produce approximately 30\% hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address hallucinations, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset consisting of 4,000 image-description pairs sourced from COCO labelled with object existence, relative positions, and attributes. We also propose Fine-grained Direct Preference Optimization (FDPO), a preference-aligned training approach that favors non-hallucinatory responses while avoiding hallucinatory content. Through human evaluation, our approach demonstrates 41-55\% reduction in hallucination rates compared to existing methods.}
}

@inproceedings{song2024moviechat,
  author    = {Enxin Song and Wenhao Chai and Guanhong Wang and Yucheng Zhang and Haoyang Zhou and Feiyang Wu and Haozhe Chi and Xun Guo and Tian Ye and Yanting Zhang and Yan Lu and Jenq-Neng Hwang and Gaoang Wang},
  title     = {{MovieChat}: From Dense Token to Sparse Memory for Long Video Understanding},
  booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages     = {18221--18232},
  year      = {2024},
  publisher = {IEEE Computer Society},
  url       = {https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html},
  pdf       = {https://openaccess.thecvf.com/content/CVPR2024/papers/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.pdf},
  abstract  = {Integrating video foundation models and large language models to build a video understanding system can overcome the limitations of understanding long videos. However, existing methods primarily use dense visual tokens, resulting in significant computational and memory costs that increase linearly with the number of frames. Taking advantage of the Atkinson-Shiffrin memory model, with tokens in Transformers serving as the carriers of memory, we propose MovieChat to overcome these challenges. MovieChat employs a memory mechanism that combines a rapidly updated short-term memory and a compact sustained long-term memory. Our approach can handle videos with over 10K frames on a 24GB graphics card, providing a 10000× advantage over other methods in terms of average GPU memory cost per frame. We create MovieChat-1K, containing 1K long videos and 14K manual annotations for comprehensive evaluation. Experimental results demonstrate MovieChat achieves state-of-the-art performance in long video understanding tasks.}
}

@article{yin2024woodpecker,
  author    = {Shukang Yin and Chaoyou Fu and Sirui Zhao and Tong Xu and Hao Wang and Dianbo Sui and Yunhang Shen and Ke Li and Xing Sun and Enhong Chen},
  title     = {{Woodpecker}: Hallucination Correction for Multimodal Large Language Models},
  journal   = {Science China Information Sciences},
  volume    = {67},
  number    = {12},
  pages     = {220105},
  year      = {2024},
  publisher = {Springer},
  doi       = {10.1007/s11432-024-4251-x},
  url       = {https://link.springer.com/article/10.1007/s11432-024-4251-x},
  abstract  = {Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. Unlike existing studies that mainly use instruction-tuning requiring model retraining with specific data, we propose a training-free method called Woodpecker to correct hallucinations. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. On the POPE benchmark, Woodpecker obtained a 30.66\%/24.33\% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. This is the first work to correct hallucination in multimodal large language models.}
}

@inproceedings{hu2024mplugdocowl,
  author    = {Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo Zhang and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou},
  title     = {{mPLUG-DocOwl} 1.5: Unified Structure Learning for {OCR}-free Document Understanding},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2024},
  pages     = {3096--3120},
  year      = {2024},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-emnlp.175/},
  pdf       = {https://aclanthology.org/2024.findings-emnlp.175.pdf},
  abstract  = {Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, charts, etc. Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding are equipped with text recognition ability but lack general structure understanding abilities for text-rich document images. In this work, we emphasize the importance of structure information in Visual Document Understanding and propose Unified Structure Learning to boost MLLM performance. Our approach comprises structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image. We design a simple and effective vision-to-text module H-Reducer, which can maintain layout information and reduce the length of visual features by merging horizontal adjacent patches through convolution. We construct DocStruct4M and DocReason25K datasets to support structure learning. Our DocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks.}
}

@article{chen2024m3docrag,
  author    = {Jaemin Cho and Debanjan Mahata and Ozan Irsoy and Yujie He and Mohit Bansal},
  title     = {{M3DocRAG}: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding},
  journal   = {arXiv preprint arXiv:2411.04952},
  year      = {2024},
  month     = {11},
  url       = {https://arxiv.org/abs/2411.04952},
  pdf       = {https://arxiv.org/pdf/2411.04952.pdf},
  abstract  = {Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that loses visual information. However, documents often contain multiple pages with important visual elements, and questions may require reasoning across these pages. To address these challenges, we introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, preserving visual information while efficiently handling multiple documents. We also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. Empirical results show that M3DocRAG achieves superior performance compared to many strong baselines across three benchmarks.}
}

@inproceedings{wang2024lemma,
  author    = {Weiran Wang and Jingwei Zhang and Xinyu Zhang and Yifan Zhang and Zhengyu Ma and Jiani Zhang and Qingsong Lv and Liang Chen},
  title     = {{LEMMA}: Learning to Mitigate Multimodal Hallucination},
  booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  year      = {2024},
  publisher = {IEEE Computer Society},
  note      = {Paper details could not be verified in CVPR 2024 proceedings during enrichment}
}

@inproceedings{kim2023videoqa,
  author    = {Heeseung Kim and Sungwon Kim and Dongyeop Kang and Sungjin Lee},
  title     = {{VideoQA-SC}: Semantically Consistent Video Question Answering},
  booktitle = {Proceedings of the {IEEE/CVF} International Conference on Computer Vision ({ICCV})},
  year      = {2023},
  publisher = {IEEE Computer Society},
  note      = {Paper details could not be verified in ICCV 2023 proceedings during enrichment}
}

@inproceedings{fujitake2024layoutllm,
  author    = {Masato Fujitake},
  title     = {{LayoutLLM}: Large Language Model Instruction Tuning for Visually Rich Document Understanding},
  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation ({LREC-COLING} 2024)},
  pages     = {10219--10224},
  year      = {2024},
  month     = {5},
  address   = {Torino, Italia},
  publisher = {ELRA and ICCL},
  url       = {https://aclanthology.org/2024.lrec-main.892/},
  pdf       = {https://aclanthology.org/2024.lrec-main.892.pdf},
  abstract  = {Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model.}
}

@inproceedings{wu2024nextgpt,
  author    = {Shengqiong Wu and Hao Fei and Leigang Qu and Wei Ji and Tat-Seng Chua},
  title     = {{NExT-GPT}: Any-to-Any Multimodal {LLM}},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {235},
  pages     = {53366--53397},
  year      = {2024},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v235/wu24e.html},
  pdf       = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wu24e/wu24e.pdf},
  abstract  = {While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1\%) of certain projection layers. The proposed NExT-GPT is capable of perceiving inputs and generating outputs in arbitrary combinations of text, images, videos, and audio.}
}

@inproceedings{zong2024vlguard,
  author    = {Yongshuo Zong and Ondrej Bohdal and Tingyang Yu and Yongxin Yang and Timothy Hospedales},
  title     = {Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {235},
  year      = {2024},
  publisher = {PMLR},
  url       = {https://arxiv.org/abs/2402.02207},
  pdf       = {https://arxiv.org/pdf/2402.02207.pdf},
  abstract  = {Current vision large language models (VLLMs) are prone to generating harmful content and are vulnerable to even simple jailbreaking attacks. This is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underlying LLM. We introduce VLGuard, the first vision-language safe instruction-following dataset covering various harmful categories. Experiments demonstrate that integrating VLGuard into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. Fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases, while maintaining model helpfulness.},
  note      = {VLGuard dataset and code available at https://github.com/ys-zong/VLGuard}
}
