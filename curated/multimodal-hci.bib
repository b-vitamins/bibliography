@article{Jaimes2007,
  author = {Jaimes, Alejandro and Sebe, Nicu},
  title = {Multimodal human-computer interaction: A survey},
  journal = {Computer Vision and Image Understanding},
  volume = {108},
  number = {1-2},
  pages = {116--134},
  year = {2007},
  publisher = {Elsevier},
  doi = {10.1016/j.cviu.2006.10.019},
  url = {https://www.sciencedirect.com/science/article/pii/S1077314206002335},
  abstract = {In this paper, we review the major approaches to multimodal human--computer interaction, giving an overview of the field from a computer vision perspective. In particular, we focus on body, gesture, gaze, and affective interaction (facial expression recognition and emotion in audio). We discuss user and task modeling, and multimodal fusion, highlighting challenges, open issues, and emerging applications for multimodal human--computer interaction (MMHCI) research.}
}

@InCollection{Dumas2009,
  author = {Dumas, Bruno and Lalanne, Denis and Oviatt, Sharon L.},
  title = {Multimodal Interfaces: A Survey of Principles, Models and Frameworks},
  booktitle = {Human Machine Interaction},
  year = {2009},
  editor = {Lalanne, Denis and Kohlas, Juerg},
  series = {Lecture Notes in Computer Science},
  volume = {5440},
  pages = {3--26},
  doi = {10.1007/978-3-642-00437-7_1},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-00437-7_1},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  abstract = {The grand challenge of multimodal interface creation is to build reliable processing systems able to analyze and understand multiple communication means in real-time. This opens a number of associated issues covered by this chapter, such as heterogeneous data types fusion, architectures for real-time processing, dialog management, machine learning for multimodal interaction, modeling languages, frameworks, etc. The chapter does not intend to cover exhaustively all the issues related to multimodal interfaces creation and some hot topics, such as error handling, have been left aside. The chapter starts with the features and advantages associated with multimodal interaction, with a focus on particular findings and guidelines, as well as cognitive foundations underlying multimodal interaction. The chapter then focuses on the driving theoretical principles, time-sensitive software architectures and multimodal fusion and fission issues.}
}

@inproceedings{Jiang2018,
  author = {Jiang, Yu-Sian and Warnell, Garrett and Stone, Peter},
  title = {Inferring User Intention using Gaze in Vehicles},
  year = {2018},
  booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
  series = {ICMI '18},
  address = {Boulder, Colorado, USA},
  month = {10},
  pages = {1--9},
  publisher = {ACM},
  doi = {10.1145/3242969.3243018},
  url = {https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICMI18-Jiang.pdf},
  pdf = {https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICMI18-Jiang.pdf},
  abstract = {Motivated by the desire to give vehicles better information about their drivers, we explore human intent inference in the setting of a human driver riding in a moving vehicle. Specifically, we consider scenarios where the driver intends to go to or learn about a specific point of interest along the vehicle's route, and an autonomous system infers this using gaze cues. The key challenge is that the scene under observation is highly dynamic---both the background and objects in the scene move independently relative to the driver---making these scenarios significantly different from the static scenes considered by most literature in the eye tracking community. We provide a formulation for determining a point of interest in a dynamic scenario, design an experimental framework to systematically evaluate initial solutions, and propose our own solution called dynamic interest point detection (DIPD). We experimentally demonstrate the success of DIPD when compared to baseline nearest-neighbor or filtering approaches.}
}

@inproceedings{Jia2020,
  author = {Jia, Jiyou and He, Yunfan and Le, Huixiao},
  title = {A Multimodal Human-Computer Interaction System and Its Application in Smart Learning Environments},
  year = {2020},
  booktitle = {Blended Learning. Education in a Smart Learning Environment},
  series = {Lecture Notes in Computer Science},
  volume = {12218},
  pages = {3--14},
  publisher = {Springer},
  address = {Cham},
  editor = {Cheung, Simon K. S. and Li, Reggie and Phusavat, Kongkiti and Paoprasert, Naraphorn and Kwok, Lam-for},
  doi = {10.1007/978-3-030-51968-1_1},
  url = {https://link.springer.com/chapter/10.1007/978-3-030-51968-1_1},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC7366450/pdf/978-3-030-51968-1_Chapter_1.pdf},
  abstract = {We present a multimodal human-computer interaction system called MMISE (Multimodal Interaction System for Education) that comprehensively uses various input and output channels. For input, it includes traditional keyboard typing, mouse clicking, screen touching, as well as the latest speech and face recognition technology. For output, it uses traditional screen display along with speech synthesis, facial expression synthesis, and gesture generation. The system employs a pedagogical objective-oriented mechanism (POOOIIM) where input and output channels are determined by specific educational objectives, designed to enhance learning through more interactive and adaptive interfaces. We describe the system's architecture and working mechanism, illustrated with practical examples. The system was pilot tested during the COVID-19 pandemic for online learning applications, helping maintain student engagement in remote learning environments through natural human-computer interaction capabilities.}
}

@article{Sumak2022,
  author = {Šumak, Boštjan and Brdnik, Saša and Pušnik, Maja},
  title = {Sensors and Artificial Intelligence Methods and Algorithms for Human--Computer Intelligent Interaction: A Systematic Mapping Study},
  journal = {Sensors},
  volume = {22},
  number = {1},
  pages = {20},
  year = {2022},
  publisher = {MDPI},
  doi = {10.3390/s22010020},
  url = {https://www.mdpi.com/1424-8220/22/1/20},
  pdf = {https://www.mdpi.com/1424-8220/22/1/20/pdf},
  abstract = {To equip computers with human communication skills and to enable natural interaction between the computer and a human, intelligent solutions are required based on artificial intelligence (AI) methods, algorithms, and sensor technology. This study aimed at identifying and analyzing the state-of-the-art AI methods and algorithms and sensors technology in existing human--computer intelligent interaction (HCII) research to explore trends in HCII research, categorize existing evidence, and identify potential directions for future research. We conduct a systematic mapping study of the HCII body of research. Four hundred fifty-four studies published in various journals and conferences between 2010 and 2021 were identified and analyzed. Studies in the HCII and IUI fields have primarily been focused on intelligent recognition of emotion, gestures, and facial expressions using sensors technology, such as the camera, EEG, Kinect, wearable sensors, eye tracker, gyroscope, and others. Researchers most often apply deep-learning and instance-based AI methods and algorithms. The support vector machine (SVM) is the most widely used algorithm for various kinds of recognition, primarily emotion, facial expression, and gesture. The convolutional neural network (CNN) is the often-used deep-learning algorithm for emotion recognition, facial recognition, and gesture recognition solutions.},
  keywords = {artificial intelligence; human--computer interaction; sensors; systematic mapping study; machine learning; intelligent user interfaces}
}

@article{Azofeifa2022,
  author = {Azofeifa, Jose Daniel and Noguez, Julieta and Ruiz, Sergio and Molina-Espinosa, José Martín and Magana, Alejandra J. and Benes, Bedrich},
  title = {Systematic Review of Multimodal Human--Computer Interaction},
  journal = {Informatics},
  volume = {9},
  number = {1},
  pages = {13},
  year = {2022},
  publisher = {MDPI},
  doi = {10.3390/informatics9010013},
  url = {https://www.mdpi.com/2227-9709/9/1/13},
  pdf = {https://www.mdpi.com/2227-9709/9/1/13/pdf},
  abstract = {This document presents a systematic review of Multimodal Human--Computer Interaction. It shows how different types of interaction technologies (virtual reality (VR) and augmented reality, force and vibration feedback devices (haptics), and tracking) are used in different domains (concepts, medicine, physics, human factors/user experience design, transportation, cultural heritage, and industry). An initial literature search identified 406 articles, from which 112 research works were selected for in-depth analysis. The analysis was conducted from three perspectives: temporal patterns, frequency of technology usage across domains, and cluster analysis. The findings show that VR and haptics are the most widely used technologies across domains. While VR is the most used technology, haptic interaction is increasingly appearing in applications. The review suggests that future work could focus on combining VR and haptic technologies to enhance multimodal human--computer interaction experiences.},
  keywords = {multimodal interaction; human--computer interaction; virtual reality; augmented reality; haptics; systematic review}
}

@article{Hasler2017,
  author = {Hasler, B{\'e}atrice S. and Salomon, Oren and Tuchman, Peleg and Lev-Tov, Amir and Friedman, Doron},
  title = {Real-time gesture translation in intercultural communication},
  journal = {AI \& Society},
  volume = {32},
  number = {1},
  pages = {25--35},
  year = {2017},
  publisher = {Springer},
  doi = {10.1007/s00146-014-0573-4},
  url = {https://link.springer.com/article/10.1007/s00146-014-0573-4},
  abstract = {Nonverbal behavior plays a crucial role in human communication and often leads to misunderstandings between people from different cultures, even if they speak the same language fluently. While translation systems are available for verbal communication, translators for nonverbal communication do not exist yet. The authors present the conceptual design and an early prototype of a real-time gesture translator using body tracking and gesture recognition in avatar-mediated intercultural interactions. It contributes to the ambitious goal of bridging between cultures by translating culture-specific gestures to enhance mutual understanding. Possible applications of the gesture translator are discussed as a facilitating tool for global business meetings and as a means of technology-enhanced conflict resolution and prevention.},
  keywords = {gesture translation; intercultural communication; nonverbal behavior; avatar-mediated interaction; body tracking}
}

@misc{Hussain2023,
  author = {Hussain, Shafiq},
  title = {Multimodal AI for Enhanced Human-Computer Interaction in Smart Environments},
  year = {2023},
  howpublished = {ResearchGate Preprint},
  url = {https://www.researchgate.net/publication/392896937_Multimodal_AI_for_Enhanced_Human-Computer_Interaction_in_Smart_Environments},
  abstract = {The rapid evolution of Artificial Intelligence (AI) has brought transformative capabilities to smart environments, enabling seamless, intuitive, and efficient human-computer interactions (HCI). Multimodal AI, which integrates multiple data modalities such as speech, vision, gestures, and contextual sensors, is emerging as a pivotal technology in shaping adaptive and intelligent interfaces. This research explores the convergence of multimodal AI and HCI within smart environments, emphasizing its role in improving user experience, system adaptability, and real-time responsiveness. The study includes detailed experimentation involving speech-gesture fusion, vision-based emotion recognition, and sensor-driven context modeling, demonstrating how multimodal AI significantly outperforms unimodal systems. The results validate that multimodal approaches yield more accurate, responsive, and personalized interactions. The paper provides an in-depth analysis of the architecture, challenges, methodologies, and outcomes, offering a roadmap for deploying effective multimodal AI systems in future smart spaces.},
  keywords = {multimodal AI; human-computer interaction; smart environments; speech-gesture fusion; emotion recognition; sensor fusion}
}

@article{Su2023,
  author = {Su, Hang and Qi, Wen and Chen, Jiahao and Yang, Chenguang and Sandoval, Juan and Laribi, Med Amine},
  title = {Recent advancements in multimodal human--robot interaction},
  journal = {Frontiers in Neurorobotics},
  volume = {17},
  pages = {1084000},
  year = {2023},
  publisher = {Frontiers Media SA},
  doi = {10.3389/fnbot.2023.1084000},
  url = {https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1084000/full},
  pdf = {https://www.frontiersin.org/articles/10.3389/fnbot.2023.1084000/pdf},
  abstract = {Robotics have advanced significantly, and human--robot interaction (HRI) now plays a crucial role in delivering better user experiences, reducing laborious tasks, and increasing public acceptance of robots. As an emerging approach to HRI, multimodal HRI enables individuals to communicate with robots using various modalities, including voice, image, text, eye movement, touch, and bio-signals such as EEG and ECG. This paper systematically reviews the state of the art of multimodal HRI, focusing on applications and research development in both input and output signals. Multimodal HRI is closely related to cognitive science, ergonomics, multimedia technology, and virtual reality, with numerous emerging applications appearing each year. The field encompasses vision-based interaction, where robots equipped with cameras can recognize and track human faces and movements, allowing them to respond to visual cues and gestures. Key capabilities include object detection and tracking, facial recognition, and gesture recognition. This comprehensive review aims to provide an overview of current developments and future trends in multimodal human--robot interaction.},
  keywords = {human--robot interaction; multimodal interaction; computer vision; gesture recognition; facial recognition; bio-signals}
}

@article{tang2024multimodal,
  title={Multimodal Alignment and Fusion: A Survey},
  author={Li, Songtao and Tang, Hao},
  year={2024},
  eprint={2411.17040},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  doi={10.48550/arXiv.2411.17040},
  url={https://arxiv.org/abs/2411.17040},
  pdf = {https://arxiv.org/pdf/2411.17040v1.pdf},
  abstract = {This survey provides a comprehensive review of recent advancements in multimodal alignment and fusion within machine learning, driven by the growing diversity of data types including text, images, audio, and video. Multimodal integration enables improved model accuracy and broader applicability by leveraging complementary information across different modalities and facilitating knowledge transfer in limited data situations. We systematically categorize and analyze existing alignment and fusion techniques based on an extensive review of more than 200 relevant papers. The survey addresses challenges of multimodal data integration including alignment issues, noise resilience, and disparities in feature representation, while focusing on applications in domains like social media analysis, medical imaging, and emotion recognition. The insights provided aim to guide future research towards optimizing multimodal learning systems to enhance their scalability, robustness, and generalizability across various applications.},
  keywords = {multimodal learning; alignment; fusion; machine learning; computer vision; survey}
}

@article{guarrasi2024systematic,
  title={A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications},
  author={Guarrasi, Valerio and Aksu, Fatih and Caruso, Camillo Maria and Di Feola, Francesco and Rofena, Aurora and Ruffini, Filippo and Soda, Paolo},
  journal={Information Fusion},
  year={2024},
  publisher={Elsevier},
  doi={10.1016/j.inffus.2025.102686},
  url={https://www.sciencedirect.com/science/article/pii/S0262885625000976},
  abstract={Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review aims to comprehensively analyze and formalize current intermediate fusion methods in biomedical applications. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a structured notation to enhance the understanding and application of these methods beyond the biomedical domain. Our findings are intended to support researchers, healthcare professionals, and the broader deep learning community in developing more sophisticated and insightful multimodal models. Through this review, we aim to provide a foundational framework for future research and practical applications in the dynamic field of MDL.}
}

@article{Fauviaux2024,
  author = {Fauviaux, Tifenn and Marin, Ludovic and Parisi, Mathilde and Schmidt, Richard and Mostafaoui, Ghilès},
  title = {From unimodal to multimodal dynamics of verbal and nonverbal cues during unstructured conversation},
  journal = {PLOS ONE},
  year = {2024},
  month = {9},
  volume = {19},
  number = {9},
  pages = {e0309831},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0309831},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0309831},
  abstract = {Conversations encompass continuous exchanges of verbal and nonverbal information. Previous research has demonstrated that gestures dynamically entrain each other and that speakers tend to align their vocal properties. While gesture and speech are known to synchronize at the intrapersonal level, few studies have investigated the multimodal dynamics of gesture/speech between individuals. Here, we investigated both the intrapersonal and interpersonal multimodal dynamics between speech and gesture during unstructured conversation. Using an online dataset of 14 dyads engaged in unstructured conversation, we measured speech and gesture synchronization using cross-wavelets at different timescales. Results confirm previous research on intrapersonal speech/gesture coordination, finding synchronization at all timescales of conversation, and extend the literature by also finding interpersonal synchronization between speech and gesture. Importantly, vocal and movement synchronization occurs particularly at middle and slower timescales between 2 and 30 seconds.}
}

@article{Xiao2024,
  author = {Xiao, Ruowei and Zhang, Rongzheng and Buruk, O{\u{g}}uz and Hamari, Juho and Virkki, Johanna},
  title = {Toward next generation mixed reality games: a research through design approach},
  journal = {Virtual Reality},
  year = {2024},
  month = {7},
  volume = {28},
  number = {142},
  publisher = {Springer},
  doi = {10.1007/s10055-024-01041-9},
  url = {https://link.springer.com/article/10.1007/s10055-024-01041-9},
  abstract = {Mixed reality games refer to games that integrate physical entities with digitally mediated contents. Currently, it entails game creators to integrate heterogeneous virtual and physical components, which is often time-consuming and labor-intensive, without the support of a coherent technology stack. The underlying technodiversity manifested by the research corpus suggests a complicated, multi-dimensional design space that goes beyond merely technical concerns. In this research, we adopted a research-through-design approach and proposed an MR game technology stack that facilitates flexible, low-code game development. As design grounding, we first surveyed 34 state-of-the-art studies, and results were synergized into three different spectra of technological affordances, respectively activity range, user interface and feedback control, to inform our next design process. We went through an iterative prototyping phase and implemented an MR game development toolset. A co-design workshop was conducted, where we invited 15 participants to try the prototype tools and co-ideate the potential use scenarios for the proposed technology stack. As a result, four conceptual game designs with three major design implications were generated, which conjointly reflect a broader understanding on MR gameful experience and contribute fresh insights to this emerging research domain.}
}

@article{Kourtesis2024,
  author = {Kourtesis, Panagiotis},
  title = {A Comprehensive Review of Multimodal {XR} Applications, Risks, and Ethical Challenges in the Metaverse},
  journal = {Multimodal Technologies and Interaction},
  volume = {8},
  number = {11},
  pages = {98},
  year = {2024},
  month = {11},
  publisher = {MDPI},
  doi = {10.3390/mti8110098},
  url = {https://www.mdpi.com/2414-4088/8/11/98},
  abstract = {This scoping review examines the broad applications, risks, and ethical challenges associated with Extended Reality (XR) technologies, including Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), within the context of Metaverse. XR is revolutionizing fields such as immersive learning in education, medical and professional training, neuropsychological assessment, therapeutic interventions, arts, entertainment, retail, e-commerce, remote work, sports, architecture, urban planning, and cultural heritage preservation. The integration of multimodal technologies—haptics, eye-, face-, and body tracking, and brain–computer interfaces—enhances user engagement and interactivity, playing a key role in shaping the immersive experiences in the Metaverse. However, XR's expansion raises serious concerns, including data privacy risks, cybersecurity vulnerabilities, cybersickness, addiction, dissociation, harassment, bullying, and misinformation. These psychological, social, and security challenges are further complicated by intense advertising, manipulation of public opinion, and social inequality, which could disproportionately affect vulnerable individuals and social groups. The review emphasizes the urgent need for robust ethical frameworks and regulatory guidelines to address these risks while promoting equitable access, privacy, autonomy, and mental well-being.}
}

@article{Dritsas2025,
  author = {Dritsas, Elias and Trigka, Maria and Troussas, Christos and Mylonas, Phivos},
  title = {Multimodal Interaction, Interfaces, and Communication: A Survey},
  journal = {Multimodal Technologies and Interaction},
  volume = {9},
  number = {1},
  pages = {6},
  year = {2025},
  month = {1},
  publisher = {MDPI},
  doi = {10.3390/mti9010006},
  url = {https://www.mdpi.com/2414-4088/9/1/6},
  keywords = {multimodal interaction, human-computer interaction, adaptive systems, multimodal fusion},
  abstract = {Multimodal interaction is a transformative human-computer interaction (HCI) approach that allows users to interact with systems through various communication channels such as speech, gesture, touch, and gaze. With advancements in sensor technology and machine learning (ML), multimodal systems are becoming increasingly important in various applications, including virtual assistants, intelligent environments, healthcare, and accessibility technologies. This survey concisely overviews recent advancements in multimodal interaction, interfaces, and communication. It delves into integrating different input and output modalities, focusing on critical technologies and essential considerations in multimodal fusion, including temporal synchronization and decision-level integration. Furthermore, the survey explores the challenges of developing context-aware, adaptive systems that provide seamless and intuitive user experiences. Lastly, by examining current methodologies and trends, this study underscores the potential of multimodal systems and sheds light on future research directions.}
}

@article{wang2025interaction,
  author = {Wang, Yun and Lu, Yan},
  title = {Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration},
  year = {2025},
  month = {6},
  eprint = {2506.11718},
  archivePrefix = {arXiv},
  primaryClass = {cs.HC},
  doi = {10.48550/arXiv.2506.11718},
  url = {https://arxiv.org/abs/2506.11718},
  note = {Preprint},
  abstract = {As AI tools proliferate across domains, from chatbots and copilots to emerging agents, they increasingly support professional knowledge work. Yet despite their growing capabilities, these systems remain fragmented: they assist with isolated tasks but lack the architectural scaffolding for sustained, adaptive collaboration. We propose a layered framework for human-agent systems that integrates three interdependent dimensions: interaction, process, and infrastructure. Crucially, our architecture elevates process to a primary focus by making it explicit, inspectable, and adaptable, enabling humans and agents to align with evolving goals and coordinate over time. This model clarifies limitations of current tools, unifies emerging system design approaches, and reveals new opportunities for researchers and AI system builders.}
}

@article{Kim2025,
  author = {Kim, Youngmin and Chung, Jiwan and Kim, Jisoo and Lee, Sunghyun and Lee, Sangkyu and Kim, Junhyeok and Yang, Cheoljong and Yu, Youngjae},
  title = {Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues},
  year = {2025},
  month = {6},
  eprint = {2506.00958},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  doi = {10.48550/arXiv.2506.00958},
  url = {https://arxiv.org/abs/2506.00958},
  note = {Accepted to ACL 2025},
  abstract = {Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. To address this gap, we introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text. Our key contribution is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. We train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Through comprehensive experiments, we demonstrate MARS's effectiveness in both understanding and generating appropriate nonverbal behaviors in conversational contexts.}
}

@article{wang2025towards,
  title={Towards spatial computing: recent advances in multimodal natural interaction for {XR} headsets},
  author={Wang, Zhimin and Rao, Maohang and Ye, Shanghua and Song, Weitao and Lu, Feng},
  year={2025},
  month = {2},
  eprint={2502.07598},
  archivePrefix={arXiv},
  primaryClass={cs.HC},
  doi={10.48550/arXiv.2502.07598},
  url={https://arxiv.org/abs/2502.07598},
  note={Also published in Frontiers of Computer Science},
  abstract={With the widespread adoption of Extended Reality (XR) headsets, spatial computing technologies are gaining increasing attention. Spatial computing enables interaction with virtual elements through natural input methods such as eye tracking, hand gestures, and voice commands, thus placing natural human-computer interaction at its core. While previous surveys have reviewed conventional XR interaction techniques, recent advancements in natural interaction, particularly driven by artificial intelligence (AI) and large language models (LLMs), have introduced new paradigms and technologies that deserve systematic investigation. This paper provides a comprehensive survey of recent research on multimodal natural interaction for wearable XR, focusing on papers published between 2022 and 2024 in six top venues. We systematically categorize and analyze the latest developments in natural interaction techniques, highlighting how AI and LLMs are transforming spatial computing experiences in XR environments.}
}

@article{Ghannam2025,
  title = {A Review of Prototyping in {XR}: Linking Extended Reality to Digital Fabrication},
  author = {Chen, Bixun and Macdonald, Shaun and Attallah, Moataz and Chapman, Paul and Ghannam, Rami},
  year = {2025},
  month = {4},
  archivePrefix = {arXiv},
  eprint = {2504.02998},
  primaryClass = {cs.HC},
  doi = {10.48550/arXiv.2504.02998},
  url = {https://arxiv.org/abs/2504.02998},
  note = {Preprint},
  abstract = {Extended Reality (XR) has expanded the horizons of entertainment and social life and shows great potential in the manufacturing industry. Prototyping in XR can help designers make initial proposals and iterations at low cost before manufacturers and investors decide whether to invest in research, development or even production. This comprehensive review systematically surveys the literature on prototyping in XR and discusses the possibility of transferring created virtual prototypes from XR to commonly used 3D modeling software and reality. We reviewed 54 manuscripts over the last 15 years and proposed five research questions regarding prototyping in XR. Based on these research questions, we summarized the components and workflows of prototyping in XR and present an overview of the latest trends in display device evolution, control technologies, digital model construction, and manufacturing processes. We also speculated on the challenges and opportunities in the field of prototyping in XR, especially in linking extended reality to digital fabrication, with the aim of guiding researchers towards new directions.}
}

@article{an2025towards,
  title={Towards {LLM}-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques},
  author={An, Jisu and Lee, Junseok and Lee, Jeoungeun and Son, Yongseok},
  year={2025},
  month={6},
  eprint={2506.04788},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  doi={10.48550/arXiv.2506.04788},
  url={https://arxiv.org/abs/2506.04788},
  note={Preprint},
  abstract={The rapid progress of Multimodal Large Language Models (MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. This survey presents an LLM-centric analysis of current approaches, examining methods for transforming and aligning diverse modal inputs into the language embedding space. We analyze 125 MLLMs developed between 2021 and 2025 and identify emerging patterns in the field. We propose a classification framework based on three key dimensions: architectural strategies for modality integration, representation learning techniques, and training paradigms. Our goal is to provide researchers with a structured overview of current integration techniques to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.}
}

@article{Aksu2025,
  author = {Fatih Aksu and Fabrizia Gelardi and Arturo Chiti and Paolo Soda},
  title = {{Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from {CT} and {PET}}},
  year = {2025},
  journal = {Pattern Recognition Letters},
  volume = {193},
  pages = {111462},
  doi = {10.1016/j.patrec.2025.04.001},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S016786552500128X},
  pdf = {https://arxiv.org/pdf/2501.12425},
  abstract = {Accurate classification of histological subtypes of non-small cell lung cancer (NSCLC) is essential in the era of precision medicine, yet current invasive techniques are not always feasible and may lead to clinical complications. This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images. The method integrates the two modalities at different stages of feature extraction, using voxel-wise fusion to exploit complementary information across varying abstraction levels while preserving spatial correlations. Experimental results demonstrate the effectiveness of the proposed approach, achieving 0.724 accuracy and 0.681 AUC for NSCLC subtype classification.}
}

@article{Hamdani2025,
  author = {Rania Hamdani and In{\'{e}}s Chihi},
  title = {{Adaptive human-computer interaction for industry 5.0: A novel concept, with comprehensive review and empirical validation}},
  journal = {Computers in Industry},
  volume = {168},
  year = {2025},
  pages = {104268},
  doi = {10.1016/j.compind.2025.104268},
  url = {https://www.sciencedirect.com/science/article/pii/S0166361525000338},
  month = {6},
  publisher = {Elsevier}
}

@article{Podapati2025,
  author = {Vyoma Podapati},
  title = {{SoK: A Systematic Review of Context- and Behavior-Aware Adaptive Authentication in Mobile Environments}},
  year = {2025},
  archivePrefix = {arXiv},
  eprint = {2507.21101},
  primaryClass = {cs.CR},
  doi = {10.48550/arXiv.2507.21101},
  url = {https://arxiv.org/abs/2507.21101},
  pdf = {https://arxiv.org/pdf/2507.21101},
  month = {7},
  abstract = {As mobile computing becomes central to digital interaction, researchers have turned their attention to adaptive authentication for its real-time, context- and behavior-aware verification capabilities. However, many implementations remain fragmented, inconsistently apply intelligent techniques, and fall short of user expectations. In this Systematization of Knowledge (SoK), we analyze 41 peer-reviewed studies since 2011 that focus on adaptive authentication in mobile environments. Our analysis spans seven dimensions: privacy and security models, interaction modalities, user behavior, risk perception, implementation challenges, usability needs, and machine learning frameworks. Our findings reveal a strong reliance on machine learning (64.3\%), especially for continuous authentication (61.9\%) and unauthorized access prevention (54.8\%). {AI}-driven approaches such as anomaly detection (57.1\%) and spatio-temporal analysis (52.4\%) increasingly shape the interaction landscape, alongside growing use of sensor-based and location-aware models.}
}

@inproceedings{Prince2025,
  author = {Brainerd Prince and Siddharth Siddharth and Subham Jalan and Hibah Ihsan Muhammad and Chaitanya Modi},
  title = {{{AI}-Driven Multimodal System for Enhancing Non-Verbal Communication in Public Speaking}},
  booktitle = {2025 {ASEE} Annual Conference \& Exposition},
  address = {Montreal, Quebec, Canada},
  month = {6},
  year = {2025},
  doi = {10.18260/1-2--57587},
  url = {https://peer.asee.org/57587},
  publisher = {ASEE},
  abstract = {This research introduces an {AI}-powered coaching system to help engineering students improve nonverbal communication skills. The system uses multimodal {AI} to analyze head pose (via {MediaPipe}), facial expressions (using {FER-2013} dataset), and verbal communication ({OpenAI} Whisper and Meta Llama3). The system provides personalized, real-time feedback on communication techniques. In performance evaluations, the {AI} outperformed human evaluators by 24.25\% in feedback quality across clarity, comprehensiveness, and specificity. The goal is to bridge the communication gap in engineering education by helping students develop both technical and interpersonal communication competencies.}
}

@inproceedings{Sharma2025,
  author = {Sahir Sharma and Conor Keighrey and James Lardner and Shane Gilligan and Niall Murray},
  title = {{Transforming Design Reviews with {XR}: A No-Code Media Experience Creation Strategy for Manufacturing Design}},
  booktitle = {Proceedings of the 2025 {ACM} International Conference on Interactive Media Experiences},
  series = {{IMX} '25},
  year = {2025},
  pages = {322--327},
  address = {Niter{\'{o}}i, Brazil},
  month = {6},
  doi = {10.1145/3706370.3727860},
  url = {https://dl.acm.org/doi/10.1145/3706370.3727860},
  publisher = {Association for Computing Machinery}
}
