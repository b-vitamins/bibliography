@inproceedings{zhang2025ngqa,
  author = {Zhang, Zheyuan and Li, Yiyang and Le, Nhi Ha Lan and Wang, Zehong and Ma, Tianyi and Galassi, Vincent and Murugesan, Keerthiram and Moniz, Nuno and Geyer, Werner and Chawla, Nitesh V and Zhang, Chuxu and Ye, Yanfang},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  note = {Introduces a new benchmark for KG question answering focused on the challenging domain of nutritional science. This task requires complex, personalized reasoning that integrates a nutritional KG with user-specific health data.},
  openalex = {W4405714818},
  title = {NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning},
  url = {https://research.ibm.com/publications/NGQA-A-Nutritional-Graph-Question-Answering-Benchmark-for-Personalized-Health-aware-Nutritional-Reasoning},
  year = {2025}
}

@inproceedings{colelough2025position,
  author = {Colelough, John and Regli, William C and Kemp, Charles and Hay, Nicholas},
  booktitle = {ICML Workshop on AI and the Pillars of Empirical Science},
  note = {A position paper arguing that monolithic black-box models like LLMs are inefficient for general intelligence. It posits that more effective systems must be built using multi-component neuro-symbolic approaches that incorporate modularity, flexible representations, and targeted prior knowledge.},
  title = {Position: Efficient General Intelligence requires Neuro-Symbolic Integration: Pillars, Benchmarks, and Beyond},
  url = {https://openreview.net/pdf?id=wjQqi7DJM2},
  year = {2025}
}

@inproceedings{hsu2025nesycoco,
  author = {Kamali, Danial and Barezi, Elham J. and Kordjamshidi, Parisa},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  note = {A modern extension of the NS-CL paradigm that leverages Large Language Models (LLMs) to generate symbolic program representations from natural language. This overcomes the limitation of predefined predicates, using the LLM's vocabulary as a flexible source of symbols and mapping them to differentiable neural modules.},
  number = {4},
  openalex = {W4409652862},
  pages = {4184--4193},
  title = {NeSyCoCo: A Neuro-Symbolic Concept Composer for Compositional Generalization},
  url = {https://arxiv.org/pdf/2412.15588},
  volume = {39},
  year = {2025}
}

@inproceedings{hu2025minictx,
  author = {Hu, Jiewen and Zhu, Thomas and Welleck, Sean},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {ICLR 2025 Oral. Introduces a benchmark that tests a model's ability to prove formal theorems that depend on new, long contexts not seen during training. Proposes "file-tuning" as a strong baseline, where models are trained using full file contexts, outperforming methods that rely only on state information.},
  openalex = {W4403853561},
  title = {miniCTX: Neural Theorem Proving with (Long-)Contexts},
  url = {https://arxiv.org/pdf/2408.03350},
  year = {2025}
}

@inproceedings{ahuja2025nesyc,
  author = {Ahuja, Ankur and Jain, Prithvijit and Agrawal, Rishabh and Bansal, Mohit},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Proposes a neuro-symbolic framework designed for continual learning in embodied AI. The system learns new skills and concepts in open-ended domains without catastrophic forgetting, a key challenge for building lifelong learning agents.},
  title = {NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains},
  url = {https://iclr.cc/virtual/2025/poster/31835},
  year = {2025}
}

@article{hu2025spherical,
  author = {Hu, Jun and Pathak, Jaideep and Tian, Yilun and Kashinath, Karthik and Kurth, Thorsten and Choudhury, Soukayna and Mardani, Morteza},
  journal = {arXiv preprint arXiv:2507.07011},
  note = {A state-of-the-art application of physics-informed principles in weather modeling. While a Neural Operator, it integrates physical constraints and a spherical multigrid architecture to push the forecasting horizon for global weather prediction beyond previous limits.},
  openalex = {W4409150548},
  title = {Spherical Multigrid Neural Operator for Improving Autoregressive Global Weather Forecasting},
  url = {https://arxiv.org/pdf/2507.07011},
  year = {2025}
}

@misc{li2025dolphin,
  archiveprefix = {arXiv},
  author = {Naik, Aaditya and Liu, Jason and Wang, Claire and Sethi, Amish and Dutta, Saikat and Naik, Mayur and Wong, Eric},
  eprint = {2410.03348},
  note = {Addresses the significant scaling challenges of neurosymbolic frameworks like LTNs. Introduces a framework that executes complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU, achieving orders-of-magnitude speedups.},
  openalex = {W4403900467},
  primaryclass = {cs.LG},
  title = {Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning},
  url = {https://arxiv.org/pdf/2410.03348},
  year = {2025}
}

@misc{liu2025proofaug,
  archiveprefix = {arXiv},
  author = {Liu, Haoxiong and Sun, Jiacheng and Li, Zhenguo and Yao, Andrew C.},
  eprint = {2501.18310},
  note = {Proposes a method to enhance LLM-based theorem provers by intelligently integrating automation tools (e.g., built-in tactics, ATPs) at multiple levels of granularity. Analyzes the structure of partial proofs to insert the right tool at the right time, setting new performance records with lower computational budgets.},
  openalex = {W4407012326},
  primaryclass = {cs.LG},
  title = {ProofAug: Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis},
  url = {https://arxiv.org/pdf/2501.18310},
  year = {2025}
}

@inproceedings{lu2025epigraph,
  author = {Lu, Songtao and Ding, Yanna and Horesh, Lior and Gao, Jianxi and Magdon-Ismail, Malik},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/ICASSP49660.2025.10889376},
  note = {Proposes a novel optimization framework to improve the logical consistency and reliability of Chain-of-Thought (CoT) reasoning in LLMs. This work represents a step towards integrating more formal, structured reasoning principles directly into the generation process of LLMs.},
  openalex = {W4408355543},
  title = {Epigraph Based Multilevel Optimization (EMO) for Enhancing Chain-of-Thought Reasoning Capabilities},
  url = {https://research.ibm.com/publications/epigraph-based-multilevel-optimization-emo-for-enhancing-chain-of-thought-reasoning-capabilities},
  year = {2025}
}

@article{mao2025neurosymbolic,
  author = {Mao, Jiayuan and Tenenbaum, Joshua B and Wu, Jiajun},
  journal = {Communications of the ACM},
  note = {to appear},
  title = {Neuro-Symbolic Concepts},
  url = {https://arxiv.org/pdf/2505.06191},
  year = {2025}
}

@inproceedings{marra2025klay,
  author = {Maene, Jaron and Derkinderen, Vincent and Dos Martires, Pedro Zuidberg},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Tackles a major scalability bottleneck in neurosymbolic AI by introducing KLay, a data structure for efficiently evaluating arithmetic circuits (used to represent logical formulas) on GPUs. Achieves speedups of multiple orders of magnitude, paving the way for scaling NeSy to larger applications.},
  title = {KLay: A GPU-Accelerated Framework for Scalable and Differentiable Reasoning},
  url = {https://arxiv.org/pdf/2410.11415},
  year = {2025}
}

@inproceedings{min2025hierarchical,
  author = {Cornelio, Cristina and Petruzzellis, Flavio and Lio, Pietro},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Enhances LLM-based planners with a neuro-symbolic approach using a KG-based Retrieval-Augmented Generation (RAG) system. Decomposes complex tasks into subtasks and uses a Symbolic Validator to ensure formal correctness and detect failures.},
  title = {Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification},
  url = {https://arxiv.org/pdf/2504.04578},
  year = {2025}
}

@article{neelan2025physics,
  author = {Neelan, Arun Govind and Bosco, Ferdin Sagai Don and Jarugumalli, Naveen Sagar and Vedarethinam, Suresh Balaji},
  journal = {arXiv preprint arXiv:2506.22413},
  note = {Investigates the sensitivity of PINNs to the choice of PDE formulation (conservative vs. non-conservative) when solving problems with shocks and discontinuities. Provides a comprehensive analysis across benchmark problems like the Burgers and Euler equations.},
  title = {Physics-Informed Neural Networks: Bridging the Divide Between Conservative and Non-Conservative Equations},
  url = {https://arxiv.org/pdf/2506.22413},
  year = {2025}
}

@inproceedings{rao2025neural,
  author = {Rao, Shubhra Kanti Karmaker and Eiers, Nathaniel and Lipizzi, Carlo},
  booktitle = {International Conference on Neurosymbolic Learning and Reasoning (NeSy)},
  note = {Introduces a framework for generalized theorem proving that bridges the gap between high proof success rates on benchmarks and practical formal verification. Uses a two-stage fine-tuning process (SFT for syntax, RL for semantics) to train an LLM to generate structured, verifiable proofs for Isabelle.},
  title = {Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification},
  url = {https://www.researchgate.net/publication/391120755_Neural_Theorem_Proving_Generating_and_Structuring_Proofs_for_Formal_Verification/fulltext/667364a594155b1731669460/Neural-Theorem-Proving-Generating-and-Structuring-Proofs-for-Formal-Verification.pdf},
  year = {2025}
}

@misc{sarker2025neurosymbolic,
  author = {Sarker, Md Kamruzzaman and Chen, Lu and Garcez, Artur d'Avila and Joty, Shafiq and Sanner, Scott and Baral, Chitta},
  booktitle = {AAAI Tutorial},
  note = {A tutorial that introduces how neurosymbolic AI can address key challenges in LLMs: Explainability (understanding decisions), Grounding (rooting knowledge in facts and domain expertise), and Instructability (guiding and controlling AI behavior).},
  title = {Neurosymbolic AI for EGI: Explainable, Grounded, and Instructable Generations},
  url = {https://aman.info/research/AAAI-2025-Tutorial_Neurosymbolic_AI_for_EGI_Explainable_Grounded_and_Instructable_Generations.pdf},
  year = {2025}
}

@inproceedings{stein2025blendrl,
  author = {Shindo, Hikaru and Delfosse, Quentin and Dhami, Devendra Singh and Kersting, Kristian},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Introduces a framework that harmoniously integrates both neural and symbolic policies. The system can leverage symbolic rules when applicable but fall back on a neural policy when symbolic knowledge is incomplete or incorrect, outperforming purely neural or symbolic baselines.},
  openalex = {W4403575085},
  title = {BlendRL: A Framework for Merging Symbolic and Neural Policy Learning},
  url = {https://openreview.net/pdf?id=60i0ksMAhd},
  year = {2025}
}

@inproceedings{chen2025copinn,
  author = {Chen, Jiayu and Han, Ling and Lu, Yang and Meng, Chuhua},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Tackles the "Unbalanced Prediction Problem" where PINNs fail near physical boundaries. Proposes a cognitive framework that imitates human learning from easy to hard, dynamically evaluating sample difficulty and using a scheduler to progressively optimize, significantly reducing errors in stubborn regions.},
  title = {CoPINN: Cognitive Physics-Informed Neural Networks},
  url = {https://icml.cc/virtual/2025/poster/46458},
  year = {2025}
}

@inproceedings{ucedasosa2025conceptual,
  author = {Uceda-Sosa, Rosario and Chang, Maria and Ramamurthy, Karthikeyan Natesan and Singh, Moninder},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  note = {Proposes a methodology for diagnosing inconsistencies and errors in KGs and LLMs by analyzing their underlying conceptual structures. This work aims to improve the reliability of neuro-symbolic systems that rely on these knowledge sources.},
  title = {Conceptual Diagnostics for Knowledge Graphs and Large Language Models},
  url = {https://research.ibm.com/publications/Conceptual-Diagnostics-for-Knowledge-Graphs-and-Large-Language-Models},
  year = {2025}
}

@article{wagner2025symdqn,
  author = {Amador, Ivo and Gierasimczuk, Nina},
  journal = {arXiv preprint arXiv:2504.02654},
  note = {Augments a Dueling Deep Q-Network (DuelDQN) architecture with modules based on LTNs. The LTN modules use logic to help the agent learn the environment's structure, predict rewards, and guide action policies, significantly improving learning performance and precision.},
  title = {SymDQN: Symbolic Knowledge and Reasoning in Neural Network-based Reinforcement Learning},
  url = {https://arxiv.org/pdf/2504.02654},
  year = {2025}
}

@article{wang2025reasoning,
  author = {Wang, Fengli and Yu, Haowei and Zhang, Liang and Tang, Jianhua},
  journal = {Mathematics},
  note = {A recent survey that provides a unified framework for understanding the evolution of AI reasoning. It categorizes representative methods, including rule learning (∂ILP), differentiable proof engines (NTPs), and concept learners (NS-CL), bridging symbolic logic, neural computation, and generative reasoning.},
  number = {11},
  openalex = {W4410694526},
  pages = {1707},
  title = {AI Reasoning in Deep Learning Era: From Symbolic AI to Neural-Symbolic AI},
  url = {https://www.mdpi.com/2227-7390/13/11/1707/pdf},
  volume = {13},
  year = {2025}
}

@article{zhang2025learning,
  author = {Zou, Zongren and Wang, Zhicheng and Karniadakis, George Em},
  journal = {arXiv preprint arXiv:2503.06320},
  note = {Addresses the challenge of capturing multiple solutions in nonlinear differential equations. Proposes a simple yet effective approach using an ensemble of PINNs with random initializations to uncover different solution modes or patterns.},
  title = {Learning and discovering multiple solutions using physics-informed neural networks with random initialization and deep ensemble},
  url = {https://arxiv.org/pdf/2503.06320},
  year = {2025}
}

@article{colelough2025neurosymbolic,
  author = {Colelough, Brandon C. and Regli, William},
  journal = {arXiv preprint arXiv:2501.05435},
  note = {A systematic review of 167 papers published between 2020 and 2024. Finds that research is concentrated in learning/inference, logic/reasoning, and knowledge representation, but identifies significant gaps in explainability, trustworthiness, and meta-cognition, pointing to key future research directions.},
  openalex = {W4406273454},
  title = {Neuro-Symbolic AI in 2024: A Systematic Review},
  url = {https://arxiv.org/pdf/2501.05435},
  year = {2025}
}

@article{delong2024neurosymbolic,
  author = {DeLong, Lauren Nicole and Mir, Ramon Fernández and Fleuriot, Jacques D.},
  doi = {10.1109/TNNLS.2024.3420218},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  note = {in press},
  openalex = {W4400771000},
  pages = {1--21},
  title = {Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey},
  url = {https://arxiv.org/pdf/2302.07200},
  year = {2024}
}

@inproceedings{desmet2024neurosymbolic,
  author = {De Smet, Lennert and Venturato, Gabriele and De Raedt, Luc and Marra, Giuseppe},
  booktitle = {ICML Workshop on Structured Probabilistic Inference and Generative Modeling (SPIGM)},
  note = {ICML 2024 Workshop SPIGM Oral. Merges deep probabilistic Markov models with logic to handle both sequential probabilistic dependencies and logical rules. Proposes a scalable inference and learning strategy combining Bayesian statistics, automated reasoning, and gradient estimation.},
  openalex = {W4405562649},
  title = {Neurosymbolic Markov Models},
  url = {https://openreview.net/pdf?id=epOXthx0ea},
  year = {2024}
}

@misc{dumancic2024declarative,
  archiveprefix = {arXiv},
  author = {Hinnerichs, Tilman and Manhaeve, Robin and Marra, Giuseppe and Dumančić, Sebastijan},
  eprint = {2405.09521},
  note = {Addresses a core limitation of neural predicates in systems like DeepProbLog: their functional, non-declarative nature. Proposes a general framework for fully declarative neural predicates based on prototype networks, enabling a single trained model to answer arbitrary queries (e.g., generating an image from a label, not just classifying an image).},
  openalex = {W4397810444},
  primaryclass = {cs.AI},
  title = {Towards a fully declarative neuro-symbolic language},
  url = {https://arxiv.org/pdf/2405.09521},
  year = {2024}
}

@inproceedings{fang2024large,
  author = {Fang, Meng and Deng, Shilong and Zhang, Yudi and Shi, Zijing and Chen, Ling and Pechenizkiy, Mykola and Wang, Jun},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v38i16.29754},
  note = {Investigates the use of LLMs as symbolic reasoners in text-based games. Proposes an LLM agent that interacts with external symbolic modules (e.g., calculators, navigators) to tackle symbolic challenges and achieve in-game objectives, demonstrating strong zero-shot reasoning capabilities.},
  number = {16},
  openalex = {W4393160365},
  pages = {17985--17993},
  title = {Large Language Models Are Neurosymbolic Reasoners},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/29754/31298},
  volume = {38},
  year = {2024}
}

@inproceedings{gao2024physics,
  author = {Gao, Siyuan and Xia, Lu and Liu, Dejun and Lu, Lu and Zhang, Bo},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Introduces Derivative-Constrained PINNs (DC-PINNs), a framework for solving PDEs that must also satisfy additional equality or inequality constraints. Employs a constraint-aware loss with self-adaptive balancing to improve solutions for problems in quantitative finance.},
  title = {Physics-Informed Neural Networks for Derivative-Constrained PDEs},
  url = {https://icml.cc/virtual/2024/poster/36827},
  year = {2024}
}

@misc{gonzalez2024neuro,
  archiveprefix = {arXiv},
  author = {Cucumides, Tamara and Daza, Daniel and Barceló, Pablo and Cochez, Michael and Geerts, Floris and Reutter, Juan L and Romero, Miguel},
  eprint = {2310.04598},
  note = {Introduces a framework for answering arbitrary graph pattern queries, including complex cyclic queries that are beyond the scope of most neuro-symbolic models. Employs an approximation scheme that unravels cyclic patterns into tree-like traversals, embedding symbolic bias into the query execution.},
  openalex = {W4387355302},
  primaryclass = {cs.AI},
  title = {A Neuro-Symbolic Framework for Answering Graph Pattern Queries in Knowledge Graphs},
  url = {https://arxiv.org/pdf/2310.04598},
  year = {2024}
}

@inproceedings{ishay2024think,
  author = {Ishay, Adam and Yang, Zhun and Lee, Joohyung and Kang, Ilgu and Lim, Dongjae},
  booktitle = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  note = {Enhances neuro-symbolic models for complex causal and temporal reasoning in video. Argues for using symbolic reasoning not just to aggregate neural outputs, but to orchestrate the entire computation, constructing a causal graph to guide neural perception and simulation for answering counterfactual questions.},
  openalex = {W4394625496},
  pages = {4180--4189},
  title = {Think Before You Simulate: Symbolic Reasoning To Orchestrate Neural Computation for Counterfactual Question Answering},
  url = {https://openaccess.thecvf.com/content/WACV2024/papers/Ishay_Think_Before_You_Simulate_Symbolic_Reasoning_To_Orchestrate_Neural_Computation_WACV_2024_paper.pdf},
  year = {2024}
}

@inproceedings{li2024logicity,
  author = {Li, Bowen and Li, Zhaoyu and Du, Qiwei and Luo, Jinqi and Wang, Wenshan and Xie, Yaqi and Stepputtis, Simon and Wang, Chen and Sycara, Katia P. and Ravikumar, Pradeep Kumar and Gray, Alexander G. and Si, Xujie and Scherer, Sebastian},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  note = {Introduces a new simulator and benchmark based on customizable first-order logic (FOL) for an urban-like environment. It is designed to test long-horizon reasoning and complex multi-agent interactions, addressing key gaps in existing NeSy benchmarks.},
  openalex = {W4404451906},
  title = {LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation},
  url = {https://arxiv.org/pdf/2411.00773},
  year = {2024}
}

@inproceedings{li2024pinnacle,
  author = {Hao, Zhongkai and Yao, Jiachen and Su, Chang and Su, Hang and Wang, Ziao and Lu, Fanzhi and Xia, Zeyu and Zhang, Yichi and Liu, Songming and Lu, Lu and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  note = {Introduces a comprehensive benchmark to address the lack of systematic comparison of PINN methods. PINNacle includes a diverse dataset of PDEs with challenges like complex geometry, multi-scale phenomena, and high dimensionality, along with a toolbox of over 10 state-of-the-art PINN variants.},
  openalex = {W4380993950},
  title = {PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs},
  url = {https://arxiv.org/pdf/2306.08827},
  year = {2024}
}

@inproceedings{li2024structure,
  author = {Li, Xin and Tang, Jinshan and Wang, Bin and Wu, Yizhou},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  note = {Proposes structure-preserving PINNs that incorporate prior knowledge about a system's energy or stability into the loss function. This approach improves numerical accuracy and enhances the robustness of downstream tasks, such as image recognition against adversarial attacks.},
  openalex = {W4390779285},
  pages = {3876--3884},
  title = {Structure-Preserving Physics-Informed Neural Networks with Energy or Lyapunov Structure},
  url = {https://www.ijcai.org/proceedings/2024/0428.pdf},
  year = {2024}
}

@inproceedings{li2024survey,
  author = {Li, Zhaoyu and Sun, Jialiang and Murphy, Logan and Su, Qidong and Li, Zenan and Zhang, Xian and Yang, Kaiyu and Si, Xujie},
  booktitle = {Conference on Language Modeling (COLM)},
  note = {A comprehensive survey of the rapidly advancing field of neural theorem proving, with a strong focus on the role of LLMs. It covers tasks from premise selection to proof generation and discusses the various ways LLMs are being integrated with formal proof assistants like Lean and Isabelle.},
  openalex = {W4394868324},
  title = {A Survey on Deep Learning for Theorem Proving},
  url = {https://arxiv.org/pdf/2404.09939},
  year = {2024}
}

@inproceedings{liu2024endtoend,
  author = {Luo, Lirui and Xu, Guoxi and Duan, Meng and Jiang, Hangtian and Li, Jing and Xu, Chongjie},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Presents a framework that jointly learns structured state representations and symbolic policies from visual input. A key contribution is a pipeline that prompts GPT-4 to generate natural language explanations for the learned symbolic policies and decisions, enhancing accessibility.},
  openalex = {W4394318950},
  title = {End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations},
  url = {https://arxiv.org/pdf/2403.12451},
  year = {2024}
}

@article{marra2024from,
  author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
  journal = {Artificial Intelligence},
  note = {A survey co-authored by key figures in both Statistical Relational AI (StarAI) and NeSy, including LTN and DeepProbLog researchers. It draws parallels between the two fields, using frameworks like LTN to illustrate the evolution from structured probabilistic models to modern neurosymbolic systems.},
  pages = {104062},
  title = {From Statistical Relational to Neuro-Symbolic Artificial Intelligence: A survey},
  url = {https://www.oru.se/english/employee/luc_de-raedt},
  volume = {328},
  year = {2024}
}

@article{marra2024statistical,
  author = {Marra, Giuseppe and Dumančić, Sebastijan and Manhaeve, Robin and De Raedt, Luc},
  journal = {Artificial Intelligence},
  note = {A comprehensive survey that traces the evolution from statistical relational AI to modern neurosymbolic systems. It uses KG reasoning as a primary case study to illustrate how principles of relational learning have been integrated with deep learning.},
  openalex = {W4391493576},
  pages = {104062},
  title = {From statistical relational to neurosymbolic artificial intelligence: A survey},
  url = {https://www.oru.se/english/employee/luc_de-raedt},
  volume = {328},
  year = {2024}
}

@inproceedings{thakur2024putnambench,
  author = {Tsoukalas, George and Lee, Jasper and Jennings, John and Xin, Jimmy and Ding, Michelle and Jennings, Michael and Thakur, Amitayush and Chaudhuri, Swarat},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
  note = {Introduces a challenging new multi-language benchmark for neural theorem provers based on problems from the premier undergraduate mathematics competition. Demonstrates that existing neural and symbolic approaches can only solve a handful of problems, establishing a difficult open challenge.},
  openalex = {W4403753499},
  title = {PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1582eaf9e0cf349e1e5a6ee453100aa1-Paper-Datasets_and_Benchmarks_Track.pdf},
  year = {2024}
}

@inproceedings{vlasselaert2024deepsoftlog,
  author = {Vlasselaert, Jonas and Manhaeve, Robin and Marra, Giuseppe and De Raedt, Luc},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Generalizes DeepProbLog by incorporating "soft unification," allowing the system to reason about the similarity of concepts. Defines key properties for soft unification operators and demonstrates improved performance on NeSy benchmarks.},
  title = {DeepSoftLog: A Differentiable Framework that Generalizes Probabilistic Logic Programming with Soft Unification},
  url = {https://openreview.net/pdf?id=s86M8naPSv},
  year = {2024}
}

@inproceedings{wang2024causality,
  author = {Wang, Ye and Wang, Tingxiong and Chen, Qinxu and Chen, Yuxing},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  note = {Addresses training difficulties for evolutionary PDEs by enforcing temporal causality. Uses implicit time differencing schemes and transfer learning to sequentially update PINNs over time, improving accuracy and efficiency by a factor of 4-40x on challenging benchmarks.},
  openalex = {W4401024902},
  pages = {4506--4514},
  title = {Causality-enhanced Discreted Physics-informed Neural Networks for Predicting Evolutionary Equations},
  url = {https://www.ijcai.org/proceedings/2024/0497.pdf},
  year = {2024}
}

@article{wang2024essential,
  author = {Wang, Wei},
  journal = {Towards Data Science},
  note = {A practitioner-focused analysis of key review papers on PINNs. It highlights papers that provide strong fundamentals, historical context, and practical resources like implementation examples and software tools, as well as those covering recent theoretical advances and applications.},
  title = {Essential Review Papers on Physics-Informed Neural Networks: A Curated Guide for Practitioners},
  url = {https://towardsdatascience.com/essential-review-papers-on-physics-informed-neural-networks-a-curated-guide-for-practitioners},
  year = {2024}
}

@inproceedings{wang2024lego,
  author = {Wang, Haiming and Xin, Huajian and Zheng, Chuanyang and Liu, Zhengying and Cao, Qingxing and Huang, Yinya and Xiong, Jing and Shi, Han and Xie, Enze and Yin, Jian and Li, Zhenguo and Liang, Xiaodan},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Presents a novel approach for neural theorem proving that utilizes a growing skill library of verified lemmas. This modular approach augments the capability of LLMs, enabling them to tackle more intricate mathematical problems and better bridge the gap between human and formal proofs.},
  title = {LEGO-Prover: Neural Theorem Proving with Growing Libraries of Lemmas},
  url = {https://openreview.net/pdf?id=3f5PALef5B},
  year = {2024}
}

@inproceedings{wang2024ropinn,
  author = {Wu, Haixu and Luo, Huakun and Ma, Yuezhou and Wang, Jianmin and Long, Mingsheng},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes a new training paradigm that extends optimization from scattered points to continuous neighborhood regions. This region-based optimization is shown to decrease generalization error and consistently boosts the performance of diverse PINNs without extra computational cost.},
  openalex = {W4398808263},
  title = {RoPINN: Region Optimized Physics-Informed Neural Networks},
  url = {https://neurips.cc/virtual/2024/poster/93144},
  year = {2024}
}

@inproceedings{winters2024neurosymbolic,
  author = {Winters, Thomas and Marra, Giuseppe and De Raedt, Luc},
  booktitle = {European Conference on Artificial Intelligence (ECAI)},
  note = {Applies a probabilistic logic-based shield to the challenging MiniHack environment. The shield, specified in DeepProbLog, guides the agent's exploration by making certain actions more or less likely based on symbolic knowledge, improving performance in complex, procedurally generated worlds.},
  openalex = {W4409360999},
  pages = {2814--2821},
  title = {Neurosymbolic Reinforcement Learning: Playing MiniHack with Probabilistic Logic Shields},
  url = {https://www.researchgate.net/publication/380696770_Neurosymbolic_Reinforcement_Learning_Playing_MiniHack_with_Probabilistic_Logic_Shields},
  year = {2024}
}

@inproceedings{yuan2024neurosymbolic,
  author = {Li, Zenan and Zhou, Zhi and Yao, Yuan and Zhang, Xian and Li, Yu-Feng and Cao, Chun and Yang, Fan and Ma, Xiaoxing},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Develops a neuro-symbolic framework to generate new, valid mathematical problems for training LLMs. It uses an LLM for intuitive "informalization" and a symbolic math solver for precise "formalization" and mutation, creating a high-quality data generation loop that improves the reasoning capabilities of fine-tuned models.},
  title = {Neuro-Symbolic Data Generation for Math Reasoning},
  url = {https://neurips.cc/virtual/2024/poster/96151},
  year = {2024}
}

@inproceedings{desmet2023deepseaproblog,
  author = {De Smet, Lennert and Dos Martires, Pedro Zuidberg and Manhaeve, Robin and Marra, Giuseppe and Kimmig, Angelika and De Raedt, Luc},
  booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
  note = {Extends DeepProbLog to handle continuous probability distributions, a major limitation of the original framework. Leverages weighted model integration (WMI) to perform inference and learning over mixed discrete-continuous domains.},
  pages = {529--538},
  title = {Neural Probabilistic Logic Programming in Discrete-Continuous Domains},
  url = {https://proceedings.mlr.press/v216/de-smet23a/de-smet23a.pdf},
  volume = {216},
  year = {2023}
}

@article{acharya2023neurosymbolic,
  author = {Acharya, K. and Raza, W. and Dourado Jr, C. M. J. M. and Velasquez, A. and Song, H.},
  doi = {10.1109/TAI.2023.3311428},
  journal = {IEEE Transactions on Artificial Intelligence},
  note = {A comprehensive survey that provides a taxonomy for Neurosymbolic RL, categorizing works into "Learning for Reasoning," "Reasoning for Learning," and "Learning-Reasoning" models. It analyzes the neural, symbolic, and RL components of numerous key papers.},
  number = {3},
  openalex = {W4386432078},
  pages = {467--484},
  title = {Neurosymbolic Reinforcement Learning and Planning: A Survey},
  url = {https://arxiv.org/pdf/2309.01038},
  volume = {4},
  year = {2023}
}

@misc{gibaut2023neurosymbolic,
  archiveprefix = {arXiv},
  author = {Gibaut, Wandemberg and Pereira, Leonardo and Grassiotto, Fabio and Osorio, Alexandre and Gadioli, Eder and Munoz, Amparo and Gomes, Sildolfo and dos Santos, Claudio},
  eprint = {2305.08876},
  note = {A recent survey that provides an overview of the state-of-the-art in neurosymbolic AI. It classifies and compares different models, including LTNs, and discusses their applications, for instance in ensuring fairness constraints in machine learning models.},
  openalex = {W4377086261},
  primaryclass = {cs.AI},
  title = {Neurosymbolic AI and its Taxonomy: a survey},
  url = {https://arxiv.org/pdf/2305.08876},
  year = {2023}
}

@inproceedings{luo2023differentiable,
  author = {Chen, Shengyuan and Cai, Yunfeng and Fang, Huang and Huang, Xiao and Sun, Mingming},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes DiffLogic, a unified neuro-symbolic framework that combines KG embedding models with rule-based models. Uses Probabilistic Soft Logic (PSL) to enable end-to-end differentiable optimization, allowing for the alternating update of embeddings and weighted logical rules.},
  title = {Differentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphs},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5965f3a748a8d41415db2bfa44635cc3-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{luo2023neurosymbolic,
  author = {Yang, Sen and Li, Xin and Cui, Leyang and Bing, Lidong and Lam, Wai},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  note = {Presents a framework where a neural LLM represents the knowledge of a problem, while a separate, LLM-free symbolic solver performs deliberate reasoning. This separation ensures that the generated reasoning proofs are causal and reliable due to the deterministic nature of the symbolic solver.},
  openalex = {W4388788666},
  pages = {10842--10856},
  title = {Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs},
  url = {https://arxiv.org/pdf/2311.09802},
  year = {2023}
}

@inproceedings{marconato2024not,
  author = {Marconato, Emanuele and Teso, Stefano and Vergari, Antonio and Passerini, Andrea},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Investigates a critical failure mode in neuro-symbolic concept learners: "reasoning shortcuts," where the model achieves high accuracy by leveraging concepts with unintended semantics. Characterizes the conditions that cause these shortcuts and proposes mitigation strategies.},
  openalex = {W4379089620},
  title = {Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e560202b6e779a82478edb46c6f8f4dd-Abstract-Conference.html},
  year = {2023}
}

@inproceedings{marra2023how,
  author = {Loconte, Lorenzo and Di Mauro, Nicola and Peharz, Robert and Vergari, Antonio},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Reframes popular KG embedding approaches as structured computational graphs whose semantics can be used for probabilistic circuits. Injects logical constraints so that predictions conform to pre-defined domain knowledge, turning discriminative embeddings into generative models.},
  openalex = {W4378501265},
  title = {How to Turn Your Knowledge Graph Embeddings into Generative Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/341e62b4cc923c40a1334c25bd450918-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{marra2023pseudo,
  author = {Ahmed, Kareem and Chang, Kai-Wei and Van den Broeck, Guy},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes a method to apply logical constraints to autoregressive models like LLMs. Uses this pseudo-semantic loss to steer LLM generation over knowledge graphs, ensuring the output better conforms to factual and logical consistency.},
  openalex = {W4389500889},
  title = {A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0c9e6a49586144e3341c8242a382216d-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{marra2023soft,
  author = {Maene, J. and De Raedt, Luc},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Shows how learned embeddings can be integrated into a probabilistic logic program to perform "soft unification" between similar concepts. This allows for more flexible reasoning over KGs where entities may be semantically related but not identical.},
  openalex = {W3205374622},
  title = {Soft-Unification in Deep Probabilistic Logic},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/717e25289895964893450974802c6e64-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{rafanelli2023symbolic,
  author = {Rafanelli, Andrea and Pozanco, Alberto and Scala, Enrico},
  booktitle = {Workshop on Recent Advances in Constraint Reasoning and Artificial Intelligence (RCRA)},
  note = {Defines an approach where logical rules, inspired by subsumption architecture, can influence an RL agent's exploration of an environment. The rules can be triggered by specific conditions or checked periodically to guide the agent's behavior at critical stages.},
  pages = {paper9},
  title = {Symbolic Knowledge-Infused RL for Abstract-Level Planning and Control},
  url = {https://ceur-ws.org/Vol-3585/paper9_RCRA8.pdf},
  year = {2023}
}

@article{sharma2023review,
  author = {Sharma, Pushan and Chung, Wai Tong and Akoush, Bassem and Ihme, Matthias},
  journal = {Energies},
  note = {A comprehensive review focusing on the application of PIML techniques to fluid mechanics. It provides a historical perspective and examines existing applications in complex turbulent flows, highlighting the challenges and opportunities.},
  number = {5},
  openalex = {W4322743280},
  pages = {2343},
  title = {A Review of Physics-Informed Machine Learning in Fluid Mechanics},
  url = {https://www.mdpi.com/1996-1073/16/5/2343/pdf},
  volume = {16},
  year = {2023}
}

@inproceedings{taniguchi2023explainable,
  author = {Kimura, Daiki and Zecevic, Stefan and Chaudhury, Subhajit and Swaminathan, Sarathkrishna and Agravante, Don Joven and Tatsubori, Michiaki and Munawar, Asim and Gray, Alexander},
  booktitle = {IJCAI Workshop on Explainable Artificial Intelligence},
  note = {Addresses the challenge of making neuro-symbolic methods truly explainable to human operators. Proposes and develops a graphical interface for NeSy-RL that allows a user to visualize and edit the trained knowledge within the network.},
  title = {Explainable Neuro-Symbolic Reinforcement Learning},
  url = {https://research.ibm.com/publications/explainable-neuro-symbolic-reinforcement-learning},
  year = {2023}
}

@inproceedings{vasan2023instructing,
  author = {Qiu, Wenjie and Mao, Wensen and Zhu, He},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Develops a framework that specifies task-oriented policies using linear temporal logic (LTL) without requiring additional training over the LTL space. This allows for flexible and compositional instruction of RL agents.},
  title = {Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a64c844b1a7337f22513a4049755146c-Paper-Conference.pdf},
  year = {2023}
}

@inproceedings{arakelyan2023adapting,
  author = {Arakelyan, Erik and Minervini, Pasquale and Daza, Daniel and Cochez, Michael and Augenstein, Isabelle},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Expands on previous work showing that complex logical queries over KGs can be answered by decomposing them into sub-queries solved by a simple link predictor. This adaptation improves data efficiency for complex query answering.},
  openalex = {W4318751789},
  pages = {31024--31052},
  title = {Adapting Neural Link Predictors for Data-Efficient Complex Query Answering},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/55c518a17bd17dcb69aa14d69d085994-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{yang2023concept,
  author = {Hsu, Joy and Mao, Jiayuan and Tenenbaum, Joshua B. and Wu, Jiajun},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Develops a model for visual reasoning across various domains by grounding concepts in KGs. Uses an LLM to generate programs in First-Order Logic (FOL) for language queries, which are then executed over the KG and visual data.},
  openalex = {W4387947659},
  title = {What's Left? Concept Grounding with Logic-Enhanced Foundation Models},
  url = {https://neurips.cc/virtual/2023/poster/70248},
  year = {2023}
}

@inproceedings{zhang2023neurosymbolic,
  author = {Zhang, Zenan and Fan, Ziqi and Chen, Jingyue and Wang, Meng and Nasrabadi, Nasser and Blasch, Erik and Ling, Haibin},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes a framework that fuses neural network training, symbol grounding, and logical constraint synthesis into a coherent end-to-end process. Cast as a game between two optimization problems (neural learning and symbolic constraint learning), it generates explicit, interpretable logical constraints alongside the neural network.},
  openalex = {W4404314654},
  title = {Neuro-symbolic Learning Yielding Logical Constraints},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/c89f588018f40c7f23c2c2f1013a18a5-Paper-Conference.pdf},
  year = {2023}
}

@article{cuomo2022scientific,
  author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
  journal = {Journal of Scientific Computing},
  note = {A forward-looking review that assesses the state of the PINN field and identifies key open challenges. It discusses issues related to training pathologies, theoretical foundations, and the need for robust software implementations.},
  number = {3},
  openalex = {W4221161478},
  pages = {88},
  title = {Scientific Machine Learning Through Physics-Informed Neural Networks: Where We Are and What's Next},
  url = {https://link.springer.com/content/pdf/10.1007/s10915-022-01939-z.pdf},
  volume = {92},
  year = {2022}
}

@article{hao2022physics,
  author = {Hao, Zhongkai and Liu, Songming and Zhang, Yichi and Ying, Chengyang and Feng, Yao and Su, Hang and Zhu, Jun},
  journal = {arXiv preprint arXiv:2211.08064},
  note = {A systematic review of the PIML paradigm, categorizing recent developments from the perspectives of machine learning tasks, representations of physical priors, and methods for incorporating that prior knowledge.},
  openalex = {W4309208391},
  title = {Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications},
  url = {https://arxiv.org/pdf/2211.08064},
  year = {2022}
}

@incollection{manhaeve2022neurosymbolic,
  author = {Manhaeve, Robin and Marra, Giuseppe and Demeester, Thomas and Dumančić, Sebastijan and Kimmig, Angelika and De Raedt, Luc},
  booktitle = {Neuro-Symbolic Artificial Intelligence: The State of the Art},
  doi = {10.3233/FAIA210354},
  note = {A chapter by the creators of DeepProbLog that positions their work within the broader NeSy landscape. It frames the ideal NeSy system as an integration of all three components, citing LTNs as a key example of the neural-logical connection.},
  openalex = {W4206290703},
  pages = {173--191},
  publisher = {IOS Press},
  title = {Neuro-Symbolic AI = Neural + Logical + Probabilistic AI},
  url = {https://ebooks.iospress.nl/volumearticle/58847},
  volume = {342},
  year = {2022}
}

@article{badreddine2022logic,
  author = {Badreddine, Samy and Garcez, Artur d'Avila and Serafini, Luciano and Spranger, Michael},
  journal = {Artificial Intelligence},
  note = {The definitive journal paper formalizing the LTN framework. Introduces "Real Logic," a fully differentiable first-order logic, and shows how it provides a uniform language to represent and compute a wide range of AI tasks, including classification, clustering, relational learning, and query answering.},
  openalex = {W3216178672},
  pages = {103649},
  title = {Logic Tensor Networks},
  url = {https://arxiv.org/pdf/2012.13635},
  volume = {303},
  year = {2022}
}

@inproceedings{mombelli2022detect,
  author = {Mitchener, Ludovico and Tuckey, David and Crosby, Matthew and Russo, Alessandra},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  note = {Introduces a three-part framework: a computer vision module to 'Detect' objects, an Inductive Logic Programming (ILP) module to 'Understand' and learn a symbolic meta-policy, and a set of pre-trained DRL policies to 'Act'. Demonstrates strong performance and sample efficiency on cognitive reasoning tasks.},
  openalex = {W4225845614},
  pages = {5139--5147},
  title = {Detect, Understand, Act: A Neuro-Symbolic Hierarchical Reinforcement Learning Framework (Extended Abstract)},
  url = {https://www.ijcai.org/proceedings/2022/742},
  year = {2022}
}

@incollection{serafini2022chapter17,
  author = {Serafini, Luciano and Garcez, Artur d'Avila},
  booktitle = {Handbook of Neuro-Symbolic AI},
  note = {A comprehensive chapter that presents the LTN framework and illustrates its use on knowledge completion, semi-supervised learning, and embedding learning. Introduces PROTO-LTN, an extension for few- and zero-shot learning.},
  openalex = {W4205280255},
  publisher = {IOS Press},
  title = {Chapter 17: Logic Tensor Networks: Theory and Applications},
  url = {https://www.researchgate.net/publication/357535461_Chapter_17_Logic_Tensor_Networks_Theory_and_Applications},
  year = {2022}
}

@misc{vanderschaar2022workshop,
  author = {van der Schaar, Mihaela and Zhang, Cheng and Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
  booktitle = {NeurIPS Workshop},
  note = {A workshop that brought together researchers interested in integrating formalizations of causality with neuro-symbolic methods. The goal is to develop next-generation AI systems that can perform causal inference as efficiently as modern neural models.},
  title = {Workshop on neuro Causal and Symbolic AI (nCSI)},
  url = {https://neurips.cc/virtual/2022/workshop/50011},
  year = {2022}
}

@inproceedings{winters2022deepstochlog,
  author = {Winters, Thomas and Marra, Giuseppe and Manhaeve, Robin and De Raedt, Luc},
  booktitle = {AAAI Conference on Artificial Intelligence},
  note = {Proposes an alternative to DeepProbLog based on stochastic logic programs, which define a distribution over derivations rather than possible worlds. This approach is shown to scale more effectively and achieves state-of-the-art results on challenging NeSy tasks.},
  openalex = {W3176736844},
  pages = {10090--10100},
  title = {DeepStochLog: Neural Stochastic Logic Programming},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/21248/20997},
  volume = {36},
  year = {2022}
}

@inproceedings{jothimurugan2021learning,
  author = {Jothimurugan, Kishor and Alur, Rajeev and Bastani, Osbert},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes an algorithm that learns probabilistic logical rules describing the safety shield from experience. This allows the agent to adapt its safety constraints over time, leading to less conservative and more efficient policies compared to relying on a fixed, hand-crafted shield.},
  pages = {13892--13904},
  title = {Learning logical rules for safe exploration in reinforcement learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f6ff0820478450059b38872a39a6748-Paper.pdf},
  year = {2021}
}

@article{karniadakis2021physics,
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal = {Nature Reviews Physics},
  note = {A comprehensive review article from one of the originators of PINNs. It provides a high-level overview of the PIML field, contextualizing PINNs within the broader landscape of scientific machine learning and outlining key challenges and opportunities.},
  number = {6},
  openalex = {W3163993681},
  pages = {422--440},
  title = {Physics-informed machine learning},
  url = {https://www.nature.com/articles/s42254-021-00314-5.pdf},
  volume = {3},
  year = {2021}
}

@inproceedings{landajuela2021deep,
  author = {Landajuela, Mikel and Petersen, Brenden K and Kim, Sookyung and Santiago, Claudio P and Glatt, Ruben and Mundhenk, Nathan and Pettit, Jacob F and Faissol, Daniel},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Proposes generating symbolic policies directly using an autoregressive recurrent neural network. Introduces an "anchoring" algorithm to distill pre-trained neural policies into fully symbolic policies for multi-dimensional action spaces.},
  pages = {29862--29874},
  title = {Deep Symbolic Policy: Unifying Neural and Symbolic Planning for Abstract-Level Decision Making},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/932c53392a013a728ebd46175b93193e-Paper.pdf},
  year = {2021}
}

@inproceedings{li2021fourier,
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Introduces a different paradigm for learning solutions to PDEs. The Neural Operator learns a mapping between function spaces, directly approximating the solution operator of a PDE family. It operates in Fourier space, making it highly efficient for a range of problems.},
  openalex = {W3118310857},
  title = {Fourier Neural Operator for Parametric Partial Differential Equations},
  url = {https://openreview.net/pdf?id=e_3f4c-hEAE},
  year = {2021}
}

@article{lu2021deepxde,
  author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  doi = {10.1137/19M1274067},
  journal = {SIAM Review},
  note = {Describes a popular and powerful open-source library for PIML. DeepXDE provides a user-friendly interface for defining and solving problems with PINNs, making the technology accessible to a broader range of scientists and engineers.},
  number = {1},
  openalex = {W3014009018},
  pages = {208--228},
  title = {DeepXDE: A deep learning library for solving differential equations},
  url = {https://epubs.siam.org/doi/10.1137/19M1274067},
  volume = {63},
  year = {2021}
}

@inproceedings{arabshahi2021conversational,
  author = {Arabshahi, Forough and Lee, Jennifer and Gawarecki, Mikayla and Mazaitis, Kathryn and Azaria, Amos and Mitchell, Tom},
  booktitle = {AAAI Conference on Artificial Intelligence},
  note = {Applies a neuro-symbolic theorem prover to the task of identifying unstated commonsense presumptions in human commands. Proposes an interactive framework where the system can conversationally evoke knowledge from humans to complete its reasoning chains when its own knowledge is incomplete.},
  openalex = {W3176075895},
  pages = {4902--4911},
  title = {Conversational Neuro-Symbolic Commonsense Reasoning},
  url = {https://cdn.aaai.org/ojs/16623/16623-13-20117-1-2-20210518.pdf},
  year = {2021}
}

@inproceedings{manhaeve2021approximate,
  author = {Manhaeve, Robin and Marra, Giuseppe and De Raedt, Luc},
  booktitle = {International Conference on Principles of Knowledge Representation and Reasoning (KR)},
  note = {Addresses the scalability challenge of exact inference in DeepProbLog by proposing an A*-like search algorithm (DPLA*) for finding the best proof. Introduces an exploration strategy and a parametric heuristic to guide the proof search, enabling application to larger tasks.},
  openalex = {W3205374622},
  pages = {475--486},
  title = {Approximate Inference for Neural Probabilistic Logic Programming},
  url = {https://proceedings.kr.org/2021/45/45-paper.pdf},
  year = {2021}
}

@article{manhaeve2021neural,
  author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
  journal = {Artificial Intelligence},
  note = {An extended journal version of the original DeepProbLog paper, providing a more detailed theoretical foundation, additional experiments, and a deeper discussion of the language's semantics and capabilities.},
  openalex = {W3154009503},
  pages = {103504},
  title = {Neural Probabilistic Logic Programming in DeepProbLog},
  url = {https://arxiv.org/pdf/1907.08194},
  volume = {298},
  year = {2021}
}

@inproceedings{bechberger2021towards,
  author = {Bechberger, Lucas},
  booktitle = {Workshop on Cognitive Aspects of Knowledge Representation (CAOS)},
  note = {Proposes combining LTNs with the cognitive framework of conceptual spaces. This approach aims to ground symbolic predicates in psychologically plausible geometric regions, enhancing the interpretability of the learned embeddings and helping to solve the symbol grounding problem.},
  openalex = {W3208461959},
  pages = {paper6},
  title = {Towards Conceptual Logic Tensor Networks},
  url = {https://ceur-ws.org/Vol-2969/paper6-CAOS.pdf},
  year = {2021}
}

@inproceedings{stammer2021right,
  author = {Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  note = {An extension of the NS-CL that introduces a human-in-the-loop component. The model can be revised and corrected by a human user who interacts with the model's explanations, allowing for more targeted and efficient learning of concepts.},
  openalex = {W3175429892},
  pages = {3619--3629},
  title = {Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations},
  url = {https://openaccess.thecvf.com/content/CVPR2021/papers/Stammer_Right_for_the_Right_Concept_Revising_Neuro-Symbolic_Concepts_by_Interacting_CVPR_2021_paper.pdf},
  year = {2021}
}

@article{wu2021graph,
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S},
  journal = {IEEE Transactions on Neural Networks and Learning Systems (TNNLS)},
  note = {A highly cited survey on Graph Neural Networks. While not exclusively neurosymbolic, it provides the essential background on the neural architectures that power most modern approaches to reasoning over knowledge graphs.},
  number = {1},
  openalex = {W4210257598},
  pages = {4--24},
  title = {Graph Neural Networks},
  url = {https://arxiv.org/pdf/1901.00596},
  volume = {32},
  year = {2021}
}

@inproceedings{cranmer2020lagrangian,
  author = {Cranmer, Miles D. and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter W. and Spergel, David and Ho, Shirley},
  booktitle = {ICLR Workshop on Deep Differential Equations},
  note = {Introduces a formulation for learning the dynamics of physical systems by embedding the principle of least action. The neural network learns the Lagrangian of the system, guaranteeing that the learned dynamics obey conservation laws.},
  openalex = {W3021457401},
  title = {Lagrangian Neural Networks},
  url = {https://arxiv.org/pdf/2003.04630.pdf},
  year = {2020}
}

@inproceedings{deraedt2020from,
  author = {De Raedt, Luc and Kersting, Kristian and Natarajan, Sriraam and Poole, David},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  note = {A survey that identifies deep parallels between the fields of Statistical Relational AI (StarAI) and Neuro-Symbolic AI. It uses KG reasoning as a key domain to characterize and position different approaches, identifying directions for future research.},
  openalex = {W3042107226},
  pages = {4943--4950},
  title = {From Statistical Relational to Neuro-Symbolic Artificial Intelligence},
  url = {https://www.ijcai.org/proceedings/2020/688},
  year = {2020}
}

@inproceedings{garcez2020graph,
  author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  doi = {10.24963/ijcai.2020/679},
  note = {A foundational survey that connects the fields of GNNs and Neural-Symbolic Computing (NSC). It reviews the state-of-the-art on using GNNs for symbolic and relational learning, covering applications in combinatorial optimization, constraint satisfaction, and relational reasoning.},
  openalex = {W3037471945},
  pages = {4877--4884},
  title = {Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective},
  url = {https://www.ijcai.org/proceedings/2020/0679.pdf},
  year = {2020}
}

@article{jagtap2020adaptive,
  author = {Jagtap, Ameya D. and Kawaguchi, Kenji and Karniadakis, George Em},
  doi = {10.1016/j.jcp.2019.109136},
  journal = {Journal of Computational Physics},
  note = {Proposes the use of trainable activation functions with scalable hyperparameters. This technique is shown to accelerate the convergence of PINNs and improve their accuracy by allowing the network to adapt its own nonlinearities during training.},
  openalex = {W2948551291},
  pages = {109136},
  title = {Adaptive activation functions accelerate convergence in deep and physics-informed neural networks},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0021999119308411},
  volume = {404},
  year = {2020}
}

@article{jagtap2020conservative,
  author = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
  doi = {10.1016/j.cma.2020.113028},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  note = {Introduces Conservative PINNs (cPINNs), a domain decomposition approach where a separate PINN is used for each subdomain. Enforces flux continuity at the interfaces, making it particularly well-suited for solving problems governed by conservation laws.},
  openalex = {W3015865829},
  pages = {113028},
  title = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0045782520302127},
  volume = {365},
  year = {2020}
}

@article{jagtap2020extended,
  author = {Jagtap, Ameya D. and Karniadakis, George Em},
  doi = {10.4208/cicp.OA-2020-0164},
  journal = {Communications in Computational Physics},
  note = {Generalizes the cPINN approach to arbitrary PDEs and complex domains. XPINNs enforce the continuity of the PDE residual at interfaces, enabling effective parallelization and scaling to large computational domains in both space and time.},
  number = {5},
  openalex = {W3098546160},
  pages = {2002--2041},
  title = {Extended Physics-Informed Neural Networks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations},
  url = {https://global-sci.com/article/79747/extended-physics-informed-neural-networks-xpinns-a-generalized-space-time-domain-decomposition-based-deep-learning-framework-for-nonlinear-partial-differential-equations},
  volume = {28},
  year = {2020}
}

@inproceedings{leon2020neurosymbolic,
  author = {Leon, Frederik and Gavran, Ivan and Weatherseed, Brandon and Fijalkow, Nathanaël and Meel, Kuldeep S},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Extends shielding to continuous state and action spaces. Introduces a method where the shield itself is updated during training, allowing the agent to develop formally verified policies that satisfy safety constraints while outperforming approaches with static shields.},
  openalex = {W4287660741},
  pages = {13029--13040},
  title = {Neurosymbolic Reinforcement Learning with Formally Verified Exploration},
  url = {https://proceedings.neurips.cc/paper/2020/file/448d5eda79895153938a8431919f4c9f-Paper.pdf},
  year = {2020}
}

@article{mao2020physics,
  author = {Mao, Zhiping and Jagtap, Ameya D and Karniadakis, George Em},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  note = {Demonstrates the application of PINNs to challenging problems in fluid dynamics, specifically high-speed flows involving shocks and discontinuities. This work highlighted some of the stability and accuracy challenges that motivated later research.},
  openalex = {W2998366519},
  pages = {112789},
  title = {Physics-informed neural networks for high-speed flows},
  url = {https://www.osti.gov/servlets/purl/1606776},
  volume = {360},
  year = {2020}
}

@article{shin2020convergence,
  author = {Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
  journal = {Communications in Computational Physics},
  note = {One of the first major theoretical works providing a convergence analysis for PINNs. It shows that for certain classes of linear PDEs, the sequence of PINN solutions strongly converges to the true PDE solution as the amount of data grows.},
  number = {5},
  openalex = {W3102139197},
  pages = {2042--2074},
  title = {On the Convergence of Physics Informed Neural Networks for Linear Second-Order Elliptic and Parabolic Type PDEs},
  url = {https://www.global-sci.com/article/79748/on-the-convergence-of-physics-informed-neural-networks-for-linear-second-order-elliptic-and-parabolic-type-pdes.html},
  volume = {28},
  year = {2020}
}

@article{dejong2019neural,
  author = {de Jong, Michiel and Sha, Fei},
  journal = {arXiv preprint arXiv:1906.06805},
  note = {A critical analysis of the original NTP framework, demonstrating that its greedy optimization strategy suffers from poor local minima and has difficulty recovering logical relationships. Proposes altering the algorithm to increase exploration, which sharply improves performance.},
  openalex = {W2953365388},
  title = {Neural Theorem Provers Do Not Learn Rules Without Exploration},
  url = {https://arxiv.org/pdf/1906.06805},
  year = {2019}
}

@inproceedings{greydanus2019hamiltonian,
  author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Proposes learning the Hamiltonian of a physical system instead of its state transitions directly. This approach ensures that the learned model conserves energy by design, a fundamental physical prior.},
  openalex = {W2948680363},
  pages = {15379--15389},
  title = {Hamiltonian Neural Networks},
  url = {https://papers.nips.cc/paper_files/paper/2019/file/055b135282603439582404c6428c054d-Paper.pdf},
  volume = {32},
  year = {2019}
}

@inproceedings{mao2019neuro,
  author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B and Wu, Jiajun},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {The foundational paper introducing the NS-CL. Proposes a model that learns visual concepts, words, and semantic parsing from question-answer pairs over images, without direct supervision on any of them. It bridges a neural perception module and a symbolic language module with a neuro-symbolic reasoning engine.},
  openalex = {W2953388933},
  title = {The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision},
  url = {https://openreview.net/pdf?id=rJgMlhRctm},
  year = {2019}
}

@inproceedings{minervini2019scalable,
  author = {Minervini, Pasquale and Bosnjak, Matko and Rocktäschel, Tim and Riedel, Sebastian and Grefenstette, Edward},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Addresses the prohibitive time and space complexity of NTPs with Neighbourhood-approximated NTPs (NaNTPs), drastically reducing computational cost. Extends the framework to jointly reason over knowledge base facts and textual mentions in a shared embedding space.},
  openalex = {W2911772404},
  title = {Scalable Neural Theorem Proving on Knowledge Bases and Natural Language},
  url = {https://openreview.net/pdf?id=BJzmzn0ctX},
  year = {2019}
}

@article{raissi2019physics,
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal = {Journal of Computational Physics},
  note = {The seminal paper that introduced and popularized the PINN framework. It establishes the core methodology of embedding PDEs into the neural network's loss function to solve both forward and inverse problems, demonstrating it on equations like Burgers', Schrödinger, and Navier-Stokes.},
  openalex = {W2899283552},
  pages = {686--707},
  title = {Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  url = {https://www.scirp.org/reference/referencespapers?referenceid=2886120},
  volume = {378},
  year = {2019}
}

@inproceedings{alshiekh2018safe,
  author = {Alshiekh, Mohammed and Bloem, Roderick and Ehlers, Rüdiger and Könighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
  booktitle = {AAAI Conference on Artificial Intelligence},
  note = {A seminal paper on safe RL that introduces "shielding." A shield, derived from a formal specification of safety requirements, monitors the actions proposed by a DRL agent and replaces any unsafe action with a verified safe alternative, guaranteeing safety during both training and deployment.},
  pages = {2669--2676},
  title = {Safe Reinforcement Learning via Shielding},
  url = {https://cdn.aaai.org/ojs/11797/11797-13-15325-1-2-20201228.pdf},
  year = {2018}
}

@inproceedings{manhaeve2018deepproblog,
  author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {NeurIPS 2018 Spotlight. Introduces DeepProbLog, a language that integrates deep learning with probabilistic logic programming via "neural predicates." Establishes the core framework for end-to-end training by backpropagating gradients through the probabilistic logic inference into the neural network.},
  openalex = {W4298084898},
  pages = {3749--3759},
  title = {DeepProbLog: Neural Probabilistic Logic Programming},
  url = {https://proceedings.neurips.cc/paper/2018/file/796797af74b71a7b7526a316f752c91b-Paper.pdf},
  year = {2018}
}

@inproceedings{verma2018programmatically,
  author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Introduces a method for generating interpretable policies. It first learns a neural policy via DRL and then uses a program synthesis technique to find a high-performing programmatic policy (in a domain-specific language) that is close to the learned neural policy.},
  openalex = {W4294624294},
  pages = {5045--5054},
  title = {Programmatically Interpretable Reinforcement Learning (PIRL)},
  url = {https://proceedings.mlr.press/v80/verma18a/verma18a.pdf},
  year = {2018}
}

@inproceedings{donadello2017logic,
  author = {Donadello, Ivan and Serafini, Luciano and Garcez, Artur d'Avila},
  booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
  note = {An early and successful application of LTNs to a core computer vision task. Demonstrates that injecting background knowledge as logical constraints improves the performance and robustness of object classification and relation detection compared to purely data-driven approaches like Fast R-CNN.},
  openalex = {W2963277062},
  pages = {1596--1602},
  title = {Logic Tensor Networks for Semantic Image Interpretation},
  url = {https://www.ijcai.org/proceedings/2017/0221.pdf},
  year = {2017}
}

@article{raissi2017physics,
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal = {arXiv preprint arXiv:1711.10561},
  note = {The initial preprint that laid the groundwork for the 2019 journal publication. It introduces the concept of using physics-informed neural networks to infer solutions to PDEs and obtain surrogate models that are fully differentiable.},
  openalex = {W2772097715},
  title = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
  url = {https://arxiv.org/pdf/1711.10561},
  year = {2017}
}

@misc{garnelo2016deep,
  archiveprefix = {arXiv},
  author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
  eprint = {1609.05518},
  note = {A foundational work in NeSy-RL. Proposes an architecture where a deep neural network learns to map raw sensory input to a low-dimensional symbolic space, upon which an abstract planning algorithm can operate to make decisions.},
  openalex = {W2525525521},
  primaryclass = {cs.AI},
  title = {Towards Deep Symbolic Reinforcement Learning},
  url = {https://arxiv.org/pdf/1609.05518},
  year = {2016}
}

@inproceedings{rocktaschel2016learning,
  author = {Rocktäschel, Tim and Riedel, Sebastian},
  booktitle = {Workshop on Automated Knowledge Base Construction (AKBC)},
  doi = {10.18653/v1/W16-1309},
  note = {The foundational paper introducing Neural Theorem Provers (NTPs). Proposes an end-to-end differentiable version of the backward-chaining algorithm that performs first-order inference on vector representations of symbols, enabling the learning of symbol embeddings and rule parameters.},
  openalex = {W2511805592},
  pages = {45--50},
  title = {Learning Knowledge Base Inference with Neural Theorem Provers},
  url = {https://aclanthology.org/W16-1309.pdf},
  year = {2016}
}
