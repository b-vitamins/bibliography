@misc{mei2025survey,
  author = {Lang Mei and Siyu Mo and Zhihan Yang and Chong Chen},
  title = {{A Survey of Multimodal Retrieval-Augmented Generation}},
  year = {2025},
  month = {3},
  eprint = {2504.08748},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR},
  url = {https://arxiv.org/abs/2504.08748},
  pdf = {https://arxiv.org/pdf/2504.08748.pdf},
  doi = {10.48550/arXiv.2504.08748},
  note = {arXiv preprint arXiv:2504.08748},
  abstract = {Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement.}
}

@misc{abootorabi2025ask,
  author = {Mohammad Mahdi Abootorabi and Amirhosein Zobeiri and Mahdi Dehghani and Mohammadali Mohammadkhani and Bardia Mohammadi and Omid Ghahroodi and Mahdieh Soleymani Baghshah and Ehsaneddin Asgari},
  title = {{Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation}},
  year = {2025},
  month = {2},
  eprint = {2502.08826},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2502.08826},
  pdf = {https://arxiv.org/pdf/2502.08826.pdf},
  doi = {10.48550/arXiv.2502.08826},
  note = {arXiv preprint arXiv:2502.08826, version 3 updated June 2, 2025},
  abstract = {Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation.}
}

@inproceedings{zhao2023retrieving,
  author = {Ruochen Zhao and Hailin Chen and Weishi Wang and Fangkai Jiao and Xuan Long Do and Chengwei Qin and Bosheng Ding and Xiaobao Guo and Minzhi Li and Xingxuan Li and Shafiq Joty},
  title = {{Retrieving Multimodal Information for Augmented Generation: A Survey}},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  year = {2023},
  address = {Singapore},
  publisher = {Association for Computational Linguistics},
  pages = {4736--4756},
  url = {https://aclanthology.org/2023.findings-emnlp.314},
  pdf = {https://aclanthology.org/2023.findings-emnlp.314.pdf},
  eprint = {2303.10868},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  abstract = {As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.}
}

@misc{xu2025comfyui,
  author = {Zhenran Xu and Yiyu Wang and Xue Yang and Longyue Wang and Weihua Luo and Kaifu Zhang and Baotian Hu and Min Zhang},
  title = {{ComfyUI-R1: Exploring Reasoning Models for Workflow Generation}},
  year = {2025},
  month = {6},
  eprint = {2506.09790},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2506.09790},
  pdf = {https://arxiv.org/pdf/2506.09790.pdf},
  doi = {10.48550/arXiv.2506.09790},
  note = {arXiv preprint arXiv:2506.09790, first large reasoning model for automated workflow generation},
  abstract = {AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. The 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series.}
}

@misc{mao2025multi,
  author = {Mingyang Mao and Mariela M. Perez-Cabarcas and Utteja Kallakuri and Nicholas R. Waytowich and Xiaomin Lin and Tinoosh Mohsenin},
  title = {{Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding}},
  year = {2025},
  month = {5},
  eprint = {2505.23990},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2505.23990},
  pdf = {https://arxiv.org/pdf/2505.23990.pdf},
  doi = {10.48550/arXiv.2505.23990},
  note = {arXiv preprint arXiv:2505.23990, revised June 14, 2025},
  abstract = {To effectively engage in human society, the ability to adapt, filter information, and make informed decisions in ever-changing situations is critical. As robots and intelligent agents become more integrated into human life, there is a growing opportunity-and need-to offload the cognitive burden on humans to these systems, particularly in dynamic, information-rich scenarios. This paper presents Multi-RAG, a multimodal retrieval-augmented generation system designed to provide adaptive assistance to humans in information-intensive circumstances by integrating video, audio, and text information streams. The system achieves superior performance compared to existing open-source video large language models while utilizing fewer resources and less input data.}
}

@misc{meng2025vlm2vec,
  author = {Rui Meng and Ziyan Jiang and Ye Liu and Mingyi Su and Xinyi Yang and Yuepeng Fu and Can Qin and Zeyuan Chen and Ran Xu and Caiming Xiong and Yingbo Zhou and Wenhu Chen and Semih Yavuz},
  title = {{VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents}},
  year = {2025},
  month = {7},
  eprint = {2507.04590},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  url = {https://arxiv.org/abs/2507.04590},
  pdf = {https://arxiv.org/pdf/2507.04590.pdf},
  doi = {10.48550/arXiv.2507.04590},
  note = {Technical Report, arXiv preprint arXiv:2507.04590},
  abstract = {Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. We introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering. Extensive experiments show that VLM2Vec-V2 achieves strong performance on the newly introduced video and document retrieval tasks, and also improves over prior baselines on the original image benchmarks.}
}

@misc{riedler2024beyond,
  author = {Monica Riedler and Stefan Langer},
  title = {{Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications}},
  year = {2024},
  month = {10},
  eprint = {2410.21943},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2410.21943},
  pdf = {https://arxiv.org/pdf/2410.21943.pdf},
  doi = {10.48550/arXiv.2410.21943},
  note = {arXiv preprint arXiv:2410.21943, code available at \url{https://github.com/riedlerm/multimodal_rag_for_industry}},
  abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in answering questions, but they lack domain-specific knowledge and are prone to hallucinations. Retrieval Augmented Generation (RAG) is one approach to address these challenges, while multimodal models are emerging as promising AI assistants for processing both text and images. This paper describes a series of experiments aimed at determining how to best integrate multimodal models into RAG systems for the industrial domain, specifically to determine whether including images alongside text from documents within the industrial domain increases RAG performance and to find the optimal configuration for such a multimodal RAG system. The results reveal that multimodal RAG can outperform single-modality RAG settings, although image retrieval poses a greater challenge than text retrieval.}
}

@inproceedings{jaiswal2025multimodal,
  author = {Amit Kumar Jaiswal and Haiming Liu and Ingo Frommholz},
  title = {{Multimodal RAG Enhanced Visual Description}},
  year = {2025},
  month = {11},
  booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
  series = {CIKM '25},
  address = {Seoul, Republic of Korea},
  publisher = {ACM},
  eprint = {2508.09170},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2508.09170},
  pdf = {https://arxiv.org/pdf/2508.09170.pdf},
  note = {5 pages, 2 figures},
  abstract = {Textual descriptions for multimodal inputs entail recurrent refinement of queries to produce relevant output images. Despite efforts to address challenges such as scaling model size and data volume, the cost associated with pre-training and fine-tuning remains substantial. The paper addresses the challenge of modality gap in pre-trained large multimodal models (LMMs), characterized by misalignment between textual and visual representations within a common embedding space. The authors propose a lightweight training-free approach utilizing Retrieval-Augmented Generation (RAG) to extend across the modality using a linear mapping, which can be computed efficiently. During inference, this mapping is applied to images embedded by an LMM enabling retrieval of closest textual descriptions from the training set, which serve as input prompts for the language model to generate new textual descriptions.}
}

@misc{bi2025everything,
  author = {Xiaowei Bi and Zheyuan Xu},
  title = {{Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment}},
  year = {2025},
  month = {3},
  eprint = {2503.09081},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  url = {https://arxiv.org/abs/2503.09081},
  pdf = {https://arxiv.org/pdf/2503.09081.pdf},
  doi = {10.48550/arXiv.2503.09081},
  note = {arXiv preprint arXiv:2503.09081, revised June 10, 2025},
  abstract = {While multi-modal learning has advanced significantly, current approaches often create inconsistencies in representation and reasoning of different modalities. We propose UMaT, a theoretically-grounded framework that unifies visual and auditory inputs as structured text for large language models, addressing semantic alignment, temporal synchronization, and efficient sparse information retrieval. UMaT is a retrieval-augmented generation (RAG) framework that efficiently processes extremely long videos while maintaining cross-modal coherence by converting visual and auditory data into a unified textual representation. It significantly improves state-of-the-art Long Video Question Answering accuracy (up to 13.7\%, and 16.9\% on long videos) via redundancy minimization and structured textual representation for unified multi-modal reasoning.}
}

@misc{liu2025hmrag,
  author = {Pei Liu and Xin Liu and Ruoyu Yao and Junming Liu and Siyuan Meng and Ding Wang and Jun Ma},
  title = {{HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation}},
  year = {2025},
  month = {4},
  eprint = {2504.12330},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2504.12330},
  pdf = {https://arxiv.org/pdf/2504.12330.pdf},
  doi = {10.48550/arXiv.2504.12330},
  note = {arXiv preprint arXiv:2504.12330, code available at \url{https://github.com/ocean-luna/HMRAG}},
  abstract = {While Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge, conventional single-agent RAG remains fundamentally limited in resolving complex queries across heterogeneous data ecosystems. This paper introduces HM-RAG, a novel Hierarchical Multi-agent Multimodal RAG framework that features a three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks, Multi-source Retrieval Agents that carry out parallel, modality-specific retrieval, and a Decision Agent that uses consistency voting to integrate multi-source answers. The proposed approach achieves a 12.95\% improvement in answer accuracy and establishes state-of-the-art results in zero-shot settings on ScienceQA and CrisisMMD benchmarks, demonstrating enhanced performance in multimodal retrieval augmented generation tasks through hierarchical multi-agent coordination.}
}

@inproceedings{xia2024rule,
  author = {Peng Xia and Kangyu Zhu and Haoran Li and Hongtu Zhu and Yun Li and Gang Li and Linjun Zhang and Huaxiu Yao},
  title = {{RULE}: {R}eliable {M}ultimodal {RAG} for {F}actuality in {M}edical {V}ision {L}anguage {M}odels},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  year = {2024},
  pages = {1081--1093},
  url = {https://aclanthology.org/2024.emnlp-main.62/},
  pdf = {https://aclanthology.org/2024.emnlp-main.62.pdf},
  doi = {10.18653/v1/2024.emnlp-main.62},
  abstract = {The paper addresses factuality challenges in Medical Large Vision Language Models (Med-LVLMs). It proposes RULE, a method with two key components: 1) calibrated selection of retrieved contexts, and 2) a preference dataset to balance model's inherent knowledge and retrieved contexts. The approach achieves a 20.8\% average improvement in factual accuracy across three medical VQA datasets.}
}

@misc{liu2025benchmarking,
  author = {Zhenghao Liu and Xingsheng Zhu and Tianshuo Zhou and Xinyi Zhang and Xiaoyuan Yi and Yukun Yan and Ge Yu and Maosong Sun},
  title = {Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts},
  year = {2025},
  month = {2},
  eprint = {2502.17297},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2502.17297},
  pdf = {https://arxiv.org/pdf/2502.17297.pdf},
  abstract = {With the rapid advancement of Multi-modal Large Language Models (MLLMs), their capability in understanding both images and text has greatly improved. However, their potential for leveraging multi-modal contextual information in Retrieval-Augmented Generation (RAG) remains largely underexplored. This paper introduces Multi-Modal Retrieval-Augmented Generation (M²RAG) benchmark comprising four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. Additionally, it presents Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Experiments show that MM-RAIT improves RAG system performance by enabling effective learning from multi-modal contexts.}
}

@misc{yang2025benchmarking,
  author = {Yuming Yang and Jiang Zhong and Li Jin and Jingwang Huang and Jingpeng Gao and Qing Liu and Yang Bai and Jingyuan Zhang and Rui Jiang and Kaiwen Wei and others},
  title = {Benchmarking Multimodal {RAG} through a Chart-based Document Question-Answering Generation Framework},
  year = {2025},
  month = {2},
  eprint = {2502.14864},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2502.14864},
  pdf = {https://arxiv.org/pdf/2502.14864.pdf},
  abstract = {Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. This paper introduces CHARGE (CHARt-based document question-answering GEneration), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation. Combined with expert validation, CHARGE constructs Chart-MRAG Bench, a comprehensive benchmark featuring 4,738 question-answering pairs across 8 domains from real-world documents for chart-based MRAG evaluation.}
}

@misc{mortaheb2025ragcheck,
  author = {Matin Mortaheb and Mohammad A. Amir Khojastepour and Srimat T. Chakradhar and Sennur Ulukus},
  title = {{RAG-Check}: Evaluating Multimodal Retrieval Augmented Generation Performance},
  year = {2025},
  month = {1},
  eprint = {2501.03995},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2501.03995},
  pdf = {https://arxiv.org/pdf/2501.03995.pdf},
  abstract = {Multi-modal RAG can introduce new hallucination sources through irrelevant retrieval and vision-language model errors. This paper proposes RAG-Check, a novel framework to evaluate multi-modal RAG reliability using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query on a scale from 0 to 1, and (ii) the correctness score (CS), evaluating the accuracy of generated response spans. The framework includes response splitting into subjective/factual spans, RS and CS models. Both models achieve approximately 88\% accuracy on test data, with the RS model aligning with human preferences 20\% better than CLIP and the CS model matching human preferences 91\% of the time. A 5000-sample human-annotated database was constructed for evaluation.}
}

@inproceedings{wasserman2025realmmrag,
  author = {Navve Wasserman and Roi Pony and Oshri Naparstek and Adi Raz Goldfarb and Eli Schwartz and Udi Barzelay and Leonid Karlinsky},
  title = {{REAL-MM-RAG}: A Real-World Multi-Modal Retrieval Benchmark},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year = {2025},
  month = {7},
  pages = {31660--31683},
  address = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url = {https://aclanthology.org/2025.acl-long.1528/},
  pdf = {https://aclanthology.org/2025.acl-long.1528.pdf},
  doi = {10.18653/v1/2025.acl-long.1528},
  abstract = {Accurate multi-modal document retrieval is crucial for Retrieval-Augmented Generation (RAG), yet existing benchmarks do not fully capture real-world challenges with their current design. This paper introduces REAL-MM-RAG, an automatically generated benchmark addressing four key properties: (i) multi-modal documents, (ii) enhanced difficulty, (iii) realistic-RAG queries and (iv) accurate labeling. The benchmark proposes a multi-difficulty-level scheme based on query rephrasing to evaluate models' semantic understanding beyond keyword matching. Results reveal significant model weaknesses, particularly in handling table-heavy documents and robustness to query rephrasing. To address these shortcomings, the authors curate a rephrased training set and introduce a new finance-focused, table-heavy dataset. Fine-tuning on these datasets enables models to achieve state-of-the-art retrieval performance on the REAL-MM-RAG benchmark.}
}

@inproceedings{xie2025filmcomposer,
  author = {Zhifeng Xie and Qile He and Youjia Zhu and Qiwei He and Mengtian Li},
  title = {{FilmComposer}: {LLM}-Driven Music Production for Silent Film Clips},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2025},
  month = {6},
  address = {Nashville, TN, USA},
  publisher = {IEEE},
  url = {https://cvpr.thecvf.com/virtual/2025/poster/33134},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Xie_FilmComposer_LLM-Driven_Music_Production_for_Silent_Film_Clips_CVPR_2025_paper.pdf},
  abstract = {This work implements music production for silent film clips using LLM-driven methods. Given the strong professional demands of film music production, the paper proposes FilmComposer, simulating the actual workflows of professional musicians. FilmComposer is the first to combine large generative models with a multi-agent approach, leveraging the advantages of both waveform music and symbolic music generation. The system consists of visual processing module, rhythm-controllable MusicGen, and multi-agent assessment, arrangement and mix. FilmComposer focuses on three core elements: audio quality, musicality, and musical development, introducing various controls such as rhythm, semantics, and visuals. The authors construct MusicPro-7k dataset comprising 7,418 video-music pairs with descriptions, main melodies, and rhythm spots. Evaluation shows clear preference for FilmComposer-generated music in user studies with experts.}
}

@misc{mohsin2025retrieval,
  author = {Muhammad Ahmed Mohsin and Ahsan Bilal and Sagnik Bhattacharya and John M. Cioffi},
  title = {Retrieval Augmented Generation with Multi-Modal {LLM} Framework for Wireless Environments},
  year = {2025},
  month = {3},
  eprint = {2503.07670},
  archivePrefix = {arXiv},
  primaryClass = {cs.NI},
  url = {https://arxiv.org/abs/2503.07670},
  pdf = {https://arxiv.org/pdf/2503.07670.pdf},
  abstract = {Future wireless networks aim to deliver high data rates and lower power consumption while ensuring seamless connectivity, necessitating robust optimization. While large language models (LLMs) have been deployed for generalized optimization scenarios, this paper proposes retrieval augmented generation (RAG) for multi-sensor wireless environment perception. The approach utilizes domain-specific prompt engineering and applies RAG to efficiently harness multimodal data inputs from sensors in wireless environments. Key pre-processing pipelines including image-to-text conversion, object detection, and distance calculations for multimodal RAG input from multi-sensor data are proposed to obtain a unified vector database crucial for optimizing LLMs in global wireless tasks. Evaluation with OpenAI's GPT and Google's Gemini models demonstrates improvements of 8\%, 8\%, 10\%, 7\%, and 12\% in relevancy, faithfulness, completeness, similarity, and accuracy compared to baseline methods. The RAG-based LLM framework provides real-time convergence under latency constraints.}
}

@misc{zheng2025retrieval,
  author = {Xu Zheng and Ziqiao Weng and Yuanhuiyi Lyu and Lutao Jiang and Haiwei Xue and Bin Ren and Danda Pani Paudel and Nicu Sebe and Luc Van Gool and Xuming Hu},
  title = {Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook},
  year = {2025},
  month = {3},
  eprint = {2503.18016},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  url = {https://arxiv.org/abs/2503.18016},
  pdf = {https://arxiv.org/pdf/2503.18016.pdf},
  abstract = {Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable knowledge sources. Recently, RAG's potential has extended beyond natural language processing into computer vision, aiming to address limitations of relying solely on internal model knowledge by incorporating external knowledge bases. This comprehensive survey reviews the current state of retrieval-augmented techniques in computer vision, focusing on two main areas: (I) visual understanding - covering tasks from basic image recognition to complex applications like medical report generation and multimodal question answering, and (II) visual generation - examining RAG applications in image, video, and 3D generation. The survey also explores recent advancements in RAG for embodied AI, focusing on planning, task execution, multimodal perception, and interaction. Given that RAG integration in computer vision is still early-stage, the authors highlight key limitations and propose future research directions.}
}

@misc{wang2024searching,
  author = {Xiaohua Wang and Zhenghua Wang and Xuan Gao and Feiran Zhang and Yixin Wu and Zhibo Xu and Tianyuan Shi and Zhengyuan Wang and Shizheng Li and Qi Qian and Ruicheng Yin and Changze Lv and Xiaoqing Zheng and Xuanjing Huang},
  title = {Searching for Best Practices in Retrieval-Augmented Generation},
  year = {2024},
  month = {7},
  eprint = {2407.01219},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2407.01219},
  pdf = {https://arxiv.org/pdf/2407.01219.pdf},
  abstract = {Retrieval-augmented generation (RAG) techniques have proven effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. However, many RAG approaches suffer from complex implementation and prolonged response times. This paper investigates existing RAG approaches and their potential combinations to identify optimal RAG practices. The authors systematically examine multiple RAG workflow components including query classification, document processing, chunking, embedding types, vector database selection, and LLM fine-tuning strategies. Through extensive experiments, key findings include: query classification modules improve accuracy and reduce latency; hybrid search with HyDE excels in retrieval; MonoT5 and reverse repacking significantly enhance performance; while Recomp addresses length constraints, omitting summarization can reduce latency. The paper also demonstrates that multimodal retrieval techniques significantly enhance question-answering capabilities about visual inputs and accelerate multimodal content generation using a "retrieval as generation" strategy. A comprehensive evaluation framework with metrics covering general, specialized, and RAG-related capabilities provides robust assessment across different performance dimensions.}
}

@inproceedings{packowski2024optimizing,
  author = {Sarah Packowski and Inge Halilovic and Jenifer Schlotfeldt and Trish Smith},
  title = {Optimizing and Evaluating Enterprise Retrieval-Augmented Generation ({RAG}): A Content Design Perspective},
  booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
  year = {2024},
  month = {10},
  publisher = {ACM},
  doi = {10.1145/3704137.3704181},
  url = {https://dl.acm.org/doi/10.1145/3704137.3704181},
  pdf = {https://arxiv.org/pdf/2410.12812.pdf},
  abstract = {Retrieval-augmented generation (RAG) is a popular technique for using large language models (LLMs) to build customer-support, question-answering solutions. This paper shares practical experience building and maintaining enterprise-scale RAG solutions that answer users' questions about software based on product documentation. The authors focus on solution strategies that are modular and model-agnostic, noting that their experience has not always matched common patterns in RAG literature. A key finding is that simple changes to knowledge base content creation can have huge impact on RAG solutions' success across different search methods, LLMs, and knowledge base collections. The paper addresses practical considerations including content structure for knowledge base indexing, chunking strategies that balance information splitting versus irrelevant inclusion, and an approach to chunk content at chapter/section level rather than by size to include complete, self-contained ideas. The authors implement RAG as "search enhancement" where natural language questions generate both regular search results and LLM-generated brief answers, requiring no new interface for users to learn.}
}

@misc{liu2023m2ugen,
  author = {Shansong Liu and Atin Sakkeer Hussain and Qilong Wu and Chenshuo Sun and Ying Shan},
  title = {{M}²{UGen}: Multi-modal Music Understanding and Generation with the Power of Large Language Models},
  year = {2023},
  month = {11},
  eprint = {2311.11255},
  archivePrefix = {arXiv},
  primaryClass = {cs.SD},
  url = {https://arxiv.org/abs/2311.11255},
  pdf = {https://arxiv.org/pdf/2311.11255.pdf},
  note = {Last revised 9 Dec 2024},
  abstract = {The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as images and audio. They typically employ frozen pre-trained encoders to represent these modalities as tokens, which are then fed into LLMs for processing. However, when it comes to music, most research has focused on discrimination tasks, such as music question answering, rather than on music generation. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M²UGen) framework that integrates LLM's abilities to comprehend and generate music for different modalities. The M²UGen model incorporates frozen pre-trained encoders and utilizes encoders capable of comprehending various modalities including music, images, and videos, with a special attention on music generation conditioned on multimodal inputs.}
}

@misc{hsu2025testtime,
  author = {Hung-Chun Hsu and Yuan-Ching Kuo and Chao-Han Huck Yang and Szu-Wei Fu and Hanrong Ye and Hongxu Yin and Yu-Chiang Frank Wang and Ming-Feng Tsai and Chuan-Ju Wang},
  title = {Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations},
  year = {2025},
  month = {8},
  eprint = {2508.18132},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR},
  url = {https://arxiv.org/abs/2508.18132},
  pdf = {https://arxiv.org/pdf/2508.18132.pdf},
  abstract = {The rapid evolution of e-commerce has exposed the limitations of traditional product retrieval systems in managing complex, multi-turn user interactions. Recent advances in multimodal generative retrieval using multimodal large language models (MLLMs) as retrievers have shown promise, but most existing methods are tailored to single-turn scenarios and struggle with the evolving intent and iterative nature of multi-turn dialogues. To address this limitation, we propose a novel framework that introduces test-time scaling into conversational multimodal product retrieval. Our approach builds on a generative retriever, further augmented with a test-time reranking (TTR) mechanism that improves retrieval accuracy and better aligns results with evolving user intent throughout the dialogue. Experiments across multiple benchmarks show consistent improvements, with average gains of 14.5 points in MRR and 10.6 points in nDCG@1.}
}

@inproceedings{soliman2025surveytables,
  author = {Hassan Soliman and Iryna Gurevych},
  title = {A Survey on Advances in Retrieval-Augmented Generation over Tabular Data and Table {QA}},
  booktitle = {ELLIS Workshop on Representation Learning and Generative Models for Structured Data},
  series = {RLGMSD},
  year = {2025},
  month = {2},
  address = {Amsterdam, Netherlands},
  url = {https://openreview.net/forum?id=AdDU2c4XfP},
  pdf = {https://openreview.net/pdf?id=AdDU2c4XfP},
  note = {Poster presentation},
  abstract = {Recent advancements in retrieval-augmented generation (RAG) and question answering (QA) over tabular data have demonstrated significant potential in addressing challenges in data retrieval, semantic understanding, and complex reasoning. This work reviews key trends, insights, and limitations across various domains, emphasizing advancements in TableQA, multi-table retrieval, multimodal table retrieval, and generative information retrieval (GenIR). These developments are critical for improving machine interaction with structured datasets, paving the way for scalable and accurate decision-making tools in real-world applications.}
}

@inproceedings{li2025benchmarkingdynvqa,
  author = {Yangning Li and Yinghui Li and Xinyu Wang and Yong Jiang and Zhen Zhang and Xinran Zheng and Hui Wang and Hai-Tao Zheng and Fei Huang and Jingren Zhou and Philip S. Yu},
  title = {Benchmarking Multimodal Retrieval Augmented Generation with Dynamic {VQA} Dataset and Self-adaptive Planning Agent},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  series = {ICLR},
  year = {2025},
  month = {1},
  note = {Poster presentation},
  url = {https://openreview.net/forum?id=VvDEuyVXkG},
  pdf = {https://openreview.net/pdf?id=VvDEuyVXkG},
  eprint = {2411.02937},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  abstract = {Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the 'hallucination' issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. To address these limitations, we construct Dyn-VQA dataset, consisting of three types of "dynamic" questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. We also propose the first self-adaptive planning agent for multimodal retrieval, OmniSearch. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes.}
}

@inproceedings{yu2025mramg,
  author = {Qinhan Yu and Zhiyou Xiao and Binghui Li and Zhengren Wang and Chong Chen and Wentao Zhang},
  title = {{MRAMG-Bench}: A Comprehensive Benchmark for Advancing Multimodal Retrieval-Augmented Multimodal Generation},
  booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  series = {SIGIR '25},
  year = {2025},
  month = {7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  location = {Padua, Italy},
  doi = {10.1145/3726302.3730288},
  url = {https://sigir2025.dei.unipd.it/detailed-program/paper?paper=3e7e0224018ab3cf51abb96464d518cd},
  eprint = {2502.04176},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  pdf = {https://arxiv.org/pdf/2502.04176.pdf},
  note = {Accepted paper},
  abstract = {Recent advances in Retrieval-Augmented Generation (RAG) have significantly improved response accuracy and relevance by incorporating external knowledge into Large Language Models (LLMs). However, existing RAG approaches primarily focus on generating text-only answers, even in Multimodal Retrieval-Augmented Generation (MRAG) scenarios, where multimodal elements are retrieved to assist in generating text answers. This overlooks the potential to leverage multimodal data for generating multimodal answers. To fill this gap, we introduce the Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) task, which aims to generate multimodal answers that combine both text and images, fully leveraging the multimodal data within a corpus. To advance this field, we present MRAMG-Bench, the first benchmark that challenges models to autonomously determine the number, selection, and ordering of images in their responses, effectively simulating the complex scenarios encountered in real-world user interactions. MRAMG-Bench consists of six carefully curated English datasets, comprising 4,346 documents, 14,190 images, and 4,800 QA pairs, sourced from three domains—Web, Academia, and Lifestyle—across seven distinct data sources. We also introduce a statistically grounded evaluation framework that systematically assesses both retrieval and generation performance.}
}

