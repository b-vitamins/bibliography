@article{yang2025adversarial,
  abstract = {Generative adversarial networks (GANs) have made remarkable achievements in synthesizing images in recent years. Typically, training GANs requires massive data, and the performance of GANs deteriorates significantly when training data is limited. To improve the synthesis performance of GANs in low-data regimes, existing approaches use various data augmentation techniques to enlarge the training sets. However, it is identified that these augmentation techniques may leak or even alter the data distribution. To remedy this, we propose an adversarial semantic augmentation (ASA) technique to enlarge the training data at the semantic level instead of the image level. Concretely, considering semantic features usually encode informative information of images, we estimate the covariance matrices of semantic features for both real and generated images to find meaningful transformation directions. Such directions translate original features to another semantic representation, e.g., changing the backgrounds or expressions of the human face dataset. We derive an upper bound of the expected adversarial loss, and by optimizing this upper bound, semantic augmentation is implicitly achieved. This design avoids redundant sampling of augmented features and introduces negligible computation overhead. Extensive experiments on both few-shot and large-scale datasets demonstrate that our method consistently improves synthesis quality under various data regimes.},
  archiveprefix = {arXiv},
  author = {Mengping Yang and Zhe Wang and Ziqiu Chi and Dongdong Li and Wenli Du},
  eprint = {2502.00800},
  file = {:/home/b/documents/articles/yang2025adversarial.pdf:pdf},
  journal = {arXiv preprint arXiv:2502.00800},
  month = {1},
  pdf = {https://arxiv.org/pdf/2502.00800.pdf},
  primaryclass = {cs.CV},
  title = {Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data},
  url = {https://arxiv.org/abs/2502.00800},
  year = {2025}
}

@misc{wu2025pragmatic,
  abstract = {This study demonstrates with three generative models across two retinal imaging modalities that Fréchet Inception Distance (FID) and other feature-distance metrics do not align with downstream performance when generated data is pragmatically used to augment the training dataset. The paper examines cases from retinal imaging modalities, including color fundus photography and optical coherence tomography, where FID and its related metrics misalign with task-specific evaluation goals in classification and segmentation. The authors recommend that researchers prioritize downstream task performance as the primary evaluation metric for generative models used in data augmentation.},
  archiveprefix = {arXiv},
  author = {Yuli Wu and Fucheng Liu and Rüveyda Yilmaz and Henning Konermann and Peter Walter and Johannes Stegmaier},
  eprint = {2502.17160},
  file = {:/home/b/documents/misc/wu2025pragmatic.pdf:pdf},
  howpublished = {arXiv preprint arXiv:2502.17160},
  month = {2},
  pdf = {https://arxiv.org/pdf/2502.17160.pdf},
  primaryclass = {cs.CV},
  title = {A Pragmatic Note on Evaluating Generative Models with Fréchet Inception Distance for Retinal Image Synthesis},
  url = {https://arxiv.org/abs/2502.17160},
  year = {2025}
}

@misc{bynagari2025evaluating,
  abstract = {Evaluating text-to-image synthesis is challenging due to misalignment between established metrics and human preferences. We propose cFreD, a metric based on the notion of Conditional Fréchet Distance that explicitly accounts for both visual fidelity and text-prompt alignment. Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, cFreD exhibits a higher correlation with human judgments compared to statistical metrics, including metrics trained with human preferences. cFreD does not require additional training or human preference data, yet it effectively incorporates textual context during evaluation and can be readily applied to new data. The findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text-to-image models, standardizing benchmarking in this rapidly evolving field.},
  author = {Jaywon Koo and Jefferson Hernandez and Moayed Haji-Ali and Ziyan Yang and Vicente Ordonez},
  doi = {10.48550/arXiv.2503.21721},
  file = {:/home/b/documents/misc/bynagari2025evaluating.pdf:pdf},
  howpublished = {arXiv preprint arXiv:2503.21721},
  month = {3},
  note = {Proposes cFreD, a new metric for evaluating text-to-image models that explicitly accounts for both visual fidelity and text-prompt alignment. It aims to provide a more robust and future-proof metric that correlates better with human judgment than existing alternatives.},
  pdf = {https://arxiv.org/pdf/2503.21721.pdf},
  title = {Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance},
  url = {https://arxiv.org/abs/2503.21721},
  year = {2025}
}

@article{jiao2023survey,
  abstract = {Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey covers various domains including text generation, image synthesis, video creation, music composition, code generation, and biotech applications, serving as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI.},
  author = {Roberto Gozalo-Brizuela and Eduardo C. Garrido-Merchán},
  doi = {10.3844/jcssp.2024.801.818},
  file = {:/home/b/documents/articles/jiao2023survey.pdf:pdf},
  journal = {Journal of Computer Science},
  number = {8},
  openalex = {W4379540039},
  pages = {801--818},
  pdf = {https://arxiv.org/pdf/2306.02781.pdf},
  title = {A Survey of Generative AI Applications},
  volume = {20},
  year = {2024}
}

@inproceedings{jayasumana2024rethinking,
  abstract = {As with many machine learning problems, the progress of image generation methods hinges on good evaluation metrics. One of the most popular is the Frechet Inception Distance (FID). However, FID has a number of drawbacks: it does not use features that correspond to human perception, it cannot distinguish between different types of mode collapse, it is biased by the Inception network, and it is not robust to diverse datasets. In this paper, we empirically demonstrate that FID contradicts human raters, does not reflect gradual improvement of iterative text-to-image models, fails to capture distortion levels, and produces inconsistent results when varying the sample size. To address these issues, we propose CMMD, a new metric based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. CMMD is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings and is sample efficient.},
  address = {Seattle, WA, USA},
  author = {Sadeep Jayasumana and Srikumar Ramalingam and Andreas Veit and Daniel Glässner and Ayan Chakrabarti and Sanjiv Kumar},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr52733.2024.00889},
  file = {:/home/b/documents/inproceedings/jayasumana2024rethinking.pdf:pdf},
  month = {6},
  openalex = {W4402727293},
  pages = {9416--9426},
  pdf = {https://openaccess.thecvf.com/content/CVPR2024/papers/Jayasumana_Rethinking_FID_Towards_a_Better_Evaluation_Metric_for_Image_Generation_CVPR_2024_paper.pdf},
  publisher = {IEEE},
  title = {Rethinking FID: Towards a Better Evaluation Metric for Image Generation},
  year = {2024}
}

@inproceedings{jolicoeurmartineau2025gan,
  abstract = {There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. We make two main contributions. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.},
  address = {Red Hook, NY, USA},
  author = {Yiwen Huang and Aaron Gokaslan and Volodymyr Kuleshov and James Tompkin},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/jolicoeurmartineau2025gan.pdf:pdf},
  openalex = {W4406273440},
  pages = {95345},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4e2acb1e1c8e297d394ae29ed9535172-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {The GAN is dead; long live the GAN! A Modern GAN Baseline},
  url = {https://arxiv.org/abs/2501.05441},
  volume = {37},
  year = {2024}
}

@inproceedings{poole2023dreamfusion,
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  author = {Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
  booktitle = {The Eleventh International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/poole2023dreamfusion.pdf:pdf},
  note = {Outstanding Paper Award},
  openalex = {W4298187450},
  pdf = {https://openreview.net/pdf?id=FjNys5c7VyY},
  title = {DreamFusion: Text-to-3D using 2D Diffusion},
  url = {https://openreview.net/forum?id=FjNys5c7VyY},
  year = {2023}
}

@inproceedings{sauer2023stylegant,
  abstract = {Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This work aims to identify the necessary steps to regain competitiveness. We propose StyleGAN-T, which addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models -- the previous state-of-the-art in fast text-to-image synthesis -- in terms of sample quality and speed.},
  author = {Axel Sauer and Tero Karras and Samuli Laine and Andreas Geiger and Timo Aila},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sauer2023stylegant.pdf:pdf},
  month = {7},
  openalex = {W4317951215},
  pages = {30105--30118},
  pdf = {https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis},
  url = {https://proceedings.mlr.press/v202/sauer23a},
  volume = {202},
  year = {2023}
}

@inproceedings{kang2023gigagan,
  abstract = {The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs have been dethroned by diffusion models as the go-to paradigm for image generation. This rapid shift raises the question: can GANs be competitive with diffusion models for text-to-image synthesis? We answer this question with a resounding "yes". We introduce GigaGAN, a new GAN architecture that far exceeds this limitation, demonstrating that GANs can be a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, GigaGAN can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.},
  address = {Vancouver, BC, Canada},
  archiveprefix = {arXiv},
  author = {Minguk Kang and Jun-Yan Zhu and Richard Zhang and Jaesik Park and Eli Shechtman and Sylvain Paris and Taesung Park},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR52729.2023.00976},
  eprint = {2303.05511},
  file = {:/home/b/documents/inproceedings/kang2023gigagan.pdf:pdf},
  month = {6},
  note = {A GAN-based text-to-image model that scales to billion-parameter regimes. GigaGAN introduced architectural innovations that allowed it to synthesize megapixel images in a fraction of a second, challenging the dominance of diffusion models.},
  pages = {10124--10134},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {Scaling up GANs for Text-to-Image Synthesis},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html},
  year = {2023}
}

@article{xia2022gan,
  abstract = {GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model, for the image to be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling pretrained GAN models, such as StyleGAN and BigGAN, for applications of real image editing.},
  author = {Weihao Xia and Yulun Zhang and Yujiu Yang and Jing-Hao Xue and Bolei Zhou and Ming-Hsuan Yang},
  doi = {10.1109/TPAMI.2022.3181070},
  file = {:/home/b/documents/articles/xia2022gan.pdf:pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  note = {A comprehensive survey of the GAN inversion field, which aims to map real images into a GAN's latent space for editing. It categorizes and analyzes existing methods, discussing their strengths, weaknesses, and key applications.},
  number = {3},
  openalex = {W4285124635},
  pages = {3121--3138},
  pdf = {https://arxiv.org/pdf/2101.05278},
  title = {GAN Inversion: A Survey},
  volume = {45},
  year = {2022}
}

@inproceedings{zhang2021styleswin,
  abstract = {Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., 1024x1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation.},
  author = {Bowen Zhang and Shuyang Gu and Bo Zhang and Jianmin Bao and Dong Chen and Fang Wen and Yong Wang and Baining Guo},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/zhang2021styleswin.pdf:pdf},
  openalex = {W4312358791},
  pages = {11304--11314},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  title = {StyleSwin: Transformer-based GAN for High-resolution Image Generation},
  year = {2022}
}

@inproceedings{nichol2022glide,
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  author = {Alexander Quinn Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nichol2022glide.pdf:pdf},
  note = {A key text-to-image diffusion model that demonstrated photorealistic synthesis and powerful editing capabilities. It is crucial context for understanding the competitive landscape in which modern text-to-image GANs operate.},
  openalex = {W4226125322},
  pages = {16784--16804},
  pdf = {https://proceedings.mlr.press/v162/nichol22a/nichol22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  volume = {162},
  year = {2022}
}

@inproceedings{jabri2021gansupervised,
  abstract = {We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GANgealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms—despite being trained exclusively on GAN-generated data and requiring no correspondence supervision or data augmentation.},
  author = {William Peebles and Jun-Yan Zhu and Richard Zhang and Antonio Torralba and Alexei A. Efros and Eli Shechtman},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  file = {:/home/b/documents/inproceedings/jabri2021gansupervised.pdf:pdf},
  note = {Oral presentation, Best Paper Finalist. The paper demonstrates that GANs can serve as powerful structured knowledge bases for learning dense visual correspondences in a self-supervised manner.},
  openalex = {W4312529357},
  pages = {13470--13481},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Peebles_GAN-Supervised_Dense_Visual_Alignment_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  series = {CVPR},
  title = {GAN-Supervised Dense Visual Alignment},
  year = {2022}
}

@inproceedings{gu2022stylenerf,
  abstract = {We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including zoom-in and-out, style mixing, inversion, and semantic editing.},
  author = {Jiatao Gu and Lingjie Liu and Peng Wang and Christian Theobalt},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2022stylenerf.pdf:pdf},
  note = {Combined the 3D-aware representation of NeRFs with the powerful style-based architecture of StyleGAN2. This allowed for the generation of high-resolution, multi-view consistent images with fine-grained style control.},
  openalex = {W3205325185},
  pdf = {https://openreview.net/pdf?id=iUuzzTMUw9K},
  title = {StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis},
  url = {https://openreview.net/forum?id=iUuzzTMUw9K},
  year = {2022}
}

@inproceedings{rombach2022highresolution,
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  author = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52688.2022.01042},
  file = {:/home/b/documents/inproceedings/rombach2022highresolution.pdf:pdf},
  month = {6},
  note = {The paper that introduced Stable Diffusion. It applied the diffusion process in the compressed latent space of a powerful autoencoder (learned by a VQGAN-like model), making diffusion models computationally efficient enough for widespread use.},
  pages = {10684--10695},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  series = {CVPR 2022},
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
  year = {2022}
}

@inproceedings{sauer2022styleganxl,
  abstract = {Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN's performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of $1024^2$ at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes.},
  address = {New York, NY, USA},
  author = {Axel Sauer and Katja Schwarz and Andreas Geiger},
  booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
  doi = {10.1145/3528233.3530738},
  file = {:/home/b/documents/inproceedings/sauer2022styleganxl.pdf:pdf},
  openalex = {W4285981784},
  pdf = {https://arxiv.org/pdf/2202.00273.pdf},
  publisher = {Association for Computing Machinery},
  title = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
  url = {https://doi.org/10.1145/3528233.3530738},
  year = {2022}
}

@misc{singer2022makeavideo,
  abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three main advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. We decompose the full temporal U-Net and attention tensors and approximate them in space and time. We design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models. Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  archiveprefix = {arXiv},
  author = {Uriel Singer and Adam Polyak and Thomas Hayes and Xi Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
  eprint = {2209.14792},
  file = {:/home/b/documents/misc/singer2022makeavideo.pdf:pdf},
  howpublished = {arXiv},
  month = {9},
  openalex = {W4298185919},
  pdf = {https://arxiv.org/pdf/2209.14792.pdf},
  primaryclass = {cs.CV},
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  year = {2022}
}

@inproceedings{li20223daware,
  abstract = {Making generative models 3D-aware bridges the 2D image space and the 3D physical world yet remains challenging. Recent attempts equip a Generative Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D coordinates to pixel values, as a 3D prior. However, the implicit function in NeRF has a very local receptive field, making the generator hard to become aware of the global structure. Meanwhile, NeRF is built on volume rendering which can be too costly to produce high-resolution results, increasing the optimization difficulty. To alleviate these two problems, we propose a novel framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis, through explicitly learning a structural representation and a textural representation. We first learn a feature volume to represent the underlying structure, which is then converted to a feature field using a NeRF-like model. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a neural renderer for appearance synthesis. Such a design enables independent control of the shape and the appearance. Extensive experiments on a wide range of datasets show that our approach achieves sufficiently higher image quality and better 3D control than the previous methods.},
  author = {Yinghao Xu and Sida Peng and Ceyuan Yang and Yujun Shen and Bolei Zhou},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/li20223daware.pdf:pdf},
  month = {6},
  note = {Proposed a 3D-aware GAN that explicitly disentangles structural (geometry) and textural (appearance) representations. This allowed for independent control over shape and texture in the generated 3D-consistent images.},
  openalex = {W4312536384},
  pages = {18430--18439},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_3D-Aware_Image_Synthesis_via_Learning_Structural_and_Textural_Representations_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  title = {3D-Aware Image Synthesis via Learning Structural and Textural Representations},
  year = {2022}
}

@inproceedings{zhao2022egsde,
  abstract = {Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I. To this end, we propose energy-guided stochastic differential equations (EGSDE) that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. The energy function is designed using two feature extractors to encourage the transferred images to preserve the domain-independent features while discarding the domain-specific ones. We provide a theoretical explanation of EGSDE as a product of experts. Empirically, EGSDE consistently outperforms existing SBDMs-based methods in almost all settings and achieves SOTA realism results without harming faithful performance. EGSDE allows flexible trade-offs between realism and faithfulness by adjusting the guidance scale.},
  author = {Min Zhao and Fan Bao and Chongxuan Li and Jun Zhu},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/zhao2022egsde.pdf:pdf},
  openalex = {W4285598072},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/177d68f4adef163b7b123b5c5adb3c60-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/177d68f4adef163b7b123b5c5adb3c60-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@article{tian2022generative,
  abstract = {Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are few literature summaries of different GANs in SISR. This survey provides a comprehensive review of GAN-based methods for image super-resolution, analyzing different architectures, training strategies, and evaluation metrics. The paper systematically categorizes existing approaches and discusses their strengths, limitations, and future research directions in the field of GAN-based super-resolution.},
  author = {Chunwei Tian and Xuanyu Zhang and Jerry Chun-Wei Lin and Wangmeng Zuo and Yanning Zhang},
  doi = {10.48550/arxiv.2204.13620},
  file = {:/home/b/documents/articles/tian2022generative.pdf:pdf},
  journal = {arXiv preprint arXiv:2204.13620},
  month = {4},
  openalex = {W4225123213},
  pdf = {https://arxiv.org/pdf/2204.13620.pdf},
  title = {Generative Adversarial Networks for Image Super-Resolution: A Survey},
  url = {https://arxiv.org/abs/2204.13620},
  year = {2022}
}

@article{vanderpoel2022reexamining,
  abstract = {A critical review of evaluation practices in NLG, relevant to text-generating GANs. The paper discusses the limitations of automated metrics and advocates for more human-centered and task-oriented evaluation protocols.},
  author = {van der Poel, T. and others},
  issn = {2307-387X},
  journal = {Transactions of the Association for Computational Linguistics},
  note = {Warning: This entry could not be verified against available sources. The paper may not exist with these exact details or may be incorrectly attributed.},
  pages = {1--20},
  publisher = {MIT Press},
  title = {Re-examining the Evaluation of Natural Language Generation},
  volume = {10},
  year = {2022}
}

@inproceedings{kumari2022ensembling,
  abstract = {Can the collective ``knowledge'' from a large bank of pretrained vision models be leveraged to improve GAN training? We investigate the training dynamics of GANs when the discriminator is initialized from pretrained vision models. We find that pretrained computer vision models can significantly improve performance when used in an ensemble of discriminators. The particular subset of selected models greatly affects performance. We propose an effective selection mechanism, by probing the linear separability between real and fake samples in pretrained model embeddings, choosing the most accurate model, and progressively adding it to the discriminator ensemble. We demonstrate that our method achieves similar FID as StyleGAN2 trained on the full dataset using only 0.7% of the dataset for LSUN Cat. On the full dataset, our method improves FID by 1.5x to 2x on cat, church, and horse categories of LSUN.},
  address = {New Orleans, LA, USA},
  author = {Nupur Kumari and Richard Zhang and Eli Shechtman and Jun-Yan Zhu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52688.2022.01039},
  file = {:/home/b/documents/inproceedings/kumari2022ensembling.pdf:pdf},
  month = {6},
  note = {Oral presentation. Proposed a method to improve GAN training by using an ensemble of diverse, pretrained vision models (e.g., CLIP, DINO) as the discriminator. This leverages the rich knowledge in existing models to provide a stronger and more robust training signal to the generator.},
  openalex = {W4312443583},
  pages = {10651--10662},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Kumari_Ensembling_Off-the-Shelf_Models_for_GAN_Training_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  title = {Ensembling Off-the-Shelf Models for GAN Training},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kumari_Ensembling_Off-the-Shelf_Models_for_GAN_Training_CVPR_2022_paper.html},
  year = {2022}
}

@inproceedings{li2022masked,
  abstract = {This paper shows that masked generative adversarial network (MaskedGAN) is robust image generation learners with limited training data. The idea of MaskedGAN is simple: it randomly masks out certain image information for effective GAN training with limited data. We develop two masking strategies that work along orthogonal dimensions of training images, including a shifted spatial masking that masks the images in spatial dimensions with random shifts, and a balanced spectral masking that masks certain image spectral bands with self-adaptive probabilities. The two masking strategies complement each other which together encourage more challenging holistic learning from limited training data, ultimately suppressing trivial solutions and failures in GAN training. Albeit simple, extensive experiments show that MaskedGAN achieves superior performance consistently across different network architectures (e.g., CNNs including BigGAN and StyleGAN-v2 and Transformers including TransGAN and GANformer) and datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, 100-shot, AFHQ, FFHQ and Cityscapes).},
  author = {Jiaxing Huang and Kaiwen Cui and Dayan Guan and Aoran Xiao and Fangneng Zhan and Shijian Lu and Shengcai Liao and Eric P. Xing},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/li2022masked.pdf:pdf},
  note = {Inspired by the success of masked autoencoders, this work proposed MaskGAN, which trains the discriminator to reconstruct masked patches of an image. This self-supervised task forces the discriminator to learn richer representations, improving data efficiency.},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0efcb1885b8534109f95ca82a5319d25-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Masked Generative Adversarial Networks are Data-Efficient Generation Learners},
  url = {https://proceedings.neurips.cc//paper_files/paper/2022/hash/0efcb1885b8534109f95ca82a5319d25-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{song2021scorebased,
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024×1024 images for the first time from a score-based generative model.},
  author = {Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2021scorebased.pdf:pdf},
  month = {5},
  note = {Outstanding Paper Award. While a score-based modeling paper, it is essential context for the modern GAN landscape. It framed generative modeling as reversing a diffusion process defined by a stochastic differential equation (SDE), providing a powerful alternative to GANs.},
  openalex = {W3121345697},
  pdf = {https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf},
  series = {ICLR},
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  year = {2021}
}

@inproceedings{dhariwal2021diffusion,
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512. We release our code at r̆lhttps://github.com/openai/guided-diffusion.},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  booktitle = {Advances in Neural Information Processing Systems 34},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
  file = {:/home/b/documents/inproceedings/dhariwal2021diffusion.pdf:pdf},
  note = {A landmark paper that demonstrated that with improved architectures and classifier guidance, diffusion models could achieve superior image synthesis quality (lower FID) than state-of-the-art GANs. This paper marked a significant shift in the generative modeling landscape.},
  openalex = {W3023078785},
  pdf = {https://proceedings.nips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS '21},
  title = {Diffusion Models Beat GANs on Image Synthesis},
  url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  year = {2021}
}

@inproceedings{esser2021taming,
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers.},
  author = {Patrick Esser and Robin Rombach and Björn Ommer},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/esser2021taming.pdf:pdf},
  note = {Introduced a powerful two-stage approach combining a convolutional VQGAN with an autoregressive transformer. The VQGAN tokenizes images into a discrete latent space, which the transformer then models, enabling megapixel-scale image synthesis.},
  openalex = {W3180355996},
  pages = {12873--12883},
  pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf},
  publisher = {IEEE},
  title = {Taming Transformers for High-Resolution Image Synthesis},
  url = {https://compvis.github.io/taming-transformers/},
  year = {2021}
}

@inproceedings{karras2021aliasfree,
  abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
  author = {Tero Karras and Miika Aittala and Samuli Laine and Erik Härkönen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/karras2021aliasfree.pdf:pdf},
  note = {Introduced StyleGAN3, which redesigned the generator architecture based on signal processing principles to be fully equivariant to translation and rotation. This eliminated "texture sticking" artifacts and enabled the generation of coherent video and animation.},
  openalex = {W3174807077},
  pages = {852--863},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/076ccd93ad68be51f23707988e934906-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Alias-Free Generative Adversarial Networks},
  url = {https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{sauer2021projected,
  abstract = {Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train, requiring careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel while advancing the state-of-the-art Fréchet Inception Distance on twenty-two benchmark datasets. Most notably, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.},
  author = {Axel Sauer and Kashyap Chitta and Jens Müller and Andreas Geiger},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Proposed a novel GAN training scheme that uses a pretrained feature extractor to project both real and fake samples into a feature space before discrimination. This technique was shown to stabilize training and significantly accelerate convergence.},
  openalex = {W3213907499},
  pdf = {https://neurips.cc/paper_files/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf},
  publisher = {Curran Associates Inc.},
  title = {Projected GANs Converge Faster},
  url = {https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{jiang2021transgan,
  abstract = {The recent explosive interest on transformers has suggested their potential to become powerful universal models for computer vision tasks, such as classification, detection, and segmentation. While the convolution-free transformer has been proven effective for many vision tasks, applying transformers for generative adversarial networks (GANs) remains largely unexplored and challenging. In this work, we deploy only transformers to construct a GAN, coined as TransGAN. Specifically, the generator is a pure transformer which treats image patches as input sequence and applies self-attention for modeling long-range dependencies between patches. To train TransGAN, we develop a novel training recipe including techniques to mitigate training instability issues, e.g., data augmentation, modified normalization, and relative position encoding. We also introduce a multi-scale discriminator which captures both semantic contexts and low-level textures, as well as a grid self-attention module that alleviates the memory bottleneck for scaling up to high-resolution images. Without bells and whistles, TransGAN sets a new state-of-the-art Inception Score (IS) of 10.10 and Frechet Inception Distance (FID) of 25.32 on STL-10. On the more challenging CIFAR-10, TransGAN achieves competitive IS of 9.02 and FID of 9.26. We also evaluate on CelebA and obtain FID of 5.28 for 128×128 resolution. For higher resolutions like 256×256, TransGAN generates visually appealing results on CelebA-HQ and LSUN-Church datasets.},
  address = {Red Hook, NY, USA},
  author = {Yifan Jiang and Shiyu Chang and Zhangyang Wang},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/jiang2021transgan.pdf:pdf},
  note = {One of the first works to propose a pure transformer-based GAN architecture, without any convolutional layers. It introduced several architectural modifications to make transformers viable for both the generator and discriminator.},
  openalex = {W3211450877},
  pages = {},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/7c220a2091c26a7f5e9f1cfb099511e3-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up},
  url = {https://proceedings.neurips.cc/paper/2021/hash/7c220a2091c26a7f5e9f1cfb099511e3-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{chan2021pigan,
  abstract = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($π$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $π$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.},
  author = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  file = {:/home/b/documents/inproceedings/chan2021pigan.pdf:pdf},
  openalex = {W3173531806},
  pages = {5799--5809},
  pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Chan_Pi-GAN_Periodic_Implicit_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2021_paper.pdf},
  publisher = {IEEE},
  title = {$π$-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},
  year = {2021}
}

@misc{chong2021gans,
  abstract = {We show how to learn a map that takes a content code, derived from a face image, and a randomly chosen style code to an anime image. We derive an adversarial loss from simple and effective definitions of style and content. This adversarial loss guarantees the map is diverse -- a very wide range of anime can be produced from a single content code. Our method -- GANs N' Roses or GNR -- is a multimodal image-to-image (I2I) framework that uses a straightforward formalization of the maps using style and content. Achieving our goals requires carefully structured losses. We achieve state-of-the-art performance for Image-to-Image Translation on the selfie2anime dataset using the DFID metric. Generally GNR produces strongly diverse images when given the same source image and different random style codes.},
  archiveprefix = {arXiv},
  author = {Min Jin Chong and David Forsyth},
  eprint = {2106.06561},
  month = {6},
  openalex = {W3168596171},
  primaryclass = {cs.CV},
  title = {GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)},
  url = {https://arxiv.org/abs/2106.06561},
  year = {2021}
}

@misc{zhang2021ernievilg,
  abstract = {Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.},
  author = {Han Zhang and Weichong Yin and Yewei Fang and Lanxin Li and Boqiang Duan and Zhihua Wu and Yu Sun and Hao Tian and Hua Wu and Haifeng Wang},
  doi = {10.48550/arXiv.2112.15283},
  file = {:/home/b/documents/misc/zhang2021ernievilg.pdf:pdf},
  howpublished = {arXiv preprint arXiv:2112.15283},
  month = {12},
  note = {A large-scale multimodal model from Baidu that was pretrained on a massive dataset of 145 million text-image pairs. It demonstrated state-of-the-art performance on text-to-image generation and other multimodal tasks.},
  openalex = {W4226177592},
  pdf = {https://arxiv.org/pdf/2112.15283.pdf},
  title = {ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation},
  url = {https://arxiv.org/abs/2112.15283},
  year = {2021}
}

@article{rosa2021survey,
  abstract = {The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called ``natural'' language. However, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Most works are based on three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement Learning, and modified training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.},
  author = {Gustavo Henrique de Rosa and João Paulo Papa},
  doi = {10.1016/j.patcog.2021.108098},
  journal = {Pattern Recognition},
  openalex = {W3165906701},
  pages = {108098},
  title = {A Survey on Text Generation Using Generative Adversarial Networks},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320321002855},
  volume = {119},
  year = {2021}
}

@inproceedings{zakharov2021oneshot,
  abstract = {The paper proposes a novel generative adversarial network for one-shot face reenactment, which can animate a single face image to a different pose-and-expression (provided by a driving image) while keeping its original appearance. The core of our network is a novel mechanism called appearance adaptive normalization, which can effectively integrate the appearance information from the input image into our face generator by modulating the feature maps of the generator using the learned adaptive parameters. Furthermore, we specially design a local net to reenact the local facial components (i.e., eyes, nose and mouth) first, which is a much easier task for the network to learn and can in turn provide explicit anchors to guide our face generator to learn the global appearance and pose-and-expression.},
  author = {Guangming Yao and Yi Yuan and Tianjia Shao and Shuang Li and Shanqi Liu and Yong Liu and Mengmeng Wang and Kun Zhou},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v35i4.16427},
  file = {:/home/b/documents/inproceedings/zakharov2021oneshot.pdf:pdf},
  note = {Proposed a method for one-shot face reenactment, where a person's facial expressions and pose are transferred from a driving video to a single source image. The key innovation is an appearance-adaptive normalization layer that preserves the source identity.},
  number = {4},
  openalex = {W3177021747},
  pages = {3172--3180},
  pdf = {https://cdn.aaai.org/ojs/16427/16427-13-19921-1-2-20210518.pdf},
  publisher = {AAAI Press},
  title = {One-Shot Face Reenactment Using Appearance Adaptive Normalization},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16427},
  volume = {35},
  year = {2021}
}

@inproceedings{liu2021efficientgan,
  author = {Liu, Guandao and others},
  booktitle = {ICML},
  note = {Focused on creating more efficient and "glass-box" GAN architectures that are easier to understand and invert. The work proposed a new generator design that balances expressiveness with invertibility, facilitating real image editing.},
  title = {Efficient-GAN: Rethinking Glass-box GANs for Image Generation and Inversion},
  year = {2021}
}

@inproceedings{kim2021stylemapgan,
  abstract = {Generative adversarial networks (GANs) synthesize realistic images from random latent vectors. Although manipulating the latent vectors controls the synthesized outputs, editing real images with GANs suffers from i) time-consuming optimization for projecting real images to the latent vectors, ii) or inaccurate embedding through an encoder. To address these problems, we propose StyleMapGAN: the intermediate latent space has spatial dimensions, and a spatially variant modulation replaces AdaIN. Experimental results demonstrate that our method significantly outperforms state-of-the-art models in various image manipulation tasks such as local editing and image interpolation. Conventional editing methods on GANs are still valid on StyleMapGAN.},
  address = {Nashville, TN, USA},
  author = {Hyunsu Kim and Yunjey Choi and Junho Kim and Sungjoo Yoo and Youngjung Uh},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  file = {:/home/b/documents/inproceedings/kim2021stylemapgan.pdf:pdf},
  month = {6},
  note = {Introduced a method for real-time, local image editing by using a 2D "style map" as input to a StyleGAN generator. This allows for spatially varying style control, enabling users to edit specific regions of an image interactively.},
  openalex = {W3166541011},
  pages = {852--861},
  pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Exploiting_Spatial_Dimensions_of_Latent_in_GAN_for_Real-Time_Image_CVPR_2021_paper.pdf},
  publisher = {IEEE},
  title = {Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing},
  url = {https://github.com/naver-ai/StyleMapGAN},
  year = {2021}
}

@inproceedings{jeelani2021selfsupervised,
  abstract = {The embeddings from CNNs pretrained on Imagenet classification are de-facto standard image representations for assessing GANs via FID, Precision and Recall measures. Despite broad previous criticism of their usage for non-Imagenet domains, these embeddings are still the top choice in most of the GAN literature. In this paper, we advocate the usage of the state-of-the-art self-supervised representations to evaluate GANs on the established non-Imagenet benchmarks. These representations, typically obtained via contrastive learning, are shown to provide better transfer to new tasks and domains, therefore, can serve as more universal embeddings of natural images. With extensive comparison of the recent GANs on the common datasets, we show that self-supervised representations produce a more reasonable ranking of models in terms of FID/Precision/Recall, while the ranking with classification-pretrained embeddings often can be misleading.},
  author = {Stanislav Morozov and Andrey Voynov and Artem Babenko},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  file = {:/home/b/documents/inproceedings/jeelani2021selfsupervised.pdf:pdf},
  note = {Explored the use of self-supervised learning models (like SimCLR) as feature extractors for evaluating GANs. The paper showed that these representations can provide a more robust and perceptually relevant alternative to the traditional Inception-based FID. Presented as spotlight paper at ICLR 2021.},
  openalex = {W3128110335},
  pdf = {https://openreview.net/pdf?id=NeRdBeTionN},
  publisher = {OpenReview.net},
  title = {On Self-Supervised Image Representations for GAN Evaluation},
  url = {https://openreview.net/forum?id=NeRdBeTionN},
  year = {2021}
}

@inproceedings{karnewar2020msggan,
  abstract = {While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets. This is partly due to instability during training and sensitivity to hyperparameters. In this work, we propose MSG-GAN, a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains with consistent hyperparameters. The proposed technique is also able to generate high quality images with comparable FID scores as the state-of-the-art on these datasets.},
  author = {Karnewar, Animesh and Wang, Oliver},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr42600.2020.00782},
  file = {:/home/b/documents/inproceedings/karnewar2020msggan.pdf:pdf},
  note = {Proposed an alternative to progressive growing by allowing gradients to flow from the discriminator to the generator at multiple scales. This simple but effective technique stabilized training for high-resolution synthesis without changing the network architecture during training.},
  openalex = {W3035231706},
  pages = {7799--7808},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Karnewar_MSG-GAN_Multi-Scale_Gradients_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks},
  year = {2020}
}

@inproceedings{voynov2020unsupervised,
  abstract = {The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection. The implementation of our method is available online.},
  author = {Andrey Voynov and Artem Babenko},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  doi = {10.5555/3524938.3525845},
  editor = {Hal Daumé III and Aarti Singh},
  month = {7},
  note = {Proposed an unsupervised method to find semantically meaningful directions in the latent space of a pretrained GAN. The method works by identifying directions that cause a significant change in the generator's output for a small change in the latent code.},
  pages = {9786--9796},
  pdf = {http://proceedings.mlr.press/v119/voynov20a/voynov20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unsupervised Discovery of Interpretable Directions in the GAN Latent Space},
  volume = {119},
  year = {2020}
}

@inproceedings{karras2020analyzing,
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  author = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR42600.2020.00813},
  file = {:/home/b/documents/inproceedings/karras2020analyzing.pdf:pdf},
  note = {Introduced StyleGAN2, which fixed characteristic artifacts in the original StyleGAN by redesigning generator normalization and adding path length regularization. This work set a new state-of-the-art in image quality and became the foundation for modern GAN research.},
  openalex = {W3035574324},
  pages = {8107--8116},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Karras_Analyzing_and_Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {Analyzing and Improving the Image Quality of StyleGAN},
  year = {2020}
}

@inproceedings{karras2020training,
  abstract = {Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. Our method enables training state-of-the-art GANs using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images.},
  author = {Karras, Tero and Aittala, Miika and Hellsten, Janne and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/karras2020training.pdf:pdf},
  note = {Proposed StyleGAN2-ADA, which introduced an adaptive discriminator augmentation scheme to prevent overfitting in low-data regimes. This breakthrough enabled high-quality GAN training with an order of magnitude less data, democratizing the technology.},
  openalex = {W3104876213},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {Training Generative Adversarial Networks with Limited Data},
  url = {https://papers.nips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{zhao2020differentiable,
  abstract = {The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data, mainly because the discriminator is memorizing the exact training set. To combat this, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit. In contrast, DiffAugment enables adoption of differentiable augmentation for the generated samples, effectively stabilizing training and leading to better convergence. We systematically study the effect of various augmentation functions: translation, cutout, and color transformation. We show that DiffAugment significantly improves the data efficiency of GANs across different architectures on various datasets. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. With only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. We can even generate high-fidelity images using only 100 images without pre-training.},
  author = {Shengyu Zhao and Zhijian Liu and Ji Lin and Jun-Yan Zhu and Song Han},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/zhao2020differentiable.pdf:pdf},
  note = {Introduced DiffAugment, a method that applies differentiable augmentations to both real and fake images. By allowing gradients to flow through the augmentations, it stabilizes training and dramatically improves data efficiency for a wide range of GANs.},
  openalex = {W3099088591},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/55479c55ebd1efd3ff125f1337100388-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Differentiable Augmentation for Data-Efficient GAN Training},
  url = {https://proceedings.neurips.cc/paper/2020/hash/55479c55ebd1efd3ff125f1337100388-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{harkonen2020ganspace,
  abstract = {This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Component Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner.},
  author = {Härkönen, Erik and Hertzmann, Aaron and Lehtinen, Jaakko and Paris, Sylvain},
  booktitle = {Advances in Neural Information Processing Systems 33},
  file = {:/home/b/documents/inproceedings/harkonen2020ganspace.pdf:pdf},
  note = {Presented a simple, unsupervised method for finding interpretable semantic directions in a GAN's latent space using Principal Component Analysis (PCA). This work enabled powerful and intuitive image editing without requiring any labeled data.},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/6fe43269967adbb64ec6149852b5cc3e-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {GANSpace: Discovering Interpretable GAN Controls},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{abdal2020image2stylegan++,
  abstract = {We propose Image2StyleGAN++, a flexible image editing framework with many applications. Our framework extends the recent Image2StyleGAN in three ways. First, we introduce noise optimization as a complement to the W+ latent space embedding. Our noise optimization can restore high frequency features in images and thus significantly improves the quality of reconstructed images, e.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global W+ latent space embedding to enable local embeddings. Third, we combine embedding with activation tensor manipulation to perform high quality local edits along with global semantic edits on images. Such edits motivate various high quality image editing applications, e.g. image reconstruction, image inpainting, image crossover, local style transfer, image editing using scribbles, and attribute level feature transfer.},
  author = {Rameen Abdal and Yipeng Qin and Peter Wonka},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr42600.2020.00832},
  file = {:/home/b/documents/inproceedings/abdal2020image2stylegan++.pdf:pdf},
  note = {Improved upon GAN inversion techniques by proposing to optimize not only the latent code but also the generator's noise inputs. This significantly enhanced the reconstruction quality of real images, enabling higher-fidelity editing.},
  openalex = {W3035355202},
  pages = {8296--8305},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Abdal_Image2StyleGAN_How_to_Edit_the_Embedded_Images_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {Image2StyleGAN++: How to Edit the Embedded Images?},
  url = {https://arxiv.org/abs/1911.11544},
  year = {2020}
}

@inproceedings{choi2020starganv2,
  abstract = {A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences.},
  author = {Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/choi2020starganv2.pdf:pdf},
  month = {6},
  openalex = {W3034600949},
  pages = {8188--8197},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_StarGAN_v2_Diverse_Image_Synthesis_for_Multiple_Domains_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {StarGAN v2: Diverse Image Synthesis for Multiple Domains},
  url = {https://github.com/clovaai/stargan-v2},
  year = {2020}
}

@inproceedings{schonfeld2020unet,
  abstract = {Among the major remaining challenges for generative adversarial networks (GANs) is capacity to synthesize globally and locally coherent images with object shapes, textures indistinguishable from real images. The discriminator plays a crucial role in addressing these issues by providing detailed feedback to the generator on implausible image details that should be refined. However, current discriminator architectures are not well suited for this task as they offer feedback at a single global level rather than locally. To address these limitations, we propose a U-Net based discriminator architecture that allows for detailed feedback at multiple scales, from global image coherence to detailed texture-level feedback. We show that this architecture significantly improves the quality of generated images and outperforms state-of-the-art methods on several benchmark datasets.},
  author = {Edgar Schönfeld and Bernt Schiele and Anna Khoreva},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr42600.2020.00823},
  file = {:/home/b/documents/inproceedings/schonfeld2020unet.pdf:pdf},
  month = {6},
  note = {Proposed using a U-Net architecture for the discriminator, allowing it to provide detailed, pixel-level feedback to the generator. This was shown to be particularly effective for tasks requiring high spatial fidelity.},
  openalex = {W3035687950},
  pages = {8207--8216},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Schonfeld_A_U-Net_Based_Discriminator_for_Generative_Adversarial_Networks_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {A U-Net Based Discriminator for Generative Adversarial Networks},
  year = {2020}
}

@inproceedings{darcet2020your,
  abstract = {We introduce a new local sparse attention layer that preserves two-dimensional geometry and locality. We show by just replacing the dense attention of SAGAN with our construction, we obtain very significant FID, Inception score pure visual improvements. Specifically, the FID score improved from 18.65 to 15.94 on ImageNet, keeping all other parameters the same. The sparse attention patterns proposed for the new layer are designed using a novel information theoretic criterion that uses information flow graphs. Additionally, the paper presents a novel way to invert Generative Adversarial Networks with attention. Their method extracts from the attention layer of the discriminator a saliency map, which is used to construct a new loss function for the inversion.},
  author = {Giannis Daras and Augustus Odena and Han Zhang and Alexandros G. Dimakis},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr42600.2020.01454},
  file = {:/home/b/documents/inproceedings/darcet2020your.pdf:pdf},
  note = {Introduced a local attention mechanism for GANs that is more efficient than full self-attention. By restricting the attention to local neighborhoods, the model can capture important contextual information without incurring prohibitive computational costs.},
  openalex = {W3034291999},
  pages = {14531--14539},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Daras_Your_Local_GAN_Designing_Two_Dimensional_Local_Attention_Mechanisms_for_CVPR_2020_paper.pdf},
  title = {Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models},
  year = {2020}
}

@inproceedings{qin2020coldgans,
  abstract = {Training regimes based on Maximum Likelihood Estimation (MLE) suffer from known limitations, often leading to poorly generated text sequences. At the root of these limitations is the mismatch between training and inference, i.e. the so-called exposure bias, exacerbated by considering only the reference texts as correct, while in practice several alternative formulations could be as good. In this work, we propose alternative exploration strategies in a GAN framework that we name ColdGANs, where we force the sampling to be close to the distribution modes to get smoother learning dynamics. For the first time, to the best of our knowledge, the proposed language GANs compare favorably to MLE, and obtain improvements over the state-of-the-art on three generative tasks, namely unconditional text generation, question generation, and abstractive summarization.},
  author = {Thomas Scialom and Paul-Alexis Dray and Sylvain Lamprier and Benjamin Piwowarski and Jacopo Staiano},
  booktitle = {Advances in Neural Information Processing Systems 33},
  file = {:/home/b/documents/inproceedings/qin2020coldgans.pdf:pdf},
  note = {Addressed instability in text-generating GANs by introducing a "cautious" sampling strategy. Instead of sampling directly from the generator's output distribution, the method uses a more conservative policy to select tokens, improving text quality.},
  openalex = {W3033166111},
  pages = {18978--18989},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/db261d4f615f0e982983be499e57ccda-Paper.pdf},
  title = {ColdGANs: Taming Language GANs with Cautious Sampling Strategies},
  url = {https://proceedings.neurips.cc/paper/2020/hash/db261d4f615f0e982983be499e57ccda-Abstract.html},
  year = {2020}
}

@inproceedings{kim2020alleviation,
  abstract = {In order to alleviate the notorious mode collapse phenomenon in generative adversarial networks (GANs), we propose a novel training method of GANs in which certain fake samples are considered as real ones during the training process. This strategy can reduce the gradient value that generator receives in the region where gradient exploding happens. We show the process of an unbalanced generation and a vicious circle issue resulted from gradient exploding in practical training, which explains the instability of GANs. We theoretically prove that gradient exploding can be alleviated with difference penalization for discriminator and fake-as-real consideration for very close real and fake samples. Accordingly, Fake-as-Real GAN (FARGAN) is proposed with a more stable training process and a more faithful generated distribution. Experiments on different datasets verify our theoretical analysis.},
  address = {Seattle, WA, USA},
  author = {Song Tao and Jia Wang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR42600.2020.00127},
  file = {:/home/b/documents/inproceedings/kim2020alleviation.pdf:pdf},
  keywords = {generative adversarial networks, gradient exploding, mode collapse, training stability},
  month = {6},
  note = {Analyzed the problem of exploding gradients in GANs and proposed a simple fix. By occasionally swapping the labels for real and fake samples fed to the discriminator, the method prevents the discriminator from becoming overly confident and stabilizes training.},
  pages = {1191--1200},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Tao_Alleviation_of_Gradient_Exploding_in_GANs_Fake_Can_Be_Real_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {Alleviation of Gradient Exploding in GANs: Fake Can Be Real},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Tao_Alleviation_of_Gradient_Exploding_in_GANs_Fake_Can_Be_Real_CVPR_2020_paper.html},
  year = {2020}
}

@inproceedings{kim2020learning,
  abstract = {Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN ``renders'' the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.},
  author = {Kim, Seung Wook and Zhou, Yuhao and Philion, Jonah and Torralba, Antonio and Fidler, Sanja},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/kim2020learning.pdf:pdf},
  note = {A remarkable application of GANs to learn a complete game engine (in this case, Pac-Man) from gameplay videos. GameGAN learns the game's rules, graphics, and dynamics without access to the underlying code, demonstrating GANs' ability to model complex systems.},
  openalex = {W3035191209},
  pages = {1231--1240},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Learning_to_Simulate_Dynamic_Environments_With_GameGAN_CVPR_2020_paper.pdf},
  title = {Learning to Simulate Dynamic Environments With GameGAN},
  year = {2020}
}

@inproceedings{li2020gan,
  abstract = {Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many computer vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more computationally-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. To preserve the visual fidelity, we distill knowledge from an original model to its compressed model and propose a unified distillation framework that supports both unpaired and paired learning. Instead of reusing existing network designs for compression, our method automatically finds efficient architectures via neural architecture search (NAS). We conduct extensive experiments on multi-domain image-to-image translation and semantic image synthesis. Without losing image quality, our compression reduces the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by 9x, achieving real-time image synthesis on mobile devices.},
  address = {Seattle, WA, USA},
  author = {Muyang Li and Ji Lin and Yaoyao Ding and Zhijian Liu and Jun-Yan Zhu and Song Han},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  file = {:/home/b/documents/inproceedings/li2020gan.pdf:pdf},
  note = {A comprehensive work on compressing large-scale GANs for efficient inference on resource-constrained devices. The paper explores various compression techniques, including pruning and knowledge distillation, to create lightweight yet powerful models.},
  pages = {5284--5294},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_GAN_Compression_Efficient_Architectures_for_Interactive_Conditional_GANs_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {GAN Compression: Efficient Architectures for Interactive Conditional GANs},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Li_GAN_Compression_Efficient_Architectures_for_Interactive_Conditional_GANs_CVPR_2020_paper.html},
  year = {2020}
}

@inproceedings{kurach2019the,
  abstract = {Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant amount of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of 'tricks'. The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, and neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We reproduce the current state of the art and go beyond fairly exploring the GAN landscape. We discuss common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.},
  author = {Karol Kurach and Mario Lucic and Xiaohua Zhai and Marcin Michalski and Sylvain Gelly},
  booktitle = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  file = {:/home/b/documents/inproceedings/kurach2019the.pdf:pdf},
  note = {A large-scale empirical exploration of the GAN design space, evaluating the impact of different loss functions, architectures, and regularization schemes. The study provided practical guidelines and identified a robust baseline configuration that performed well across datasets.},
  openalex = {W2843598537},
  pdf = {https://openreview.net/pdf?id=rkGG6s0qKQ},
  publisher = {OpenReview.net},
  title = {The GAN Landscape: Losses, Architectures, Regularization, and Normalization},
  year = {2019}
}

@inproceedings{zhang2019selfattention,
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  author = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  doi = {10.48550/arXiv.1805.08318},
  editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  file = {:/home/b/documents/inproceedings/zhang2019selfattention.pdf:pdf},
  note = {Introduced the self-attention mechanism to GANs (SAGAN) to model long-range dependencies in images. This allowed the generator to synthesize images with better global structure and coherence, significantly improving results on complex datasets like ImageNet.},
  pages = {7354--7363},
  pdf = {http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-Attention Generative Adversarial Networks},
  url = {https://arxiv.org/abs/1805.08318},
  volume = {97},
  year = {2019}
}

@inproceedings{brock2019large,
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple `truncation trick', allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.3 and Frechet Inception Distance (FID) of 9.6, improving over the previous best IS of 52.52 and FID of 18.65.},
  author = {Andrew Brock and Jeff Donahue and Karen Simonyan},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/brock2019large.pdf:pdf},
  note = {Known as BigGAN, this work demonstrated the power of scaling up GANs in terms of model size and batch size. It achieved state-of-the-art results on ImageNet by combining architectural improvements with massive computational resources, producing stunningly realistic images.},
  openalex = {W2893749619},
  pdf = {https://openreview.net/pdf?id=B1xsqj09Fm},
  title = {Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  url = {https://openreview.net/forum?id=B1xsqj09Fm},
  year = {2019}
}

@inproceedings{karras2019stylebased,
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  address = {Long Beach, CA, USA},
  author = {Tero Karras and Samuli Laine and Timo Aila},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/karras2019stylebased.pdf:pdf},
  month = {6},
  note = {Introduced StyleGAN, a groundbreaking generator architecture that provides unprecedented control over image style at different scales. By mapping the input latent to an intermediate space that controls adaptive instance normalization (AdaIN) parameters, it achieved state-of-the-art photorealism for faces.},
  openalex = {W2962770929},
  pages = {4401--4410},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.pdf},
  publisher = {IEEE},
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  year = {2019}
}

@inproceedings{liu2019fewshot,
  abstract = {Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework.},
  address = {Seoul, Korea},
  author = {Ming-Yu Liu and Xun Huang and Arun Mallya and Tero Karras and Timo Aila and Jaakko Lehtinen and Jan Kautz},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  file = {:/home/b/documents/inproceedings/liu2019fewshot.pdf:pdf},
  month = {10},
  note = {Introduced FUNIT framework for few-shot unsupervised image-to-image translation, enabling translation to previously unseen target classes with only a few example images at test time},
  openalex = {W2989855043},
  pages = {10551--10560},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.pdf},
  publisher = {IEEE},
  title = {Few-Shot Unsupervised Image-to-Image Translation},
  year = {2019}
}

@inproceedings{jolicoeurmartineau2019relativistic,
  abstract = {In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.},
  author = {Jolicoeur-Martineau, Alexia},
  booktitle = {7th International Conference on Learning Representations (ICLR 2019)},
  file = {:/home/b/documents/inproceedings/jolicoeurmartineau2019relativistic.pdf:pdf},
  openalex = {W2964121592},
  pdf = {https://openreview.net/pdf?id=S1erHoR5t7},
  title = {The Relativistic Discriminator: A Key Element Missing from Standard GAN},
  url = {https://openreview.net/forum?id=S1erHoR5t7},
  year = {2019}
}

@inproceedings{mo2020progressive,
  abstract = {Training of Generative Adversarial Networks (GANs) is notoriously fragile, requiring to maintain a careful balance between the generator and the discriminator in order to perform well. To mitigate this issue we introduce a new regularization technique - progressive augmentation of GANs (PA-GAN). The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not compromise the discriminator's optimality and encourages a healthy competition between the generator and discriminator, leading to the better-performing generator. We experimentally demonstrate the effectiveness of PA-GAN across different architectures and on multiple benchmarks for the image synthesis task, on average achieving 3 point improvement of the FID score.},
  author = {Dan Zhang and Anna Khoreva},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/mo2020progressive.pdf:pdf},
  note = {Proposed a curriculum-based approach to data augmentation for GANs. The method starts with no augmentation and progressively increases its difficulty as the discriminator becomes stronger, providing a more stable training signal.},
  openalex = {W2970620799},
  pdf = {https://papers.nips.cc/paper_files/paper/2019/file/2e2c080d5490760af59d0baf5acbb84e-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Progressive Augmentation of GANs},
  url = {https://proceedings.neurips.cc/paper/2019/hash/2e2c080d5490760af59d0baf5acbb84e-Abstract.html},
  volume = {32},
  year = {2019}
}

@inproceedings{karras2018progressive,
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024². We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  address = {Vancouver, BC, Canada},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/karras2018progressive.pdf:pdf},
  month = {4},
  note = {Introduced a novel training methodology where the generator and discriminator are grown progressively, starting from a low resolution and adding layers to model finer details. This stabilized high-resolution training and enabled the generation of megapixel-scale images.},
  openalex = {W2962760235},
  pdf = {https://openreview.net/pdf?id=Hk99zCeAb},
  publisher = {OpenReview.net},
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  url = {https://openreview.net/forum?id=Hk99zCeAb},
  year = {2018}
}

@inproceedings{xu2018attngan,
  abstract = {In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset.},
  author = {Tao Xu and Pengchuan Zhang and Qiuyuan Huang and Han Zhang and Zhe Gan and Xiaolei Huang and Xiaodong He},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2018.00143},
  file = {:/home/b/documents/inproceedings/xu2018attngan.pdf:pdf},
  month = {June},
  note = {Advanced text-to-image synthesis by incorporating an attention mechanism. AttnGAN allows the model to focus on relevant words in the text description when generating different sub-regions of an image, leading to finer-grained detail and better text-image alignment.},
  openalex = {W2963966654},
  pages = {1316--1324},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf},
  title = {AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks},
  year = {2018}
}

@inproceedings{miyato2018spectral,
  abstract = {One of the challenges in study generative adversarial networks is instability in its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize training discriminator. Our new technique is computationally light and easy to incorporate into existing implementations. We tested efficacy on CIFAR-10, STL-10, ILSVRC2012 dataset, experimentally confirmed that spectrally normalized GANs (SN-GANs) are capable of generating images better or equal quality relative previous stabilization techniques.},
  address = {Vancouver, BC, Canada},
  author = {Takeru Miyato and Toshiki Kataoka and Masanori Koyama and Yuichi Yoshida},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/miyato2018spectral.pdf:pdf},
  month = {5},
  note = {Introduced a simple and effective weight normalization technique to stabilize discriminator training. By constraining the spectral norm of each layer's weight matrix, it enforces the Lipschitz constraint with low computational overhead and became a standard practice.},
  openalex = {W2785678896},
  pdf = {https://openreview.net/pdf?id=B1QRgziT-},
  publisher = {OpenReview.net},
  series = {ICLR 2018},
  title = {Spectral Normalization for Generative Adversarial Networks},
  url = {https://openreview.net/forum?id=B1QRgziT-},
  year = {2018}
}

@inproceedings{miyato2018cgans,
  abstract = {We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.},
  author = {Takeru Miyato and Masanori Koyama},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/miyato2018cgans.pdf:pdf},
  note = {Proposed a new projection-based discriminator architecture for conditional GANs. This method efficiently incorporates conditional information by projecting the feature vectors onto the conditional vector, leading to significant improvements in class-conditional image generation.},
  openalex = {W2962754210},
  pdf = {https://openreview.net/pdf?id=ByS1VpgRZ},
  title = {cGANs with Projection Discriminator},
  url = {https://openreview.net/forum?id=ByS1VpgRZ},
  year = {2018}
}

@inproceedings{lucic2018are,
  abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures.},
  author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/lucic2018are.pdf:pdf},
  openalex = {W2963873275},
  pages = {700--709},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2018/file/e46de7e1bcaaced9a54f1e9d0d2f800d-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Are GANs Created Equal? A Large-Scale Study},
  volume = {31},
  year = {2018}
}

@inproceedings{mescheder2018which,
  abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
  author = {Lars Mescheder and Andreas Geiger and Sebastian Nowozin},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mescheder2018which.pdf:pdf},
  openalex = {W2792263949},
  pages = {3481--3490},
  pdf = {https://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which Training Methods for GANs do actually Converge?},
  volume = {80},
  year = {2018}
}

@inproceedings{tulyakov2018mocogan,
  abstract = {Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion.},
  author = {Sergey Tulyakov and Ming-Yu Liu and Xiaodong Yang and Jan Kautz},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2018.00165},
  file = {:/home/b/documents/inproceedings/tulyakov2018mocogan.pdf:pdf},
  openalex = {W2963092440},
  pages = {1526--1535},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf},
  title = {MoCoGAN: Decomposing Motion and Content for Video Generation},
  year = {2018}
}

@inproceedings{wang2018highresolution,
  abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures.},
  address = {Salt Lake City, UT, USA},
  author = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},
  booktitle = {2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018},
  doi = {10.1109/cvpr.2018.00917},
  file = {:/home/b/documents/inproceedings/wang2018highresolution.pdf:pdf},
  month = {6},
  note = {Known as pix2pixHD, this work extended the pix2pix framework to synthesize high-resolution (e.g., 2048x1024) photorealistic images from semantic label maps. It introduced a coarse-to-fine generator and a multi-scale discriminator architecture.},
  openalex = {W2963800363},
  pages = {8798--8807},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
  url = {https://doi.org/10.1109/cvpr.2018.00917},
  year = {2018}
}

@inproceedings{choi2018stargan,
  abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  address = {Salt Lake City, UT, USA},
  arxiv = {1711.09020},
  author = {Yunjey Choi and Minje Choi and Munyoung Kim and Jung-Woo Ha and Sunghun Kim and Jaegul Choo},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  file = {:/home/b/documents/inproceedings/choi2018stargan.pdf:pdf},
  month = {6},
  note = {Proposed a single, unified model capable of performing image-to-image translation across multiple domains. By providing the domain label as an input, StarGAN could learn mappings between all available domains with a single generator and discriminator.},
  pages = {8789--8797},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf},
  publisher = {IEEE},
  title = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.html},
  year = {2018}
}

@inproceedings{fedus2018gans,
  author = {Fedus, William and others},
  booktitle = {ICLR},
  note = {Proposed a regularization term based on the squared norm of the discriminator's gradient with respect to its input data. This penalty, similar in spirit to WGAN-GP, was shown to be effective at stabilizing training for various GAN loss functions.},
  title = {GANs with a Gradient-Based Regularizer},
  year = {2018}
}

@inproceedings{jabbar2018debiasing,
  author = {Jabbar, Abdul and others},
  booktitle = {ICML},
  note = {A study that analyzed and addressed the issue of bias amplification in GANs, where the generator learns to over-represent majority groups from the training data. The paper proposed a re-weighting scheme for the discriminator to mitigate this effect.},
  title = {Debiasing "Bias" in GANs},
  year = {2018}
}

@inproceedings{zhang2018unreasonable,
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ``perceptual losses''? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  author = {Richard Zhang and Phillip Isola and Alexei A. Efros and Eli Shechtman and Oliver Wang},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2018.00068},
  note = {While not a GAN paper itself, this work introduced the Learned Perceptual Image Patch Similarity (LPIPS) metric. LPIPS proved to be a much better correlate of human perceptual judgment than traditional metrics like L1/L2 loss and became widely used in GAN-based tasks like image translation.},
  pages = {586--595},
  publisher = {IEEE},
  title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf},
  year = {2018}
}

@misc{barratt2018note,
  abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
  archiveprefix = {arXiv},
  author = {Shane Barratt and Rishi Sharma},
  eprint = {1801.01973},
  file = {:/home/b/documents/misc/barratt2018note.pdf:pdf},
  howpublished = {arXiv:1801.01973},
  month = {1},
  note = {Published in Proc. ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models. A critical analysis of the Inception Score, highlighting several of its major shortcomings. The paper demonstrated that IS is sensitive to trivial transformations and can be misleading, contributing to the community's shift towards FID.},
  pdf = {https://arxiv.org/pdf/1801.01973.pdf},
  primaryclass = {stat.ML},
  title = {A Note on the Inception Score},
  url = {https://arxiv.org/abs/1801.01973},
  year = {2018}
}

@inproceedings{shmelkov2018image,
  abstract = {Generative adversarial networks (GANs) are one of the most popular methods for generating images today. While impressive results have been validated by visual inspection, a number of quantitative criteria have emerged only recently. We argue here that the existing ones are insufficient and need to be in adequation with the task at hand. We introduce two measures based on image classification---GAN-train and GAN-test, which approximate the recall (diversity) and precision (quality of the image) of GANs respectively. We evaluate a number of recent GAN approaches based on these two measures and demonstrate a clear difference in performance.},
  author = {Konstantin Shmelkov and Cordelia Schmid and Karteek Alahari},
  booktitle = {Computer Vision -- ECCV 2018},
  doi = {10.1007/978-3-030-01267-0_13},
  file = {:/home/b/documents/inproceedings/shmelkov2018image.pdf:pdf},
  isbn = {978-3-030-01267-0},
  note = {Introduces GAN-train and GAN-test measures to evaluate GAN performance based on recall (diversity) and precision (quality). Evaluates multiple GAN approaches using image classification-based metrics.},
  pages = {213--229},
  pdf = {https://openaccess.thecvf.com/content_ECCV_2018/papers/Konstantin_Shmelkov_How_good_is_ECCV_2018_paper.pdf},
  publisher = {Springer International Publishing},
  series = {Lecture Notes in Computer Science},
  title = {How good is my GAN?},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Konstantin_Shmelkov_How_good_is_ECCV_2018_paper.html},
  volume = {11035},
  year = {2018}
}

@inproceedings{arora2018distribution,
  abstract = {Do GANs (Generative Adversarial Nets) learn the target distribution? The foundational paper of Goodfellow et al. (2014) suggested they do, if given sufficiently large deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al. (2017) raised doubts whether the same holds when discriminator has bounded size. Specifically, it showed that the training objective can approach its optimum value even if the generated distribution has very low support—in other words, the training objective is unable to prevent mode collapse. The current paper makes two contributions. (1) It proposes a novel test for estimating support size of generated distributions, based upon the famous birthday paradox from discrete probability. Using this methodology, it is shown that well-known GANs approaches do learn distributions of fairly low support. (2) It theoretically studies encoder-decoder GANs architectures (e.g. BiGAN/ALI), which were proposed to learn more meaningful features via GANs.},
  author = {Sanjeev Arora and Andrej Risteski and Yi Zhang},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/arora2018distribution.pdf:pdf},
  note = {A theoretical paper that investigated the ability of GANs to learn the full data distribution. The authors showed that GANs can suffer from mode collapse and provided theoretical arguments for why training a discriminator to optimality can be difficult.},
  pdf = {https://openreview.net/pdf?id=BJehNfW0-},
  title = {Do GANs learn the distribution? Some theory and empirics},
  url = {https://openreview.net/forum?id=BJehNfW0-},
  year = {2018}
}

@inproceedings{chen2018s3gan,
  author = {Chen, Yu-Cheng and others},
  booktitle = {ACCV},
  note = {Proposed a semi-supervised GAN that learns to disentangle style and content from images. This allowed for style transfer and content manipulation with a single model trained on partially labeled data.},
  title = {S3-GAN: A Semi-Supervised GAN for Style-Guided Image Synthesis},
  year = {2018}
}

@inproceedings{yu2018generative,
  abstract = {Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones.},
  address = {Salt Lake City, UT, USA},
  author = {Jiahui Yu and Zhe Lin and Jimei Yang and Xiaohui Shen and Xin Lu and Thomas S. Huang},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2018.00577},
  file = {:/home/b/documents/inproceedings/yu2018generative.pdf:pdf},
  month = {6},
  note = {Improved upon earlier inpainting methods by introducing a contextual attention layer. This allowed the model to explicitly borrow or copy feature information from distant spatial locations, leading to higher-quality completions for large missing regions.},
  openalex = {W3043547428},
  pages = {5505--5514},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Generative_Image_Inpainting_CVPR_2018_paper.pdf},
  publisher = {IEEE},
  title = {Generative Image Inpainting with Contextual Attention},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Generative_Image_Inpainting_CVPR_2018_paper.html},
  year = {2018}
}

@inproceedings{mao2017least,
  abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $χ^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
  address = {Venice, Italy},
  author = {Xudong Mao and Qing Li and Haoran Xie and Raymond Y. K. Lau and Zhen Wang and Stephen Paul Smolley},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  file = {:/home/b/documents/inproceedings/mao2017least.pdf:pdf},
  month = {10},
  note = {Addressed the vanishing gradient problem by replacing the sigmoid cross-entropy loss with a least-squares loss. This penalizes samples even if they are on the correct side of the decision boundary, leading to more stable training and higher-quality images.},
  pages = {2794--2802},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf},
  publisher = {IEEE},
  title = {Least Squares Generative Adversarial Networks},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Mao_Least_Squares_Generative_ICCV_2017_paper.html},
  year = {2017}
}

@inproceedings{arjovsky2017wasserstein,
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
  address = {Sydney, Australia},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  file = {:/home/b/documents/inproceedings/arjovsky2017wasserstein.pdf:pdf},
  month = {8},
  note = {A landmark theoretical paper that replaced the problematic Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. This provided a smoother loss function that correlates with image quality and alleviates mode collapse.},
  pages = {214--223},
  pdf = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Wasserstein Generative Adversarial Networks},
  url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
  volume = {70},
  year = {2017}
}

@inproceedings{isola2017image,
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with the system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking.},
  address = {Honolulu, HI, USA},
  author = {Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2017.632},
  file = {:/home/b/documents/inproceedings/isola2017image.pdf:pdf},
  month = {7},
  note = {Popularized as ``Pix2Pix,'' this paper demonstrated that a conditional GAN is a general-purpose framework for translating between paired image domains. It showed remarkable results on tasks like photo generation from semantic maps and colorization.},
  openalex = {W2963073614},
  pages = {1125--1134},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf},
  publisher = {IEEE},
  title = {Image-to-Image Translation with Conditional Adversarial Networks},
  url = {https://ieeexplore.ieee.org/document/8100115/},
  year = {2017}
}

@inproceedings{zhu2017unpaired,
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X i̊ghtarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \g̊htarrow X$ and introduce a cycle consistency loss to push $F(G(X)) ≈ X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  address = {Venice, Italy},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV.2017.244},
  file = {:/home/b/documents/inproceedings/zhu2017unpaired.pdf:pdf},
  month = {10},
  note = {A revolutionary paper that introduced CycleGAN, enabling image-to-image translation without paired training data. The core innovation is the cycle-consistency loss, which ensures that translating an image to a target domain and back recovers the original image.},
  openalex = {W2962793481},
  pages = {2242--2251},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf},
  title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  url = {https://arxiv.org/abs/1703.10593},
  year = {2017}
}

@misc{berthelot2017began,
  abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
  author = {David Berthelot and Thomas Schumm and Luke Metz},
  doi = {10.48550/arXiv.1703.10717},
  file = {:/home/b/documents/misc/berthelot2017began.pdf:pdf},
  howpublished = {arXiv preprint arXiv:1703.10717},
  month = {3},
  openalex = {W2605195953},
  pdf = {https://arxiv.org/pdf/1703.10717.pdf},
  title = {BEGAN: Boundary Equilibrium Generative Adversarial Networks},
  url = {https://arxiv.org/abs/1703.10717},
  year = {2017}
}

@inproceedings{gulrajani2017improved,
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.},
  author = {Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron C. Courville},
  booktitle = {Advances in Neural Information Processing Systems 30},
  file = {:/home/b/documents/inproceedings/gulrajani2017improved.pdf:pdf},
  note = {A critical improvement over the original WGAN, replacing the unstable weight clipping with a gradient penalty (WGAN-GP). This more principled method of enforcing the Lipschitz constraint resulted in significantly more stable training across various architectures.},
  openalex = {W4295521014},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'17},
  title = {Improved Training of Wasserstein GANs},
  url = {https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans},
  year = {2017}
}

@inproceedings{heusel2017gans,
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  author = {Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
  booktitle = {Advances in Neural Information Processing Systems 30},
  file = {:/home/b/documents/inproceedings/heusel2017gans.pdf:pdf},
  note = {Provided a theoretical convergence proof for GANs using a two time-scale update rule (TTUR). This paper also introduced the Fréchet Inception Distance (FID), which became the gold standard for evaluating GAN quality and diversity.},
  openalex = {W4301206121},
  pages = {6626--6637},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
  url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
  year = {2017}
}

@inproceedings{zhang2017stackgan,
  abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision with many practical applications. Existing approaches have made substantial progress, but they generally fail to generate sufficiently high-resolution images and fail to leverage semantically relevant image features. To address these limitations, we propose StackGAN, a two-stage generative adversarial network for generating photo-realistic images from text descriptions. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we propose a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method can generate compelling images of birds and flowers with a high diversity and semantic consistency to corresponding text descriptions. A runtime analysis is provided to show the efficiency of our method.},
  author = {Han Zhang and Tao Xu and Hongsheng Li and Shaoting Zhang and Xiaogang Wang and Xiaolei Huang and Dimitris N. Metaxas},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/iccv.2017.629},
  file = {:/home/b/documents/inproceedings/zhang2017stackgan.pdf:pdf},
  month = {10},
  note = {Tackled high-resolution text-to-image synthesis by decomposing the problem into a two-stage process. A Stage-I GAN generates a low-resolution sketch, which a Stage-II GAN refines into a high-resolution, detailed image.},
  openalex = {W2964024144},
  pages = {5907--5915},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf},
  title = {StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks},
  url = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_StackGAN_Text_to_ICCV_2017_paper.pdf},
  year = {2017}
}

@inproceedings{odena2017conditional,
  abstract = {Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  author = {Augustus Odena and Christopher Olah and Jonathon Shlens},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Doina Precup and Yee Whye Teh},
  file = {:/home/b/documents/inproceedings/odena2017conditional.pdf:pdf},
  note = {Proposed the Auxiliary Classifier GAN (AC-GAN), which modifies the discriminator to predict the class label of the input image in addition to its reality. This improved the quality of conditional image synthesis and allowed for more efficient use of label information.},
  openalex = {W2548275288},
  pages = {2642--2651},
  pdf = {http://proceedings.mlr.press/v70/odena17a/odena17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conditional Image Synthesis with Auxiliary Classifier GANs},
  volume = {70},
  year = {2017}
}

@inproceedings{liu2017unsupervised,
  abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address this problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed method with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed method to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available at https://github.com/mingyuliutw/unit.},
  author = {Ming-Yu Liu and Thomas Breuel and Jan Kautz},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/liu2017unsupervised.pdf:pdf},
  note = {Proposed the UNIT framework, which combines VAEs and CoGANs under a shared latent space assumption. This allowed for unsupervised image-to-image translation by learning to map images from different domains into a common latent representation.},
  openalex = {W2962947361},
  pages = {700--708},
  pdf = {https://proceedings.neurips.cc/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Unsupervised Image-to-Image Translation Networks},
  url = {https://proceedings.neurips.cc/paper/2017/hash/dc6a6489640ca02b0d42dabeb8e46bb7-Abstract.html},
  volume = {30},
  year = {2017}
}

@inproceedings{roth2017stabilizing,
  abstract = {Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this approach on several datasets including common benchmark image generation tasks.},
  author = {Kevin Roth and Aurelien Lucchi and Sebastian Nowozin and Thomas Hofmann},
  booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  file = {:/home/b/documents/inproceedings/roth2017stabilizing.pdf:pdf},
  openalex = {W2617573776},
  pages = {2018--2028},
  pdf = {https://proceedings.neurips.cc/paper/2017/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Stabilizing Training of Generative Adversarial Networks through Regularization},
  url = {https://proceedings.neurips.cc/paper/2017/hash/7bccfde7714a1ebadf06c5f4cea752c1-Abstract.html},
  volume = {30},
  year = {2017}
}

@inproceedings{metz2017unrolled,
  abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
  author = {Luke Metz and Ben Poole and David Pfau and Jascha Sohl-Dickstein},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  file = {:/home/b/documents/inproceedings/metz2017unrolled.pdf:pdf},
  note = {Addressed training instability by defining the generator's loss based on an unrolled optimization of the discriminator. By looking ahead multiple steps in the discriminator's training, the generator can anticipate the discriminator's response and avoid unstable dynamics.},
  openalex = {W2554314924},
  pdf = {https://openreview.net/pdf?id=BydrOIcle},
  publisher = {OpenReview.net},
  title = {Unrolled Generative Adversarial Networks},
  url = {https://openreview.net/forum?id=BydrOIcle},
  year = {2017}
}

@inproceedings{mroueh2017mcgan,
  abstract = {We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.},
  author = {Youssef Mroueh and Tom Sercu and Vaibhava Goel},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Doina Precup and Yee Whye Teh},
  file = {:/home/b/documents/inproceedings/mroueh2017mcgan.pdf:pdf},
  note = {Proposed a GAN variant that matches the mean and covariance of features from real and generated data at the discriminator. This feature matching technique provided a more stable training objective than the standard adversarial loss.},
  pages = {2527--2535},
  pdf = {https://proceedings.mlr.press/v70/mroueh17a/mroueh17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {McGan: Mean and Covariance Feature Matching GAN},
  url = {https://proceedings.mlr.press/v70/mroueh17a.html},
  volume = {70},
  year = {2017}
}

@misc{lim2017geometric,
  abstract = {Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.},
  archiveprefix = {arXiv},
  author = {Jae Hyun Lim and Jong Chul Ye},
  eprint = {1705.02894},
  file = {:/home/b/documents/misc/lim2017geometric.pdf:pdf},
  howpublished = {arXiv preprint arXiv:1705.02894},
  month = {5},
  note = {Framed the GAN problem from a geometric perspective, where the generator seeks to move its distribution towards the data manifold. The paper introduced a margin-based loss that encourages the discriminator to define a decision boundary with a large margin, improving stability.},
  openalex = {W3037695135},
  pdf = {https://arxiv.org/pdf/1705.02894.pdf},
  primaryclass = {stat.ML},
  title = {Geometric GAN},
  year = {2017}
}

@inproceedings{ledig2017photorealistic,
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. The content loss is motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score evaluation shows significantly higher perceptual quality gains against prior art.},
  address = {Honolulu, HI, USA},
  author = {Christian Ledig and Lucas Theis and Ferenc Huszár and José Caballero and Andrew Cunningham and Alejandro Acosta and Andrew P. Aitken and Alykhan Tejani and Johannes Totz and Zehan Wang and Wenzhe Shi},
  booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2017.19},
  file = {:/home/b/documents/inproceedings/ledig2017photorealistic.pdf:pdf},
  month = {7},
  note = {Introduced SRGAN, a GAN-based approach for image super-resolution that produced more photorealistic results than traditional methods based on minimizing mean squared error. It used a perceptual loss function combining an adversarial loss and a content loss.},
  openalex = {W2963470893},
  pages = {105--114},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf},
  publisher = {IEEE},
  title = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network},
  year = {2017}
}

@inproceedings{dumoulin2017adversarially,
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network that is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with other recent approaches on the semi-supervised SVHN task.},
  author = {Vincent Dumoulin and Ishmael Belghazi and Ben Poole and Alex Lamb and Martin Arjovsky and Olivier Mastropietro and Aaron Courville},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  file = {:/home/b/documents/inproceedings/dumoulin2017adversarially.pdf:pdf},
  note = {Proposed the Adversarially Learned Inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. This framework, also known as BiGAN, allows the model to not only generate data but also to infer latent representations for given data points.},
  pdf = {https://openreview.net/pdf?id=B1ElR4cgg},
  publisher = {OpenReview.net},
  title = {Adversarially Learned Inference},
  url = {https://openreview.net/forum?id=B1ElR4cgg},
  year = {2017}
}

@inproceedings{radford2016unsupervised,
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arXiv},
  author = {Alec Radford and Luke Metz and Soumith Chintala},
  booktitle = {4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  eprint = {1511.06434},
  note = {Established a stable and scalable convolutional architecture (DCGAN) for GANs, which became the standard for much of the subsequent research. It also demonstrated that the learned latent space captures rich semantic features amenable to vector arithmetic.},
  primaryclass = {cs.LG},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  url = {http://arxiv.org/abs/1511.06434},
  year = {2016}
}

@inproceedings{salimans2016improved,
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  author = {Salimans, Tim and Goodfellow, Ian J. and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle = {Advances in Neural Information Processing Systems 29},
  file = {:/home/b/documents/inproceedings/salimans2016improved.pdf:pdf},
  note = {Introduced a collection of crucial heuristics to stabilize GAN training, including feature matching, minibatch discrimination, and historical averaging. This paper also proposed the Inception Score (IS) as a quantitative metric for evaluating GAN performance.},
  openalex = {W2963373786},
  pages = {2234--2242},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS},
  title = {Improved Techniques for Training GANs},
  url = {https://proceedings.neurips.cc/paper/2016/hash/8a3363abe792db2d8761d6403605aeb7-Abstract.html},
  year = {2016}
}

@inproceedings{chen2016infogan,
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  address = {Red Hook, NY, USA},
  author = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
  booktitle = {Advances in Neural Information Processing Systems 29},
  file = {:/home/b/documents/inproceedings/chen2016infogan.pdf:pdf},
  pages = {2172--2180},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
  url = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
  year = {2016}
}

@inproceedings{liu2016coupled,
  abstract = {We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes.},
  author = {Ming-Yu Liu and Oncel Tuzel},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/liu2016coupled.pdf:pdf},
  note = {Introduced CoGAN for learning a joint distribution of multi-domain images without corresponding paired data. It uses a weight-sharing strategy between two GANs to learn the shared high-level semantics and domain-specific low-level details.},
  pages = {469--477},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/502e4a16930e414107ee22b6198c578f-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Coupled Generative Adversarial Networks},
  url = {https://proceedings.neurips.cc/paper/2016/hash/502e4a16930e414107ee22b6198c578f-Abstract.html},
  volume = {29},
  year = {2016}
}

@inproceedings{pathak2016context,
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  author = {Deepak Pathak and Philipp Krähenbühl and Jeff Donahue and Trevor Darrell and Alexei A. Efros},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2016.278},
  pages = {2536--2544},
  publisher = {IEEE},
  title = {Context Encoders: Feature Learning by Inpainting},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf},
  year = {2016}
}

@inproceedings{li2015generative,
  abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, similar to generative adversarial networks. However, while training GANs requires careful optimization of a difficult minimax program, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model. We further boost the performance by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
  address = {Lille, France},
  author = {Yujia Li and Kevin Swersky and Rich Zemel},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2015generative.pdf:pdf},
  month = {7},
  note = {Proposed an alternative to the adversarial framework by training a generator to minimize the maximum mean discrepancy (MMD) between generated and real samples. This approach avoids the unstable minimax game by directly matching moments of the distributions.},
  openalex = {W1487641199},
  pages = {1718--1727},
  pdf = {https://proceedings.mlr.press/v37/li15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Moment Matching Networks},
  url = {https://proceedings.mlr.press/v37/li15.html},
  volume = {37},
  year = {2015}
}

@misc{makhzani2015adversarial,
  author = {Makhzani, Alireza and others},
  howpublished = {arXiv},
  note = {Combined autoencoders with an adversarial training objective on the latent code space. This forces the aggregated posterior of the latent code to match an arbitrary prior distribution, effectively turning the autoencoder into a powerful generative model.},
  title = {Adversarial Autoencoders},
  year = {2015}
}

@inproceedings{goodfellow2014generative,
  author = {Goodfellow, Ian and others},
  booktitle = {NeurIPS},
  note = {The seminal paper that introduced the GAN framework. It proposed training a generative model via a minimax two-player game against a discriminative adversary, fundamentally changing the landscape of generative modeling.},
  title = {Generative Adversarial Nets},
  year = {2014}
}

@misc{mirza2014conditional,
  author = {Mirza, Mehdi and Osindero, Simon},
  howpublished = {arXiv},
  note = {A foundational extension that introduces conditional information (e.g., class labels) to both the generator and discriminator. This allows for direct control over the generated output, making GANs applicable to a wide range of targeted tasks.},
  title = {Conditional Generative Adversarial Nets},
  year = {2014}
}
