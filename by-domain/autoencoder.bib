@incollection{rumelhart1986learning,
  abstract      = {This chapter presents a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal hidden units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The procedure has been found to be highly successful in learning to solve a wide variety of tasks, and has been applied to problems ranging from speech recognition to medical diagnosis.},
  address       = {Cambridge, MA},
  author        = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  booktitle     = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations},
  chapter       = {8},
  doi           = {10.7551/mitpress/4943.003.0128},
  editor        = {Rumelhart, David E. and McClelland, James L.},
  note          = {TLDR: This seminal chapter introduces the backpropagation algorithm and the concept of auto-association, where a neural network is trained to reconstruct its own input. It establishes the autoencoder as a method for unsupervised learning of distributed internal representations.},
  openalex      = {W4390583470},
  pages         = {318--362},
  pdf           = {https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf},
  publisher     = {MIT Press},
  title         = {Learning Internal Representations by Error Propagation},
  year          = {1986}
}

@article{baldi1989neural,
  abstract      = {We consider the problem of learning from examples in layered linear feedforward neural networks, using optimization methods such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. In particular, we show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order principal vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  author        = {Pierre Baldi and Kurt Hornik},
  doi           = {10.1016/0893-6080(89)90014-2},
  journal       = {Neural Networks},
  note          = {TLDR: This paper provides the crucial theoretical link between linear autoencoders and Principal Component Analysis (PCA). It proves that a linear autoencoder learns to project data onto the principal subspace, establishing autoencoders as a non-linear generalization of PCA.},
  number        = {1},
  openalex      = {W2078626246},
  pages         = {53--58},
  pdf           = {http://www.vision.jhu.edu/teaching/learning/deeplearning19/assets/Baldi_Hornik-89.pdf},
  title         = {Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima},
  url           = {https://www.sciencedirect.com/science/article/abs/pii/0893608089900142},
  volume        = {2},
  year          = {1989}
}

@inproceedings{baldi2012autoencoders,
  abstract      = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear autoencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoencoder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections to clustering, Hebbian learning, and information theory.},
  address       = {Bellevue, Washington, USA},
  author        = {Pierre Baldi},
  booktitle     = {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  editor        = {Isabelle Guyon and Gideon Dror and Vincent Lemaire and Graham Taylor and Daniel Silver},
  openalex      = {W2181347294},
  pages         = {37--49},
  pdf           = {http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Autoencoders, Unsupervised Learning, and Deep Architectures},
  url           = {https://proceedings.mlr.press/v27/baldi12a.html},
  volume        = {27},
  year          = {2012}
}

@article{olshausen1996emergence,
  abstract      = {The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  author        = {Olshausen, Bruno A. and Field, David J.},
  doi           = {10.1038/381607a0},
  journal       = {Nature},
  month         = {6},
  note          = {TLDR: While not an autoencoder paper, this is the foundational work on sparse coding that inspired the Sparse Autoencoder. It demonstrates that an algorithm seeking sparse linear codes for natural images develops Gabor-like receptive fields, mirroring those found in the mammalian primary visual cortex.},
  number        = {6583},
  openalex      = {W2145889472},
  pages         = {607--609},
  pmid          = {8637596},
  title         = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  volume        = {381},
  year          = {1996}
}

@article{kramer1992autoassociative,
  abstract      = {Autoassociative neural networks are feedforward nets trained to produce an approximation of the identity mapping between network inputs and outputs using backpropagation or similar learning procedures. The key feature of an autoassociative network is a dimensional bottleneck between input and output. Compression of information by the bottleneck results in the acquisition of a correlation model of the input data, useful for performing a variety of data screening tasks. The network reduces measurement noise by mapping inputs into the space of the correlation model, and the residuals of this mapping can be used to detect sensor failures. Values for missing and faulty sensors can be estimated using the network. A related approach, robust autoassociative networks, filter both random noise and gross errors in data resulting from faulty sensors. These networks replace with a single forward pass the conventional multiple-step procedure of data rectification, gross error detection, failure identification and sensor value replacement estimation. Autoassociative networks can be used to preprocess data so that sensor-based calculations can be performed correctly even in the presence of large sensor biases and failures.},
  author        = {Mark A. Kramer},
  doi           = {10.1016/0098-1354(92)80051-a},
  issn          = {0098-1354},
  journal       = {Computers & Chemical Engineering},
  month         = {4},
  note          = {TLDR: An early and influential paper that applies non-linear auto-associative networks (autoencoders) to chemical process data. It clearly demonstrates their superiority over linear PCA for data compression and feature extraction in real-world industrial applications.},
  number        = {4},
  openalex      = {W2084229232},
  pages         = {313--328},
  publisher     = {Elsevier BV},
  title         = {Auto-Associative Neural Networks},
  url           = {https://doi.org/10.1016/0098-1354(92)80051-a},
  volume        = {16},
  year          = {1992}
}

@article{hinton2006reducing,
  abstract      = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ``autoencoder'' networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  author        = {Geoffrey E. Hinton and Ruslan Salakhutdinov},
  doi           = {10.1126/science.1127647},
  journal       = {Science},
  month         = {7},
  note          = {TLDR: This landmark paper ignited the deep learning revolution by introducing a method to train deep autoencoders. It uses a greedy, layer-by-layer pre-training strategy with Restricted Boltzmann Machines to initialize weights effectively, overcoming the vanishing gradient problem and demonstrating superior dimensionality reduction compared to PCA.},
  number        = {5786},
  openalex      = {W2100495367},
  pages         = {504--507},
  pdf           = {https://www.cs.toronto.edu/~hinton/absps/science.pdf},
  pmid          = {16873662},
  publisher     = {American Association for the Advancement of Science},
  title         = {Reducing the Dimensionality of Data with Neural Networks},
  volume        = {313},
  year          = {2006}
}

@article{hinton2006fast,
  abstract      = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels.},
  author        = {Geoffrey E. Hinton and Simon Osindero and Yee-Whye Teh},
  doi           = {10.1162/neco.2006.18.7.1527},
  journal       = {Neural Computation},
  month         = {7},
  note          = {TLDR: This paper provides the technical details for the greedy layer-wise unsupervised pre-training algorithm that was central to the success of the 2006 Science paper. It shows how a stack of Restricted Boltzmann Machines can be trained to initialize a deep generative model, which can then be fine-tuned.},
  number        = {7},
  openalex      = {W2136922672},
  pages         = {1527--1554},
  pdf           = {https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf},
  title         = {A Fast Learning Algorithm for Deep Belief Nets},
  volume        = {18},
  year          = {2006}
}

@inproceedings{bengio2007greedy,
  abstract      = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. This paper proposes a greedy layer-wise unsupervised learning algorithm that can help address this problem by learning simpler concepts first and progressively building more abstract representations. The approach involves: (1) pre-training one layer at a time in a greedy way, (2) using unsupervised learning at each layer to preserve information from the input, and (3) fine-tuning the whole network with respect to the ultimate criterion of interest.},
  address       = {Cambridge, MA},
  author        = {Yoshua Bengio and Pascal Lamblin and Dan Popovici and Hugo Larochelle},
  booktitle     = {Advances in Neural Information Processing Systems 19},
  note          = {TLDR: This paper provides a theoretical and empirical analysis of the greedy layer-wise unsupervised pre-training strategy. It argues that this approach acts as a form of regularization and helps guide the learning towards basins of attraction of better local minima.},
  openalex      = {W2110798204},
  pages         = {153--160},
  pdf           = {https://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf},
  publisher     = {MIT Press},
  title         = {Greedy Layer-Wise Training of Deep Networks},
  url           = {https://proceedings.neurips.cc/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html},
  year          = {2006}
}

@article{lecun1998gradient,
  abstract      = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. The key advantage is that the entire system is trainable end to end. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2-D shapes, are shown to outperform all other techniques.},
  author        = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  doi           = {10.1109/5.726791},
  journal       = {Proceedings of the IEEE},
  month         = {11},
  note          = {TLDR: While famous for introducing Convolutional Neural Networks (CNNs), this paper is foundational for representation learning in general. It champions the principle of minimizing hand-engineered features in favor of learning hierarchical representations directly from data, a philosophy that underpins the entire autoencoder paradigm.},
  number        = {11},
  openalex      = {W2112796928},
  pages         = {2278--2324},
  pdf           = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=726791},
  title         = {Gradient-Based Learning Applied to Document Recognition},
  volume        = {86},
  year          = {1998}
}

@inproceedings{kingma2014autoencoding,
  abstract      = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.},
  author        = {Diederik P. Kingma and Max Welling},
  booktitle     = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  month         = {4},
  note          = {TLDR: A paradigm-shifting paper that introduced the Variational Autoencoder (VAE), a principled framework for combining the representational power of neural networks with the statistical rigor of variational inference. It introduced the reparameterization trick, enabling the training of powerful generative models with backpropagation.},
  openalex      = {W1959608418},
  pdf           = {https://openreview.net/pdf?id=33X9fd2-9FyZd},
  title         = {Auto-Encoding Variational Bayes},
  url           = {http://arxiv.org/abs/1312.6114},
  year          = {2014}
}

@inproceedings{vincent2008extracting,
  abstract      = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective.},
  address       = {Helsinki, Finland},
  author        = {Pascal Vincent and Hugo Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},
  booktitle     = {Proceedings of the 25th International Conference on Machine Learning},
  doi           = {10.1145/1390156.1390294},
  note          = {TLDR: This paper introduced the Denoising Autoencoder (DAE), a simple yet highly effective modification to the standard autoencoder. By training the model to reconstruct a clean input from a stochastically corrupted version, the DAE is forced to learn robust features that capture the underlying data manifold.},
  openalex      = {W2025768430},
  pages         = {1096--1103},
  pdf           = {http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf},
  publisher     = {ACM},
  series        = {ICML '08},
  title         = {Extracting and Composing Robust Features with Denoising Autoencoders},
  year          = {2008}
}

@article{vincent2010stacked,
  abstract      = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  author        = {Pascal Vincent and Hugo Larochelle and Isabelle Lajoie and Yoshua Bengio and Pierre-Antoine Manzagol},
  journal       = {Journal of Machine Learning Research},
  month         = {3},
  note          = {TLDR: This work extends the DAE concept to deep architectures by proposing the Stacked Denoising Autoencoder (SDA). It demonstrates that greedy, layer-wise pre-training, where each layer is trained as a DAE to denoise the output of the previous layer, is an effective strategy for initializing deep networks.},
  openalex      = {W2145094598},
  pages         = {3371--3408},
  pdf           = {https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf},
  title         = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
  volume        = {11},
  year          = {2010}
}

@inproceedings{rifai2011contractive,
  abstract      = {We present a novel approach for training deterministic auto-encoders by adding a well chosen penalty term to the classical reconstruction cost function, achieving results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. The penalty term results in a localized space contraction which in turn yields robust features on the activation layer.},
  address       = {Bellevue, WA, USA},
  author        = {Salah Rifai and Pascal Vincent and Xavier Muller and Xavier Glorot and Yoshua Bengio},
  booktitle     = {Proceedings of the 28th International Conference on Machine Learning},
  month         = {6},
  note          = {TLDR: This paper introduced the Contractive Autoencoder (CAE), which adds a penalty term to the loss function corresponding to the Frobenius norm of the encoder's Jacobian. This explicitly encourages the learned representation to be robust to infinitesimal perturbations of the input, learning an invariant mapping.},
  openalex      = {W2218318129},
  pages         = {833--840},
  pdf           = {https://icml.cc/2011/papers/455_icmlpaper.pdf},
  publisher     = {Omnipress},
  series        = {ICML'11},
  title         = {Contractive Auto-Encoders: Explicit Invariance During Feature Extraction},
  year          = {2011}
}

@inproceedings{chen2012msda,
  abstract      = {Stacked denoising autoencoders (SDA) have been successfully used to learn new representations for domain adaptation. However, they are limited by two crucial limitations: (1) high computational cost and (2) lack of scalability to high-dimensional features. We propose a marginalized SDA (mSDA) that addresses both of these limitations. In contrast to SDAs, mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters. In fact, they are computed in closed-form. The mSDA algorithm only requires going through the data once to compute expectations, and the optimization problem is convex thus guaranteeing a global optimum. We can implement this algorithm in only 20 lines of MATLAB and we find that it speeds up SDAs by two orders of magnitude. Furthermore, we can scale it to features with millions of dimensions. Despite its simplicity and efficiency, mSDA achieves almost identical accuracies to standard SDAs on 14 different domain adaptation tasks.},
  address       = {Edinburgh, Scotland},
  author        = {Minmin Chen and Zhixiang Xu and Kilian Q. Weinberger and Fei Sha},
  booktitle     = {Proceedings of the 29th International Conference on Machine Learning},
  note          = {TLDR: This work proposes a highly efficient version of the Stacked Denoising Autoencoder by marginalizing out the noise. For linear denoisers, this allows for a closed-form, non-iterative solution, dramatically speeding up training by orders of magnitude while achieving comparable performance.},
  openalex      = {W2096873754},
  pages         = {767--774},
  pdf           = {https://icml.cc/2012/papers/416.pdf},
  publisher     = {Omnipress},
  series        = {ICML'12},
  title         = {Marginalized Denoising Autoencoders for Domain Adaptation},
  year          = {2012}
}

@inproceedings{geras2015scheduled,
  abstract      = {We present a representation learning method that learns features at multiple different levels of scale. Working within the unsupervised framework of denoising autoencoders, we observe that when the input is heavily corrupted during training, the network tends to learn coarse-grained features, whereas when the input is only slightly corrupted, the network tends to learn fine-grained features. This motivates the scheduled denoising autoencoder, which starts with a high level of noise that lowers as training progresses. We find that the resulting representation yields a significant boost on a later supervised task compared to the original input, or to a standard denoising autoencoder trained at a single noise level.},
  author        = {Krzysztof J. Geras and Charles Sutton},
  booktitle     = {International Conference on Learning Representations},
  month         = {5},
  note          = {TLDR: This paper observes that the level of noise in a DAE affects the scale of learned features, with high noise leading to coarse features and low noise leading to fine-grained ones. It proposes a "scheduled" approach where the noise level is gradually decreased during training, allowing the model to learn a multi-scale representation.},
  openalex      = {W2962676938},
  pdf           = {https://homepages.inf.ed.ac.uk/csutton/publications/scheda-iclr2015.pdf},
  title         = {Scheduled Denoising Autoencoders},
  url           = {https://arxiv.org/abs/1406.3269},
  year          = {2015}
}

@inproceedings{bengio2013generalized,
  abstract      = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
  author        = {Yoshua Bengio and Li Yao and Guillaume Alain and Pascal Vincent},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  note          = {TLDR: This paper establishes a formal link between denoising autoencoders and generative modeling. It shows that a DAE implicitly learns to estimate the score (gradient of the log-density) of the data distribution, and provides a method to sample from this implicit model via a Markov chain.},
  openalex      = {W2134842679},
  pages         = {899--907},
  pdf           = {https://proceedings.neurips.cc/paper/2013/file/559cb990c9dffd8675f6bc2186971dc2-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NIPS},
  title         = {Generalized Denoising Auto-Encoders as Generative Models},
  url           = {https://proceedings.neurips.cc/paper/2013/hash/559cb990c9dffd8675f6bc2186971dc2-Abstract.html},
  volume        = {26},
  year          = {2013}
}

@inproceedings{rifai2012generative,
  abstract      = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks.},
  author        = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann N. and Vincent, Pascal},
  booktitle     = {Proceedings of the 29th International Conference on Machine Learning},
  note          = {TLDR: This work proposes a method to turn a trained Contractive Autoencoder into a generative model. It defines a stochastic sampling procedure that leverages the learned manifold structure captured by the CAE's contractive penalty, allowing for efficient sampling that mixes well between data modes.},
  openalex      = {W2950320139},
  pdf           = {https://proceedings.mlr.press/v2012/rifai12a/rifai12a.pdf},
  publisher     = {ICML.cc},
  series        = {ICML'12},
  title         = {A Generative Process for Sampling Contractive Auto-Encoders},
  url           = {https://icml.cc/2012/papers/910.pdf},
  year          = {2012}
}

@article{alain2014what,
  abstract      = {Recent work suggests that some auto-encoder variants do a good job of capturing local manifold structure of data. This paper explores what regularized auto-encoders learn about data-generating distributions. The main finding is that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. More specifically, the auto-encoder captures the score (derivative of the log-density with respect to the input). The results are generic and not dependent on specific auto-encoder parameterization. The paper also demonstrates how an approximate Metropolis-Hastings MCMC can recover samples from the estimated distribution.},
  author        = {Guillaume Alain and Yoshua Bengio},
  journal       = {Journal of Machine Learning Research},
  note          = {TLDR: This paper provides a theoretical analysis connecting regularized autoencoders (like DAEs and CAEs) to score matching. It shows that under certain conditions, these autoencoders learn to estimate the score of the data-generating distribution, providing a unified perspective on their underlying statistical objective.},
  number        = {1},
  openalex      = {W2614634292},
  pages         = {3563--3593},
  pdf           = {https://jmlr.org/papers/volume15/alain14a/alain14a.pdf},
  title         = {What Regularized Auto-Encoders Learn from the Data-Generating Distribution},
  volume        = {15},
  year          = {2014}
}

@article{spigler2019denoising,
  abstract      = {Despite recent developments that allowed neural networks to achieve impressive performance on a variety of applications, these models are intrinsically affected by the problem of overgeneralization, due to their partitioning of the full input space into the fixed set of target classes used during training. Thus it is possible for novel inputs belonging to categories unknown during training, or even completely unrecognizable inputs to humans, to fool the system into classifying them as one of the known classes, with high degree of confidence. This can lead to security problems in critical applications and is closely linked to open set recognition and 1-class recognition. This paper presents a novel way to compute a confidence score using the reconstruction error of denoising autoencoders and shows that such confidence score can correctly identify regions of the input space close to the training data distribution. The proposed solution is tested on benchmarks and 'fooling' datasets constructed from MNIST and Fashion-MNIST datasets.},
  author        = {Giacomo Spigler},
  doi           = {10.1109/TPAMI.2019.2909876},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number        = {4},
  openalex      = {W2756388459},
  pages         = {998--1004},
  pdf           = {https://ieeexplore.ieee.org/iel7/34/4359286/08684304.pdf},
  title         = {Denoising Autoencoders for Overgeneralization in Neural Networks},
  volume        = {42},
  year          = {2020}
}

@inproceedings{hinton2011transforming,
  abstract      = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT, that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community.},
  address       = {Berlin, Heidelberg},
  author        = {Geoffrey E. Hinton and Alex Krizhevsky and Sida D. Wang},
  booktitle     = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
  doi           = {10.1007/978-3-642-21735-7_6},
  editor        = {Timo Honkela and Wlodek Duch and Mark Girolami and Samuel Kaski},
  note          = {TLDR: This paper proposes a novel ``transforming auto-encoder'' designed to explicitly disentangle pose information from object identity. The model learns to recognize a visual entity and output both its presence and a vector of instantiation parameters (e.g., position, orientation), representing a step towards equivariant representations.},
  openalex      = {W2966661},
  pages         = {44--51},
  pdf           = {https://www.cs.toronto.edu/~bonner/courses/2022s/csc2547/papers/capsules/transforming-autoencoders,-hinton,-icann-2011.pdf},
  publisher     = {Springer},
  series        = {Lecture Notes in Computer Science},
  title         = {Transforming Auto-Encoders},
  volume        = {6791},
  year          = {2011}
}

@techreport{ng2011sparse,
  abstract      = {This work describes the sparse autoencoder learning algorithm, which is one approach to automatically learn features from unlabeled data. The algorithm addresses the limitation that most applications of supervised learning still require manually specified input features. The notes first describe feedforward neural networks and the backpropagation algorithm for supervised learning, then show how this is used to construct an autoencoder (an unsupervised learning algorithm), and finally build on this to derive a sparse autoencoder using KL-divergence penalty to enforce sparsity.},
  author        = {Andrew Ng},
  institution   = {Stanford University},
  note          = {TLDR: These influential lecture notes provided one of the clearest and most widely circulated formulations of the sparse autoencoder. They detailed the use of a KL-divergence penalty to enforce sparsity and provided the full backpropagation derivations, serving as the de facto introduction to the topic for a generation of students and practitioners. Part of CS294A Lecture Notes.},
  number        = {CS294A},
  pages         = {1--19},
  pdf           = {http://nlp.stanford.edu/~socherr/sparseAutoencoder_2011new.pdf},
  title         = {Sparse Autoencoder},
  type          = {Lecture Notes},
  url           = {https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf},
  year          = {2011}
}

@inproceedings{lee2007efficient,
  abstract      = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
  author        = {Honglak Lee and Alexis Battle and Rajat Raina and Andrew Y. Ng},
  booktitle     = {Advances in Neural Information Processing Systems},
  doi           = {10.7551/mitpress/7503.003.0105},
  note          = {TLDR: This paper connects sparse coding with an autoencoder-like framework and proposes efficient algorithms for learning the basis functions (dictionary). It introduces methods that are significantly faster than previous approaches, making large-scale sparse coding practical and paving the way for its use in deep learning.},
  openalex      = {W2113606819},
  pages         = {801--808},
  pdf           = {https://papers.nips.cc/paper_files/paper/2006/file/2d71b2ae158c7c5912cc0bbde2bb9d95-Paper.pdf},
  publisher     = {MIT Press},
  title         = {Efficient Sparse Coding Algorithms},
  url           = {https://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms},
  year          = {2007}
}

@inproceedings{lee2009unsupervised,
  abstract      = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these approaches have not been extensively studied for auditory data. We apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. For speech data, we show that the learned features correspond to phones/phonemes. For music data, we demonstrate that the learned features yield improved classification performance relative to audio features commonly used in the literature.},
  address       = {Vancouver, BC, Canada},
  author        = {Honglak Lee and Peter T. Pham and Yan Largman and Andrew Y. Ng},
  booktitle     = {Advances in Neural Information Processing Systems},
  month         = {12},
  note          = {TLDR: This work demonstrates the power of unsupervised pre-training with sparse representations for audio data. It develops convolutional RBMs and sparse coding techniques to learn hierarchical features from raw audio spectrograms, achieving state-of-the-art results on audio classification tasks.},
  openalex      = {W2107789863},
  pages         = {1096--1104},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2009/file/a113c1ecd3cace2237256f4c712f61b5-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Unsupervised Feature Learning for Audio Classification using Convolutional Deep Belief Networks},
  url           = {https://proceedings.neurips.cc/paper/2009/hash/a113c1ecd3cace2237256f4c712f61b5-Abstract.html},
  volume        = {22},
  year          = {2009}
}

@inproceedings{ngiam2010tiled,
  abstract      = {Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolutional neural networks (Tiled CNNs), which use a regular ``tiled'' pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units $k$ steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. The results show that learning such invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.},
  author        = {Jiquan Ngiam and Zhenghao Chen and Daniel Chia and Pang Wei Koh and Quoc Viet Le and Andrew Y. Ng},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W2147860648},
  pdf           = {https://papers.nips.cc/paper_files/paper/2010/file/01f78be6f7cad02658508fe4616098a9-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Tiled Convolutional Neural Networks},
  volume        = {23},
  year          = {2010}
}

@inproceedings{makhzani2014ksparse,
  abstract      = {Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.},
  author        = {Alireza Makhzani and Brendan J. Frey},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This paper introduces the k-sparse autoencoder, which enforces sparsity directly by allowing only the top-k largest hidden unit activations to be non-zero. This avoids the need to tune a sparsity penalty coefficient and prevents the feature shrinkage bias associated with L1 regularization.},
  openalex      = {W1853900790},
  pdf           = {https://openreview.net/pdf?id=QDm4QXNOsuQVE},
  title         = {k-Sparse Autoencoders},
  url           = {https://openreview.net/forum?id=QDm4QXNOsuQVE},
  year          = {2014}
}

@inproceedings{rolfe2013discriminative,
  abstract      = {We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.},
  author        = {Jason Tyler Rolfe and Yann LeCun},
  booktitle     = {1st International Conference on Learning Representations, ICLR 2013, Scottsdale, AZ, USA, May 2-4, 2013, Conference Track Proceedings},
  month         = {5},
  openalex      = {W2963623200},
  pdf           = {https://openreview.net/pdf?id=aJh-lFL2dFJ21},
  title         = {Discriminative Recurrent Sparse Auto-Encoders},
  url           = {https://openreview.net/forum?id=aJh-lFL2dFJ21},
  year          = {2013}
}

@inproceedings{coates2011analysis,
  abstract      = {The goal of unsupervised feature learning is to take unlabeled data as input and produce good features as output -- features that can then be used for other tasks like classification. A great deal of work has been done on this problem, yet it remains unclear which methods work well in practice and when. In this paper we aim to address this problem by carrying out a large-scale empirical study. We tested several feature learning algorithms (sparse autoencoders, sparse RBMs, K-means clustering, and Gaussian mixtures) on three datasets (CIFAR-10, NORB, and STL). Our experiments suggest that certain factors may be more important to achieving high performance than the precise algorithm used. Specifically, we found that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance -- so critical, in fact, that when these parameters are pushed to their limits, several methods achieve roughly equal performance.},
  address       = {Fort Lauderdale, FL, USA},
  author        = {Adam Coates and Honglak Lee and Andrew Y. Ng},
  booktitle     = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  month         = {6},
  note          = {TLDR: This paper provides a large-scale empirical study comparing various single-layer unsupervised feature learning methods, including sparse autoencoders, sparse RBMs, and k-means. It surprisingly finds that with proper preprocessing and a large number of hidden units, simpler algorithms can perform as well as or better than more complex ones.},
  openalex      = {W2118858186},
  pages         = {215--223},
  pdf           = {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
  url           = {https://proceedings.mlr.press/v15/coates11a.html},
  volume        = {15},
  year          = {2011}
}

@book{prince2023understanding,
  abstract      = {An authoritative, accessible, and up-to-date treatment of deep learning that strikes a pragmatic middle ground between theory and practice. Understanding Deep Learning provides an authoritative, accessible, and up-to-date treatment of deep learning, offering a pragmatic approach that focuses on the techniques most likely to solve real-world problems. The book covers key topics including backpropagation, initialization, optimization, convolutional networks, sequence models, attention, transformers, graph neural networks, autoencoders, variational autoencoders, generative adversarial networks, normalizing flows, and diffusion models.},
  author        = {Simon J. D. Prince},
  isbn          = {9780262048644},
  month         = {12},
  note          = {Free PDF available online. Covers k-sparse autoencoders and other deep learning architectures with mathematical rigor and practical insights. Each concept is presented in lay terms and detailed precisely in mathematical form with visual illustrations.},
  pdf           = {https://anthology-of-data.science/resources/prince2023udl.pdf},
  publisher     = {The MIT Press},
  title         = {Understanding Deep Learning},
  url           = {https://udlbook.github.io/udlbook/},
  year          = {2023}
}

@inproceedings{pouyan2017new,
  abstract      = {Data representation plays an important role in performance of machine learning algorithms. Since data usually lacks the desired quality, many efforts have been made to provide a more desirable representation of data. In this paper, we propose a new sparse autoencoder by imposing the power two of smoothed L0 norm of data representation on the hidden layer of regular autoencoder. The square of smoothed L0 norm increases the tendency that each data representation is ``individually'' sparse. Moreover, by using the proposed sparse autoencoder, once the model parameters are learned, the sparse representation of any new data is obtained simply by a matrix-vector multiplication without performing any optimization. When applied to the MNIST, CIFAR-10, and OPTDIGITS datasets, the results show that the proposed model guarantees a sparse representation for each input data which leads to better classification.},
  address       = {Kos, Greece},
  author        = {Ali Shahin Shamsabadi and Massoud Babaie-Zadeh and Seyyede Zohreh Seyyedsalehi and Hamid R. Rabiee and Christian Jutten},
  booktitle     = {2017 25th European Signal Processing Conference (EUSIPCO)},
  doi           = {10.23919/EUSIPCO.2017.8081588},
  month         = {8},
  note          = {TLDR: This paper proposes a novel sparsity penalty based on a smoothed approximation of the L0 norm. The goal is to more directly encourage sparsity in the representation for each individual data point, addressing a limitation where traditional SAEs only enforce sparsity on average over a minibatch.},
  openalex      = {W2766854072},
  pages         = {2141--2145},
  pdf           = {https://zenodo.org/records/1159876/files/1570347253.pdf},
  publisher     = {IEEE},
  title         = {A New Algorithm for Training Sparse Autoencoders},
  url           = {https://ieeexplore.ieee.org/document/8081588/},
  year          = {2017}
}

@inproceedings{hinton1993autoencoders,
  abstract      = {An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
  author        = {Hinton, Geoffrey E. and Zemel, Richard S.},
  booktitle     = {Advances in Neural Information Processing Systems 6},
  editor        = {J. D. Cowan and G. Tesauro and J. Alspector},
  note          = {TLDR: This early work connects autoencoders to the Minimum Description Length (MDL) principle. It frames the training objective as minimizing the information required to describe both the latent code and the reconstruction error, providing a theoretical justification for using stochastic codes.},
  pages         = {3--10},
  pdf           = {https://proceedings.neurips.cc/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf},
  publisher     = {Morgan-Kaufmann},
  title         = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
  url           = {https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html},
  year          = {1993}
}

@inproceedings{kingma2014semisupervised,
  abstract      = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  address       = {Red Hook, NY, USA},
  author        = {Diederik P. Kingma and Shakir Mohamed and Danilo Jimenez Rezende and Max Welling},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper extends the VAE framework to the semi-supervised setting. It shows how a VAE can be used to learn a generative model of both the data and labels, allowing the model to leverage vast amounts of unlabeled data to improve classification performance when only a few labeled examples are available.},
  openalex      = {W2108501770},
  pages         = {3581--3589},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2014/file/6d42b1217a6996997ead5a8398c1f944-Paper.pdf},
  publisher     = {Curran Associates Inc.},
  series        = {NIPS'14},
  title         = {Semi-Supervised Learning with Deep Generative Models},
  url           = {https://arxiv.org/abs/1406.5298},
  volume        = {27},
  year          = {2014}
}

@inproceedings{burda2016importance,
  abstract      = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  author        = {Yuri Burda and Roger B. Grosse and Ruslan Salakhutdinov},
  booktitle     = {4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  note          = {TLDR: This work introduces the Importance Weighted Autoencoder (IWAE), which provides a tighter evidence lower bound (ELBO) for VAEs by using multiple samples from the approximate posterior. This simple modification consistently leads to models that learn richer representations and achieve better log-likelihood scores.},
  pdf           = {https://arxiv.org/pdf/1509.00519},
  publisher     = {ICLR},
  title         = {Importance Weighted Autoencoders},
  year          = {2016}
}

@inproceedings{higgins2017betavae,
  abstract      = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce $β$-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. We introduce an adjustable hyperparameter $β$ that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that $β$-VAE with appropriately tuned $β > 1$ qualitatively outperforms VAE ($β = 1$), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (CelebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, $β$-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  author        = {Higgins, Irina and Matthey, Loïc and Pal, Arka and Burgess, Christopher P. and Glorot, Xavier and Botvinick, Matthew M. and Mohamed, Shakir and Lerchner, Alexander},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This highly influential paper proposes the $β$-VAE, a modification to the VAE objective that introduces a tunable hyperparameter $β$ to control the strength of the KL-divergence term. It demonstrates that with $β > 1$, the model is encouraged to learn more disentangled latent representations, where single latent units correspond to distinct, interpretable factors of variation in the data.},
  pdf           = {https://openreview.net/pdf?id=Sy2fzU9gl},
  title         = {$β$-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  url           = {https://openreview.net/forum?id=Sy2fzU9gl},
  year          = {2017}
}

@inproceedings{makhzani2016adversarial,
  abstract      = {In this paper, we propose the `adversarial autoencoder' (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We demonstrate how adversarial autoencoders can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. On the MNIST, Street View House Numbers and Toronto Face datasets, we show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  author        = {Alireza Makhzani and Jonathon Shlens and Navdeep Jaitly and Ian Goodfellow and Brendan Frey},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This paper proposes matching the aggregated posterior distribution of the VAE's latent code to an arbitrary prior distribution using an adversarial training objective. This replaces the KL-divergence term with a discriminator network, providing more flexibility and often resulting in better generative samples.},
  openalex      = {W4293568373},
  pdf           = {https://openreview.net/pdf/2xwp4Zwr3TpKBZvXtWoj.pdf},
  title         = {Adversarial Autoencoders},
  url           = {https://openreview.net/forum?id=2xwp4Zwr3TpKBZvXtWoj},
  year          = {2016}
}

@inproceedings{radford2016unsupervised,
  abstract      = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks -- demonstrating their applicability as general image representations.},
  author        = {Alec Radford and Luke Metz and Soumith Chintala},
  booktitle     = {4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  doi           = {10.48550/arxiv.1511.06434},
  note          = {TLDR: While primarily a GAN paper, DCGAN demonstrated the power of learning a good latent space for generation. The paper showed that the encoder-like discriminator learned a hierarchy of features and that vector arithmetic in the latent space was meaningful, heavily influencing the goals of VAE-based representation learning.},
  openalex      = {W2963684088},
  pdf           = {https://arxiv.org/pdf/1511.06434.pdf},
  title         = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  url           = {http://iclr.cc/archive/www/doku.php%3Fid=iclr2016:accepted-main.html},
  year          = {2016}
}

@inproceedings{chen2016infogan,
  abstract      = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  author        = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
  booktitle     = {Advances in Neural Information Processing Systems 29},
  note          = {TLDR: This paper introduces an information-theoretic extension to GANs that is able to learn disentangled representations in a completely unsupervised manner. It maximizes the mutual information between a small subset of latent variables and the observation, providing a powerful alternative to VAE-based disentanglement.},
  openalex      = {W2963226019},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
  url           = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
  volume        = {29},
  year          = {2016}
}

@inproceedings{tolstikhin2018wasserstein,
  abstract      = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  author        = {Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schölkopf},
  booktitle     = {6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  month         = {5},
  note          = {TLDR: This work proposes the Wasserstein Autoencoder (WAE), which minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution. It offers a more stable training objective than adversarial methods and provides a clear theoretical connection between VAEs and optimal transport.},
  pdf           = {https://openreview.net/pdf?id=HkL7n1-0b},
  title         = {Wasserstein Auto-Encoders},
  url           = {https://openreview.net/forum?id=HkL7n1-0b},
  year          = {2018}
}

@inproceedings{bowman2016generating,
  abstract      = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. The authors present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  address       = {Berlin, Germany},
  author        = {Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},
  booktitle     = {Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL 2016)},
  doi           = {10.18653/v1/K16-1002},
  note          = {TLDR: This was a pioneering work in applying VAEs to natural language generation. It combines an RNN-based autoencoder with the VAE framework to learn a continuous latent space for sentences, demonstrating smooth interpolation and sentence generation capabilities, while also highlighting the "KL vanishing" problem.},
  openalex      = {W2963223306},
  pages         = {10--21},
  pdf           = {https://aclanthology.org/K16-1002.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Generating Sentences from a Continuous Space},
  url           = {https://aclanthology.org/K16-1002/},
  year          = {2016}
}

@inproceedings{vahdat2020nvae,
  abstract      = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256×256 pixels. The source code is available at https://github.com/NVlabs/NVAE.},
  author        = {Arash Vahdat and Jan Kautz},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper introduces a deep hierarchical VAE that scales to large, high-resolution images. By using residual connections and carefully designing the hierarchy of latent variables, NVAE achieves state-of-the-art likelihood scores on standard benchmarks, demonstrating that VAEs can be competitive with other top generative models.},
  openalex      = {W3041956526},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/e3b21256183cf7c2c7a66be163579d37-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {NVAE: A Deep Hierarchical Variational Autoencoder},
  url           = {https://papers.nips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html},
  volume        = {33},
  year          = {2020}
}

@article{kingma2019introduction,
  abstract      = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  author        = {Diederik P. Kingma and Max Welling},
  doi           = {10.1561/2200000056},
  journal       = {Foundations and Trends in Machine Learning},
  note          = {TLDR: A comprehensive review and tutorial on Variational Autoencoders written by the original authors. It provides a deep dive into the theory, extensions, and applications of VAEs, serving as an essential reference for anyone looking to understand the topic in depth.},
  number        = {4},
  openalex      = {W2948978827},
  pages         = {307--392},
  pdf           = {https://arxiv.org/pdf/1906.02691},
  publisher     = {now publishers},
  title         = {An Introduction to Variational Autoencoders},
  volume        = {12},
  year          = {2019}
}

@article{bengio2013representation,
  abstract      = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  author        = {Yoshua Bengio and Aaron Courville and Pascal Vincent},
  doi           = {10.1109/TPAMI.2013.50},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month         = {8},
  note          = {TLDR: A highly influential and comprehensive survey that defined the field of representation learning. It provides a unified view of probabilistic models, autoencoders, manifold learning, and deep networks, contextualizing the role of autoencoders within the broader quest for learning useful features from data.},
  number        = {8},
  openalex      = {W2163922914},
  pages         = {1798--1828},
  pdf           = {https://arxiv.org/pdf/1206.5538},
  title         = {Representation Learning: A Review and New Perspectives},
  volume        = {35},
  year          = {2013}
}

@inproceedings{masci2011stacked,
  address       = {Berlin, Heidelberg},
  author        = {Jonathan Masci and Ueli Meier and Dan Cireşan and Jürgen Schmidhuber},
  booktitle     = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
  doi           = {10.1007/978-3-642-21735-7_7},
  editor        = {Timo Honkela and Włodzisław Duch and Mark Girolami and Samuel Kaski},
  note          = {TLDR: This paper introduces the Convolutional Autoencoder (CAE) and demonstrates how stacking them can learn a hierarchy of features. It applies this method to traffic sign classification, showing that unsupervised pre-training with CAEs can significantly improve performance.},
  openalex      = {W2136655611},
  pages         = {52--59},
  pdf           = {http://www.idsia.ch/~juergen/icann2011stack.pdf},
  publisher     = {Springer},
  series        = {Lecture Notes in Computer Science},
  title         = {Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction},
  volume        = {6791},
  year          = {2011}
}

@inproceedings{kipf2016variational,
  abstract      = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arXiv},
  author        = {Thomas N. Kipf and Max Welling},
  booktitle     = {NIPS Workshop on Bayesian Deep Learning},
  eprint        = {1611.07308},
  month         = {12},
  pdf           = {http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf},
  primaryclass  = {stat.ML},
  title         = {Variational Graph Auto-Encoders},
  url           = {https://arxiv.org/abs/1611.07308},
  year          = {2016}
}

@article{lecun2015deep,
  abstract      = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  author        = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  doi           = {10.1038/nature14539},
  journal       = {Nature},
  month         = {5},
  note          = {TLDR: This seminal review article provides a high-level overview of the field of deep learning for a broad scientific audience. It highlights autoencoders as a key method for unsupervised learning and discusses their role in pre-training deep networks.},
  number        = {7553},
  openalex      = {W2919115771},
  pages         = {436--444},
  pdf           = {https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf},
  title         = {Deep Learning},
  volume        = {521},
  year          = {2015}
}

@inproceedings{sedhain2015autorec,
  abstract      = {This paper proposes AutoRec, a novel autoencoder framework for collaborative filtering (CF). Empirically, AutoRec's compact and efficiently trainable model outperforms state-of-the-art CF techniques on the Movielens and Netflix datasets.},
  address       = {New York, NY, USA},
  author        = {Suvash Sedhain and Aditya Krishna Menon and Scott Sanner and Lexing Xie},
  booktitle     = {Proceedings of the 24th International Conference on World Wide Web},
  doi           = {10.1145/2740908.2742726},
  openalex      = {W1720514416},
  pages         = {111--112},
  pdf           = {https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf},
  publisher     = {Association for Computing Machinery},
  series        = {WWW '15 Companion},
  title         = {AutoRec: Autoencoders Meet Collaborative Filtering},
  url           = {https://doi.org/10.1145/2740908.2742726},
  year          = {2015}
}

@inproceedings{liang2018variational,
  abstract      = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research. We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  address       = {New York, NY, USA},
  author        = {Dawen Liang and Rahul G. Krishnan and Matthew D. Hoffman and Tony Jebara},
  booktitle     = {Proceedings of the 2018 World Wide Web Conference},
  doi           = {10.1145/3178876.3186150},
  isbn          = {978-1-4503-5639-8},
  location      = {Lyon, France},
  month         = {4},
  note          = {TLDR: This work extends VAEs to the task of collaborative filtering for implicit feedback data. It proposes a generative model based on a multinomial likelihood that outperforms a wide range of baselines, including linear models and neural network approaches.},
  openalex      = {W2963085847},
  pages         = {689--698},
  pdf           = {https://dl.acm.org/doi/pdf/10.1145/3178876.3186150},
  publisher     = {ACM},
  title         = {Variational Autoencoders for Collaborative Filtering},
  year          = {2018}
}

@inproceedings{srivastava2015unsupervised,
  abstract      = {We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (percepts) of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.},
  address       = {Lille, France},
  author        = {Nitish Srivastava and Elman Mansimov and Ruslan Salakhutdinov},
  booktitle     = {Proceedings of the 32nd International Conference on Machine Learning},
  month         = {7},
  note          = {TLDR: This paper presents an influential early approach to unsupervised video representation learning using an LSTM-based autoencoder. The model is trained to reconstruct input video sequences and also to predict future frames, learning features that are useful for action recognition tasks.},
  openalex      = {W2116435618},
  pages         = {843--852},
  pdf           = {http://proceedings.mlr.press/v37/srivastava15.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Unsupervised Learning of Video Representations using LSTMs},
  url           = {https://proceedings.mlr.press/v37/srivastava15.html},
  volume        = {37},
  year          = {2015}
}

@inproceedings{wu2016learning,
  abstract      = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
  author        = {Jiajun Wu and Chengkai Zhang and Tianfan Xue and William T. Freeman and Joshua B. Tenenbaum},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This work combines a VAE with a GAN to learn a probabilistic latent space of 3D object shapes. The VAE encoder maps 3D shapes into a latent space, and a GAN is used to regularize this space, enabling shape completion, interpolation, and generation.},
  openalex      = {W2546066744},
  pdf           = {https://proceedings.neurips.cc/paper/2016/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf},
  series        = {NIPS'16},
  title         = {Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling},
  url           = {https://proceedings.neurips.cc/paper/2016/hash/44f683a84163b3523afe57c2e008bc8c-Abstract.html},
  volume        = {29},
  year          = {2016}
}

@inproceedings{yao2020implicit,
  abstract      = {An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, we show theoretically and empirically that the system spontaneously learns representations with a low effective dimension. The model, dubbed implicit rank-minimizing autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate superior representation learning performance against a standard deterministic autoencoder and comparable performance to a variational autoencoder on MNIST and CelebA datasets through various generative tasks.},
  author        = {Li Jing and Jure Zbontar and Yann LeCun},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper proposes a simple, deterministic autoencoder that learns compact latent spaces by implicitly minimizing the rank of the code's covariance matrix. By inserting extra linear layers between the encoder and decoder, the model spontaneously learns representations with a low effective dimension without needing a probabilistic framework.},
  openalex      = {W3103129800},
  pages         = {14736--14746},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/a9078e8653368c9c291ae2f8b74012e7-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Implicit Rank-Minimizing Autoencoder},
  url           = {https://proceedings.neurips.cc/paper/2020/hash/a9078e8653368c9c291ae2f8b74012e7-Abstract.html},
  volume        = {33},
  year          = {2020}
}

@article{liu2023autoencodersreview,
  abstract      = {Deep learning, which is a subfield of machine learning, has opened a new era for the development of neural networks. The auto-encoder is a key component of deep structure, which can be used to realize transfer learning and plays an important role in both unsupervised learning and non-linear feature extraction. In this survey, we aim to review state-of-the-art auto-encoder algorithms, discussing their basic concepts, variants, applications, and future trends in deep learning. The paper provides a comprehensive overview of auto-encoder models, collecting and categorizing studies on auto-encoders and their variants, analyzing them from different perspectives, and discussing their applications in numerous domains including pattern recognition, computer vision, data generation, and recommender systems.},
  author        = {Chen, Shuangshuang and Guo, Wei},
  doi           = {10.3390/math11081777},
  journal       = {Mathematics},
  month         = {4},
  number        = {8},
  openalex      = {W4363650848},
  pages         = {1777},
  pdf           = {https://www.mdpi.com/2227-7390/11/8/1777/pdf?version=1681094465},
  title         = {Auto-Encoders in Deep Learning---A Review with New Perspectives},
  url           = {https://www.mdpi.com/2227-7390/11/8/1777},
  volume        = {11},
  year          = {2023}
}

@inproceedings{he2022masked,
  abstract      = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we show that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3$ imes$ or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  author        = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Dollár and Ross Girshick},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  note          = {TLDR: This landmark paper introduces the Masked Autoencoder (MAE), a scalable self-supervised learning approach for Vision Transformers. It masks a high proportion of input patches and learns to reconstruct them from the unmasked ones, demonstrating that a simple autoencoding objective can learn powerful representations and achieve state-of-the-art results.},
  openalex      = {W4313156423},
  pages         = {16000--16009},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf},
  publisher     = {IEEE Computer Society},
  title         = {Masked Autoencoders Are Scalable Vision Learners},
  year          = {2022}
}

@inproceedings{bao2022beit,
  abstract      = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first 'tokenize' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.},
  author        = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: A concurrent and influential work to MAE, BEiT proposes a masked image modeling task analogous to BERT. It first tokenizes the image into discrete visual tokens using a dVAE, then masks some tokens and trains a Transformer to predict the original visual tokens of the masked patches.},
  openalex      = {W3170863103},
  pdf           = {https://openreview.net/pdf?id=p-BhZSz59o4},
  title         = {BEiT: BERT Pre-Training of Image Transformers},
  url           = {https://openreview.net/forum?id=p-BhZSz59o4},
  year          = {2022}
}

@inproceedings{tong2022videomae,
  abstract      = {Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.},
  author        = {Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper successfully adapts the MAE framework to video data for self-supervised representation learning. It proposes a "tube masking" strategy with a very high masking ratio, demonstrating that VideoMAE can learn strong spatio-temporal representations efficiently and achieve excellent performance on downstream action recognition tasks.},
  openalex      = {W4221167396},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/416f9cb3276121c42eebb86352a4354a-Paper-Conference.pdf},
  title         = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/416f9cb3276121c42eebb86352a4354a-Paper-Conference.pdf},
  volume        = {35},
  year          = {2022}
}

@inproceedings{cai2023marlin,
  abstract      = {This paper proposes a self-supervised approach to learn universal facial representations from videos, that can transfer across a variety of facial analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encoding generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN.},
  author        = {Zhixi Cai and Shreya Ghosh and Kalin Stefanov and Abhinav Dhall and Jianfei Cai and Hamid Rezatofighi and Reza Haffari and Munawar Hayat},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi           = {10.1109/cvpr52729.2023.00150},
  note          = {TLDR: This work proposes MARLIN, a masked autoencoder specifically designed to learn universal facial representations from videos. By densely masking key facial regions (eyes, mouth, etc.), the model is forced to learn robust local and global features that transfer well to diverse downstream tasks like expression recognition and DeepFake detection.},
  openalex      = {W4386076638},
  pages         = {1493--1504},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.pdf},
  publisher     = {IEEE},
  title         = {MARLIN: Masked Autoencoder for Facial Video Representation LearnINg},
  url           = {https://openaccess.thecvf.com/content/CVPR2023/html/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.html},
  year          = {2023}
}

@inproceedings{baevski2022masked,
  abstract      = {This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder is lightweight and reconstructs masked patches in the spectrogram. When pre-trained on AudioSet, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training.},
  author        = {Po-Yao Huang and Hu Xu and Juncheng Li and Alexei Baevski and Michael Auli and Wojciech Galuba and Florian Metze and Christoph Feichtenhofer},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper explores the application of masked autoencoders to self-supervised learning of speech representations. It investigates different masking strategies and reconstruction targets, showing that a simple MAE-style objective can learn powerful representations for speech recognition and other audio tasks.},
  openalex      = {W4285483774},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b89d5e209990b19e33b418e14f323998-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Masked Autoencoders that Listen},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b89d5e209990b19e33b418e14f323998-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{gao2022mcmae,
  abstract      = {Vision Transformers (ViT) become widely-adopted architectures for various vision tasks. Masked auto-encoding for feature pretraining and multi-scale hybrid convolution-transformer architectures can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation. In this work, we propose a multi-scale hybrid convolution-transformer that can learn more discriminative representations via the mask auto-encoding scheme, named MCMAE. We adopt the masked convolution to prevent information leakage in the convolution blocks. Our method outperforms vanilla MAE and other recent strong baselines. MCMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared with MAE-Base, and on object detection, MCMAE-Base finetuned for only 25 epochs surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP respectively.},
  author        = {Peng Gao and Teli Ma and Hongsheng Li and Ziyi Lin and Jifeng Dai and Yu Qiao},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Conference venue: NeurIPS 2022. GitHub: https://github.com/Alpha-VL/ConvMAE},
  pages         = {35663--35678},
  pdf           = {https://openreview.net/pdf?id=qm5LpHyyOUO},
  publisher     = {Curran Associates, Inc.},
  title         = {MCMAE: Masked Convolution Meets Masked Autoencoders},
  url           = {https://proceedings.neurips.cc/paper/2022/hash/e7938ede51225b490bb69f7b361a9259-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{pei2024videomac,
  abstract      = {Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image/video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed VideoMAC, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Moreover, we present a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency. Due to the hierarchical feature extraction capabilities inherent in ConvNets, our approach demonstrates superior proficiency in spatial information extraction in contrast to ViT-based MVM methods. We conduct comprehensive experiments to validate the effectiveness of our method. The experimental results indicate that VideoMAC achieves state-of-the-art performance compared to previous MVM methods on several downstream video understanding tasks.},
  author        = {Gensheng Pei and Tao Chen and Xiruo Jiang and Huafeng Liu and Zeren Sun and Yazhou Yao},
  booktitle     = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  note          = {TLDR: This paper proposes VideoMAC, an efficient masked video modeling approach that uses ConvNets instead of resource-intensive Transformers. It employs a dual-encoder architecture and sparse convolutions to facilitate inter-frame reconstruction consistency, outperforming ViT-based methods on several downstream video understanding tasks.},
  openalex      = {W4402780389},
  pages         = {27332--27341},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2024/papers/Pei_VideoMAC_Video_Masked_Autoencoders_Meet_ConvNets_CVPR_2024_paper.pdf},
  publisher     = {Computer Vision Foundation / IEEE},
  title         = {VideoMAC: Video Masked Autoencoders Meet ConvNets},
  year          = {2024}
}

@inproceedings{roet2023masked,
  abstract      = {Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how self-supervised deep learning approaches scale when training larger models on larger microscopy datasets.},
  author        = {Oren Kraus and Kian Kenyon-Dean and Saber Saberian and Maryam Fallah and Peter McLean and Jess Leung and Vasudev Sharma and Ayla Khan and Jia Balakrishnan and Safiye Celik and Maciej Sypetkowski and Chi Cheng and Kristen Morse and Maureen Makes and Ben Mabey and Berton Earnshaw},
  booktitle     = {NeurIPS 2023 Generative AI and Biology (GenBio) Workshop},
  openalex      = {W4387228684},
  pdf           = {https://openreview.net/pdf?id=jasx3fgu4G},
  title         = {Masked autoencoders are scalable learners of cellular morphology},
  url           = {https://openreview.net/forum?id=jasx3fgu4G},
  year          = {2023}
}

@inproceedings{zhou2022ibot,
  abstract      = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the necessity and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3% linear probing accuracy and an 87.8% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, e.g., object detection, instance segmentation, and semantic segmentation.},
  author        = {Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Yuille and Tao Kong},
  booktitle     = {International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2111.07832},
  openalex      = {W3213836217},
  pdf           = {https://openreview.net/pdf?id=ydopy-e6Dg},
  title         = {Image BERT Pre-training with Online Tokenizer},
  url           = {https://openreview.net/forum?id=ydopy-e6Dg},
  year          = {2022}
}

@inproceedings{parpart2024transformer,
  address       = {Washington, DC, USA},
  author        = {Gavin Parpart and Jonathan H. Tu and Bradley D. Clymer and Jung H. Lee and Jason S. Babcock},
  booktitle     = {2024 IEEE Military Communications Conference (MILCOM)},
  doi           = {10.1109/milcom61039.2024.10773654},
  isbn          = {979-8-3503-7423-0},
  month         = {10},
  note          = {TLDR: This paper applies transformer-based masked autoencoders to the task of Radio Frequency (RF) device fingerprinting. It demonstrates that pre-training a transformer on RF signals using an MAE objective improves classification accuracy for identifying specific physical devices from their radio emissions.},
  openalex      = {W4405103975},
  pages         = {859--862},
  publisher     = {IEEE},
  title         = {Transformer Masked Autoencoders for RF Device Fingerprinting},
  year          = {2024}
}

@inproceedings{rombach2022high,
  abstract      = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  author        = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi           = {10.1109/cvpr52688.2022.01042},
  month         = {6},
  note          = {TLDR: This paper introduces Latent Diffusion Models (LDMs), the architecture behind Stable Diffusion. It proposes using a powerful autoencoder to map images into a compact latent space, where a diffusion model can be trained much more efficiently, enabling high-resolution image synthesis at a fraction of the computational cost.},
  openalex      = {W4312933868},
  pages         = {10684--10695},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf},
  title         = {High-Resolution Image Synthesis with Latent Diffusion Models},
  year          = {2022}
}

@inproceedings{preechakul2022diffusion,
  abstract      = {Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This work explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. The key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. The method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction.},
  author        = {Konpat Preechakul and Nattanat Chatthee and Suttisak Wizadwongsa and Supasorn Suwajanakorn},
  booktitle     = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi           = {10.1109/cvpr52688.2022.01036},
  note          = {TLDR: This work proposes an autoencoder that uses a learnable encoder to capture high-level semantic features and a diffusion model as a powerful decoder to model stochastic details. This architecture produces a meaningful, decodable latent representation that allows for high-fidelity reconstruction and enables challenging tasks like attribute manipulation on real images.},
  openalex      = {W3217030260},
  pages         = {10609--10619},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2022/papers/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.pdf},
  title         = {Diffusion Autoencoders: Toward a Meaningful and Decodable Representation},
  year          = {2022}
}

@inproceedings{xiao2021vaebm,
  abstract      = {Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256×256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection.},
  author        = {Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This paper proposes VAEBM, a hybrid model that combines a VAE and an Energy-Based Model (EBM). The VAE captures the global data structure, while the EBM refines the samples to produce sharp, high-quality images, leveraging the strengths of both generative frameworks.},
  openalex      = {W3118605064},
  pdf           = {https://openreview.net/pdf?id=5m3SEczOV8L},
  title         = {VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models},
  url           = {https://openreview.net/forum?id=5m3SEczOV8L},
  year          = {2021}
}

@inproceedings{han2020joint,
  abstract      = {This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energy-based model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.},
  author        = {Tian Han and Erik Nijkamp and Linqi Zhou and Bo Pang and Song-Chun Zhu and Ying Nian Wu},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi           = {10.1109/cvpr42600.2020.00800},
  note          = {TLDR: This paper proposes a joint training scheme for a VAE and a latent EBM based on an elegant objective function of Kullback-Leibler divergences. The EBM acts as a critic to improve the VAE's generator, leading to significantly improved synthesis quality and enabling anomaly detection.},
  openalex      = {W3035101153},
  pages         = {7975--7984},
  pdf           = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Han_Joint_Training_of_Variational_Auto-Encoder_and_Latent_Energy-Based_Model_CVPR_2020_paper.pdf},
  title         = {Joint Training of Variational Auto-Encoder and Latent Energy-Based Model},
  url           = {https://openaccess.thecvf.com/content_CVPR_2020/html/Han_Joint_Training_of_Variational_Auto-Encoder_and_Latent_Energy-Based_Model_CVPR_2020_paper.html},
  year          = {2020}
}

@inproceedings{kim2023diffusion,
  abstract      = {Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.},
  address       = {Vancouver, Canada},
  author        = {Gyeongman Kim and Hajin Shim and Hyunsu Kim and Yunjey Choi and Junho Kim and Eunho Yang},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint        = {2212.02802},
  eprinttype    = {arXiv},
  month         = {6},
  openalex      = {W4386076391},
  pages         = {6091--6100},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Diffusion_Video_Autoencoders_Toward_Temporally_Consistent_Face_Video_Editing_via_CVPR_2023_paper.pdf},
  publisher     = {IEEE},
  title         = {Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding},
  url           = {https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Diffusion_Video_Autoencoders_Toward_Temporally_Consistent_Face_Video_Editing_via_CVPR_2023_paper.html},
  year          = {2023}
}

@inproceedings{tang2023vector,
  abstract      = {Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.},
  author        = {Long Tung Vuong and Trung Le and He Zhao and Chuanxia Zheng and Mehrtash Harandi and Jianfei Cai and Dinh Phung},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  note          = {TLDR: This paper combines the strengths of Vector Quantized VAEs (VQ-VAEs) and Wasserstein Autoencoders (WAEs). The proposed model uses a discrete latent space for powerful representation while optimizing the Wasserstein distance for more stable training and higher-quality image generation.},
  openalex      = {W4320854089},
  pages         = {35223--35242},
  pdf           = {https://proceedings.mlr.press/v202/vuong23a/vuong23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Vector Quantized Wasserstein Auto-Encoder},
  volume        = {202},
  year          = {2023}
}

@inproceedings{ricker2024aeroblade,
  abstract      = {With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs, including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. We release our code and data at https://github.com/jonasricker/aeroblade.},
  address       = {Seattle, WA, USA},
  author        = {Jonas Ricker and Denis Lukovnikov and Asja Fischer},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi           = {10.1109/CVPR52733.2024.00871},
  month         = {6},
  note          = {TLDR: This paper proposes a novel, training-free method for detecting images generated by Latent Diffusion Models. It cleverly exploits the fact that the pre-trained autoencoder used in LDMs reconstructs synthetic images with lower error than real images, providing a simple yet effective forensic signal.},
  openalex      = {W4402716434},
  pages         = {9130--9140},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2024/papers/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.pdf},
  publisher     = {IEEE},
  title         = {AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error},
  url           = {https://openaccess.thecvf.com/content/CVPR2024/html/Ricker_AEROBLADE_Training-Free_Detection_of_Latent_Diffusion_Images_Using_Autoencoder_Reconstruction_CVPR_2024_paper.html},
  year          = {2024}
}

@inproceedings{nazari2023geometric,
  abstract      = {Visualization is a crucial step in exploratory data analysis. One approach is to train an autoencoder with low-dimensional latent space. Large networks can help unfolding the data. However, such expressive networks achieve low reconstruction error even when representation is distorted. To avoid misleading visualizations, we propose a first differential geometric perspective on the decoder, leading to diagnostics for embedding distortion, and a new regularizer for mitigating such distortion. Our 'Geometric Autoencoder' avoids spuriously stretching embeddings, so visualization captures structure more faithfully, and flags areas where little distortion could not be achieved, guarding against misinterpretation.},
  author        = {Philipp Nazari and Sebastian Damrich and Fred A. Hamprecht},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  editor        = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  openalex      = {W4383046993},
  pages         = {25834--25857},
  pdf           = {https://proceedings.mlr.press/v202/nazari23a/nazari23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Geometric Autoencoders -- What You See is What You Decode},
  url           = {https://proceedings.mlr.press/v202/nazari23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{wang2024autobasisencoder,
  abstract      = {We introduce AutoBasisEncoder, a novel framework designed for operator learning -- the task of learning to map from one function to another. This approach autonomously discovers a basis of functions optimized for the target function space and utilizes this pre-trained basis for efficient operator learning. By introducing an intermediary auto-encoding task to the popular DeepONet framework, AutoBasisEncoder disentangles the learning of the basis functions and of the coefficients, simplifying the operator learning process. The framework operates in two stages: Initially, the framework learns basis functions through auto-encoding, followed by leveraging this basis to predict the coefficients of the target function. According to preliminary experiments, AutoBasisEncoder's basis functions exhibit superior suitability for operator learning and function reconstruction compared to DeepONet.},
  author        = {Thomas X. Wang and Nicolas Baskiotis and Patrick Gallinari},
  booktitle     = {ICLR 2024 Workshop on AI for Differential Equations in Science},
  keywords      = {operator learning, auto-encoding, neural fields, pre-training, basis},
  month         = {3},
  note          = {Presented at ICLR 2024 Workshop on AI for Differential Equations in Science},
  pdf           = {https://openreview.net/pdf?id=8bVPkfswQG},
  title         = {AutoBasisEncoder: Pre-trained Neural Field Basis via Autoencoding for Operator Learning},
  url           = {https://openreview.net/forum?id=8bVPkfswQG},
  year          = {2024}
}

@inproceedings{zhang2025meshgen,
  abstract      = {In this paper, we introduce MeshGen, an advanced image-to-3D pipeline that generates high-quality 3D meshes with detailed geometry and physically based rendering (PBR) textures. Addressing the challenges faced by existing 3D native diffusion models, such as suboptimal auto-encoder performance, limited controllability, poor generalization, and inconsistent image-based PBR texturing, MeshGen employs several key innovations to overcome these limitations. We pioneer a render-enhanced point-to-shape auto-encoder that compresses meshes into a compact latent space by designing perceptual optimization with ray-based regularization. This ensures that the 3D shapes are accurately represented and reconstructed to preserve geometric details within the latent space. To address data scarcity and image-shape misalignment, we further propose geometric augmentation and generative rendering augmentation techniques, which enhance the model's controllability and generalization ability, allowing it to perform well even with limited public datasets. For the texture generation, MeshGen employs a reference attention-based multi-view ControlNet for consistent appearance synthesis. This is further complemented by our multi-view PBR decomposer that estimates PBR components and a UV inpainter that fills invisible areas, ensuring a seamless and consistent texture across the 3D mesh. Our extensive experiments demonstrate that MeshGen largely outperforms previous methods in both shape and texture generation, setting a new standard for the quality of 3D meshes generated with PBR textures.},
  address       = {},
  arxiv         = {2505.04656},
  author        = {Chen, Zilong and Wang, Yikai and Sun, Wenqiang and Wang, Feng and Chen, Yiwen and Liu, Huaping},
  booktitle     = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  code          = {https://github.com/heheyas/MeshGen},
  note          = {CVPR 2025 Highlight paper},
  pages         = {},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative_CVPR_2025_paper.pdf},
  publisher     = {IEEE Computer Society},
  title         = {MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation},
  url           = {https://heheyas.github.io/MeshGen/},
  year          = {2025}
}

@misc{cunningham2023sparse,
  abstract      = {This work explores the use of sparse autoencoders to identify interpretable features in language models, addressing the challenge of superposition where neural networks represent more features than they have neurons. The approach provides a scalable and unsupervised method for extracting monosemantic features from the internal activations of language models, contributing to mechanistic interpretability research.},
  archiveprefix = {arXiv},
  author        = {Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
  eprint        = {2309.08600},
  note          = {TLDR: A foundational paper in the modern resurgence of SAEs for interpretability. It demonstrates that training a very large, sparse autoencoder on the internal activations of a language model can uncover thousands of interpretable, monosemantic features, providing a scalable method to resolve superposition.},
  pdf           = {https://openreview.net/pdf?id=F76bwRSLeK},
  primaryclass  = {cs.LG},
  title         = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  url           = {https://arxiv.org/abs/2309.08600},
  year          = {2023}
}

@techreport{gao2024scaling,
  abstract      = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders (Makhzani and Frey, 2013) to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release code and autoencoders for open-source models, as well as a visualizer.},
  archiveprefix = {arXiv},
  author        = {Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  eprint        = {2406.04093},
  institution   = {OpenAI},
  month         = {6},
  note          = {TLDR: This work presents a state-of-the-art methodology for reliably training extremely wide and sparse autoencoders on frontier models like GPT-4. It introduces k-sparse autoencoders to directly control sparsity, finds clean scaling laws, and proposes new metrics for evaluating feature quality beyond reconstruction loss.},
  number        = {arXiv:2406.04093},
  openalex      = {W4399455238},
  pdf           = {https://arxiv.org/pdf/2406.04093.pdf},
  primaryclass  = {cs.LG},
  title         = {Scaling and Evaluating Sparse Autoencoders},
  url           = {https://arxiv.org/abs/2406.04093},
  year          = {2024}
}

@inproceedings{cunningham2024towards,
  abstract      = {One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. This prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. Polysemanticity may arise from "superposition", where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. We use a scalable and unsupervised method called Sparse Autoencoders to find interpretable, monosemantic features in the residual streams of real LLMs (Pythia-70M/410M).},
  author        = {Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2309.08600},
  keywords      = {language model, interpretability, representation learning, sparsity, dictionary learning, unsupervised learning},
  openalex      = {W4386839891},
  pdf           = {https://openreview.net/pdf?id=F76bwRSLeK},
  title         = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  url           = {https://openreview.net/forum?id=F76bwRSLeK},
  year          = {2024}
}

@misc{kissane2024sparse,
  abstract      = {This work replicates the success of applying sparse autoencoders to MLP layers and demonstrates that the same technique works well on attention layer outputs. We train sparse autoencoders on the second attention layer of a two-layer language model, specifically on hook_z concatenated over all attention heads. The learned features are sparse and interpretable, providing insights into what attention layers learn and how they compute their outputs. We estimate that 82% of non-dead features in our SAE are interpretable, with 24% of the SAE features being dead. This approach allows us to see how much of each feature's weights come from each head, which is a promising direction for investigating attention head superposition. The research was conducted as part of a 2-week research sprint project during the training phase of Neel Nanda's MATS stream.},
  author        = {Connor Kissane and Robert Krzyzanowski and Arthur Conmy and Neel Nanda},
  howpublished  = {AI Alignment Forum},
  month         = {1},
  note          = {Result of a 2-week research sprint project during Neel Nanda's MATS stream. Demonstrates that SAEs applied to attention layer outputs learn sparse, interpretable features with 82% of non-dead features being interpretable.},
  openalex      = {W4400065290},
  pdf           = {https://arxiv.org/pdf/2406.17759.pdf},
  title         = {Sparse Autoencoders Work on Attention Layer Outputs},
  url           = {https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs},
  year          = {2024}
}

@misc{chanin2024absorption,
  abstract      = {Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. However, naively using SAEs can lead to inadequate interpretability if the SAE's latents are polysemantic (i.e., activated by multiple unrelated concepts). Recent work suggests that SAEs suffer from feature splitting, where a single ground-truth feature is represented by multiple SAE features, which could result in lower-quality interpretations. In this work, we introduce a new evaluation methodology that allows us to robustly measure the quality of SAE features. Using this methodology, we find that feature splitting is not the only concern. We identify a new phenomenon, feature absorption, where seemingly monosemantic features fail to fire where they should, and instead get absorbed into their children features. We show that feature absorption is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We validate our findings empirically on hundreds of SAEs trained on LLMs of varying sizes. Our results suggest that varying SAE size or sparsity is insufficient to solve this issue, and we discuss the implications of feature absorption for SAE interpretability and potential approaches to solve the issue.},
  archiveprefix = {arXiv},
  author        = {David I. Chanin and James Wilken-Smith and Tomáš Dulka and Hardik Bhatnagar and Satvik Golechha and Joseph Bloom},
  doi           = {10.48550/arXiv.2409.14507},
  eprint        = {2409.14507},
  howpublished  = {arXiv preprint},
  month         = {9},
  openalex      = {W4403851170},
  pdf           = {https://arxiv.org/pdf/2409.14507.pdf},
  primaryclass  = {cs.LG},
  title         = {A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders},
  url           = {https://arxiv.org/abs/2409.14507},
  year          = {2024}
}

@misc{thasarathan2025universal,
  abstract      = {We present Universal Sparse Autoencoders (USAEs), a framework for uncovering and aligning interpretable concepts spanning multiple pretrained deep neural networks. Unlike existing concept-based interpretability methods, which focus on a single model, USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models at once. The core insight is to train a single, overcomplete sparse autoencoder (SAE) that ingests activations from any model and decodes them to approximate the activations of any other model under consideration. By optimizing a shared objective, the learned dictionary captures common factors of variation-concepts-across different tasks, architectures, and datasets. USAEs discover semantically coherent and important universal concepts across vision models; ranging from low-level features (e.g., colors and textures) to higher-level structures (e.g., parts and objects).},
  archiveprefix = {arXiv},
  author        = {Harrish Thasarathan and Jason Forsyth and Thomas Fel and Matthew Kowal and Konstantinos G. Derpanis},
  doi           = {10.48550/arxiv.2502.03714},
  eprint        = {2502.03714},
  howpublished  = {arXiv preprint},
  month         = {2},
  openalex      = {W4407244839},
  pdf           = {https://arxiv.org/pdf/2502.03714},
  title         = {Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment},
  url           = {https://arxiv.org/abs/2502.03714},
  year          = {2025}
}

@inproceedings{rajamanoharan2024improving,
  abstract      = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
  author        = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramár, János and Shah, Rohin and Nanda, Neel},
  booktitle     = {Advances in Neural Information Processing Systems},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/01772a8b0420baec00c4d59fe2fbace6-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders},
  url           = {https://papers.nips.cc/paper_files/paper/2024/hash/01772a8b0420baec00c4d59fe2fbace6-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{marks2024evaluating,
  abstract      = {Sparse Autoencoders (SAEs) are an interpretability technique aimed at decomposing neural network activations into interpretable units. However, a major bottleneck for SAE development has been the lack of high-quality performance metrics, with prior work largely relying on unsupervised proxies. This work addresses this critical bottleneck by proposing evaluation frameworks that move beyond unsupervised proxies. We extend SHIFT evaluation using LLMs and introduce Targeted Probe Perturbation (TPP) metric to measure SAEs' ability to disentangle and reduce spurious correlations by removing spurious cues from classifiers through ablating SAE features.},
  address       = {Red Hook, NY, USA},
  author        = {Adam Karvonen and Can Rager and Samuel Marks and Neel Nanda},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Workshop: Attributing Model Behavior at Scale (ATTRIB)},
  pages         = {TBD},
  pdf           = {https://arxiv.org/pdf/2411.18895},
  publisher     = {Curran Associates Inc.},
  series        = {NeurIPS '24},
  title         = {Evaluating Sparse Autoencoders on Targeted Concept Removal Tasks},
  url           = {https://neurips.cc/virtual/2024/105351},
  volume        = {37},
  year          = {2024}
}

@misc{mudide2024efficient,
  abstract      = {Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE architecture aimed at reducing the compute cost of training SAEs. Inspired by sparse mixture of experts models, Switch SAEs route activation vectors between smaller ``expert'' SAEs, enabling SAEs to efficiently scale to many more features. We present experiments comparing Switch SAEs with other SAE architectures, and find that Switch SAEs deliver a substantial Pareto improvement in the reconstruction vs. sparsity frontier for a given fixed training compute budget. We also study the geometry of features across experts, analyze features duplicated across experts, and verify that Switch SAE features are as interpretable as features found by other SAE architectures.},
  archiveprefix = {arXiv},
  author        = {Anish Mudide and Joshua Engels and Eric J. Michaud and Max Tegmark and Christian Schroeder de Witt},
  eprint        = {2410.08201},
  howpublished  = {arXiv preprint},
  month         = {10},
  openalex      = {W4403365499},
  pdf           = {https://arxiv.org/pdf/2410.08201.pdf},
  primaryclass  = {cs.LG},
  title         = {Efficient Dictionary Learning with Switch Sparse Autoencoders},
  url           = {https://arxiv.org/abs/2410.08201},
  year          = {2024}
}

@misc{lepori2023neurosurgeon,
  abstract      = {Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits. We introduce NeuroSurgeon, a Python library for discovering and manipulating subnetworks within HuggingFace Transformers models. NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.},
  archiveprefix = {arXiv},
  author        = {Michael A. Lepori and Ellie Pavlick and Thomas Serre},
  doi           = {10.48550/arXiv.2309.00244},
  eprint        = {2309.00244},
  openalex      = {W4386437578},
  pdf           = {https://arxiv.org/pdf/2309.00244.pdf},
  primaryclass  = {cs.LG},
  title         = {NeuroSurgeon: A Toolkit for Subnetwork Analysis},
  url           = {https://arxiv.org/abs/2309.00244},
  year          = {2023}
}

@inproceedings{teshima2020autoencoding,
  abstract      = {Does a Variational AutoEncoder (VAE) consistently encode typical samples generated from its decoder? This paper shows that the perhaps surprising answer to this question is 'No'; a (nominally trained) VAE does not necessarily amortize inference for typical samples that it is capable of generating. We study the implications of this behaviour on the learned representations and also the consequences of fixing it by introducing a notion of self consistency. Our approach hinges on an alternative construction of the variational approximation distribution to the true posterior of an extended VAE model with a Markov chain alternating between the encoder and the decoder. The method can be used to train a VAE model from scratch or given an already trained VAE, it can be run as a post processing step in an entirely self supervised way without access to the original training data. Our experimental analysis reveals that encoders trained with our self-consistency approach lead to representations that are robust (insensitive) to perturbations in the input introduced by adversarial attacks. We provide experimental results on the ColorMnist and CelebA benchmark datasets that quantify the properties of the learned representations and compare the approach with a baseline that is specifically trained for the desired property.},
  author        = {Taylan Cemgil and Sumedh Ghaisas and Krishnamurthy Dvijotham and Sven Gowal and Pushmeet Kohli},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {TLDR: This paper identifies a key issue in VAEs: they don't necessarily encode their own generated samples well. It proposes a self-consistency objective to fix this, leading to representations that are more robust to adversarial attacks.},
  openalex      = {W3097987777},
  pages         = {15077--15087},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ac10ff1941c540cd87c107330996f4f6-Paper.pdf},
  title         = {The Autoencoding Variational Autoencoder},
  volume        = {33},
  year          = {2020}
}

@inproceedings{shao2020controlvae,
  abstract      = {Variational Autoencoders (VAE) and their variants have been widely used in various applications, such as dialog generation, image generation, and disentangled representation learning. However, existing VAE models have limitations in different applications. For example, they easily suffer from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller inspired by automatic control theory with basic VAE to improve performance. We design a new non-linear PI (proportional-integral) control method to automatically tune hyperparameters during model training. The framework is evaluated across three applications: language modeling, disentangled representation learning, and image generation. Results show ControlVAE can achieve better disentangling than previous methods, avert KL-vanishing, and improve diversity of generated text and images compared to original VAE approaches.},
  author        = {Shao, Huajie and Yao, Shuochao and Sun, Dachun and Zhang, Aston and Liu, Shengzhong and Liu, Dongxin and Wang, Jun and Abdelzaher, Tarek},
  booktitle     = {Proceedings of the 37th International Conference on Machine Learning},
  editor        = {Daumé III, Hal and Singh, Aarti},
  month         = {7},
  openalex      = {W3034182955},
  pages         = {8655--8664},
  pdf           = {http://proceedings.mlr.press/v119/shao20b/shao20b.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {ControlVAE: Controllable Variational Autoencoder},
  url           = {https://proceedings.mlr.press/v119/shao20b.html},
  volume        = {119},
  year          = {2020}
}

@inproceedings{liu2020towards,
  abstract      = {Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g. variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset.},
  address       = {Seattle, WA, USA},
  author        = {Wenqian Liu and Runze Li and Meng Zheng and Srikrishna Karanam and Ziyan Wu and Bir Bhanu and Richard J. Radke and Octavia Camps},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month         = {6},
  openalex      = {W3034648032},
  pages         = {8642--8651},
  pdf           = {https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.pdf},
  publisher     = {IEEE},
  title         = {Towards Visually Explaining Variational Autoencoders},
  url           = {https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Towards_Visually_Explaining_Variational_Autoencoders_CVPR_2020_paper.html},
  year          = {2020}
}

@inproceedings{ghosh2020from,
  abstract      = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. Our deterministic regularized autoencoders can learn a smooth, meaningful latent space as VAEs without having to force some arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show in a rigorous empirical study that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules.},
  author        = {Partha Ghosh and Mehdi S. M. Sajjadi and Antonio Vergari and Michael J. Black and Bernhard Schölkopf},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This paper investigates the connection between variational and deterministic autoencoders. It proposes a method to gradually anneal the VAE's stochasticity, transitioning it into a deterministic model that retains a well-structured latent space without the "holes" typical of naively trained deterministic autoencoders.},
  openalex      = {W2994610548},
  pdf           = {https://openreview.net/pdf?id=S1g7tpEYDS},
  title         = {From Variational to Deterministic Autoencoders},
  url           = {https://openreview.net/forum?id=S1g7tpEYDS},
  year          = {2020}
}

@inproceedings{skopek2020mixed,
  abstract      = {Euclidean space has historically been the typical workhorse geometry for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. We introduce a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature is fixed or learnable. Our Mixed-curvature VAE generalizes the Euclidean VAE to curved latent spaces and recovers it when curvatures of all latent space components go to 0. We train our model end-to-end using the reparameterization trick and show that it improves upon VAE and its hyperbolic and spherical counterparts on density modeling tasks.},
  author        = {Ondrej Skopek and Octavian-Eugen Ganea and Gary Bécigneul},
  booktitle     = {International Conference on Learning Representations},
  doi           = {10.3929/ethz-b-000372387},
  month         = {4},
  openalex      = {W2995315221},
  pdf           = {https://openreview.net/pdf?id=S1g6xeSKDS},
  title         = {Mixed-curvature Variational Autoencoders},
  url           = {https://openreview.net/forum?id=S1g6xeSKDS},
  year          = {2020}
}

@inproceedings{xu2021anytime,
  abstract      = {Autoregressive models are widely used for tasks such as image and audio generation. The sampling process of these models, however, does not allow interruptions and cannot adapt to real-time computational resources. This challenge impedes the deployment of powerful autoregressive models, which involve a slow sampling process that is sequential in nature and typically scales linearly with respect to the data dimension. To address this difficulty, we propose a new family of autoregressive models that enables anytime sampling. Inspired by Principal Component Analysis, we learn a structured representation space where dimensions are ordered based on their importance with respect to reconstruction. Using an autoregressive model in this latent space, we trade off sample quality for computational efficiency by truncating the generation process before decoding into the original data space. Experimentally, we demonstrate in several image and audio generation tasks that sample quality degrades gracefully as we reduce the computational budget for sampling. The approach suffers almost no loss in sample quality (measured by FID) using only 60% to 80% of all latent dimensions for image data.},
  author        = {Yilun Xu and Yang Song and Sahaj Garg and Linyuan Gong and Rui Shu and Aditya Grover and Stefano Ermon},
  booktitle     = {International Conference on Learning Representations},
  note          = {TLDR: This paper proposes an "ordered autoencoder" that learns a latent space where dimensions are ordered by importance, similar to PCA. This ordered structure enables "anytime sampling" for an autoregressive model trained on this space, allowing a trade-off between sample quality and computational cost at inference time.},
  openalex      = {W3131764679},
  pdf           = {https://openreview.net/pdf?id=TSRTzJnuEBS},
  title         = {Anytime Sampling for Autoregressive Models via Ordered Autoencoding},
  url           = {https://openreview.net/forum?id=TSRTzJnuEBS},
  year          = {2021}
}

@inproceedings{ribeiro2021model,
  abstract      = {We develop a novel method for carrying out model selection for Bayesian autoencoders (BAEs) by means of prior hyper-parameter optimization. Inspired by the common practice of type-II maximum likelihood optimization and its equivalence to Kullback-Leibler divergence minimization, we propose to optimize the distributional sliced-Wasserstein distance (DSWD) between the output of the autoencoder and the empirical data distribution. The advantages of this formulation are that we can estimate the DSWD based on samples and handle high-dimensional problems. We carry out posterior estimation of the BAE parameters via stochastic gradient Hamiltonian Monte Carlo and turn our BAE into a generative model by fitting a flexible Dirichlet mixture model in the latent space. Consequently, we obtain a powerful alternative to variational autoencoders, which are the preferred choice in modern applications of autoencoders for representation learning with uncertainty. We evaluate our approach qualitatively and quantitatively using a vast experimental campaign on a number of unsupervised learning tasks and show that, in small-data regimes where priors matter, our approach provides state-of-the-art results, outperforming multiple competitive baselines.},
  author        = {Ba-Hien Tran and Simone Rossi and Dimitrios Milios and Pietro Michiardi and Edwin V. Bonilla and Maurizio Filippone},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4287120965},
  pages         = {13656--13668},
  pdf           = {https://neurips.cc/paper_files/paper/2021/file/a41db61e2728ef963614a8c8755b9b9a-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Model Selection for Bayesian Autoencoders},
  url           = {https://proceedings.neurips.cc/paper/2021/hash/a41db61e2728ef963614a8c8755b9b9a-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{phillips2021basisdevae,
  abstract      = {The Variational Autoencoder (VAE) performs effective nonlinear dimensionality reduction in a variety of problem settings. However, the black-box neural network decoder function typically employed limits the ability of the decoder function to be constrained and interpreted, making the use of VAEs problematic in settings where prior knowledge should be embedded within the decoder. We present DeVAE, a novel VAE-based model with a derivative-based forward mapping, allowing for greater control over decoder behaviour via specification of the decoder function in derivative space. Additionally, we show how DeVAE can be paired with a sparse clustering prior to create BasisDeVAE and perform interpretable simultaneous dimensionality reduction and feature-level clustering. We demonstrate the performance and scalability of the DeVAE and BasisDeVAE models on synthetic and real-world data and present how the derivative-based approach allows for expressive yet interpretable forward models which respect prior knowledge.},
  author        = {Dominic Danks and Christopher Yau},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  openalex      = {W3170048766},
  pages         = {2410--2420},
  pdf           = {https://proceedings.mlr.press/v139/danks21a/danks21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders},
  volume        = {139},
  year          = {2021}
}

@inproceedings{wang2021paul,
  abstract      = {Recent success in casting Non-rigid Structure from Motion (NRSfM) as an unsupervised deep learning problem has raised fundamental questions about what novelty in NRSfM prior could the deep learning offer. In this paper we advocate for a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior. The framework is unique as: (i) it learns the 3D auto-encoder weights solely from 2D projected measurements, and (ii) it is Procrustean in that it jointly resolves the unknown rigid pose for each shape instance. We refer to this architecture as a Procustean Autoencoder for Unsupervised Lifting (PAUL), and demonstrate state-of-the-art performance across a number of benchmarks in comparison to recent innovations such as Deep NRSfM and C3PDO.},
  address       = {Nashville, TN, USA},
  arxiv         = {2103.16773},
  author        = {Chaoyang Wang and Simon Lucey},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi           = {10.1109/CVPR46437.2021.00050},
  month         = {6},
  openalex      = {W3173299751},
  pages         = {434--443},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PAUL_Procrustean_Autoencoder_for_Unsupervised_Lifting_CVPR_2021_paper.pdf},
  publisher     = {IEEE},
  title         = {PAUL: Procrustean Autoencoder for Unsupervised Lifting},
  url           = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PAUL_Procrustean_Autoencoder_for_Unsupervised_Lifting_CVPR_2021_paper.html},
  year          = {2021}
}

@inproceedings{chen2024frequency,
  abstract      = {Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder (bioFAME) that learns to parameterize the representation of biosignals in the frequency space. bioFAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. We demonstrate the robustness of the proposed architecture on multiple biosignal datasets, where we show the proposed architecture does not only perform better than single-modality models, but also outperform in transfer learning tasks.},
  author        = {Ran Liu and Ellen L. Zippi and Hadi Pouransari and Christopher Michael Sandino and Jingping Nie and Hanlin Goh and Erdrin Azemi and Ali Moin},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  openalex      = {W4386721612},
  pdf           = {https://openreview.net/pdf?id=t5LXyWbs5p},
  title         = {Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals},
  url           = {https://openreview.net/forum?id=t5LXyWbs5p},
  year          = {2024}
}

@article{bick2024unsupervised,
  abstract      = {Although high-dimensional clinical data (HDCD) are increasingly available in biobank-scale datasets, their use for genetic discovery remains challenging. Here we introduce an unsupervised deep learning model, Representation Learning for Genetic Discovery on Low-Dimensional Embeddings (REGLE), for discovering associations between genetic variants and HDCD. REGLE leverages variational autoencoders to compute nonlinear disentangled embeddings of HDCD, which become the inputs to genome-wide association studies (GWAS). REGLE can uncover features not captured by existing expert-defined features and enables the creation of accurate disease-specific polygenic risk scores (PRSs) in datasets with very few labeled data. We apply REGLE to perform GWAS on respiratory and circulatory HDCD---spirograms measuring lung function and photoplethysmograms measuring blood volume changes. REGLE replicates known loci while identifying others not previously detected. REGLE are predictive of overall survival, and PRSs constructed from REGLE loci improve disease prediction across multiple biobanks. Overall, REGLE contain clinically relevant information beyond that captured by existing expert-defined features, leading to improved genetic discovery and disease prediction.},
  author        = {Taedong Yun and Justin Cosentino and Babak Behsaz and Zachary R. McCaw and Davin Hill and Robert Luben and Dongbing Lai and John Bates and Howard Yang and Tae-Hwi Schwantes-An and Yuchen Zhou and Anthony P. Khawaja and Andrew Carroll and Brian D. Hobbs and Michael H. Cho and Cory Y. McLean and Farhad Hormozdiari},
  doi           = {10.1038/s41588-024-01831-6},
  journal       = {Nature Genetics},
  month         = {8},
  number        = {8},
  openalex      = {W4400423257},
  pages         = {1604--1613},
  pdf           = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11319202/pdf/},
  title         = {Unsupervised representation learning on high-dimensional clinical data improves genomic discovery and prediction},
  volume        = {56},
  year          = {2024}
}

@inproceedings{huang2022threedlinker,
  abstract      = {Deep learning has achieved tremendous success in designing novel chemical compounds with desirable pharmaceutical properties. In this work, we focus on a new type of drug design problem -- generating a small "linker" to physically attach two independent molecules with their distinct functions. The main computational challenges include: 1) the generation of linkers is conditional on the two given molecules, in contrast to generating full molecules from scratch in previous works; 2) linkers heavily depend on the anchor atoms of the two molecules to be connected, which are not known beforehand; 3) 3D structures and orientations of the molecules need to be considered to avoid atom clashes, for which equivariance to E(3) group are necessary. To address these problems, we propose a conditional generative model, named 3DLinker, which is able to predict anchor atoms and jointly generate linker graphs and their 3D structures based on an E(3) equivariant graph variational autoencoder. So far as we know, there are no previous models that could achieve this task. We compare our model with multiple conditional generative models modified from other molecular design tasks and find that our model has a significantly higher rate in recovering molecular graphs, and more importantly, accurately predicting the 3D coordinates of all the atoms.},
  author        = {Yinan Huang and Xingang Peng and Jianzhu Ma and Muhan Zhang},
  booktitle     = {39th International Conference on Machine Learning},
  editor        = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  note          = {TLDR: This paper tackles the problem of designing molecular linkers to connect two functional molecules. It proposes 3DLinker, an E(3) equivariant VAE that can predict anchor atoms and jointly generate the 3D structure and graph of a valid linker, a novel capability in drug design.},
  openalex      = {W4280559736},
  pages         = {9280--9294},
  pdf           = {https://proceedings.mlr.press/v162/huang22g/huang22g.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design},
  volume        = {162},
  year          = {2022}
}

@inproceedings{lerch2022convolutional,
  abstract      = {Ensemble weather predictions typically show systematic errors that have to be corrected via post-processing. Even state-of-the-art post-processing methods based on neural networks often solely rely on location-specific predictors that require an interpolation of the physical weather model's spatial forecast fields to the target locations. However, potentially useful predictability information contained in large-scale spatial structures within the input fields is potentially lost in this interpolation step. We propose the use of convolutional autoencoders to learn compact representations of spatial input fields which can then be used to augment location-specific information as additional inputs to post-processing models. The benefits of including this spatial information is demonstrated in a case study of 2-m temperature forecasts at surface stations in Germany.},
  author        = {Sebastian Lerch and Kai Lars Polsterer},
  booktitle     = {ICLR Workshop on AI for Earth and Space Science},
  doi           = {10.48550/arxiv.2204.05102},
  month         = {4},
  note          = {TLDR: This work applies convolutional autoencoders to improve weather forecasting. The autoencoder learns a compact representation of spatial forecast fields, which is then used as an additional input to a post-processing model, allowing it to leverage large-scale spatial structures and improve temperature predictions.},
  openalex      = {W4223533121},
  pdf           = {https://arxiv.org/pdf/2204.05102.pdf},
  title         = {Convolutional Autoencoders for Spatially-Informed Ensemble Post-Processing},
  url           = {https://arxiv.org/abs/2204.05102},
  year          = {2022}
}

@inproceedings{schneider2022autoencoders,
  abstract      = {This paper provides a comparative analysis of convolutional autoencoders (CAE), variational autoencoders (VAEs), and Adversarial Autoencoders (AAEs) for anomaly detection. The study applies these three autoencoder types to MNIST and CIFAR10 datasets to compare their anomaly detection performances both qualitatively and quantitatively. The research found that all three models perform well on the simple-structured MNIST dataset, but none achieved convincing anomaly detection results on the more complex CIFAR10 dataset. Notably, the simplest model (CAE) demonstrated nearly as accurate results as the more complex models, and in some cases even better performance, highlighting important practical trade-offs in model complexity versus performance.},
  address       = {New Orleans, LA, USA},
  author        = {Sarah Schneider and Doris Antensteiner and Daniel Soukup and Matthias Scheutz},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  month         = {6},
  note          = {TLDR: This paper provides a comparative analysis of convolutional autoencoders (CAE), VAEs, and Adversarial Autoencoders (AAEs) for anomaly detection. It finds that for simple datasets like MNIST, all models perform well, but for more complex data like CIFAR-10, the simpler CAE can be as good or better, highlighting important practical trade-offs.},
  openalex      = {W4292794125},
  pages         = {1986--1992},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Schneider_Autoencoders_-_A_Comparative_Analysis_in_the_Realm_of_Anomaly_CVPRW_2022_paper.pdf},
  publisher     = {IEEE},
  title         = {Autoencoders - A Comparative Analysis in the Realm of Anomaly Detection},
  url           = {https://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Schneider_Autoencoders_-_A_Comparative_Analysis_in_the_Realm_of_Anomaly_CVPRW_2022_paper.html},
  year          = {2022}
}

@inproceedings{liu2021variational,
  abstract      = {Existing state-of-the-art methods mainly focus on single image super-resolution which cannot perform well on large upsampling factors, e.g., 8×. This paper proposes a novel reference based image super-resolution approach via Variational AutoEncoder (RefVAE). The proposed method allows any arbitrary image to act as a reference for super-resolution. Even using random map or low-resolution image itself, the proposed RefVAE can transfer the knowledge from the reference to the super-resolved images. Depending upon different references, the proposed method can generate different versions of super-resolved images from a hidden super-resolution space.},
  author        = {Zhi-Song Liu and Wan-Chi Siu and Li-Wen Wang},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  note          = {TLDR: This paper proposes using a VAE for reference-based image super-resolution. The model, RefVAE, can leverage an arbitrary reference image to transfer textural details to the super-resolved output, generating diverse and high-quality results from a learned super-resolution space.},
  openalex      = {W3176793854},
  pages         = {516--525},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/papers/Liu_Variational_AutoEncoder_for_Reference_Based_Image_Super-Resolution_CVPRW_2021_paper.pdf},
  publisher     = {IEEE},
  title         = {Variational AutoEncoder for Reference Based Image Super-Resolution},
  url           = {https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Liu_Variational_AutoEncoder_for_Reference_Based_Image_Super-Resolution_CVPRW_2021_paper.html},
  year          = {2021}
}

@inproceedings{laturnus2021morphvae,
  abstract      = {For the past century, the anatomy of a neuron has been considered one of its defining features: The shape of a neuron's dendrites and axon fundamentally determines what other neurons it can connect to. These neurites have been described using mathematical tools e.g. in the context of cell type classification, but generative models of these structures have only rarely been proposed and are often computationally inefficient. Here we propose MorphVAE, a sequence-to-sequence variational autoencoder with spherical latent space as a generative model for neural morphologies. The model operates on walks within the tree structure of a neuron and can incorporate expert annotations on a subset of the data using semi-supervised learning. We develop our model on artificially generated toy data and evaluate its performance on dendrites of excitatory cells and axons of inhibitory cells of mouse motor cortex (M1) and dendrites of retinal ganglion cells. We show that the learned latent feature space allows for better cell type discrimination than other commonly used features. By sampling new walks from the latent space we can easily construct new morphologies with a specified degree of similarity to their reference neuron, providing an efficient generative model for neural morphologies.},
  author        = {Sophie C. Laturnus and Philipp Berens},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  openalex      = {W3168203489},
  pages         = {6021--6031},
  pdf           = {http://proceedings.mlr.press/v139/laturnus21a/laturnus21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {MorphVAE: Generating Neural Morphologies from 3D-Walks using a Variational Autoencoder with Spherical Latent Space},
  volume        = {139},
  year          = {2021}
}

@inproceedings{lee2024everest,
  abstract      = {Masked Video Autoencoder (MVA) approaches have demonstrated their potential by significantly outperforming previous video representation learning methods. However, they waste an excessive amount of computations and memory in predicting uninformative tokens/frames due to random masking strategies. To resolve this issue, we present EVEREST, an efficient approach to video representation learning that finds tokens containing rich motion features and discards uninformative ones during both pre-training and fine-tuning. Specifically, our method exploits the unequal information density among the patches in videos. We present a redundancy-robust mask generator that selects tokens with a large disparity with the paired ones in the previous time dimension, indicating that they include rich motion features. We also present an information-intensive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Through extensive experiments, we demonstrate that our approach achieves comparable performance to existing methods while significantly reducing the computations, memory, and training time. EVEREST has great potential to lower the barrier for video-related research that requires enormous computing power and cost.},
  author        = {Sunil Hwang and Jaehong Yoon and Youngwan Lee and Sung Ju Hwang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  pages         = {20889--20907},
  pdf           = {https://proceedings.mlr.press/v235/hwang24d/hwang24d.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens},
  url           = {https://proceedings.mlr.press/v235/hwang24d.html},
  volume        = {235},
  year          = {2024}
}

@article{mienye2024deep,
  abstract      = {Autoencoders have become a fundamental technique in deep learning, significantly enhancing representation learning across various domains, including image processing, anomaly detection, and generative modelling. This paper provides a comprehensive review of autoencoder architectures, from their inception and fundamental concepts to advanced implementations such as adversarial autoencoders, convolutional autoencoders, and variational autoencoders, examining their operational mechanisms, mathematical foundations, typical applications, and their role in generative modelling. The study contributes to the field by synthesizing existing knowledge, discussing recent advancements, new perspectives, and the practical implications of autoencoders in tackling modern machine learning challenges.},
  author        = {Ibomoiye Domor Mienye and Theo G. Swart},
  doi           = {10.1007/s11831-025-10260-5},
  journal       = {Archives of Computational Methods in Engineering},
  note          = {TLDR: This recent review provides a comprehensive overview of autoencoder architectures, from foundational concepts to advanced models like VAEs and AAEs. It examines their mathematical foundations, operational mechanisms, and evolving role in cross-disciplinary settings, offering a modern perspective on the field.},
  openalex      = {W4408473962},
  title         = {Deep Autoencoder Neural Networks: A Comprehensive Review and New Perspectives},
  year          = {2025}
}

@article{li2023comprehensive,
  abstract      = {Autoencoder is an unsupervised learning model, which can automatically learn data features from a large number of samples and can act as a dimensionality reduction method. With the development of deep learning technology, autoencoder has attracted the attention of many scholars. Researchers have proposed several improved versions of autoencoder based on different application fields. This paper explains the principle of a conventional autoencoder and investigates the primary development process of an autoencoder. A taxonomy of autoencoders according to their structures and principles is proposed, with related autoencoder models comprehensively analyzed and discussed. The application progress of autoencoders in different fields, such as image classification and natural language processing, is introduced. The shortcomings of the current autoencoder algorithm are summarized and prospects for future development directions are addressed.},
  author        = {Pengzhi Li and Yan Pei and Jianqiang Li},
  doi           = {10.1016/j.asoc.2023.110176},
  journal       = {Applied Soft Computing},
  month         = {5},
  note          = {TLDR: This survey provides a roadmap of autoencoder methods, categorizing them based on structure and principle. It reviews applications in machine vision, NLP, recommender systems, and anomaly detection, serving as a guide for researchers and practitioners.},
  openalex      = {W4323545743},
  pages         = {110176},
  publisher     = {Elsevier},
  title         = {A comprehensive survey on design and application of autoencoder in deep learning},
  url           = {https://www.sciencedirect.com/science/article/abs/pii/S1568494623001941},
  volume        = {138},
  year          = {2023}
}

@misc{khan2024review,
  abstract      = {Traditional mathematical models used in designing next-generation communication systems often fall short due to inherent simplifications, narrow scope, and computational limitations. In recent years, the incorporation of deep learning (DL) methodologies into communication systems has made significant progress in system design and performance optimisation. Autoencoders (AEs) have become essential, enabling end-to-end learning that allows for the combined optimisation of transmitters and receivers. Consequently, AEs offer a data-driven methodology capable of bridging the gap between theoretical models and real-world complexities. This review comprehensively investigates the application of AE in communication systems, drawing on 120 recent studies published in reputed publishers, including ScienceDirect, Scopus, Springer and IEEE Xplore. The analysis encompasses various communication domains, including wireless, optical, semantic, and quantum communication, to identify suitable AE approaches for each. The works are categorized based on research issues such as transceiver design, channel models, digital signal processing (DSP) techniques, computational complexity, and non-differentiable channels.},
  archiveprefix = {arXiv},
  author        = {Omar Alnaseri and Laith Alzubaidi and Yassine Himeur and Mohammed Alaa Ala'anzy and Jens Timmermann and Mohammed S. M. Gismalla},
  eprint        = {2412.13843},
  howpublished  = {arXiv preprint},
  month         = {12},
  note          = {Comprehensive survey reviewing 120 recent studies on the application of autoencoders in communication systems across wireless, optical, semantic, and quantum domains},
  pdf           = {https://arxiv.org/pdf/2412.13843.pdf},
  title         = {A Review on Deep Learning Autoencoder in the Design of Next-Generation Communication Systems},
  url           = {https://arxiv.org/abs/2412.13843},
  year          = {2024}
}
