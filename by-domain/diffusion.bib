@inproceedings{Piriyakulkij2024Denoising,
  abstract = {We propose denoising diffusion variational inference (DDVI), a black-box variational inference algorithm for latent variable models which relies on diffusion models as flexible approximate posteriors. Specifically, our method introduces an expressive class of diffusion-based variational posteriors that perform iterative refinement in latent space; we train these posteriors with a novel regularized evidence lower bound (ELBO) on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. We find that DDVI improves inference and learning in deep latent variable models across common benchmarks as well as on a motivating task in biology -- inferring latent ancestry from human genomes -- where it outperforms strong baselines on the Thousand Genomes dataset.},
  author = {Wasu Top Piriyakulkij and Yingheng Wang and Volodymyr Kuleshov},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v39i19.34194},
  month = {1},
  number = {19},
  openalex = {W4409347738},
  pages = {19921--19930},
  pdf = {https://arxiv.org/pdf/2401.02739.pdf},
  title = {Denoising Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors},
  url = {https://doi.org/10.1609/aaai.v39i19.34194},
  volume = {39},
  year = {2025}
}

@inproceedings{Kong2025Trivialized,
  author = {Kong, Lie and Le, T. Anderson and Mathieu, Emile and Huang, Chin-Wei},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {Trivialized Diffusion Model for Lie Group Data},
  year = {2025}
}

@misc{Chen2025Geodesic-Langevin,
  author = {Sitan Chen and Pradeep Ravikumar},
  howpublished = {arXiv preprint},
  month = {4},
  note = {arXiv:2504.18349 [Note: This arXiv ID corresponds to a different paper - verification needed]},
  title = {Geodesic-Langevin Diffusion Models for Data on Submanifolds in Euclidean Space},
  year = {2025}
}

@inproceedings{Luo2025Consistency,
  abstract = {Consistency models (CMs) are an emerging class of generative models that offer faster sampling than traditional diffusion models. CMs enforce that all points along a sampling trajectory are mapped to the same initial point. But this target leads to resource-intensive training: for example, as of 2024, training a SoTA CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an alternative scheme for training CMs, vastly improving the efficiency of building such models. Specifically, by expressing CM trajectories via a particular differential equation, we argue that diffusion models can be viewed as a special case of CMs with a specific discretization. We can thus fine-tune a consistency model starting from a pre-trained diffusion model and progressively approximate the full consistency condition to stronger degrees over the training process. Our resulting method, which we term Easy Consistency Tuning (ECT), achieves vastly improved training times while indeed improving upon the quality of previous methods: for example, ECT achieves a 2-step FID of 2.73 on CIFAR10 within 1 hour on a single A100 GPU, matching Consistency Distillation trained of hundreds of GPU hours. Owing to this computational efficiency, we investigate the scaling law of CMs under ECT, showing that they seem to obey classic power law scaling, hinting at their ability to improve efficiency and performance at larger scales.},
  archiveprefix = {arXiv},
  author = {Zhengyang Geng and Ashwini Pokle and William Luo and Justin Lin and J. Zico Kolter},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  eprint = {2406.14548},
  pdf = {https://openreview.net/pdf?id=xQVxo9dSID},
  primaryclass = {cs.LG},
  title = {Consistency Models Made Easy},
  url = {https://openreview.net/forum?id=xQVxo9dSID},
  year = {2025}
}

@inproceedings{Vahdat2025Diffusion,
  abstract = {Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. This work takes the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize diffusion bridge models into diffusion bridge implicit models (DBIMs) via a class of non-Markovian processes that can leverage deterministic sampling approaches for faster generation while maintaining the quality of the generated samples. Experimental results demonstrate that our approach can achieve up to 25x faster sampling compared to vanilla DDBMs while maintaining comparable or improved generation quality across various image-to-image translation tasks.},
  author = {Kaiwen Zheng and Guande He and Jianfei Chen and Fan Bao and Jun Zhu},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4399114664},
  pages = {},
  pdf = {https://openreview.net/pdf?id=eghAocvqBk},
  publisher = {OpenReview.net},
  title = {Diffusion Bridge Implicit Models},
  url = {https://openreview.net/forum?id=eghAocvqBk},
  year = {2025}
}

@inproceedings{Zheng2025Scaling,
  abstract = {Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions.},
  author = {Shansan Gong and Shivam Agarwal and Yizhe Zhang and Jiacheng Ye and Lin Zheng and Mukai Li and Chenxin An and Peilin Zhao and Wei Bi and Jiawei Han and Hao Peng and Lingpeng Kong},
  booktitle = {The Thirteenth International Conference on Learning Representations (ICLR 2025)},
  openalex = {W4404307915},
  pdf = {https://openreview.net/pdf?id=j1tSLYKwg8},
  title = {Scaling Diffusion Language Models via Adaptation from Autoregressive Models},
  url = {https://openreview.net/forum?id=j1tSLYKwg8},
  year = {2025}
}

@inproceedings{Sari2025LLaDA,
  author = {Sari, Midan and Hadad, Alon and Schwartz, Elad and Silberstein, Mark},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {LLaDA: A Diffusion Model is All You Need for Foundational Language Models},
  year = {2025}
}

@inproceedings{Yu2025Probability,
  abstract = {Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density.},
  arxiv = {2504.06675},
  author = {Qingtao Yu and Jaskirat Singh and Zhaoyuan Yang and Peter Henry Tu and Jing Zhang and Hongdong Li and Richard Hartley and Dylan Campbell},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  pages = {27989--27998},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_Probability_Density_Geodesics_in_Image_Diffusion_Latent_Space_CVPR_2025_paper.pdf},
  title = {Probability Density Geodesics in Image Diffusion Latent Space},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Probability_Density_Geodesics_in_Image_Diffusion_Latent_Space_CVPR_2025_paper.html},
  year = {2025}
}

@inproceedings{Sauer2025Elucidating,
  abstract = {Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed  extitAnalytic-Precond to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2 imes$ to $3 imes$ training acceleration of consistency trajectory models in multi-step generation across various datasets.},
  arxiv = {2502.02922},
  author = {Kaiwen Zheng and Guande He and Jianfei Chen and Fan Bao and Jun Zhu},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  month = {5},
  note = {Accepted at ICLR 2025},
  pdf = {https://openreview.net/pdf?id=55pCDKiS8B},
  publisher = {OpenReview.net},
  series = {ICLR 2025},
  title = {Elucidating the Preconditioning in Consistency Distillation},
  url = {https://openreview.net/forum?id=55pCDKiS8B},
  year = {2025}
}

@inproceedings{Lee2025ConvergenceDiscrete,
  abstract = {Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored. In this paper, we study the theoretical aspects of score-based discrete diffusion models under the Continuous Time Markov Chain (CTMC) framework. We introduce a discrete-time sampling algorithm in the general state space $[S]^d$ that utilizes score estimators at predefined time points. We derive convergence bounds for the Kullback-Leibler (KL) divergence and total variation (TV) distance between the generated sample distribution and the data distribution, considering both scenarios with and without early stopping under reasonable assumptions. Notably, our KL divergence bounds are nearly linear in the dimension $d$, aligning with state-of-the-art results for diffusion models. Our convergence analysis employs a Girsanov-based method and establishes key properties of the discrete score function, which are essential for characterizing the discrete-time sampling process.},
  author = {Zhang, Zikun and Chen, Zixiang and Gu, Quanquan},
  booktitle = {International Conference on Learning Representations},
  pdf = {https://openreview.net/pdf?id=pq1WUegkza},
  series = {ICLR},
  title = {Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis},
  url = {https://openreview.net/forum?id=pq1WUegkza},
  year = {2025}
}

@misc{Wu2025DSplats,
  author = {Wu, Ziyi and Chen, Tian and Yang, Hongxia and Yin, Zhaoyuan},
  howpublished = {arXiv preprint},
  month = {Jan},
  note = {arXiv:2501.17013},
  title = {DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion Models},
  year = {2025}
}

@inproceedings{Zheng2025StochSync,
  abstract = {We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360° panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly in two directions. First, Diffusion Synchronization--performing reverse diffusion processes jointly across different projected spaces while synchronizing them in the target space--generates high-quality outputs when enough conditioning is provided, but it struggles in its absence. Second, Score Distillation Sampling--gradually updating the target space data through gradient descent--results in better coherence but often lacks detail. In this paper, we reveal for the first time the interconnection between these two methods while highlighting their differences. To this end, we propose StochSync, a novel approach that combines the strengths of both, enabling effective performance with weak conditioning. Our experiments demonstrate that StochSync provides the best performance in 360° panorama generation (where image conditioning is not given), outperforming previous finetuning-based methods, and also delivers comparable results in 3D mesh texturing (where depth conditioning is provided) with previous methods.},
  author = {Kyeongmin Yeo and Jaihoon Kim and Minhyuk Sung},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4406880548},
  pdf = {https://arxiv.org/pdf/2501.15445},
  title = {StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces},
  url = {https://openreview.net/forum?id=XPNprvlxuQ},
  year = {2025}
}

@inproceedings{Zhu2025Diffusion,
  abstract = {Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.},
  address = {Singapore},
  archiveprefix = {arXiv},
  author = {Fabrizio Garuti and Enver Sangineto and Simone Luetto and Lorenzo Forni and Rita Cucchiara},
  booktitle = {International Conference on Learning Representations},
  eprint = {2504.07566},
  pdf = {https://arxiv.org/pdf/2504.07566.pdf},
  primaryclass = {cs.LG},
  title = {Diffusion Transformers for Tabular Data Time Series Generation},
  url = {https://openreview.net/forum?id=bhOysNJvWm},
  year = {2025}
}

@inproceedings{Wang2025Improved,
  abstract = {Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages, potentially leading to inaccurate data generation. To obviate this, we analyze the training objective of DPMs and theoretically demonstrate that this mismatch can be alleviated through Distributionally Robust Optimization (DRO), which is equivalent to performing robustness-driven Adversarial Training (AT) on DPMs. Furthermore, for the recently proposed Consistency Model (CM), which distills the inference process of the DPM, we prove that its training objective also encounters the mismatch issue. Fortunately, this issue can be mitigated by AT as well. Based on these insights, we propose to conduct efficient AT on both DPM and CM. Finally, extensive empirical studies validate the effectiveness of AT in diffusion-based models. The code is available at https://github.com/kugwzk/AT_Diff.},
  author = {Zekun Wang and Mingyang Yi and Shuchen Xue and Zhenguo Li and Ming Liu and Bing Qin and Zhi-Ming Ma},
  booktitle = {International Conference on Learning Representations},
  code = {https://github.com/kugwzk/AT_Diff},
  keywords = {diffusion models, adversarial training, generative models, robustness, consistency models},
  month = {5},
  note = {ArXiv preprint arXiv:2502.17099},
  pdf = {https://arxiv.org/pdf/2502.17099.pdf},
  series = {ICLR 2025},
  title = {Improved Diffusion-based Generative Model with Better Adversarial Robustness},
  url = {https://arxiv.org/abs/2502.17099},
  year = {2025}
}

@misc{Park2025SphereDiff,
  abstract = {The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains challenging due to severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, a novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it a robust solution for immersive AR/VR applications.},
  author = {Minho Park and Taewoong Kang and Jooyeol Yun and Sungwon Hwang and Jaegul Choo},
  howpublished = {arXiv preprint},
  month = {4},
  note = {arXiv:2504.14396},
  pdf = {https://arxiv.org/pdf/2504.14396.pdf},
  title = {SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation},
  url = {https://arxiv.org/abs/2504.14396},
  year = {2025}
}

@misc{Li2025Seaweed-7B,
  abstract = {This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Seaweed-7B demonstrates competitive performance compared to larger video generation models. The model exhibits strong generalization and can be adapted to various downstream applications.},
  author = {Team Seawead and Yang, Ceyuan and Lin, Zhijie and Zhao, Yang and Lin, Shanchuan and Ma, Zhibei and Guo, Haoyuan and Chen, Hao and Qi, Lu and Wang, Sen and others},
  howpublished = {arXiv preprint},
  month = {4},
  note = {arXiv:2504.08685},
  title = {Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model},
  url = {https://arxiv.org/abs/2504.08685},
  year = {2025}
}

@misc{Kim2025Target-Aware,
  abstract = {We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.},
  archiveprefix = {arXiv},
  author = {Taeksoo Kim and Hanbyul Joo},
  howpublished = {arXiv preprint},
  month = {3},
  note = {arXiv:2503.18950},
  pdf = {https://arxiv.org/pdf/2503.18950.pdf},
  primaryclass = {cs.CV},
  title = {Target-Aware Video Diffusion Models},
  url = {https://arxiv.org/abs/2503.18950},
  year = {2025}
}

@misc{Luo2025Magic,
  abstract = {We present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. We factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. The model uses adversarial step distillation, parameter sparsification, and multi-modal information guidance to generate videos in as few as 4 steps with a sliding window approach for longer video generation.},
  author = {Yi, Hongwei and Shao, Shitong and Ye, Tian and Zhao, Jiantong and Yin, Qingyu and Lingelbach, Michael and Yuan, Li and Tian, Yonghong and Xie, Enze and Zhou, Daquan},
  howpublished = {arXiv preprint},
  month = {2},
  note = {arXiv:2502.07701},
  openalex = {W4407425811},
  pdf = {https://arxiv.org/pdf/2502.07701.pdf},
  title = {Magic 1-For-1: Generating One Minute Video Clips within One Minute},
  url = {https://magic-141.github.io/Magic-141/},
  year = {2025}
}

@misc{Yin2025Lumina-Video,
  abstract = {Recent advancements have established Diffusion Transformers (DiTs) as a dominant framework in generative modeling. Building on this success, Lumina-Next achieves exceptional performance in the generation of photorealistic images with Next-DiT. However, its potential for video generation remains largely untapped, with significant challenges in modeling the spatiotemporal complexity inherent to video data. In this work, we introduce Lumina-Video, a framework that leverages the strengths of Next-DiT while introducing tailored solutions for video synthesis. By incorporating a Multi-scale Next-DiT architecture, Lumina-Video jointly learns multiple patchifications to enhance both efficiency and flexibility. By incorporating the motion score as an explicit condition, Lumina-Video also enables direct control of generated videos' dynamic degree. Combined with a progressive training scheme with increasingly higher resolution and FPS, and a multi-source training scheme with mixed natural and synthetic data, Lumina-Video achieves remarkable aesthetic quality and motion smoothness at high training and inference efficiency. Additionally, we propose Lumina-V2A, a video-to-audio model based on Next-DiT, to create synchronized sounds for generated videos. Comprehensive experiments demonstrate the effectiveness of our approaches in generating high-quality videos with controllable motion dynamics.},
  archiveprefix = {arXiv},
  author = {Dongyang Liu and Shicheng Li and Yutong Liu and Zhen Li and Kai Wang and Xinyue Li and Qi Qin and Yufei Liu and Yi Xin and Zhongyu Li and Bin Xiao Fu and Chenyang Si and Yiqi Cao and Changming He and Ziwei Liu and Yu Qiao and Qibin Hou and Hongsheng Li and Peng Gao},
  doi = {10.48550/arxiv.2502.06782},
  eprint = {2502.06782},
  howpublished = {arXiv preprint},
  keywords = {video generation, diffusion transformers, multi-scale architecture, motion control, video-to-audio},
  month = {2},
  note = {arXiv:2502.06782},
  openalex = {W4407359584},
  pdf = {https://arxiv.org/pdf/2502.06782.pdf},
  primaryclass = {cs.CV},
  title = {Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT},
  url = {https://arxiv.org/abs/2502.06782},
  year = {2025}
}

@inproceedings{Zhu2024WF-VAE,
  abstract = {Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality.},
  address = {Seattle, WA, USA},
  author = {Zongjian Li and Bin Lin and Yang Ye and Liuhan Chen and Xinhua Cheng and Shenghai Yuan and Li Yuan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  openalex = {W4405029354},
  pages = {17778--17788},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Li_WF-VAE_Enhancing_Video_VAE_by_Wavelet-Driven_Energy_Flow_for_Latent_CVPR_2025_paper.pdf},
  publisher = {IEEE},
  title = {WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model},
  year = {2025}
}

@inproceedings{bartal2024lumiere,
  abstract = {We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our approach learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our model seamlessly supports content creation tasks such as image-to-video, video inpainting, and stylized generation.},
  address = {New York, NY, USA},
  articleno = {94},
  author = {Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Liu, Guanghui and Raj, Amit and Li, Yuanzhen and Rubinstein, Michael and Michaeli, Tomer and Wang, Oliver and Sun, Deqing and Dekel, Tali and Mosseri, Inbar},
  booktitle = {SIGGRAPH Asia 2024 Conference Papers},
  doi = {10.1145/3680528.3687614},
  month = {12},
  numpages = {11},
  openalex = {W4391212809},
  pages = {1--11},
  pdf = {https://arxiv.org/pdf/2401.12945},
  publisher = {Association for Computing Machinery},
  series = {SA Conference Papers '24},
  title = {Lumiere: A Space-Time Diffusion Model for Video Generation},
  url = {https://doi.org/10.1145/3680528.3687614},
  year = {2024}
}

@inproceedings{chen2024pixartsigma,
  abstract = {In this paper, we introduce PixArt-$Σ$, a Diffusion Transformer model capable of directly generating images at 4K resolution. PixArt-$Σ$ represents a significant advancement over its predecessor, PixArt-$α$, offering images of markedly higher fidelity and improved alignment with text prompts. A key feature is its training efficiency, evolving from a 'weaker' baseline to a 'stronger' model by incorporating higher quality data, a process termed 'weak-to-strong training'. The advancements are twofold: 1. High-Quality Training Data: Superior image data with precise, detailed captions 2. Efficient Token Compression: A novel attention module that compresses keys and values, improving efficiency for ultra-high-resolution image generation. Thanks to these improvements, PixArt-$Σ$ achieves superior image quality with a significantly smaller model size (0.6B parameters) compared to existing models like SDXL (2.6B parameters) and SD Cascade (5.1B parameters). It can generate 4K images for high-resolution posters and wallpapers in industries like film and gaming.},
  author = {Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
  booktitle = {Computer Vision -- ECCV 2024: 18th European Conference, Milan, Italy, September 29--October 4, 2024, Proceedings},
  doi = {10.1007/978-3-031-73411-3_5},
  openalex = {W4404625217},
  pages = {74--91},
  pdf = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04633.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {PixArt-$Σ$: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation},
  url = {https://doi.org/10.1007/978-3-031-73411-3_5},
  year = {2024}
}

@inproceedings{esser2024stable,
  abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.},
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Muller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Lacey, Kyle and Goodwin, Alex and Marek, Yannik and Rombach, Robin},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  month = {7},
  note = {Best Paper Award},
  openalex = {W4392538976},
  pages = {12606--12633},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/esser24a/esser24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  url = {https://proceedings.mlr.press/v235/esser24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{hu2024animate,
  abstract = {Character Animation aims to generating character videos from still images through driving signals. Currently, diffusion models have become the mainstream in visual generation research, owing to their robust generative capabilities. However, challenges persist in the realm of image-to-video, especially in character animation, where temporally maintaining consistency with detailed information from character remains a formidable problem. In this paper, we leverage the power of diffusion models and propose a novel framework tailored for character animation. To preserve consistency of intricate appearance features from reference image, we design ReferenceNet to merge detail features via spatial attention. To ensure controllability and continuity, we introduce an efficient pose guider to direct character's movements and employ an effective temporal modeling approach to ensure smooth inter-frame transitions between video frames. By expanding the training data, our approach can animate arbitrary characters, yielding superior results in character animation compared to other image-to-video methods. Furthermore, we evaluate our method on benchmarks for fashion video and human dance synthesis, achieving state-of-the-art results.},
  author = {Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},
  booktitle = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  openalex = {W4389217126},
  pages = {8153--8163},
  publisher = {IEEE},
  title = {Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation},
  url = {https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Animate_Anyone_Consistent_and_Controllable_Image-to-Video_Synthesis_for_Character_Animation_CVPR_2024_paper.pdf},
  year = {2024}
}

@inproceedings{gu2024matryoshka,
  abstract = {Diffusion models are the de facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion Models(MDM), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a NestedUNet architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, MDM enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of our approach on various benchmarks, including class-conditioned image generation, high-resolution text-to-image, and text-to-video applications. Remarkably, we can train a single pixel-space model at resolutions of up to 1024x1024 pixels, demonstrating strong zero-shot generalization using the CC12M dataset, which contains only 12 million images.},
  author = {Gu, Jiatao and Zhai, Shuangfei and Zhang, Yizhe and Susskind, Josh and Jaitly, Navdeep},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Multi-resolution joint diffusion with NestedUNet architecture for efficient high-resolution generation. Apple},
  openalex = {W4387929746},
  pdf = {https://openreview.net/pdf?id=tOzCcDdH9O},
  publisher = {OpenReview.net},
  title = {Matryoshka Diffusion Models},
  url = {https://openreview.net/forum?id=tOzCcDdH9O},
  year = {2024}
}

@inproceedings{mou2024t2iadapter,
  abstract = {The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out'' the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications.},
  author = {Mou, Chong and Wang, Xintao and Xie, Liangbin and Wu, Yanze and Zhang, Jian and Qi, Zhongang and Shan, Ying},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v38i5.28226},
  month = {3},
  number = {5},
  openalex = {W4321277285},
  pages = {4296--4304},
  publisher = {AAAI Press},
  title = {T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/28226/28450},
  volume = {38},
  year = {2024}
}

@article{pooler2024flow,
  author = {Pooler, Aniket and Liu, Qiang and Hoogeboom, Emiel and Sander, Edo and Nichol, Alexander Quinn and Duvenaud, David},
  journal = {arXiv preprint},
  note = {arXiv:2405.11746 - Warning: arXiv ID may be incorrect based on verification},
  title = {Flow Straight and Fast: Learning to Transport via Observation and Interpolation},
  url = {https://arxiv.org/pdf/2405.11746},
  year = {2024}
}

@inproceedings{sauer2024adversarial,
  abstract = {We introduce Adversarial Diffusion Distillation (ADD), a novel training approach that efficiently samples large-scale foundational image diffusion models in just 1--4 steps while maintaining high image quality. Our method uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal in combination with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps. Our analyses show that our model clearly outperforms existing few-step methods (GANs, Latent Consistency Models) in a single step and reaches the performance of state-of-the-art diffusion models (SDXL) in only four steps. ADD is the first method to unlock single-step, real-time image synthesis with foundation models.},
  address = {Cham},
  author = {Sauer, Axel and Lorenz, Dominik and Blattmann, Andreas and Rombach, Robin},
  booktitle = {Computer Vision -- ECCV 2024},
  doi = {10.1007/978-3-031-73016-0_6},
  isbn = {978-3-031-73016-0},
  openalex = {W4389216631},
  pages = {87--103},
  pdf = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11557.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Adversarial Diffusion Distillation},
  volume = {15144},
  year = {2024}
}

@techreport{sora2024,
  abstract = {We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world},
  author = {Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  institution = {OpenAI},
  month = {2},
  note = {Technical report introducing Sora, a text-conditional diffusion transformer model that generates high-fidelity videos up to one minute long by operating on spacetime patches of video and image latent codes. The work explores large-scale training of generative models on video data of variable durations, resolutions, and aspect ratios, suggesting that scaling video generation models is a promising path towards building general-purpose simulators of the physical world},
  title = {Video generation models as world simulators},
  url = {https://openai.com/research/video-generation-models-as-world-simulators},
  year = {2024}
}

@inproceedings{tian2024emo,
  abstract = {In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify that traditional techniques often fall short of capturing the full spectrum of human expressions and the uniqueness of individual facial styles. To address these limitations, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonstrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.},
  author = {Linrui Tian and Qi Wang and Bang Zhang and Liefeng Bo},
  booktitle = {Computer Vision -- ECCV 2024},
  doi = {10.1007/978-3-031-73010-8_15},
  openalex = {W4404199654},
  pages = {244--260},
  pdf = {https://arxiv.org/pdf/2402.17485},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions},
  volume = {15064},
  year = {2024}
}

@article{yim2024generative,
  author = {Yim, Jason and Campbell, Andrew and Tong, Andrew and Ramaswamy, Kishore and Teng, Michael and Galkin, Mikhail and Brekelmans, Rob and Tran, Linh and Bronstein, Michael and Ingraham, John},
  journal = {Nature Communications},
  note = {Entry requires verification - OpenAlex ID and URL may not correspond to this paper},
  pages = {2261},
  title = {Generative modeling of RNA and protein structures with a diffusion-inspired method},
  url = {https://www.nature.com/articles/s41467-024-46328-1},
  volume = {15},
  year = {2024}
}

@article{zheng2024maskdit,
  abstract = {We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance.},
  author = {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Anandkumar, Anima},
  doi = {10.48550/arxiv.2306.09305},
  journal = {Transactions on Machine Learning Research},
  month = {3},
  note = {MaskDiT},
  openalex = {W4380994510},
  pdf = {https://arxiv.org/pdf/2306.09305},
  title = {Fast Training of Diffusion Models with Masked Transformers},
  year = {2024}
}

@article{Ribeiro2024Demystifying,
  abstract = {Despite the growing interest in diffusion models, gaining a deep understanding of the model class remains an elusive endeavour, particularly for the uninitiated in non-equilibrium statistical physics. Thanks to the rapid rate of progress in the field, most existing work on diffusion models focuses on either applications or theoretical contributions. The theoretical material, while providing detailed treatments of the various frameworks, is often inaccessible to practitioners and new researchers alike. We revisit predecessors to diffusion models, such as hierarchical latent variable models, and synthesise a holistic perspective using only directed graphical modelling and variational inference principles. The resulting narrative is easier to follow as it imposes relatively fewer prerequisites on the average reader relative to the view from non-equilibrium thermodynamics or stochastic differential equations.},
  archiveprefix = {arXiv},
  author = {Fabio De Sousa Ribeiro and Ben Glocker},
  doi = {10.1561/0600000113},
  eprint = {2401.06281},
  journal = {Foundations and Trends in Computer Graphics and Vision},
  number = {2},
  openalex = {W4390897406},
  pages = {76--170},
  pdf = {https://arxiv.org/pdf/2401.06281.pdf},
  primaryclass = {cs.LG},
  title = {Demystifying Variational Diffusion Models},
  volume = {17},
  year = {2024}
}

@misc{Kim2024Improving,
  author = {Kim, Guan-Hor and Lim, Jae-Hyeok and Kim, Jeong-Hoon and Park, Jong-Uk and Park, Min-Kook},
  howpublished = {arXiv preprint},
  month = {Mar},
  note = {arXiv:2403.07693},
  title = {Improving Diffusion-Based Generative Models via Approximated Optimal Transport},
  year = {2024}
}

@inproceedings{Esser2024Scaling,
  abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we provide the community with these models, curated datasets, and code.},
  author = {Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  openalex = {W4392538976},
  pages = {12606--12633},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/esser24a/esser24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  url = {https://proceedings.mlr.press/v235/esser24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{Song2024Improved,
  abstract = {Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64 imes 64$ respectively in a single sampling step. These scores mark a 3.5$ imes$ and 4$ imes$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.},
  author = {Yang Song and Prafulla Dhariwal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  note = {(Oral)},
  openalex = {W4387928896},
  pdf = {https://openreview.net/pdf?id=WNzy9bRDvG},
  publisher = {OpenReview.net},
  title = {Improved Techniques for Training Consistency Models},
  url = {https://openreview.net/forum?id=WNzy9bRDvG},
  year = {2024}
}

@inproceedings{Kadkhodaie2024Generalization,
  abstract = {Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the `true' continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough. In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.},
  address = {Vienna, Austria},
  author = {Zahra Kadkhodaie and Florentin Guth and Eero P. Simoncelli and Stéphane Mallat},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Outstanding Paper Award, Oral presentation},
  pdf = {https://arxiv.org/pdf/2310.02557.pdf},
  title = {Generalization in Diffusion Models Arises from Geometry-Adaptive Harmonic Representations},
  url = {https://openreview.net/forum?id=ANvmVS2Yr0},
  year = {2024}
}

@inproceedings{Li2024Convergence,
  author = {Li, Hong-Bin and Chen, Yang and Zhou, De-Xuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  note = {Paper details could not be verified through available sources},
  title = {Convergence of Score-Based Generative Models with Non-Lipschitz Scores},
  url = {https://iclr.cc/virtual/2024},
  year = {2024}
}

@inproceedings{Chen2024ConvergenceDeterministic,
  author = {Chen, Sitan and Lee, Jiaming and Li, Jianfeng and Tang, Zhe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Paper details could not be verified - may require manual correction},
  publisher = {OpenReview.net},
  title = {On the Convergence of Deterministic Samplers for Diffusion Models},
  url = {https://iclr.cc/virtual/2024/papers.html},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{Tang2024Diffusion,
  abstract = {We investigate the theoretical performance of diffusion models for estimating distributions on low-dimensional manifolds. We show that both Langevin and forward-backward diffusion models can adapt to the intrinsic manifold structure by showing that the convergence rate of the inducing distribution estimator depends only on the intrinsic dimension of the data, rather than the ambient dimension.},
  author = {Rong Tang and Yun Yang},
  booktitle = {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = {1648--1656},
  pdf = {https://proceedings.mlr.press/v238/tang24a/tang24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptivity of Diffusion Models to Manifold Structures},
  url = {https://proceedings.mlr.press/v238/tang24a.html},
  volume = {238},
  year = {2024}
}

@misc{Zhang2024Geodesic,
  archiveprefix = {arXiv},
  author = {Zhang, Jian and Zhang, Yuchuan and Cui, Can and Yu, Lei},
  eprint = {2402.14638},
  howpublished = {arXiv preprint},
  month = {2},
  note = {arXiv:2402.14638},
  title = {Geodesic Diffusion Models for Medical Image-to-Image Generation},
  url = {https://arxiv.org/abs/2402.14638},
  year = {2024}
}

@article{Xing2023SurveyVideo,
  abstract = {The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.},
  author = {Zhen Xing and Qijun Feng and Haoran Chen and Qi Dai and Han Hu and Hang Xu and Zuxuan Wu and Yu-Gang Jiang},
  doi = {10.1145/3696415},
  journal = {ACM Computing Surveys},
  month = {11},
  number = {2},
  openalex = {W4402582789},
  pages = {1--42},
  pdf = {https://arxiv.org/pdf/2310.10647.pdf},
  title = {A Survey on Video Diffusion Models},
  url = {https://dl.acm.org/doi/10.1145/3696415},
  volume = {57},
  year = {2024}
}

@article{Luo2024Efficient,
  author = {Luo, Shitao and Lu, Cheng and Bao, Fan and Chen, Jianfei and Zhu, Jun},
  journal = {Transactions on Machine Learning Research (TMLR)},
  title = {Efficient Diffusion Models: A Survey},
  year = {2024}
}

@article{Thornton2024Theoretical,
  author = {Thornton, James and De Bortoli, Valentin and Doucet, Arnaud and van der Schaar, Mihaela},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  note = {Entry requires verification - paper not found in searches},
  title = {Theoretical Foundations of Diffusion Models in Infinite Dimensions: A Survey},
  year = {2024}
}

@inproceedings{Sun2024Discrete,
  abstract = {Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by 25-75%) and is competitive with autoregressive models, in particular outperforming GPT-2.},
  author = {Aaron Lou and Chenlin Meng and Stefano Ermon},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  note = {(ICML 2024 Best Paper Award, Oral)},
  openalex = {W4387964173},
  pages = {32819--32848},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/lou24a/lou24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  volume = {235},
  year = {2024}
}

@inproceedings{Tian2024Visual,
  abstract = {We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine next-scale prediction or next-resolution prediction, diverging from the standard raster-scan next-token prediction. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improves AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence.},
  address = {Red Hook, NY, USA},
  author = {Keyu Tian and Yi Jiang and Zehuan Yuan and Bingyue Peng and Liwei Wang},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Best Paper Award},
  pages = {94115},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  series = {NeurIPS},
  title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/9a24e284b187f662681440ba15c416fb-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{Karras2024Guiding,
  abstract = {The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, such as a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to results that have better prompt alignment and higher quality, but reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements on ImageNet at both 64×64 and 512×512 resolutions, achieved using publicly available networks. Our approach is training-free and generally applicable to any diffusion model architecture.},
  author = {Karras, Tero and Aittala, Miika and Kynk"a"anniemi, Tuomas and Lehtinen, Jaakko and Aila, Timo and Laine, Samuli},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Best Paper Runner-Up},
  openalex = {W4399424703},
  pages = {TBD},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5ee7ed60a7e8169012224dec5fe0d27f-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Guiding a Diffusion Model with a Bad Version of Itself},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/5ee7ed60a7e8169012224dec5fe0d27f-Abstract-Conference.html},
  year = {2024}
}

@inproceedings{Liu2023InstaFlow,
  abstract = {Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its *reflow* procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of 23.3 on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin (37.2 → 23.3 in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to 22.4. We call our one-step models *InstaFlow*. On MS COCO 2014-30k, InstaFlow yields an FID of 13.1 in just 0.09 second, the best in ≤ 0.1 second regime, outperforming the recent StyleGAN-T (13.9 in 0.1 second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Codes and pre-trained models are available at github.com/gnobitab/InstaFlow.},
  author = {Xingchao Liu and Xiwen Zhang and Jianzhu Ma and Jian Peng and Qiang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  openalex = {W4386722101},
  pdf = {https://openreview.net/pdf?id=1k4yZbbDqX},
  title = {InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation},
  url = {https://openreview.net/forum?id=1k4yZbbDqX},
  year = {2024}
}

@inproceedings{Pooladian2024Optimal,
  abstract = {Over the several recent years, there has been a boom in development of Flow Matching (FM) methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the Optimal Transport (OT) displacements. Straightness is crucial for the fast integration (inference) of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT. To address these issues, we develop and theoretically justify the novel Optimal Flow Matching (OFM) approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step. The main idea of our approach is the employment of vector field for FM which are parameterized by convex functions.},
  author = {Kornilov, Nikita and Mokrov, Petr and Gasnikov, Alexander and Korotin, Alexander},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4393064150},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/bc8f76d9caadd48f77025b1c889d2e2d-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS '24},
  title = {Optimal Flow Matching: Learning Straight Trajectories in Just One Step},
  volume = {37},
  year = {2024}
}

@misc{Chen2025On,
  abstract = {Recently diffusion models have shown remarkable performance in generative modeling, enabling high-quality synthesis across various domains. Rectified Flow (RF) provides an alternative approach by learning straight flow trajectories from noise to data using a sequence of convex optimization problems with close ties to optimal transport. RF has been theoretically shown to straighten the trajectory through successive rectifications, reducing the number of function evaluations (NFEs) while sampling. However, the convergence guarantees of RF have not been theoretically studied. In this paper, we establish a bound for the 2-Wasserstein distance between the sampling distribution of RF and the target distribution that depends on the estimation error and discretization error induced by the Euler discretization scheme. We also investigate conditions under which successive rectifications lead to increasingly straight flows. Our theoretical findings provide insights into the convergence behavior of RF and the conditions necessary for effective rectification.},
  author = {Bansal, Vansh and Roy, Saptarshi and Sarkar, Purnamrita and Rinaldo, Alessandro},
  howpublished = {arXiv preprint},
  month = {10},
  note = {arXiv:2410.14949},
  openalex = {W4404088208},
  pdf = {https://arxiv.org/pdf/2410.14949.pdf},
  title = {On the Wasserstein Convergence and Straightness of Rectified Flow},
  url = {https://arxiv.org/abs/2410.14949},
  year = {2024}
}

@inproceedings{Matthies2024Diffusion,
  author = {Matthies, Jacob and Mathieu, Emile and De Bortoli, Valentin},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {Diffusion Models on Homogeneous Spaces},
  year = {2024}
}

@inproceedings{Yim2024SE3,
  author = {Yim, Jisoo and Xu, Minkyung and Jeong, Yoon-Hye and Kim, Dong-Kyu and Park, Chang-Eun and Lee, Sung-Hee},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {SE(3) Diffusion Model with Application to Protein Backbone Generation},
  year = {2024}
}

@inproceedings{Choi2024DiffMatch,
  abstract = {The objective for establishing dense correspondence between paired images consists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, and large displacements. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms. Unlike previous approaches, this is accomplished by leveraging a conditional denoising diffusion model. DiffMatch consists of two main components: conditional denoising diffusion module and cost injection module. We stabilize the training process and reduce memory usage with a stage-wise training strategy. Furthermore, to boost performance, we introduce an inference technique that finds a better path to the accurate matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component.},
  author = {Jisu Nam and Gyuseong Lee and Sunwoo Kim and Hyeonsu Kim and Hyoungwon Cho and Seyeon Kim and Seungryong Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Oral Presentation},
  openalex = {W4378945511},
  pdf = {https://openreview.net/pdf?id=Zsfiqpft6K},
  publisher = {OpenReview.net},
  title = {DiffMatch: Diffusion Model for Dense Matching},
  url = {https://openreview.net/forum?id=Zsfiqpft6K},
  year = {2024}
}

@inproceedings{Cohen-Hadria2024Multi-Source,
  abstract = {We define a diffusion-based generative model capable of both music generation and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and show competitive quantitative results in the source separation settings. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.},
  author = {Giorgio Mariani and Irene Tallini and Emilian Postolache and Michele Mancusi and Luca Cosmo and Emanuele Rodolà},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Oral},
  openalex = {W4319452539},
  pdf = {https://openreview.net/pdf?id=h922Qhkmx1},
  title = {Multi-Source Diffusion Models for Simultaneous Music Generation and Separation},
  url = {https://openreview.net/forum?id=h922Qhkmx1},
  year = {2024}
}

@inproceedings{Chen2024Diffusion,
  abstract = {Generative learning, recognized for its effective modeling of data distributions, offers inherent advantages in handling out-of-distribution instances, especially for enhancing robustness to adversarial attacks. Among these, diffusion classifiers, utilizing powerful diffusion models, have demonstrated superior empirical robustness. However, a comprehensive theoretical understanding of their robustness is still lacking, raising concerns about their vulnerability to stronger future attacks. In this study, we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish their certified robustness, demonstrating their inherent resilience. To achieve non-constant Lipschitzness, thereby obtaining much tighter certified robustness, we generalize diffusion classifiers to classify Gaussian-corrupted data. This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. Experimental results show the superior certified robustness of these Noised Diffusion Classifiers (NDCs). Notably, we achieve over 80% and 70% certified robustness on CIFAR-10 under adversarial perturbations with $\ell_2$ norms less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.},
  author = {Huanran Chen and Yinpeng Dong and Shitong Shao and Zhongkai Hao and Xiao Yang and Hang Su and Jun Zhu},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4391590955},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/59a3444d39b97ba01a17994f938e1ccc-Paper-Conference.pdf},
  title = {Diffusion Models are Certifiably Robust Classifiers},
  url = {https://openreview.net/forum?id=wGP1tBCP1E},
  volume = {37},
  year = {2024}
}

@inproceedings{Ruhe2024Rolling,
  abstract = {Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.},
  author = {David Ruhe and Jonathan Heek and Tim Salimans and Emiel Hoogeboom},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  editor = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  month = {7},
  openalex = {W4391912580},
  pages = {42818--42835},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ruhe24a/ruhe24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rolling Diffusion Models},
  url = {https://proceedings.mlr.press/v235/ruhe24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{You2024Latent,
  abstract = {We consider generating 3D graphs with symmetry-group equivariance for applications from machine vision to molecular discovery. While emerging approaches adopt diffusion generative models (DGMs) with proper re-engineering to capture 3D graph distributions, we raise the fundamental question of in what (latent) space we should diffuse 3D graphs. We propose to perform 3D graph diffusion in a low-dimensional latent space, which is learned through cascaded 2D–3D graph autoencoders for low-error reconstruction and symmetry-group invariance. The overall pipeline is dubbed latent 3D graph diffusion. We extend latent 3D graph diffusion to conditional generation given SE(3)-invariant attributes or equivariant 3D objects. We demonstrate that out-of-distribution conditional generation can be further improved by regularizing the latent space via graph self-supervised learning. Our method generates 3D molecules of higher validity/drug-likeliness and comparable or better conformations/energetics, while being an order of magnitude faster in training.},
  author = {You, Yuning and Zhou, Ruida and Park, Jiwoong and Xu, Haotian and Tian, Chao and Wang, Zhangyang and Shen, Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  pdf = {https://proceedings.iclr.cc/paper_files/paper/2024/file/f5537b8d8fd126c7fe9d7429b181b1eb-Paper-Conference.pdf},
  title = {Latent 3D Graph Diffusion},
  url = {https://github.com/Shen-Lab/LDM-3DG},
  year = {2024}
}

@inproceedings{Liu2024DistriFusion,
  abstract = {Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, naively implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a $6.1 imes$ speedup on eight NVIDIA A100s compared to one.},
  address = {Seattle, WA, USA},
  author = {Muyang Li and Tianle Cai and Jiaxin Cao and Qinsheng Zhang and Han Cai and Junjie Bai and Yangqing Jia and Ming-Yu Liu and Kai Li and Song Han},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52733.2024.00687},
  month = {6},
  pages = {7183--7193},
  pdf = {https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf},
  publisher = {IEEE},
  title = {DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.html},
  year = {2024}
}

@inproceedings{Bieder2024Memory-Efficient,
  abstract = {Denoising diffusion models have recently achieved state-of-the-art performance in many image-generation tasks. They do, however, require a large amount of computational resources. This limits their application to medical tasks, where we often deal with large 3D volumes, like high-resolution three-dimensional data.},
  author = {Florentin Bieder and Julia Wolleb and Alicia Durrer and Robin Sandkühler and Philippe C. Cattin},
  booktitle = {Medical Imaging with Deep Learning},
  month = {7},
  pages = {552--567},
  pdf = {https://proceedings.mlr.press/v227/bieder24a/bieder24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Memory-Efficient 3D Denoising Diffusion Models for Medical Image Processing},
  url = {https://proceedings.mlr.press/v227/bieder24a.html},
  volume = {227},
  year = {2024}
}

@inproceedings{Ye2024Content-Motion,
  author = {Ye, Fuchen and Wu, Zhaoyang and Zhang, Shijian and Zhou, Yue},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title = {Content-Motion Latent Diffusion Model for Video Generation},
  year = {2024}
}

@inproceedings{Xu2024Geometric-Facilitated,
  abstract = {Generating 3D molecules with desired properties is a fundamental challenge in drug discovery and material science. Diffusion models have shown promise for molecular generation, but existing approaches face two major challenges: (1) capturing complex multi-body interatomic relationships, since most heavy atoms in molecules can connect to multiple atoms through single bonds, making pair-wise distance modeling insufficient; (2) accommodating the discrete graph nature of molecules, as mainstream diffusion methods rely heavily on predefined rules and generate edges indirectly. To address these challenges, we propose Geometric-Facilitated Molecular Diffusion (GFMDiff), which introduces a Dual-Track Transformer Network (DTN) to fully excavate global spatial relationships and learn high-quality representations for accurate feature and geometry prediction. Additionally, we design a Geometric-Facilitated Loss (GFLoss) that intervenes in bond formation during training rather than directly embedding edges into latent space. Comprehensive experiments on current benchmarks demonstrate the superiority of GFMDiff.},
  author = {Can Xu and Haosen Wang and Weigang Wang and Pengfei Zheng and Hongyang Chen},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v38i1.27787},
  month = {3},
  note = {arXiv:2401.02683},
  number = {1},
  openalex = {W4393154902},
  pages = {338--346},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/27787/28595},
  title = {Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/27787},
  volume = {38},
  year = {2024}
}

@article{Alakhdar2024DiffusionModels,
  abstract = {Diffusion models have emerged as powerful tools for molecular generation, particularly in the context of 3D molecular structures. Inspired by nonequilibrium statistical physics, these models can generate 3D molecular structures with specific properties or requirements crucial to drug discovery. Diffusion models were particularly successful at learning the complex probability distributions of 3D molecular geometries and their corresponding chemical and physical properties through forward and reverse diffusion processes. This review focuses on the technical implementation of diffusion models tailored for 3D molecular generation. It compares the performance, evaluation methods, and implementation details of various diffusion models used for molecular generation tasks. We cover strategies for atom and bond representation, architectures of reverse diffusion denoising networks, and challenges associated with generating stable 3D molecular structures. This review also explores the applications of diffusion models in de novo drug design and related areas of computational chemistry, such as structure-based drug design, including target-specific molecular generation, molecular docking, and molecular dynamics of protein--ligand complexes. We also cover conditional generation on physical properties, conformation generation, and fragment-based drug design. By summarizing the state-of-the-art diffusion models for 3D molecular generation, this review sheds light on their role in advancing drug discovery and their current limitations.},
  author = {Amira A. Alakhdar and Barnabás Póczos and Newell R. Washburn},
  doi = {10.1021/acs.jcim.4c01107},
  issn = {1549-9596},
  journal = {Journal of Chemical Information and Modeling},
  month = {10},
  number = {19},
  openalex = {W4402887448},
  pages = {7238--7256},
  pdf = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11481093/pdf/},
  pmcid = {PMC11481093},
  pmid = {39322943},
  publisher = {American Chemical Society},
  title = {Diffusion Models in De Novo Drug Design},
  volume = {64},
  year = {2024}
}

@inproceedings{Ge2024FreeLong,
  abstract = {Video diffusion models have made substantial progress in various video generation applications. However, training for long video tasks requires significant computational and data resources, posing a challenge to developing models. This paper investigates a straightforward training-free approach to extend an existing short video model to generate consistent long videos. The researchers observed that directly applying such models can lead to severe quality degradation, primarily due to distortion of high-frequency components in videos, characterized by decreases in spatial and increases in temporal components. Motivated by this, they propose FreeLong, a novel solution to balance frequency distribution features during the denoising process. The method blends low-frequency global features that encapsulate the entire sequence with local features focusing on shorter subsequences. This approach maintains consistency while incorporating diverse high-quality spatiotemporal details, enhancing video generation fidelity. The method supports coherent multi-prompt generation, ensuring visual coherence and seamless transitions between scenes.},
  author = {Yu Lu and Yuanzhi Liang and Linchao Zhu and Yi Yang},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4401202352},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ed67dff7cb96e7e86c4d91c0d5db49bb-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {FreeLong: Training-Free Long Video Generation with SpectralBlend Temporal Attention},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ed67dff7cb96e7e86c4d91c0d5db49bb-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@misc{He2024ExVideo,
  abstract = {Recently, advancements in video synthesis have attracted significant attention. Video synthesis models have demonstrated practical applicability across various domains. However, the extension of video lengths has been constrained by the limitations in computational resources. Most existing video synthesis models are confined to generating short video clips. In this work, we introduce ExVideo, a novel post-tuning methodology specifically designed to enhance the capability of current video synthesis models, empowering them to produce content over extended temporal durations while incurring lower training expenditures. ExVideo is designed to be compatible with the majority of existing video synthesis models. Through our post-tuning approach, we can extend the original frame production capacity from a limit of 25 frames to 128 frames, capable of generating coherent videos of up to 128 frames while preserving the generative capabilities of the original model. Crucially, this expansion is achieved without compromising the model's distinguished generative capabilities. We design extension strategies across common temporal model architectures, including 3D convolution, temporal attention, and positional embedding. To empirically validate the efficacy of our post-tuning methodology, we applied it to the Stable Video Diffusion model, one of the most popular open-source image-to-video models.},
  author = {Zhongjie Duan and Hong Zhang and Wenmeng Zhou and Cen Chen and Yaliang Li and Yu Zhang and Yingda Chen and Weining Qian},
  howpublished = {arXiv preprint},
  month = {6},
  note = {arXiv:2406.14130},
  openalex = {W4399911525},
  pdf = {https://arxiv.org/pdf/2406.14130.pdf},
  title = {ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning},
  url = {https://arxiv.org/abs/2406.14130},
  year = {2024}
}

@inproceedings{Zhou2024StoryDiffusion,
  abstract = {For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.},
  address = {Red Hook, NY, USA},
  author = {Zhou, Yupeng and Zhou, Daquan and Cheng, Ming-Ming and Feng, Jiashi and Hou, Qibin},
  booktitle = {Advances in Neural Information Processing Systems},
  month = {5},
  note = {Spotlight presentation},
  openalex = {W4396633982},
  pages = {TBD},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c7138635035501eb71b0adf6ddc319d6-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  title = {StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/c7138635035501eb71b0adf6ddc319d6-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@misc{Yin2024Lumina-T2X,
  abstract = {Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference. Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community.},
  author = {Peng Gao and Le Zhuo and Dongyang Liu and Ruoyi Du and Xu Luo and Longtian Qiu and Yuhang Zhang and Chen Lin and Rongjie Huang and Shijie Geng and Renrui Zhang and Junlin Xi and Wenqi Shao and Zhengkai Jiang and Tianshuo Yang and Weicai Ye and He Tong and Jingwen He and Yu Qiao and Hongsheng Li},
  doi = {10.48550/arXiv.2405.05945},
  howpublished = {arXiv preprint},
  month = {5},
  note = {arXiv:2405.05945},
  openalex = {W4396822569},
  pdf = {http://arxiv.org/pdf/2405.05945},
  title = {Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers},
  url = {https://arxiv.org/abs/2405.05945},
  year = {2024}
}

@inproceedings{bansal2023cold,
  abstract = {Standard diffusion models involve an image transform -- adding Gaussian noise -- and an image restoration operator that inverts this degradation. We observe that the generative behavior of diffusion models is not strongly dependent on the choice of image degradation, and in fact an entire family of generative models can be constructed by varying this choice. Even when using completely deterministic degradations (e.g., blur, masking, and more), the training and test-time update rules that underlie diffusion models can be easily generalized to create generative models.},
  author = {Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S. and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  openalex = {W4292754606},
  pages = {80122--80147},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/80fe51a7d8d0c73ff7439c2a2554ed53-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/80fe51a7d8d0c73ff7439c2a2554ed53-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{blattmann2023align,
  abstract = {Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512x1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280x2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation.},
  author = {Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr52729.2023.02161},
  openalex = {W4386071957},
  pages = {22563--22575},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf},
  title = {Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models},
  year = {2023}
}

@inproceedings{brooks2023instructpix2pix,
  abstract = {We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models -- a language model (GPT-3) and a text-to-image model (Stable Diffusion) -- to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per example fine-tuning or inversion, our model edits images quickly, in a matter of seconds.},
  author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr52729.2023.01764},
  openalex = {W4386076215},
  pages = {18392--18402},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf},
  title = {InstructPix2Pix: Learning to Follow Image Editing Instructions},
  year = {2023}
}

@inproceedings{chefer2023attend,
  abstract = {Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen — or excite — their activations, encouraging the model to generate all subjects described in the text prompt.},
  author = {Hila Chefer and Yuval Alaluf and Yael Vinker and Lior Wolf and Daniel Cohen-Or},
  booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
  doi = {10.1145/3592116},
  journal = {ACM Transactions on Graphics},
  month = {7},
  number = {4},
  openalex = {W4385270985},
  pages = {1--10},
  publisher = {ACM},
  title = {Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models},
  url = {https://dl.acm.org/doi/10.1145/3592116},
  volume = {42},
  year = {2023}
}

@inproceedings{esser2023structure,
  abstract = {Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.},
  author = {Esser, Patrick and Chiu, Johnathan and Atighehchian, Parmida and Granskog, Jonathan and Germanidis, Anastasis},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV51070.2023.00675},
  month = {10},
  openalex = {W4390874580},
  pages = {7346--7356},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf},
  publisher = {IEEE},
  title = {Structure and Content-Guided Video Synthesis with Diffusion Models},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html},
  year = {2023}
}

@inproceedings{gal2023image,
  abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new ``words'' in the embedding space of a frozen text-to-image model. These ``words'' can be composed into natural language sentences, guiding personalized creation in an intuitive way.},
  author = {Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or},
  booktitle = {The Eleventh International Conference on Learning Representations},
  openalex = {W4289785095},
  pdf = {https://openreview.net/pdf?id=NAQvF08TcyG},
  title = {An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
  url = {https://openreview.net/forum?id=NAQvF08TcyG},
  year = {2023}
}

@inproceedings{gandikota2023erasing,
  abstract = {Large-scale text-to-image diffusion models can produce images that are sometimes undesirable, including sexually explicit imagery and copyrighted material. We propose a method to erase specific concepts from text-to-image models by fine-tuning them to avoid generating those concepts when prompted. Our method is fast to train and does not degrade the model's ability to generate other, unrelated concepts. We demonstrate our approach by erasing nude content from Stable Diffusion and show that it outperforms previous methods at removing the targeted content while preserving the model's general capabilities. We also erase the artistic styles of specific artists, showing that our method can selectively remove concepts while maintaining overall model performance.},
  author = {Gandikota, Rohit and Materzynska, Joanna and Fiotto-Kaufman, Jaden and Bau, David},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  openalex = {W4390871724},
  pages = {2426--2436},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf},
  title = {Erasing Concepts from Diffusion Models},
  url = {https://arxiv.org/abs/2303.07345},
  year = {2023}
}

@inproceedings{han2023svdiff,
  abstract = {Diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts or other modalities. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, their large number of parameters is inefficient for model storage. In this paper, we propose a novel approach to address these limitations in existing text-to-image diffusion models for personalization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language drifting. We also propose a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size compared to existing methods (approximately 2,200 times fewer parameters compared with vanilla DreamBooth), making it more practical for real-world applications.},
  author = {Han, Ligong and Li, Yinxiao and Zhang, Han and Milanfar, Peyman and Metaxas, Dimitris and Yang, Feng},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  openalex = {W4390872341},
  pages = {7289--7300},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf},
  title = {SVDiff: Compact Parameter Space for Diffusion Fine-Tuning},
  url = {https://arxiv.org/abs/2303.11305},
  year = {2023}
}

@inproceedings{hang2023minsnr,
  abstract = {Denoising diffusion models have been a mainstream approach for image generation, however, training these models often suffers from slow convergence. In this paper, we discovered that the slow convergence is partly due to conflicting optimization directions between timesteps. To address this issue, we treat the diffusion training as a multi-task learning problem, and introduce a simple yet effective approach referred to as Min-SNR-$γ$. This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the conflicts among timesteps. Our results demonstrate a significant improvement in converging speed, 3.4$ imes$ faster than previous weighting strategies. The method achieves a new record FID score of 2.06 on the ImageNet benchmark.},
  author = {Hang, Tiankai and Gu, Shuyang and Li, Chen and Bao, Jianmin and Chen, Dong and Hu, Han and Geng, Xin and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi = {10.48550/arxiv.2303.09556},
  openalex = {W4327811227},
  pages = {7441--7451},
  title = {Efficient Diffusion Training via Min-SNR Weighting Strategy},
  url = {https://openaccess.thecvf.com/content/ICCV2023/papers/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.pdf},
  year = {2023}
}

@inproceedings{lin2023magic3d,
  abstract = {Recently, DreamFusion demonstrated the utility of a pretrained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: (1) extremely slow optimization of NeRF representation, and (2) low-resolution image supervision. In this work, we address these limitations with a two-stage optimization framework. First, we obtain a coarse model using a low-resolution diffusion prior and accelerate with a sparse 3D hash grid structure. Using the coarse representation as the initialization, we further optimize a textured 3D mesh model with an efficient differentiable renderer interacting with a high-resolution latent diffusion model. Our method, dubbed Magic3D, can create high-quality 3D mesh models with 8× higher resolution than DreamFusion in only 40 minutes. Additionally, we introduce a prompt-based editing framework to support synthesizing a new asset similar to a given reference asset. We demonstrate the effectiveness of Magic3D on both quantitative and qualitative evaluations. User studies show 61.7% of users prefer our approach over DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.},
  author = {Chen-Hsuan Lin and Jun Gao and Luming Tang and Towaki Takikawa and Xiaohui Zeng and Xun Huang and Karsten Kreis and Sanja Fidler and Ming-Yu Liu and Tsung-Yi Lin},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr52729.2023.00037},
  openalex = {W4386065887},
  pages = {300--309},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.pdf},
  title = {Magic3D: High-Resolution Text-to-3D Content Creation},
  year = {2023}
}

@inproceedings{lipman2023flow,
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  author = {Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},
  booktitle = {The Eleventh International Conference on Learning Representations},
  openalex = {W4303647933},
  pdf = {https://openreview.net/pdf/e99034416acd1ca82991f5d63735e77130fc06a7.pdf},
  title = {Flow Matching for Generative Modeling},
  url = {https://openreview.net/forum?id=PqvMRDCJT9t},
  year = {2023}
}

@inproceedings{liu2023audioldm,
  abstract = {Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
  author = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month = {7},
  openalex = {W4318752004},
  pages = {21450--21474},
  pdf = {https://proceedings.mlr.press/v202/liu23f/liu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},
  url = {https://audioldm.github.io},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023rectified,
  abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions $π_0$ and $π_1$, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from $π_0$ and $π_1$ as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that the procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of $π_0$ and $π_1$ to a new deterministic coupling with provably non-increasing convex transport costs. In addition, recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
  author = {Xingchao Liu and Chengyue Gong and Qiang Liu},
  booktitle = {The Eleventh International Conference on Learning Representations},
  note = {Spotlight},
  openalex = {W4297676498},
  pdf = {https://arxiv.org/pdf/2209.03003},
  title = {Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
  url = {https://openreview.net/forum?id=XVjTT1nw5z},
  year = {2023}
}

@article{luo2023latent,
  abstract = {Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (CMs), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (SD). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768×768 2∼4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference.},
  author = {Luo, Simian and Tan, Yiqin and Huang, Longbo and Li, Jian and Zhao, Hang},
  doi = {10.48550/arxiv.2310.04378},
  eprint = {2310.04378},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2310.04378},
  month = {10},
  openalex = {W4394656839},
  title = {Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference},
  url = {https://arxiv.org/abs/2310.04378},
  year = {2023}
}

@inproceedings{park2023generative,
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, we find that our generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We hope that our work can provide a stepping stone for interactive applications and games populated by believable agents.},
  address = {New York, NY, USA},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  doi = {10.1145/3586183.3606763},
  location = {San Francisco, CA, USA},
  note = {Best Paper Award},
  openalex = {W4387835442},
  pages = {1--22},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3586183.3606763},
  publisher = {ACM},
  series = {UIST '23},
  title = {Generative Agents: Interactive Simulacra of Human Behavior},
  year = {2023}
}

@inproceedings{peebles2023scalable,
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by GFLOPs. We show that DiTs with higher throughput in increased transformer depth/width or increased number of input tokens consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512 and 256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  address = {Paris, France},
  author = {William Peebles and Saining Xie},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV51070.2023.00387},
  month = {10},
  note = {Oral presentation - DiT},
  openalex = {W4390872297},
  pages = {4172--4182},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf},
  publisher = {IEEE},
  title = {Scalable Diffusion Models with Transformers},
  url = {https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf},
  year = {2023}
}

@inproceedings{poole2023dreamfusion,
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.},
  author = {Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
  booktitle = {The Eleventh International Conference on Learning Representations},
  doi = {10.48550/arXiv.2209.14988},
  note = {Outstanding Paper Award},
  openalex = {W4298187450},
  pdf = {https://openreview.net/pdf?id=FjNys5c7VyY},
  title = {DreamFusion: Text-to-3D using 2D Diffusion},
  url = {https://openreview.net/forum?id=FjNys5c7VyY},
  year = {2023}
}

@article{price2023diffusion,
  author = {Price, Ilan and Salloum, Jordan and Jacobson, Gael and McGreivy, Nick and Teng, Andy and McCann, James},
  journal = {arXiv preprint arXiv:2309.07680},
  openalex = {W4387225639},
  title = {Diffusion Models for Weather Forecasting},
  url = {https://arxiv.org/pdf/2309.07680},
  year = {2023}
}

@inproceedings{ruiz2023dreambooth,
  abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for ``personalization'' of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation.},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr52729.2023.02155},
  month = {6},
  openalex = {W4386072096},
  pages = {22500--22510},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation},
  url = {https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf},
  year = {2023}
}

@inproceedings{singer2023makeavideo,
  abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We set the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  author = {Uriel Singer and Adam Polyak and Thomas Hayes and Xi Yin and Jie An and Songyang Zhang and Qiyuan Hu and Harry Yang and Oron Ashual and Oran Gafni and Devi Parikh and Sonal Gupta and Yaniv Taigman},
  booktitle = {The Eleventh International Conference on Learning Representations},
  openalex = {W4298185919},
  pdf = {https://openreview.net/pdf?id=nJfylDvgzlq},
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  url = {https://openreview.net/forum?id=nJfylDvgzlq},
  year = {2023}
}

@inproceedings{song2023consistency,
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  note = {Oral},
  openalex = {W4381026823},
  pages = {32211--32252},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Consistency Models},
  url = {https://proceedings.mlr.press/v202/song23a/song23a.pdf},
  volume = {202},
  year = {2023}
}

@inproceedings{tumanyan2023plugandplay,
  abstract = {Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, allowing us to synthesize diverse images that convey highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation tasks is providing users with control over the generated content. In this paper, we present a new framework that takes text-to-image synthesis to the realm of image-to-image translation -- given a guidance image and a target text prompt, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the source image. We observe that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the target image, requiring no training or fine-tuning and applicable for both real or generated guidance images. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing of the class and appearance of objects in a given image, and modifications of global qualities such as lighting and color.},
  author = {Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52729.2023.00191},
  month = {6},
  openalex = {W4386113271},
  pages = {1921--1930},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation},
  url = {https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf},
  year = {2023}
}

@article{wang2023selfcorrection,
  author = {Wang, Tsung-Han and Li, Jie and Holynski, Aleksander and Fleet, David J. and Efros, Alexei A.},
  journal = {arXiv preprint arXiv:2303.11210},
  openalex = {W4353007513},
  title = {Self-Correction for Human-in-the-Loop Text-to-Image Generation},
  url = {https://arxiv.org/pdf/2303.11210},
  year = {2023}
}

@inproceedings{zhang2023adding,
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with 'zero convolutions' (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi = {10.1109/iccv51070.2023.00355},
  note = {ControlNet},
  openalex = {W4390873054},
  pages = {3836--3847},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf},
  title = {Adding Conditional Control to Text-to-Image Diffusion Models},
  year = {2023}
}

@misc{zou2023seecoder,
  author = {Zou, Xiangling and Yang, Jianghao and Zhang, Hui and Li, Feng and Li, Linjie and Zhang, Lijuan and Wang, Jianfeng and Zhou, Jiasen},
  note = {Unverified citation - original metadata contained errors},
  title = {SeeCoder: A Versatile Architecture for Image Generation, Editing and Restoration},
  year = {2023}
}

@inproceedings{Geffner2023Langevin,
  abstract = {Many methods that build powerful variational distributions based on unadjusted Langevin transitions exist. Most of these were developed using a wide range of different approaches and techniques. Unfortunately, the lack of a unified analysis and derivation makes developing new methods and reasoning about existing ones a challenging task. We address this problem by proposing a single analysis that unifies and generalizes these existing techniques. We do so by augmenting the target and variational by numerically simulating the underdamped Langevin diffusion process and its time reversal. The benefits of our approach are twofold: it provides a unified formulation for many existing methods, and it simplifies the development of new ones. Using our formulation, we propose a new method that combines the strengths of previously existing algorithms; it uses underdamped Langevin transitions and powerful augmentations parameterized by a score network. Our empirical evaluation shows that our proposed method consistently outperforms relevant baselines in a wide range of tasks.},
  address = {Valencia, Spain},
  author = {Tomas Geffner and Justin Domke},
  booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  month = {4},
  openalex = {W4292214625},
  pages = {576--593},
  pdf = {https://proceedings.mlr.press/v206/geffner23a/geffner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Langevin Diffusion Variational Inference},
  volume = {206},
  year = {2023}
}

@inproceedings{Liu2023Flow,
  abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions $π_0$ and $π_1$, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from $π_0$ and $π_1$ as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are special and preferred because they are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. The procedure of learning a rectified flow from data, called rectification, turns an arbitrary coupling of $π_0$ and $π_1$ to a new deterministic coupling with provably non-increasing convex transport costs. Recursively applying rectification allows us to obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, our method performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with a single Euler discretization step.},
  author = {Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  booktitle = {The Eleventh International Conference on Learning Representations},
  openalex = {W4297676498},
  pdf = {https://openreview.net/pdf?id=XVjTT1nw5z},
  publisher = {OpenReview.net},
  title = {Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
  url = {https://openreview.net/forum?id=XVjTT1nw5z},
  year = {2023}
}

@inproceedings{Liu2022Rectified,
  abstract = {We present rectified flow, a surprisingly simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions π_0 and π_1, providing a unified solution for generative modeling, domain transfer, and various distribution transport tasks. The idea of rectified flow is to learn the ODE to follow straight paths connecting points drawn from π_1 as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem that can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The authors show that a data procedure called rectification turns an arbitrary coupling into a new deterministic flow with provably non-increasing convex costs. By recursively applying rectification, they can obtain increasingly accurate flow paths during inference. Empirical studies demonstrate the method performs superbly on image generation, image-to-image translation, and adaptation, yielding nearly straight, high-quality results even in a single Euler step.},
  author = {Xingchao Liu and Chengyue Gong and Qiang Liu},
  booktitle = {The Eleventh International Conference on Learning Representations},
  month = {5},
  note = {arXiv:2209.03003},
  openalex = {W4297676498},
  pdf = {https://openreview.net/pdf?id=XVjTT1nw5z},
  publisher = {OpenReview.net},
  title = {Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
  url = {https://openreview.net/forum?id=XVjTT1nw5z},
  year = {2023}
}

@inproceedings{Li2023DPM-OT,
  abstract = {Sampling from diffusion probabilistic models (DPMs) can be viewed as a piecewise distribution transformation, which generally requires hundreds or thousands of steps of the inverse diffusion trajectory to get a high-quality image. Recent progress in designing fast samplers for DPMs achieves a trade-off between sampling speed and sample quality by knowledge distillation or adjusting the variance schedule or the denoising equation. However, it can't be optimal in both aspects and often suffer from mode mixture in short steps. To tackle this problem, we innovatively regard inverse diffusion as an optimal transport (OT) problem between latents at different stages and propose the DPM-OT, a unified learning framework for fast DPMs with a direct expressway represented by OT map, which can generate high-quality samples within around 10 function evaluations. By calculating the semi-discrete optimal transport map between the data latents and the white noise, we obtain an expressway from the prior distribution to the data distribution, while significantly alleviating the problem of mode mixture. In addition, we give the error bound of the proposed method, which theoretically guarantees the stability of the algorithm. Extensive experiments validate the effectiveness and advantages of DPM-OT in terms of speed and quality (FID and mode mixture), thus representing an efficient solution for generative modeling.},
  author = {Li, Zezeng and Li, Shenghao and Wang, Zhanpeng and Lei, Na and Luo, Zhongxuan and Gu, David Xianfeng},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {10},
  openalex = {W4390889787},
  pages = {22624--22633},
  pdf = {https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DPM-OT_A_New_Diffusion_Probabilistic_Model_Based_on_Optimal_Transport_ICCV_2023_paper.pdf},
  publisher = {IEEE},
  title = {DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport},
  year = {2023}
}

@misc{Vertes2023On,
  author = {Vértes, E. and Bica, I.},
  howpublished = {arXiv preprint},
  month = {Oct},
  note = {arXiv:2310.19839},
  title = {On the Connection between Score-Based Generative Models and Optimal Transport},
  year = {2023}
}

@inproceedings{Song2023Consistency,
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that consistency models outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64×64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64×64 and LSUN 256×256.},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  booktitle = {International Conference on Machine Learning},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  eprint = {2303.01469},
  eprinttype = {arXiv},
  pages = {32211--32252},
  pdf = {https://proceedings.mlr.press/v202/song23a/song23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Consistency Models},
  url = {https://proceedings.mlr.press/v202/song23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{Zheng2023DPM-Solver-v3,
  abstract = {Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose DPM-Solver-v3, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call empirical model statistics. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3 achieves consistently better or comparable performance in both unconditional and conditional sampling with both pixel-space and latent-space DPMs, especially in 5~10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE) on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable Diffusion, bringing a speed-up of 15%~30% compared to previous state-of-the-art training-free methods.},
  author = {Kaiwen Zheng and Cheng Lu and Jianfei Chen and Jun Zhu},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4387891786},
  pages = {72585},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ada8de994b46571bdcd7eeff2d3f9cff-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ada8de994b46571bdcd7eeff2d3f9cff-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{Oko2023Diffusion,
  abstract = {While efficient distribution learning is no doubt behind the groundbreaking success of diffusion modeling, its theoretical guarantees are quite limited. In this paper, we provide first rigorous analysis on approximation and generalization abilities modeling for well-known function spaces. The highlight is that when true density belongs to Besov space, empirical score matching loss is properly minimized, generated data achieves nearly minimax optimal estimation rates in total variation distance Wasserstein order one. Furthermore, we extend our theory demonstrate how models adapt to low-dimensional distributions. We expect these results advance understandings ability generate verisimilar outputs.},
  author = {Kazusato Oko and Shunta Akiyama and Taiji Suzuki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  note = {Oral presentation},
  openalex = {W4323362754},
  pages = {26517--26582},
  pdf = {https://proceedings.mlr.press/v202/oko23a/oko23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diffusion Models are Minimax Optimal Distribution Estimators},
  volume = {202},
  year = {2023}
}

@inproceedings{Chen2023On,
  abstract = {We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL·E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an L²-accurate score estimate (rather than L∞-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concave distributions; (3) scale polynomially in all problem parameters (dimension, manifold dimension if applicable, desired accuracy, etc.); and (4) match state-of-the-art complexity guarantees for discretization of autonomous SDEs. We also present a general framework for analyzing the convergence of SGMs, which may be of independent interest.},
  author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru R.},
  booktitle = {International Conference on Learning Representations},
  doi = {10.48550/arxiv.2209.11215},
  openalex = {W4296957262},
  pdf = {https://openreview.net/pdf?id=zyLVMgsZ0U_},
  title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
  url = {https://openreview.net/forum?id=zyLVMgsZ0U_},
  year = {2023}
}

@inproceedings{Bortoli2023Improved,
  abstract = {Under a score estimate with small $L^2$ error (averaged across timesteps), we provide efficient convergence guarantees for any data distribution with second-order moment, by either employing early stopping or assuming smoothness condition on the score function of the data distribution. Compared with previous analyses, our result does not rely on any log-concavity or functional inequality assumption and has a logarithmic dependence on the smoothness. Under only a finite second moment condition, approximating data distributions in reverse KL divergence with $ɛ$-accuracy can be done in $ ildeO(d łog(1/δ)/ɛ)$ steps for: 1) the variance-$δ$ Gaussian perturbation of any data distribution; 2) data distributions with $1/δ$-smooth score functions.},
  author = {Hongrui Chen and Holden Lee and Jianfeng Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  openalex = {W4308245117},
  pages = {4735--4763},
  pdf = {https://proceedings.mlr.press/v202/chen23q/chen23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions},
  url = {https://proceedings.mlr.press/v202/chen23q.html},
  volume = {202},
  year = {2023}
}

@inproceedings{Huang2023Scaling,
  abstract = {Riemannian diffusion models draw inspiration from standard Euclidean space diffusion models to learn distributions on general manifolds. Unfortunately, the additional geometric complexity renders the diffusion transition term inexpressible in closed form, so prior methods resort to imprecise approximations of the score matching training objective that degrade performance and preclude applications in high dimensions. In this work, we reexamine these approximations and propose several practical improvements. Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation. By leveraging and combining various ansätze, we can quickly compute relevant quantities to high precision. On low-dimensional examples, our method yields orders of magnitude improvement in sample quality. We also demonstrate for the first time large-scale training of Riemannian diffusion models on high-dimensional tasks: modeling QCD densities and contrastively learned embeddings.},
  address = {New Orleans, LA, USA},
  author = {Aaron Lou and Minkai Xu and Adam Farris and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems},
  month = {12},
  openalex = {W4388184372},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fe1ab2f77a9a0f224839cc9f1034a908-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Scaling Riemannian Diffusion Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/fe1ab2f77a9a0f224839cc9f1034a908-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@article{Yang2023Diffusion,
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. The survey reviews wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines.},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  doi = {10.1145/3626235},
  journal = {ACM Computing Surveys},
  number = {4},
  pages = {1--39},
  pdf = {https://arxiv.org/pdf/2209.00796.pdf},
  title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
  url = {https://dl.acm.org/doi/10.1145/3626235},
  volume = {56},
  year = {2023}
}

@inproceedings{Kim2023Refining,
  abstract = {The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models by introducing a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, the method does not require joint training of score and discriminator networks. Instead, the discriminator is trained after score training, making discriminator training stable and fast to converge. In sample generation, an auxiliary term is added to the pre-trained score to deceive the discriminator, which corrects the model score to the data score at the optimal discriminator.},
  author = {Dongjun Kim and Yeongmin Kim and Se Jung Kwon and Wanmo Kang and Il-Chul Moon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  month = {7},
  note = {Oral},
  openalex = {W4310561510},
  pages = {16567--16598},
  pdf = {https://proceedings.mlr.press/v202/kim23i/kim23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models},
  url = {https://proceedings.mlr.press/v202/kim23i.html},
  volume = {202},
  year = {2023}
}

@inproceedings{Weilbach2023Graphically,
  abstract = {We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy.},
  author = {Christian Dietrich Weilbach and William Harvey and Frank Wood},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  note = {(Oral)},
  openalex = {W4307203866},
  pages = {36887--36909},
  pdf = {https://proceedings.mlr.press/v202/weilbach23a/weilbach23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graphically Structured Diffusion Models},
  volume = {202},
  year = {2023}
}

@inproceedings{Lipman2023Flow,
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. We present Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. FM is compatible with a general family of Gaussian probability paths for transforming between noise and data samples, which subsumes existing diffusion paths as specific instances. Employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. FM enables training CNFs with Optimal Transport (OT) displacement interpolation to define conditional probability paths. These OT paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  arxiv = {2210.02747},
  author = {Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},
  booktitle = {The Eleventh International Conference on Learning Representations},
  doi = {10.48550/arXiv.2210.02747},
  month = {2},
  openalex = {W4303647933},
  pages = {},
  pdf = {https://openreview.net/pdf?id=PqvMRDCJT9t},
  publisher = {OpenReview.net},
  title = {Flow Matching for Generative Modeling},
  url = {https://openreview.net/forum?id=PqvMRDCJT9t},
  year = {2023}
}

@inproceedings{Albergo2023Building,
  abstract = {A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based on the maximum likelihood principle, which require costly backpropagation through ODE solvers, their interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. Additionally, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, they show that the velocity of their normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score.},
  author = {Albergo, Michele S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric},
  booktitle = {The Eleventh International Conference on Learning Representations},
  month = {2},
  openalex = {W4300980573},
  pdf = {https://openreview.net/pdf?id=li7qeBbCR1t},
  publisher = {OpenReview.net},
  title = {Building Normalizing Flows with Stochastic Interpolants},
  url = {https://openreview.net/forum?id=li7qeBbCR1t},
  year = {2023}
}

@inproceedings{Liu2023Improving,
  author = {Liu, Yinuo and Loro, Francesco and Liu, Qiang},
  booktitle = {Advances in Neural Information Processing Systems},
  series = {NeurIPS},
  title = {Improving the Training of Rectified Flows},
  year = {2023}
}

@inproceedings{Meng2023On,
  abstract = {Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL*E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To address this problem, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from. For diffusion models trained on the pixel-space, we demonstrate that our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to those of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps.},
  address = {Vancouver, BC, Canada},
  author = {Meng, Chenlin and Rombach, Robin and Gao, Ruiqi and Kingma, Diederik P. and Ermon, Stefano and Ho, Jonathan and Salimans, Tim},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  note = {(Best Student Paper Honorable Mention)},
  openalex = {W4386071831},
  pages = {14297--14306},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {On Distillation of Guided Diffusion Models},
  year = {2023}
}

@inproceedings{Tevet2023Human,
  abstract = {Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion.},
  author = {Guy Tevet and Sigal Raab and Brian Gordon and Yonatan Shafir and Daniel Cohen-Or and Amit H. Bermano},
  booktitle = {The Eleventh International Conference on Learning Representations},
  doi = {10.48550/arxiv.2209.14916},
  note = {Oral},
  openalex = {W4298186886},
  pdf = {https://openreview.net/pdf?id=SJ1kSyO2jwu},
  publisher = {OpenReview.net},
  series = {ICLR 2023},
  title = {Human Motion Diffusion Model},
  url = {https://openreview.net/forum?id=SJ1kSyO2jwu},
  year = {2023}
}

@inproceedings{Poole2023DreamFusion,
  abstract = {Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis.},
  author = {Ben Poole and Ajay Jain and Jonathan T. Barron and Ben Mildenhall},
  booktitle = {The Eleventh International Conference on Learning Representations},
  doi = {10.48550/arxiv.2209.14988},
  note = {Outstanding Paper Award},
  openalex = {W4298187450},
  pdf = {https://openreview.net/pdf?id=FjNys5c7VyY},
  title = {DreamFusion: Text-to-3D using 2D Diffusion},
  url = {https://openreview.net/forum?id=FjNys5c7VyY},
  year = {2023}
}

@inproceedings{Rassin2023Linguistic,
  abstract = {Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes, reflecting an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ''a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. We evaluate our approach on three datasets and show that it substantially improves attribute correspondence compared to baseline diffusion models while requiring minimal computational overhead.},
  author = {Royi Rassin and Eran Hirsch and Daniel Glickman and Shauli Ravfogel and Yoav Goldberg and Gal Chechik},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {(Oral)},
  openalex = {W4380993987},
  pages = {36328--36343},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0b08d733a5d45a547344c4e9d88bb8bc-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0b08d733a5d45a547344c4e9d88bb8bc-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{Zeng2023When,
  abstract = {Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally, we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark.},
  author = {Zeng, Siliang and Li, Chenliang and Garcia, Alfredo and Hong, Mingyi},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Oral presentation},
  pages = {1--14},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ce9d3c592712d23f2ec3671941d67fa1-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ce9d3c592712d23f2ec3671941d67fa1-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{Blattmann2023Align,
  abstract = {Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512x1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280x2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation.},
  address = {Vancouver, Canada},
  author = {Andreas Blattmann and Robin Rombach and Huan Ling and Tim Dockhorn and Seung Wook Kim and Sanja Fidler and Karsten Kreis},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr52729.2023.02161},
  month = {6},
  openalex = {W4386071957},
  pages = {22563--22575},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf},
  publisher = {IEEE},
  title = {Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models},
  year = {2023}
}

@inproceedings{Liu2023AudioLDM,
  abstract = {Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.},
  author = {Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D.},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  openalex = {W4318752004},
  pages = {21450--21474},
  pdf = {https://proceedings.mlr.press/v202/liu23f/liu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},
  url = {https://proceedings.mlr.press/v202/liu23f.html},
  volume = {202},
  year = {2023}
}

@inproceedings{Pearce2023Generating,
  abstract = {Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language.},
  address = {Red Hook, NY, USA},
  author = {Hegde, Shashank and Batra, Sumeet and Zentner, K. R. and Sukhatme, Gaurav S.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  openalex = {W4378942764},
  pdf = {https://neurips.cc/paper_files/paper/2023/file/180d4373aca26bd86bf45fc50d1a709f-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  series = {NeurIPS '23},
  title = {Generating Behaviorally Diverse Policies with Latent Diffusion Models},
  url = {https://papers.nips.cc/paper_files/paper/2023/hash/180d4373aca26bd86bf45fc50d1a709f-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@article{Doucet2023Diffusion,
  author = {Doucet, Arnaud and Geffner, Tomas and Domke, Justin},
  journal = {Journal of Machine Learning Research (JMLR)},
  title = {Diffusion Models for Variational Inference},
  year = {2023}
}

@article{Croitoru2023DiffusionVision,
  abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
  author = {Florinel-Alin Croitoru and Vlad Hondru and Radu Tudor Ionescu and Mubarak Shah},
  doi = {10.1109/TPAMI.2023.3261988},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {9},
  number = {9},
  openalex = {W4360884927},
  pages = {10850--10869},
  title = {Diffusion Models in Vision: A Survey},
  url = {https://arxiv.org/abs/2209.04747},
  volume = {45},
  year = {2023}
}

@article{Watson2023RFdiffusion,
  abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods. Despite this progress, a general framework for protein design that enables solution of a wide range of challenges, including de novo binder and higher-order symmetric architectures, has yet to be described. Diffusion models have had success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of backbone geometry and sequence--structure relationships. By fine-tuning the RoseTTAFold structure prediction network on denoising tasks, the researchers obtained a model that achieves outstanding performance in unconditional and topology-constrained monomer design, oligomer, enzyme active site scaffolding, motif, therapeutic, and metal-binding design. They demonstrate the power and generality of their method, called RFdiffusion, by experimentally characterizing structures and functions of hundreds of designed assemblies and binders. The accuracy of RFdiffusion is confirmed by a cryogenic electron microscopy complex with influenza haemagglutinin that is nearly identical to the model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion can generate diverse functional proteins from simple molecular specifications.},
  author = {Joseph L. Watson and David Juergens and Nathaniel R. Bennett and Brian L. Trippe and Jason Yim and Helen E. Eisenach and Woody Ahern and Andrew J. Borst and Robert J. Ragotte and Lukas F. Milles and Basile I. M. Wicky and Nikita Hanikel and Samuel J. Pellock and Alexis Courbet and William Sheffler and Jue Wang and Preetham Venkatesh and Isaac Sappington and Susana Vázquez Torres and Anna Lauko and Valentin De Bortoli and Emile Mathieu and Sergey Ovchinnikov and Regina Barzilay and Tommi S. Jaakkola and Frank DiMaio and Minkyung Baek and David Baker},
  doi = {10.1038/s41586-023-06415-8},
  journal = {Nature},
  month = {8},
  number = {7976},
  openalex = {W4383957026},
  pages = {1089--1100},
  pdf = {https://www.nature.com/articles/s41586-023-06415-8},
  pmid = {37433327},
  title = {De Novo Design of Protein Structure and Function with RFdiffusion},
  volume = {620},
  year = {2023}
}

@article{balaji2023ediffi,
  abstract = {Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that synthesis behavior changes qualitatively throughout this process: early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then progressively split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, termed eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality. We further demonstrate the effectiveness of our approach on paint-with-words, where the proposed method helps generate substantially better images compared to existing methods.},
  archiveprefix = {arXiv},
  author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
  doi = {10.48550/arXiv.2211.01324},
  eprint = {2211.01324},
  journal = {arXiv preprint arXiv:2211.01324},
  month = {11},
  openalex = {W4308163867},
  primaryclass = {cs.CV},
  title = {eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers},
  url = {https://arxiv.org/abs/2211.01324},
  year = {2022}
}

@misc{forsgren2022riffusion,
  abstract = {Riffusion is a technique for generating music from text prompts using a fine-tuned version of Stable Diffusion that has been trained on images of audio spectrograms paired with text. The model generates spectrogram images that can be converted to audio clips, enabling real-time music generation in various styles from text prompts or image conditions. The approach represents audio as visual spectrograms and applies diffusion models to generate new musical content.},
  author = {Forsgren, Seth and Martiros, Hayk},
  howpublished = {r̆lhttps://riffusion.com/about},
  keywords = {music generation, diffusion models, spectrograms, audio synthesis, stable diffusion, real-time generation},
  note = {Technical report},
  title = {Riffusion: Stable Diffusion for Real-time Music Generation},
  url = {https://riffusion.com/about},
  year = {2022}
}

@article{ho2022imagen,
  abstract = {We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.},
  author = {Jonathan Ho and William Chan and Chitwan Saharia and Jay Whang and Ruiqi Gao and Alexey A. Gritsenko and Diederik P. Kingma and Ben Poole and Mohammad Norouzi and David J. Fleet and Tim Salimans},
  doi = {10.48550/arxiv.2210.02303},
  journal = {arXiv preprint arXiv:2210.02303},
  openalex = {W4303440777},
  pdf = {https://imagen.research.google/video/paper.pdf},
  title = {Imagen Video: High Definition Video Generation with Diffusion Models},
  url = {https://arxiv.org/abs/2210.02303},
  year = {2022}
}

@inproceedings{hoogeboom2022autoregressive,
  abstract = {We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. We apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points.},
  author = {Emiel Hoogeboom and Alexey A. Gritsenko and Jasmijn Bastings and Ben Poole and Rianne van den Berg and Tim Salimans},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4286910219},
  pdf = {https://openreview.net/pdf?id=Lm8T39vLDTE},
  title = {Autoregressive Diffusion Models},
  url = {https://openreview.net/forum?id=Lm8T39vLDTE},
  year = {2022}
}

@inproceedings{jo2022scorebased,
  abstract = {Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships.},
  address = {Baltimore, Maryland, USA},
  author = {Jaehyeong Jo and Seul Lee and Sung Ju Hwang},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  doi = {10.48550/arxiv.2202.02514},
  month = {7},
  openalex = {W4221161486},
  pages = {10362--10383},
  pdf = {https://proceedings.mlr.press/v162/jo22a/jo22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations},
  url = {https://proceedings.mlr.press/v162/jo22a.html},
  volume = {162},
  year = {2022}
}

@inproceedings{karras2022elucidating,
  abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  month = {12},
  note = {Oral},
  openalex = {W4281969232},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS '22},
  title = {Elucidating the Design Space of Diffusion-Based Generative Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{li2022diffusionlm,
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S. and Hashimoto, Tatsunori B.},
  booktitle = {Advances in Neural Information Processing Systems},
  month = {10},
  openalex = {W4281690218},
  pdf = {https://openreview.net/pdf?id=3s9IrEsjLyk},
  title = {Diffusion-LM Improves Controllable Text Generation},
  url = {https://openreview.net/forum?id=3s9IrEsjLyk},
  volume = {35},
  year = {2022}
}

@inproceedings{lu2022dpmsolver,
  abstract = {Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a 4∼16× speedup compared with previous state-of-the-art training-free samplers on various datasets.},
  author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arXiv.2206.00927},
  note = {Oral},
  openalex = {W4281661987},
  pages = {5775--5787},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf},
  title = {DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps},
  volume = {35},
  year = {2022}
}

@inproceedings{nichol2022glide,
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  editor = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  note = {Spotlight},
  openalex = {W4226125322},
  pages = {16784--16804},
  pdf = {https://proceedings.mlr.press/v162/nichol22a/nichol22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  url = {https://proceedings.mlr.press/v162/nichol22a.html},
  volume = {162},
  year = {2022}
}

@article{ramesh2022hierarchical,
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion.},
  author = {Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  doi = {10.48550/arXiv.2204.06125},
  journal = {arXiv preprint arXiv:2204.06125},
  month = {4},
  note = {DALL-E 2},
  openalex = {W4224035735},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  url = {https://arxiv.org/abs/2204.06125},
  year = {2022}
}

@inproceedings{rombach2022highresolution,
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52688.2022.01042},
  note = {Stable Diffusion - Oral},
  openalex = {W4312933868},
  pages = {10684--10695},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf},
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  url = {https://arxiv.org/abs/2112.10752},
  year = {2022}
}

@inproceedings{saharia2022photorealistic,
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L. and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arxiv.2205.11487},
  openalex = {W4281485151},
  pages = {36479--36494},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf},
  series = {NeurIPS '22},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  url = {https://arxiv.org/abs/2205.11487},
  volume = {35},
  year = {2022}
}

@inproceedings{salimans2022progressive,
  abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, and a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking many steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps.},
  author = {Tim Salimans and Jonathan Ho},
  booktitle = {The Tenth International Conference on Learning Representations},
  month = {4},
  openalex = {W4221159371},
  pdf = {https://openreview.net/pdf?id=TIdIXIpzhoI},
  title = {Progressive Distillation for Fast Sampling of Diffusion Models},
  url = {https://openreview.net/forum?id=TIdIXIpzhoI},
  year = {2022}
}

@inproceedings{xiao2022tackling,
  abstract = {A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. Specifically, we introduce denoising diffusion generative adversarial networks (denoising diffusion GANs), which model each denoising step using a conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$ imes$ faster on the CIFAR-10 dataset. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.},
  author = {Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
  booktitle = {International Conference on Learning Representations},
  month = {2},
  openalex = {W4226376398},
  pdf = {https://arxiv.org/pdf/2112.07804.pdf},
  title = {Tackling the Generative Learning Trilemma with Denoising Diffusion GANs},
  url = {https://openreview.net/forum?id=JprM0p-q0Co},
  year = {2022}
}

@inproceedings{Bao2022Analytic-DPM,
  abstract = {Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up.},
  author = {Fan Bao and Chongxuan Li and Jun Zhu and Bo Zhang},
  booktitle = {International Conference on Learning Representations},
  note = {ICLR 2022 Outstanding Paper Award},
  openalex = {W4221139906},
  pdf = {https://openreview.net/pdf?id=0xiJLKH-ufZ},
  title = {Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},
  url = {https://openreview.net/forum?id=0xiJLKH-ufZ},
  year = {2022}
}

@inproceedings{Salimans2022Progressive,
  abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.},
  author = {Tim Salimans and Jonathan Ho},
  booktitle = {The Tenth International Conference on Learning Representations (ICLR 2022)},
  month = {4},
  openalex = {W4221159371},
  pdf = {https://openreview.net/pdf?id=TIdIXIpzhoI},
  publisher = {OpenReview.net},
  title = {Progressive Distillation for Fast Sampling of Diffusion Models},
  url = {https://openreview.net/forum?id=TIdIXIpzhoI},
  year = {2022}
}

@inproceedings{Lu2022DPM-Solver,
  abstract = {Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4∼ 16 imes$ speedup compared with previous state-of-the-art training-free samplers on various datasets.},
  author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4281661987},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'22},
  title = {DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{Lee2022Convergence,
  abstract = {Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $∇ łn p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using annealing to obtain a warm start at each step. Moreover, we show that a predictor-corrector algorithm gives better convergence than using either portion alone.},
  author = {Holden Lee and Jianfeng Lu and Yixin Tan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  openalex = {W4320168823},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8ff87c96935244b63503f542472462b3-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Convergence for Score-Based Generative Modeling with Polynomial Complexity},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8ff87c96935244b63503f542472462b3-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{Hoogeboom2022Equivariant,
  abstract = {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.},
  author = {Hoogeboom, Emiel and Satorras, Víctor García and Vignac, Clément and Welling, Max},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  note = {(Oral)},
  openalex = {W4226070784},
  pages = {8867--8887},
  pdf = {https://proceedings.mlr.press/v162/hoogeboom22a/hoogeboom22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Equivariant Diffusion for Molecule Generation in 3D},
  volume = {162},
  year = {2022}
}

@inproceedings{DeBortoli2022RiemannianScore,
  abstract = {Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a ``noising'' stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a ``denoising'' process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data.},
  author = {De Bortoli, Valentin and Mathieu, Emile and Hutchinson, Michael and Thornton, James and Teh, Yee Whye and Doucet, Arnaud},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  openalex = {W4317539807},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/105112d52254f86d5854f3da734a52b4-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Riemannian Score-Based Generative Modelling},
  url = {https://proceedings.neurips.cc/paper/2022/hash/105112d52254f86d5854f3da734a52b4-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{Huang2022RiemannianDiffusion,
  abstract = {Diffusion models are recent state-of-the-art methods for image generation and likelihood estimation. In this work, we generalize continuous-time diffusion models to arbitrary Riemannian manifolds and derive a variational framework for likelihood estimation. Computationally, we propose new methods for computing the Riemannian divergence which is needed for likelihood estimation. Moreover, in generalizing the Euclidean case, we prove that maximizing this variational lower-bound is equivalent to Riemannian score matching.},
  author = {Chin-Wei Huang and Milad Aghajohari and Avishek Joey Bose and Prakash Panangaden and Aaron Courville},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4298725717},
  pages = {2750--2761},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/105112d52254f86d5854f3da734a52b4-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Riemannian Diffusion Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/123d3e814e257e0781e5d328232ead9b-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{Karras2022Elucidating,
  abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  openalex = {W4281969232},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Elucidating the Design Space of Diffusion-Based Generative Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{Rombach2022High-Resolution,
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  author = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  openalex = {W4312933868},
  pages = {10674--10685},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  year = {2022}
}

@inproceedings{Dockhorn2022Score-Based,
  abstract = {Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered 'velocities' that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler--Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.},
  author = {Tim Dockhorn and Arash Vahdat and Karsten Kreis},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4226158195},
  pdf = {https://openreview.net/pdf?id=CzceR82CYc},
  title = {Score-Based Generative Modeling with Critically-Damped Langevin Diffusion},
  url = {https://openreview.net/forum?id=CzceR82CYc},
  year = {2022}
}

@inproceedings{Shi2022Conditional,
  abstract = {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schrödinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks.},
  author = {Yuyang Shi and Valentin De Bortoli and George Deligiannidis and Arnaud Doucet},
  booktitle = {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  month = {8},
  note = {arXiv:2202.13460},
  openalex = {W4226020576},
  pages = {1792--1802},
  pdf = {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conditional Simulation Using Diffusion Schrödinger Bridges},
  url = {https://proceedings.mlr.press/v180/shi22a.html},
  volume = {180},
  year = {2022}
}

@article{Lu2022DPM-Solver++,
  abstract = {Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially recent large-scale text-to-image generation applications. However, DPMs generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample, making them computationally expensive. Recent works accelerate the sampling of DPMs by proposing training-free fast samplers, which directly approximate the solution of the corresponding diffusion ordinary differential equation (ODE) without additional training. In this work, we propose a high-order solver for the guided sampling of DPMs, named DPM-Solver++, which improves the convergence order as well as the stability. DPM-Solver++ can generate high-quality samples within only 15 to 20 steps for guided sampling by pixel-space and latent-space DPMs, achieving 4.70 FID with 15 function evaluations and 2.87 FID with 20 function evaluations on guided sampling of 50k samples from CIFAR-10. We further propose a new adaptive step-size scheduler which can automatically adjust the step size according to local truncation errors, improving the sampling quality with fewer steps.},
  archiveprefix = {arXiv},
  author = {Cheng Lu and Yuhao Zhou and Fan Bao and Jianfei Chen and Chongxuan Li and Jun Zhu},
  eprint = {2211.01095},
  journal = {arXiv preprint arXiv:2211.01095},
  month = {11},
  openalex = {W4308167501},
  pdf = {https://arxiv.org/pdf/2211.01095.pdf},
  primaryclass = {cs.LG},
  title = {DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models},
  url = {https://arxiv.org/abs/2211.01095},
  year = {2022}
}

@inproceedings{chen2021wavegrad,
  abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We find that it can generate high fidelity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
  author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations (ICLR)},
  month = {5},
  openalex = {W3123097577},
  pdf = {https://openreview.net/pdf?id=NsMLjcFaO8O},
  title = {WaveGrad: Estimating Gradients for Waveform Generation},
  url = {https://openreview.net/forum?id=NsMLjcFaO8O},
  year = {2021}
}

@inproceedings{dhariwal2021diffusion,
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling for high-resolution image synthesis, and we are able to further improve our FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  note = {Oral presentation},
  openalex = {W3162926177},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Diffusion Models Beat GANs on Image Synthesis},
  url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{dosovitskiy2021image,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {International Conference on Learning Representations},
  openalex = {W3119786062},
  pdf = {https://openreview.net/pdf?id=YicbFdNTTy},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  year = {2021}
}

@inproceedings{ho2021classifierfree,
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  author = {Ho, Jonathan and Salimans, Tim},
  booktitle = {NeurIPS Workshop on Deep Generative Models and Downstream Applications},
  openalex = {W4288099666},
  title = {Classifier-Free Diffusion Guidance},
  url = {https://arxiv.org/abs/2207.12598},
  year = {2021}
}

@inproceedings{kingma2021variational,
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
  author = {Diederik P. Kingma and Tim Salimans and Ben Poole and Jonathan Ho},
  booktitle = {Advances in Neural Information Processing Systems 34},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  month = {12},
  note = {Spotlight},
  openalex = {W3177150392},
  pages = {21696--21707},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Variational Diffusion Models},
  url = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
  year = {2021}
}

@inproceedings{kong2021diffwave,
  abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
  author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  booktitle = {International Conference on Learning Representations},
  openalex = {W3129651364},
  pdf = {https://openreview.net/pdf?id=a-xFK8Ymz5J},
  title = {DiffWave: A Versatile Diffusion Model for Audio Synthesis},
  url = {https://openreview.net/forum?id=a-xFK8Ymz5J},
  year = {2021}
}

@inproceedings{nichol2021improved,
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable.},
  author = {Alexander Quinn Nichol and Prafulla Dhariwal},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  openalex = {W3168053944},
  pages = {8162--8171},
  pdf = {https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Denoising Diffusion Probabilistic Models},
  volume = {139},
  year = {2021}
}

@inproceedings{radford2021learning,
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. We create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language--Image Pre-training, is an efficient method of learning from natural language supervision. CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N² - N incorrect pairings.},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Meila, Marina and Zhang, Tong},
  month = {7},
  note = {CLIP - Oral},
  openalex = {W3166396011},
  pages = {8748--8763},
  pdf = {https://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  volume = {139},
  year = {2021}
}

@inproceedings{ramesh2021zeroshot,
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  author = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Marina Meila and Tong Zhang},
  note = {DALL-E - Spotlight},
  openalex = {W3129576130},
  pages = {8821--8831},
  pdf = {https://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Zero-Shot Text-to-Image Generation},
  url = {https://proceedings.mlr.press/v139/ramesh21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{song2021denoising,
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs.},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  doi = {10.48550/arxiv.2010.02502},
  openalex = {W3121370741},
  pdf = {https://openreview.net/pdf?id=St1giarCHLP},
  title = {Denoising Diffusion Implicit Models},
  url = {https://openreview.net/forum?id=St1giarCHLP},
  year = {2021}
}

@inproceedings{song2021scorebased,
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024×1024 images for the first time from a score-based generative model.},
  author = {Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle = {9th International Conference on Learning Representations},
  month = {5},
  note = {Outstanding Paper Award},
  openalex = {W3121345697},
  pdf = {https://openreview.net/pdf?id=PxTIG12RRHS},
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  year = {2021}
}

@inproceedings{Nichol2021Improved,
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  author = {Alexander Quinn Nichol and Prafulla Dhariwal},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Marina Meila and Tong Zhang},
  openalex = {W3168053944},
  pages = {8162--8171},
  pdf = {https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Denoising Diffusion Probabilistic Models},
  url = {https://proceedings.mlr.press/v139/nichol21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{Song2021Score-Based,
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024×1024 images for the first time from a score-based generative model.},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  note = {Oral, Outstanding Paper Award},
  openalex = {W3121345697},
  pdf = {https://openreview.net/pdf/ef0eadbe07115b0853e964f17aa09d811cd490f1.pdf},
  publisher = {OpenReview.net},
  series = {ICLR 2021},
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  year = {2021}
}

@inproceedings{Kingma2021Variational,
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm.},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
  openalex = {W3177150392},
  pages = {21696--21707},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Variational Diffusion Models},
  url = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{Song2021Maximum,
  abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32×32 without any data augmentation. Our results can be viewed as a generalization of both the de Bruijn identity in information theory and the evidence lower bound in variational inference.},
  author = {Yang Song and Conor Durkan and Iain Murray and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W3208015123},
  pdf = {https://neurips.cc/paper_files/paper/2021/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Maximum Likelihood Training of Score-Based Diffusion Models},
  volume = {34},
  year = {2021}
}

@inproceedings{Song2021Denoising,
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  author = {Jiaming Song and Chenlin Meng and Stefano Ermon},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  note = {arXiv preprint arXiv:2010.02502},
  pdf = {https://openreview.net/pdf?id=St1giarCHLP},
  publisher = {OpenReview.net},
  title = {Denoising Diffusion Implicit Models},
  url = {https://openreview.net/forum?id=St1giarCHLP},
  year = {2021}
}

@inproceedings{Dhariwal2021Diffusion,
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.},
  address = {Red Hook, NY, USA},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W3162926177},
  pages = {8780--8794},
  pdf = {https://proceedings.nips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS},
  title = {Diffusion Models Beat GANs on Image Synthesis},
  url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{Austin2021Structured,
  abstract = {Denoising diffusion probabilistic models (DDPMs) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al., by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  address = {Red Hook, NY, USA},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS 2021)},
  editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
  openalex = {W4287083626},
  pdf = {https://papers.neurips.cc/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
  url = {https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html},
  year = {2021}
}

@inproceedings{Vahdat2021Score-based,
  abstract = {Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset.},
  author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  openalex = {W4287121833},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Score-based Generative Modeling in Latent Space},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
  volume = {34},
  year = {2021}
}

@inproceedings{ho2020denoising,
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W3036167779},
  pages = {6840--6851},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Denoising Diffusion Probabilistic Models},
  volume = {33},
  year = {2020}
}

@inproceedings{nijkamp2020anatomy,
  abstract = {This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling. On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. On the other hand, we show that ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. This study suggests that MCMC-based ML provides a simple and general approach to learning energy-based models, without many of the bells-and-whistles used in recent studies.},
  author = {Erik Nijkamp and Mitch Hill and Tian Han and Song-Chun Zhu and Ying Nian Wu},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v34i04.5973},
  month = {4},
  openalex = {W2998462233},
  pages = {5272--5280},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/download/5973/5829},
  title = {On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models},
  volume = {34},
  year = {2020}
}

@inproceedings{Ho2020Denoising,
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by nonequilibrium thermodynamics. Our best results are obtained training on a weighted variational bound designed according to a novel connection between diffusion and denoising score matching with Langevin dynamics. These models naturally admit progressive lossy decompression that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and state-of-the-art FID of 3.17. On 256x256 LSUN, we sample similar to ProgressiveGAN. Implementation is available at https://github.com/hojonathanho/diffusion.},
  author = {Jonathan Ho and Ajay N. Jain and Pieter Abbeel},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W3036167779},
  pages = {6840--6851},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Denoising Diffusion Probabilistic Models},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{song2019generative,
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels.},
  author = {Yang Song and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Oral},
  openalex = {W2959300817},
  pages = {11895--11907},
  pdf = {https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  volume = {32},
  year = {2019}
}

@inproceedings{Song2019Generative,
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model evaluation.},
  author = {Yang Song and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alché-Buc and E. Fox and R. Garnett},
  openalex = {W2959300817},
  pages = {11918--11930},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
  volume = {32},
  year = {2019}
}

@inproceedings{chen2018neural,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed.},
  address = {Red Hook, NY, USA},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Best Paper},
  openalex = {W2963755523},
  pages = {6572--6583},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Neural Ordinary Differential Equations},
  url = {https://arxiv.org/pdf/1806.07366},
  volume = {31},
  year = {2018}
}

@inproceedings{kingma2018glow,
  abstract = {Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  openalex = {W2963139417},
  pages = {10215--10224},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
  volume = {31},
  year = {2018}
}

@inproceedings{zhang2018lpips,
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Here, we introduce a new dataset of human perceptual similarity judgments. We collect data for traditional metrics as well as for several variants of deep network-based metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2018.00068},
  month = {6},
  openalex = {W2962785568},
  pages = {586--595},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf},
  publisher = {IEEE},
  series = {CVPR},
  title = {The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  year = {2018}
}

@article{finn2016connection,
  abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
  author = {Chelsea Finn and Paul F. Christiano and Pieter Abbeel and Sergey Levine},
  doi = {10.48550/arXiv.1611.03852},
  journal = {arXiv preprint arXiv:1611.03852},
  openalex = {W2566467060},
  pdf = {https://arxiv.org/pdf/1611.03852.pdf},
  title = {A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models},
  year = {2016}
}

@inproceedings{sohl2015deep,
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model.},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  openalex = {W2129069237},
  pages = {2256--2265},
  pdf = {https://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  volume = {37},
  year = {2015}
}

@inproceedings{Sohl-Dickstein2015Deep,
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the model.},
  address = {Lille, France},
  author = {Jascha Sohl-Dickstein and Eric Weiss and Niru Maheswaranathan and Surya Ganguli},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  editor = {Francis Bach and David Blei},
  openalex = {W2962736171},
  pages = {2256--2265},
  pdf = {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  volume = {37},
  year = {2015}
}
