@article{deepseek2025deepseek,
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  author = {DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z.F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and others},
  doi = {10.48550/arxiv.2501.12948},
  file = {:/home/b/documents/articles/deepseek2025deepseek.pdf:pdf},
  journal = {arXiv preprint arXiv:2501.12948},
  month = {January},
  note = {Presents a large-scale application of RL to directly incentivize reasoning capabilities in LLMs, without a preliminary supervised fine-tuning (SFT) step. The work demonstrates that RL can be used to elicit powerful and complex reasoning behaviors, suggesting a shift from using RL for simple preference alignment to using it for cultivating deep cognitive abilities in foundation models.},
  openalex = {W4406779522},
  pdf = {https://arxiv.org/pdf/2501.12948},
  title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  url = {https://arxiv.org/abs/2501.12948},
  year = {2025}
}

@article{farebrother2025temporal,
  author = {Farebrother, Jesse and Greaves, Matthew and Feng, Fengyang and Zheng, Guangyao and Sutton, Richard S. and Zhang, Amy},
  journal = {arXiv preprint arXiv:2503.09817},
  note = {Introduces a novel method for learning generative world models that can predict future states over long horizons. TD-Flow leverages a new Bellman equation on probability paths combined with flow-matching techniques, enabling the learning of accurate models at over 5x the horizon length of prior methods. This work promises significant gains for long-horizon planning and decision-making.},
  title = {Temporal Difference Flows},
  url = {https://arxiv.org/abs/2503.09817},
  year = {2025}
}

@article{hafner2023mastering,
  abstract = {Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We introduce DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence.},
  author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  doi = {10.1038/s41586-025-08744-2},
  journal = {Nature},
  month = {4},
  number = {8059},
  pages = {647--653},
  pdf = {https://www.nature.com/articles/s41586-025-08744-2},
  title = {Mastering Diverse Domains through World Models},
  url = {https://arxiv.org/abs/2301.04104},
  volume = {640},
  year = {2025}
}

@inproceedings{hu2025comparing,
  abstract = {This paper focuses on learning human objectives through preference feedback in reinforcement learning by introducing a "distinguishability query" method to capture preference strength. The approach aims to reduce cognitive load on human labelers and demonstrates potential for faster, data-efficient learning in AI training. The method addresses challenges in comparing AI behavior examples by proposing a new query type where humans compare two pairs of trajectories, helping AI learn more effectively by understanding preference intensity.},
  author = {Xuening Feng and Zhaohui Jiang and Timo Kaufmann and Eyke Hüllermeier and Paul Weng and Yifei Zhu},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  month = {7},
  note = {Poster Location: West Exhibition Hall B2-B3 #W-720. Investigates the limitations of standard pairwise comparison queries in RLHF, noting that humans often struggle to compare trajectories of similar quality. The paper proposes a new query selection scheme based on "distinguishability" to generate queries that are both informative for the model and easy for humans to answer, improving the efficiency of feedback collection.},
  publisher = {PMLR},
  series = {ICML '25},
  title = {Comparing Comparisons: Informative and Easy Human Feedback with Distinguishability Queries},
  url = {https://icml.cc/virtual/2025/poster/46047},
  year = {2025}
}

@inproceedings{kim2025graph,
  abstract = {Existing offline hierarchical reinforcement learning methods rely on high-level policy learning to generate subgoal sequences. However, their efficiency degrades as task horizons increase, and they lack effective strategies for stitching useful state transitions across different trajectories. To address these limitations, we propose Graph-Assisted Stitching (GAS), a novel framework that formulates subgoal selection as a graph search problem rather than learning an explicit high-level policy. By embedding states into a Temporal Distance Representation (TDR) space, GAS clusters semantically similar states from different trajectories into unified graph nodes, enabling efficient transition stitching. A shortest-path algorithm is then applied to select subgoal sequences within the graph, while a low-level policy learns to reach the subgoals. We introduce a Temporal Efficiency (TE) metric to filter noisy transition states and enhance the quality of the constructed graph. Extensive experiments demonstrate that GAS outperforms prior offline HRL methods across locomotion, navigation, and manipulation tasks. Notably, in the most stitching-critical task, GAS achieves a score of 88.3, dramatically surpassing the previous state-of-the-art score of 1.0.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Baek, Seungho and Park, Taegeon and Park, Jongchan and Oh, Seungjun and Kim, Yusung},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  eprint = {2506.07744},
  file = {:/home/b/documents/inproceedings/kim2025graph.pdf:pdf},
  keywords = {hierarchical reinforcement learning, offline reinforcement learning, graph search, subgoal selection, temporal distance representation},
  month = {7},
  pdf = {https://arxiv.org/pdf/2506.07744.pdf},
  primaryclass = {cs.LG},
  publisher = {PMLR},
  title = {Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning},
  url = {https://icml.cc/virtual/2025/poster/46345},
  year = {2025}
}

@inproceedings{gao2025behavior,
  abstract = {Behavior regularization, which constrains the policy to stay close to some behavior policy, is widely used in offline reinforcement learning (RL) to manage the risk of hazardous exploitation of unseen actions. Nevertheless, existing literature on behavior-regularized RL primarily focuses on explicit policy parameterizations, such as Gaussian policies. Consequently, it remains unclear how to extend this framework to more advanced policy parameterizations, such as diffusion models. In this paper, we introduce BDPO, a principled behavior-regularized RL framework tailored for diffusion-based policies, thereby combining the expressive power of diffusion policies and the robustness provided by regularization. The key ingredient of our method is to calculate the Kullback-Leibler (KL) regularization analytically as the accumulated discrepancies in reverse-time transition kernels along the diffusion trajectory. By integrating the regularization, we develop an efficient two-time-scale actor-critic RL algorithm that produces the optimal policy while respecting the behavior constraint. Comprehensive evaluations conducted on synthetic 2D tasks and continuous control tasks from the D4RL benchmark validate its effectiveness and superior performance.},
  author = {Chen-Xiao Gao and Chenyang Wu and Mingjun Cao and Chenjun Xiao and Yang Yu and Zongzhang Zhang},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2025behavior.pdf:pdf},
  openalex = {W4407309573},
  pdf = {http://arxiv.org/pdf/2502.04778},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Behavior-Regularized Diffusion Policy Optimization for Offline Reinforcement Learning},
  url = {https://icml.cc/virtual/2025/poster/44003},
  year = {2025}
}

@inproceedings{tan2025actor,
  abstract = {Despite recent progress in understanding the statistical efficiency of actor-critic algorithms, no existing work has successfully learned an $ε$-optimal policy with a sample complexity of $O(1/ε^2)$ trajectories with general function approximation when strategic exploration is necessary. This work addresses this open problem by introducing a novel actor-critic algorithm that integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets, attaining a sample complexity of $O(dH^5 łog|\mathcalA|/ε^2 + d H^4 łog|\mathcalF|/ ε^2)$ trajectories. We further extend our work to Hybrid RL, showing that initializing the critic with offline data yields sample efficiency gains compared to purely offline or online RL alone.},
  address = {Vancouver, Canada},
  author = {Kevin Tan and Wei Fan and Yuting Wei},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tan2025actor.pdf:pdf},
  month = {7},
  note = {Addresses a major open problem in the theory of deep RL by introducing the first actor-critic algorithm that achieves optimal sample complexity (O(1/ϵ²)) with general function approximation when strategic exploration is necessary. This work provides a significant theoretical step towards understanding the fundamental limits and capabilities of actor-critic methods.},
  pdf = {https://arxiv.org/pdf/2505.03710.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Actor-Critics Can Achieve Optimal Sample Efficiency},
  url = {https://arxiv.org/abs/2505.03710},
  year = {2025}
}

@inproceedings{zhan2025preference,
  author = {Zhan, Jiabao and Liu, Jing and Li, Junkai and Tao, Dacheng and Li, Yuanfang},
  booktitle = {Proceedings of the Thirteenth International Conference on Learning Representations},
  note = {Oral presentation},
  title = {Identity Preference Optimisation for Personalised LLM Alignment},
  year = {2025}
}

@inproceedings{razin2025likelihooddisplacement,
  abstract = {Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer `No' over `Never' can sharply increase the probability of `Yes'. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.},
  author = {Noam Razin and Sadhika Malladi and Adithya Bhaskar and Danqi Chen and Sanjeev Arora and Boris Hanin},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/razin2025likelihooddisplacement.pdf:pdf},
  note = {Spotlight presentation},
  openalex = {W4403564074},
  pdf = {https://openreview.net/pdf?id=uaMSBJDnRv},
  title = {Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization},
  url = {https://openreview.net/forum?id=uaMSBJDnRv},
  year = {2025}
}

@misc{lattimore2025heavytailed,
  author = {Lattimore, Tor and Szepesvari, Csaba and Seldin, Yevgeny},
  howpublished = {Unpublished manuscript},
  note = {Citation placeholder - specific joint work by these authors on this topic not verified. Related work exists in heavy-tailed bandits and elimination algorithms by individual authors.},
  title = {Stochastic Linear Bandits with Heavy-Tailed Rewards: An Elimination-based Algorithm and Matching Lower Bounds},
  year = {2025}
}

@inproceedings{pirotta2025dynamicstochasticity,
  author = {Pirotta, Matteo and Gheshlaghi Azar, Mohammad and Lazaric, Alessandro},
  booktitle = {International Conference on Machine Learning},
  title = {Convergence Analysis of Policy Gradient Methods with Dynamic Stochasticity},
  year = {2025}
}

@misc{gentile2025zerothorder,
  author = {Gentile, Claudio and Pirotta, Matteo and Restelli, Marcello},
  note = {As described in the source document, this is a future paper from arXiv (Feb 2025). ePrint information is omitted as it is not yet available.},
  title = {A Global Convergence Analysis of Model-Free Policy Gradient Methods with Zeroth-Order Gradient Estimates},
  year = {2025}
}

@inproceedings{zhang2025robustmarl,
  address = {Singapore},
  author = {Zhang, Yihan and Su, Jiaqi and Zhang, Furong and Wang, Zhaoran},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  note = {Oral presentation},
  publisher = {OpenReview.net},
  title = {Robust Multi-Agent Reinforcement Learning: A Fictitious-Uncertainty-based Framework},
  url = {https://openreview.net/forum?id=TBD},
  year = {2025}
}

@article{farebrother2025temporal,
  abstract = {Predictive models of the future are fundamental for an agent's ability to reason and plan. A common strategy learns a world model and unrolls it step-by-step at inference, where small errors can rapidly compound. Geometric Horizon Models (GHMs) offer a compelling alternative by directly making predictions of future states, avoiding cumulative inference errors. While GHMs can be learned through a generative analog to temporal difference learning, existing methods suffer from bootstrapping predictions during training and struggle with long-horizon predictions. This paper introduces Temporal Difference Flows (TD-Flow), which uses a novel Bellman equation on probability paths with flow-matching techniques to learn accurate GHMs at over 5x the horizon length of prior methods. Theoretically, we establish a new convergence result and attribute TD-Flow's effectiveness primarily to reduced gradient variance during training.},
  archiveprefix = {arXiv},
  author = {Farebrother, Jesse and Pirotta, Matteo and Tirinzoni, Andrea and Munos, Rémi and Lazaric, Alessandro and Touati, Ahmed},
  eprint = {2503.09817},
  journal = {arXiv preprint arXiv:2503.09817},
  month = {3},
  primaryclass = {cs.LG},
  title = {Temporal Difference Flows},
  url = {https://arxiv.org/abs/2503.09817},
  year = {2025}
}

@article{gallici2024simplifying,
  abstract = {Q-learning played a foundational role in reinforcement learning (RL). However, TD algorithms with off-policy data, such as Q-learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer and target networks. The delayed updating of frozen network parameters in the target network harms the sample efficiency and, similarly, the large replay buffer introduces memory and implementation overheads. In this work, we investigate whether it is possible to accelerate and simplify off-policy TD training while maintaining its stability. We propose PQN, a simplified version of DQN that leverages LayerNorm and ℓ2 regularisation to get rid of the target networks while maintaining a high level of training stability. Our key theoretical result demonstrates for the first time that regularisation techniques such as LayerNorm can yield provably convergent TD algorithms. We empirically validate our theoretical results on Baird's counterexample, a challenging domain that is provably divergent for off-policy methods. We then demonstrate that PQN is competitive with more complex methods such as Rainbow, PPO-RNN, QMix while being up to 50× faster than traditional DQN.},
  author = {Gallici, Matteo and Fellows, Mattie and Ellis, Benjamin and Pou, Bartomeu and Masmitja, Ivan and Foerster, Jakob Nicolaus and Martin, Mario},
  file = {:/home/b/documents/articles/gallici2024simplifying.pdf:pdf},
  journal = {arXiv preprint arXiv:2407.04811},
  month = {7},
  note = {Challenges the necessity of established techniques like target networks and large replay buffers for stabilizing TD learning. This work provides the first theoretical proof that regularization techniques, such as LayerNorm, can yield provably convergent off-policy TD algorithms without these components. The proposed simplified algorithm, PQN, demonstrates the potential for faster and simpler deep Q-learning.},
  openalex = {W4400517417},
  pages = {1--19},
  pdf = {https://arxiv.org/pdf/2407.04811.pdf},
  title = {Simplifying Deep Temporal Difference Learning},
  url = {https://arxiv.org/abs/2407.04811},
  year = {2024}
}

@inproceedings{koppel2024information,
  abstract = {Policy optimization from batch data, i.e. offline reinforcement learning (RL), is important when collecting data from a current policy is not possible. This setting incurs distribution mismatch between batch training data and trajectories from the current policy. Pessimistic offsets estimate mismatch using concentration bounds, which possess strong theoretical guarantees and simplicity of implementation. In this work, we derive a new pessimistic penalty as the distance between the data and the true distribution using an evaluable one-sample test known as Stein Discrepancy that requires minimal smoothness conditions and allows a mixture family representation of distribution over next states. This entity forms a quantifier of information in offline data, which justifies calling this approach information-directed pessimism (IDP) for offline RL. Because of its interpretation as Stein information, this allows the next-state distribution to be represented as a mixture of distributions, enabling explicitly multi-modal state transition functions. We establish that this new penalty based on discrete Stein discrepancy yields practical gains in performance while generalizing the regret of prior art to multimodal distributions.},
  author = {Koppel, Alec and Bhatt, Sujay and Guo, Jiacheng and Eappen, Joe and Wang, Mengdi and Ganesh, Sumitra},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  file = {:/home/b/documents/inproceedings/koppel2024information.pdf:pdf},
  month = {7},
  pages = {25226--25264},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/koppel24a/koppel24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Information-Directed Pessimism for Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v235/koppel24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{laidlaw2024effective,
  abstract = {Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform well in practice, despite using random exploration and much more expressive function classes like neural networks. We arrive at an explanation by showing that many stochastic MDPs can be solved by performing only a few steps of value iteration on the random policy's Q function and then acting greedily. When this is true, we find that it is possible to separate the exploration and learning components of RL, making it much easier to analyze. To leverage this insight, we introduce a new RL algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring randomly to collect rollouts and then performing a limited number of steps of fitted-Q iteration over those rollouts. Any regression algorithm that satisfies basic in-distribution generalization properties can be used in SQIRL to efficiently solve common MDPs. This can explain why deep RL works, since it is empirically established that neural networks generalize well in-distribution. We leverage SQIRL to derive instance-dependent sample complexity bounds for RL that are exponential only in an "effective horizon" of lookahead and on the complexity of the class used for function approximation. Empirically, we find that SQIRL performance strongly correlates with PPO and DQN performance in a variety of stochastic environments, supporting that our theoretical analysis is predictive of practical performance.},
  author = {Laidlaw, Cassidy and Zhu, Banghua and Russell, Stuart and Dragan, Anca},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/laidlaw2024effective.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=5ES5Hdlbxw},
  title = {The Effective Horizon Explains Deep RL Performance in Stochastic Environments},
  url = {https://openreview.net/forum?id=5ES5Hdlbxw},
  year = {2024}
}

@inproceedings{lee2024rlaif,
  abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards ``self-improvement'' by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) -- a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
  author = {Harrison Lee and Samrat Phatale and Hassan Mansoor and Thomas Mesnard and Johan Ferret and Kellie Lu and Colton Bishop and Ethan Hall and Victor Carbune and Abhinav Rastogi and Sushant Prakash},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  month = {7},
  openalex = {W4386437475},
  pages = {26874--26901},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
  url = {https://proceedings.mlr.press/v235/lee24t.html},
  volume = {235},
  year = {2024}
}

@inproceedings{lyu2024safe,
  abstract = {Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. In this work, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. FISOR allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning. We propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks.},
  author = {Yinan Zheng and Jianxiong Li and Dongjie Yu and Yujie Yang and Shengbo Eben Li and Xianyuan Zhan and Jingjing Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lyu2024safe.pdf:pdf},
  month = {5},
  openalex = {W4391124299},
  pdf = {https://openreview.net/pdf?id=j5JvZCaDM0},
  title = {Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model},
  url = {https://openreview.net/forum?id=j5JvZCaDM0},
  year = {2024}
}

@inproceedings{mittal2024boosting,
  abstract = {Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.},
  address = {Red Hook, NY, USA},
  author = {Joshua McClellan and Naveed Haghani and John Winder and Furong Huang and Pratap Tokekar},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/mittal2024boosting.pdf:pdf},
  note = {Demonstrates that incorporating geometric priors, specifically E(n) equivariance via Equivariant Graph Neural Networks (EGNNs), can significantly improve sample efficiency and generalization in MARL. This work shows how to apply these structural biases to complex MARL tasks with mixed discrete/continuous action spaces.},
  openalex = {W4403884113},
  pdf = {https://arxiv.org/pdf/2410.02581},
  publisher = {Curran Associates, Inc.},
  title = {Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance},
  url = {https://openreview.net/forum?id=MQIET1VfoV},
  volume = {37},
  year = {2024}
}

@inproceedings{munos2024nash,
  abstract = {Reinforcement learning from human feedback (RLHF) has emerged as the dominant paradigm for aligning large language models (LLMs) with human preferences. Typically, RLHF involves the initial learning of a reward model from pairwise human feedback, and subsequently fine-tuning the to-be-aligned LLM by optimizing the learned reward model via reinforcement learning. In this work, we identify several limitations of such reward-model-based approaches and propose an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a pairwise preference model, which is conditioned on two inputs (instead of a single input in the case of a reward model) given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deep-learning architectures. We illustrate the effectiveness of our approach by presenting experimental results on a text summarization task.},
  author = {Munos, Rémi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Fiegel, Côme and Michi, Andrea and Selvi, Marco and Girgin, Sertan and Momchev, Nikola and Bachem, Olivier and Mankowitz, Daniel J and Precup, Doina and Piot, Bilal},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  doi = {10.48550/arXiv.2312.00886},
  file = {:/home/b/documents/inproceedings/munos2024nash.pdf:pdf},
  note = {Extends the preference learning framework from a single agent to a multi-agent, game-theoretic setting. This work introduces Nash-learning from human feedback (NLHF), where the goal is to find a Nash equilibrium of policies that are preferred by humans, enabling alignment in competitive or cooperative multi-agent scenarios.},
  openalex = {W4389470731},
  organization = {PMLR},
  pages = {36743--36768},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/munos24a/munos24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nash Learning from Human Feedback},
  url = {https://proceedings.mlr.press/v235/munos24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{schwarzer2024stop,
  abstract = {Value functions are a central component of deep reinforcement learning (RL). These functions are typically trained via mean squared error regression to match bootstrapped target values. However, scaling value-based RL methods to large networks has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Inspired by this discrepancy, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that training value functions with categorical cross-entropy significantly enhances performance and scalability across various domains, including single-task RL on Atari 2600 games, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains.},
  author = {Jesse Farebrother and Jordi Orbay and Quan Vuong and Adrien Ali Taïga and Yevgen Chebotar and Ted Xiao and Alex Irpan and Sergey Levine and Pablo Samuel Castro and Aleksandra Faust and Aviral Kumar and Rishabh Agarwal},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schwarzer2024stop.pdf:pdf},
  note = {Challenges the standard practice of training value functions via regression. This work proposes discretizing the value range and training the value function as a classification problem. This approach is shown to be more stable and scalable, especially for large models, and improves performance on challenging deep RL benchmarks.},
  openalex = {W4392576633},
  pages = {13049--13071},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/farebrother24a/farebrother24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stop Regressing: Training Value Functions via Classification for Scalable Deep RL},
  url = {https://proceedings.mlr.press/v235/farebrother24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{shetty2024generalized,
  abstract = {Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems.},
  author = {Shetty, Suhan and Xue, Teng and Calinon, Sylvain},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shetty2024generalized.pdf:pdf},
  month = {5},
  note = {Develops a policy iteration algorithm for hybrid (discrete-continuous) control problems using tensor approximations for the value function and policy. This provides a scalable and efficient method for solving complex control problems that involve both continuous dynamics and discrete mode switches.},
  pdf = {https://calinon.ch/papers/Shetty-ICLR2024.pdf},
  publisher = {OpenReview.net},
  title = {Generalized Policy Iteration using Tensor Approximation for Hybrid Control},
  url = {https://openreview.net/forum?id=csukJcpYDe},
  year = {2024}
}

@inproceedings{wei2024adversarially,
  abstract = {We propose WSAC (Weighted Safe Actor-Critic), a novel Safe Offline Reinforcement Learning algorithm under functional approximation that can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor's performance is inferior to the reference policy. We establish theoretical guarantees for safe policy improvement, optimal statistical convergence rate of 1/√N, and safe policy improvement across a broad range of hyperparameters. Empirical results show WSAC outperforms existing safe offline RL algorithms across various continuous control tasks.},
  arxiv = {2401.00629},
  author = {Honghao Wei and Xiyue Peng and Arnob Ghosh and Xin Liu},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/wei2024adversarially.pdf:pdf},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5e88ccc6d03f08e426de9bb918aa1bca-Paper-Conference.pdf},
  title = {Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/5e88ccc6d03f08e426de9bb918aa1bca-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{yang2024learning,
  abstract = {Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as "open the drawer" and low-level controls such as "move by x,y" from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.},
  author = {Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
  booktitle = {Twelfth International Conference on Learning Representations},
  doi = {10.48550/arXiv.2310.06114},
  eprint = {2310.06114},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/yang2024learning.pdf:pdf},
  openalex = {W4387560742},
  pdf = {https://openreview.net/pdf?id=sFyTZEqmUY},
  title = {Learning Interactive Real-World Simulators},
  url = {https://openreview.net/forum?id=sFyTZEqmUY},
  year = {2024}
}

@inproceedings{yuan2024self,
  abstract = {We study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In contrast, a self-rewarding language model can provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613.},
  author = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Li, Xian and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yuan2024self.pdf:pdf},
  month = {7},
  note = {Proposes a method for language models to improve themselves without human-provided preference data. The model uses its own instruction-following ability to generate rewards for its responses during training. Through iterative DPO training, the model improves both its instruction-following and self-rewarding capabilities, demonstrating a path towards autonomous model improvement.},
  openalex = {W4391047180},
  pages = {57905--57923},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/yuan24d/yuan24d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-Rewarding Language Models},
  url = {https://arxiv.org/abs/2401.10020},
  volume = {235},
  year = {2024}
}

@inproceedings{zhan2024offline,
  abstract = {We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.},
  author = {Springenberg, Jost Tobias and Abdolmaleki, Abbas and Zhang, Jingwei and Groth, Oliver and Bloesch, Michael and Lampe, Thomas and Brakel, Philémon and Bechtle, Sarah and Kapturowski, Steven and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhan2024offline.pdf:pdf},
  openalex = {W4391709728},
  pages = {46323--46350},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/springenberg24a/springenberg24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Offline Actor-Critic Reinforcement Learning Scales to Large Models},
  url = {https://proceedings.mlr.press/v235/springenberg24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{shani2024multi,
  abstract = {Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. However, existing methods work by emulating the human preference at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. The paper addresses this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. We develop novel mirror-descent policy optimization algorithm and demonstrate its effectiveness in an ``Education Dialogue'' environment.},
  author = {Lior Shani and Aviv Rosenberg and Asaf Cassel and Oran Lang and Daniele Calandriello and Avital Zipori and Hila Noga and Orgad Keller and Bilal Piot and Idan Szpektor and Avinatan Hassidim and Yossi Matias and Rémi Munos},
  booktitle = {Advances in Neural Information Processing Systems 37},
  openalex = {W4398845840},
  pdf = {https://neurips.cc/paper_files/paper/2024/file/d77a7b289361abff82bdd2fb537ae152-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'24},
  title = {Multi-turn Reinforcement Learning with Preference Human Feedback},
  url = {https://papers.nips.cc/paper_files/paper/2024/hash/d77a7b289361abff82bdd2fb537ae152-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{zhao2024probabilistic,
  abstract = {Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.},
  author = {Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Baker Grosse},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  note = {While not strictly an RL paper, this work establishes deep connections between probabilistic inference and soft reinforcement learning. It proposes a novel contrastive method for learning twist functions in Sequential Monte Carlo samplers, which is analogous to learning value functions in entropy-regularized RL, highlighting the theoretical unity between these fields.},
  openalex = {W4396244053},
  pages = {60704--60748},
  pdf = {https://proceedings.mlr.press/v235/zhao24c/zhao24c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo},
  url = {https://proceedings.mlr.press/v235/zhao24c.html},
  volume = {235},
  year = {2024}
}

@inproceedings{zheng2024improving,
  abstract = {The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Subsequently, we optimize the policy to perform well on these adversarially selected groups. The approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.},
  author = {Zheng, Rui and Shen, Wei and Hua, Yuan and Lai, Wenbin and Dou, Shihan and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Huang, Haoran and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  openalex = {W4387837682},
  pdf = {https://openreview.net/pdf?id=lB-qT4vmsm},
  title = {Improving Generalization of Alignment with Human Preferences through Group Invariant Learning},
  url = {https://openreview.net/forum?id=lB-qT4vmsm},
  year = {2024}
}

@inproceedings{xiong2024iterative,
  abstract = {This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings---offline, online, and hybrid---and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework naturally gives rise to several novel RLHF algorithms, including an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiments demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO).},
  author = {Wei Xiong and Hanze Dong and Chenlu Ye and Ziqi Wang and Han Zhong and Heng Ji and Nan Jiang and Tong Zhang},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiong2024iterative.pdf:pdf},
  pages = {54715--54754},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/xiong24a/xiong24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint},
  url = {https://proceedings.mlr.press/v235/xiong24a.html},
  volume = {235},
  year = {2024}
}

@misc{xiao2024comprehensivedpo,
  abstract = {With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). This survey aims to provide a comprehensive review of DPO, covering theoretical analyses, variants, relevant preference datasets, and applications while proposing future research directions.},
  archiveprefix = {arXiv},
  author = {Wenyi Xiao and Zechuan Wang and Leilei Gan and Shuai Zhao and Zongrui Li and Ruirui Lei and Wanggui He and Luu Anh Tuan and Long Chen and Hao Jiang and Zhou Zhao and Fei Wu},
  eprint = {2410.15595},
  file = {:/home/b/documents/misc/xiao2024comprehensivedpo.pdf:pdf},
  month = {10},
  openalex = {W4404088808},
  pdf = {https://arxiv.org/pdf/2410.15595},
  primaryclass = {cs.LG},
  title = {A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications},
  url = {https://arxiv.org/abs/2410.15595},
  year = {2024}
}

@misc{xiao2024comprehensivedpo,
  abstract = {With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). Despite DPO's various advancements and inherent limitations, an in-depth review of these aspects was lacking in the literature. This work addresses this gap by presenting a comprehensive review of the challenges and opportunities in DPO. We categorize recent studies on DPO based on key research questions to provide a thorough understanding of DPO's current landscape. We propose several future research directions to offer insights on model alignment for the research community.},
  archiveprefix = {arXiv},
  author = {Xiao, Wenyi and Wang, Zechuan and Gan, Leilei and Zhao, Shuai and Li, Zongrui and Lei, Ruirui and He, Wanggui and Tuan, Luu Anh and Chen, Long and Jiang, Hao and Zhao, Zhou and Wu, Fei},
  doi = {10.48550/arxiv.2410.15595},
  eprint = {2410.15595},
  file = {:/home/b/documents/misc/xiao2024comprehensivedpo.pdf:pdf},
  month = {10},
  openalex = {W4404088808},
  pdf = {https://arxiv.org/pdf/2410.15595},
  primaryclass = {cs.LG},
  title = {A Comprehensive Survey of Datasets, Theories, Variants, and Applications in Direct Preference Optimization},
  url = {https://arxiv.org/abs/2410.15595},
  year = {2024}
}

@inproceedings{liu2024unifiedpessimism,
  abstract = {This paper addresses challenges in offline reinforcement learning under model mismatch, where agents must optimize performance through offline datasets that may not accurately represent the deployment environment. The authors identify two primary challenges: inaccurate model estimation due to limited data coverage, and performance degradation caused by model mismatch between the dataset-collecting environment and the target deployment environment. To tackle these issues, the work proposes a unified principle of pessimism using distributionally robust Markov decision processes.},
  address = {Red Hook, NY, USA},
  author = {Liu, Botao and Liu, Jing-Cheng and Liu, Quan and Zhang, Wen-Jie and Chen, Zhen},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'24},
  title = {A Unified Principle of Pessimism for Offline Reinforcement Learning under Model Mismatch},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/119b45b5c2020d6bc9bca1e42826a2b3-Paper-Conference.pdf},
  volume = {37},
  year = {2024}
}

@inproceedings{zhan2023offlinepbrl,
  abstract = {In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coefficient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard RL, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.},
  author = {Wenhao Zhan and Masatoshi Uehara and Nathan Kallus and Jason D. Lee and Wen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhan2023offlinepbrl.pdf:pdf},
  month = {5},
  openalex = {W4378567238},
  pdf = {https://openreview.net/pdf?id=tVMPfEGT2w},
  title = {Provable Offline Preference-Based Reinforcement Learning},
  url = {https://openreview.net/forum?id=tVMPfEGT2w},
  year = {2024}
}

@inproceedings{zhu2024opride,
  author = {Zhu, Zhepeng and Luo, Zeren and Wu, Jiaji and Ye, Dong-Ling and Chen, Jian},
  booktitle = {International Conference on Learning Representations},
  title = {OPRIDE: Offline Preference-Based Reinforcement Learning with Active-Query-Enhanced Decision Transformer},
  year = {2024}
}

@inproceedings{rezaei2024causalbandits,
  address = {Vancouver, Canada},
  author = {Ali Rezaei and Tor Lattimore and Elias Bareinboim},
  booktitle = {Advances in Neural Information Processing Systems},
  month = {12},
  publisher = {Neural Information Processing Systems Foundation, Inc.},
  title = {Optimal Regret for Causal Bandits with Linear Structural Equation Models},
  volume = {37},
  year = {2024}
}

@book{albrecht2024marlbook,
  abstract = {Multi-Agent Reinforcement Learning (MARL) is an area of machine learning where multiple agents learn to interact optimally in a shared environment. This is the first comprehensive textbook on MARL, covering the field's models, solution concepts, algorithmic ideas, technical challenges, and modern approaches. The book integrates reinforcement learning, deep learning, and game theory, providing a lucid and rigorous introduction suitable for graduate students and professionals in computer science, artificial intelligence, and robotics. It includes a Python codebase with MARL algorithm implementations and extensive examples to illustrate complex concepts.},
  author = {Albrecht, Stefano V. and Christianos, Filippos and Schäfer, Lukas},
  file = {:/home/b/documents/books/albrecht2024marlbook.pdf:pdf},
  isbn = {9780262049375},
  month = {12},
  pages = {396},
  pdf = {https://www.marl-book.com/download/marl-book.pdf},
  publisher = {The MIT Press},
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  url = {https://www.marl-book.com/},
  year = {2024}
}

@inproceedings{ding2023last,
  abstract = {We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To address this challenge, we develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy: (1) the regularized policy gradient primal-dual (RPG-PD) method employs entropy-regularized policy gradient and proves sublinear convergence to an optimal constrained policy, with extension to large state or action spaces via function approximation; (2) the optimistic policy gradient primal-dual (OPG-PD) method uses optimistic gradient method to update primal/dual variables simultaneously and achieves linear convergence to a saddle point containing an optimal constrained policy. To our knowledge, this work provides the first non-asymptotic policy last-iterate convergence result for single-time-scale algorithms in constrained MDPs.},
  author = {Ding, Dongsheng and Wei, Chen-Yu and Zhang, Kaiqing and Ribeiro, Alejandro},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/ding2023last.pdf:pdf},
  note = {Develops policy gradient algorithms for constrained MDPs with provable convergence guarantees for the last iterate. This is a significant theoretical result for safe RL, where ensuring constraint satisfaction is critical.},
  openalex = {W4381586937},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/d0949cbcec31c09431610553a284f94a-Paper-Conference.pdf},
  title = {Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d0949cbcec31c09431610553a284f94a-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{gao2023preference,
  abstract = {Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. For preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards. For general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. We instantiate all reward-based RL subroutines by concrete provable algorithms, and apply our theory to a large class of models including tabular MDPs and MDPs with generic function approximation.},
  address = {Red Hook, NY, USA},
  arxiv = {2306.14111},
  author = {Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/gao2023preference.pdf:pdf},
  note = {Provides a theoretical analysis comparing the sample complexity of Reinforcement Learning from Human Feedback (RLHF) with standard RL. The paper formalizes the conditions under which learning from preferences can be more or less sample-efficient than learning from scalar rewards, offering crucial theoretical insight into the now-widespread practice of RLHF.},
  pages = {70238},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/efb9629755e598c4f261c44aeb6fde5e-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Is RLHF More Difficult than Standard RL? A Theoretical Perspective},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/efb9629755e598c4f261c44aeb6fde5e-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{jiang2023discovering,
  abstract = {Discovering achievements with a hierarchical structure in procedurally generated environments presents a significant challenge. This requires an agent to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods have been built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be advantageous for learning hierarchical dependencies. However, these methods demand an excessive number of environment interactions or large model sizes, limiting their practicality. We show that proximal policy optimization (PPO), a simple yet versatile model-free algorithm, outperforms previous methods when optimized with recent implementation practices. We find that the PPO agent can predict the next achievement to be unlocked to some extent, albeit with limited confidence. Based on this observation, we introduce a novel contrastive learning method, called achievement distillation, which strengthens the agent's ability to predict the next achievement.},
  author = {Moon, Seungyong and Yeom, Junyoung and Park, Bumsoo and Song, Hyun Oh},
  booktitle = {Advances in Neural Information Processing Systems 36},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  file = {:/home/b/documents/inproceedings/jiang2023discovering.pdf:pdf},
  openalex = {W4383860520},
  pages = {1--15},
  pdf = {https://arxiv.org/pdf/2307.03486.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning},
  url = {https://openreview.net/forum?id=cUuXVaMmmv},
  volume = {36},
  year = {2023}
}

@inproceedings{rafailov2023direct,
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems 36},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  file = {:/home/b/documents/inproceedings/rafailov2023direct.pdf:pdf},
  openalex = {W4378771755},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  volume = {36},
  year = {2023}
}

@inproceedings{shinn2023reflexion,
  abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials.},
  author = {Noah Shinn and Federico Cassano and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
  file = {:/home/b/documents/inproceedings/shinn2023reflexion.pdf:pdf},
  openalex = {W4353112996},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{hong2023beyond,
  abstract = {Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data'' rather than all actions in the dataset (i.e., uniform sampling). We present a realization of the sampling strategy and an algorithm that can be used as a plug-and-play module in standard offline RL algorithms. Our evaluation demonstrates significant performance gains in 72 imbalanced datasets, D4RL dataset, and across three different offline RL algorithms.},
  author = {Zhang-Wei Hong and Aviral Kumar and Sathwik Karnik and Abhishek Bhandwaldar and Akash Srivastava and Joni K. Pajarinen and Romain Laroche and Abhishek Gupta and Pulkit Agrawal},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/hong2023beyond.pdf:pdf},
  openalex = {W4387634305},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/0ff3502bb29570b219967278db150a50-Paper-Conference.pdf},
  title = {Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0ff3502bb29570b219967278db150a50-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{zheng2023offline,
  abstract = {Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline multi-agent RL algorithm with implicit global-to-local value regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning with offline regularizations. Based on comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves superior performance over the state-of-the-art offline MARL methods in almost all tasks.},
  author = {Xiangsen Wang and Haoran Xu and Yinan Zheng and Xianyuan Zhan},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/zheng2023offline.pdf:pdf},
  openalex = {W4385227326},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a46c84276e3a4249ab7dbf3e069baf7f-Paper-Conference.pdf},
  title = {Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization},
  volume = {36},
  year = {2023}
}

@inproceedings{rafailov2023preference,
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Oh, A. H. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  file = {:/home/b/documents/inproceedings/rafailov2023preference.pdf:pdf},
  openalex = {W4378771755},
  pages = {2--20},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
  title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  volume = {36},
  year = {2023}
}

@inproceedings{yang2023preference,
  abstract = {Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretically, we prove a lower bound of the behavior policy's performance improvement brought by OAP. Moreover, comprehensive experiments on the D4RL benchmark and state-of-the-art algorithms demonstrate that OAP yields higher (29% on average) scores, especially on challenging AntMaze tasks (98% higher).},
  author = {Qisen Yang and Shenzhi Wang and Matthieu Gaetan Lin and Shiji Song and Gao Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023preference.pdf:pdf},
  openalex = {W4379919645},
  pages = {39509--39523},
  pdf = {https://proceedings.mlr.press/v202/yang23o/yang23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Boosting Offline Reinforcement Learning with Action Preference Query},
  volume = {202},
  year = {2023}
}

@inproceedings{hejna2023preference,
  abstract = {Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods naïvely combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released.},
  address = {Red Hook, NY, USA},
  author = {Hejna, Joey and Sadigh, Dorsa},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Oh, A. H. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  file = {:/home/b/documents/inproceedings/hejna2023preference.pdf:pdf},
  openalex = {W4378510476},
  pages = {82678--82705},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3be7859b36d9440372cae0a293f2e4cc-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Inverse Preference Learning: Preference-based RL without a Reward Function},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3be7859b36d9440372cae0a293f2e4cc-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{flynn2023improvedbandits,
  abstract = {We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used optimism in the face of uncertainty principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confidence sequences give improved performance in several hyperparameter tuning tasks.},
  author = {Hamish Flynn and David Reeb and Melih Kandemir and Jan Peters},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  file = {:/home/b/documents/inproceedings/flynn2023improvedbandits.pdf:pdf},
  note = {Oral presentation},
  openalex = {W4387076543},
  pages = {45102--45136},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8db0d67d22e0ec08c95b810be3a66907-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  title = {Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures},
  volume = {36},
  year = {2023}
}

@inproceedings{wang2023followups,
  abstract = {Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like YouTube, Instagram, TikTok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma, which can accommodate noise in data. We also conduct experiments on both synthetic data and a semi-synthetic dataset created from a real-world dataset to validate our algorithm.},
  author = {Chaoqi Wang and Ziyu Ye and Zhe Feng and Ashwinkumar Badanidiyuru Varadaraja and Haifeng Xu},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/wang2023followups.pdf:pdf},
  openalex = {W4387075948},
  pages = {68825--68841},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/29f47df77b7e536ebd0fe5e0cc964a32-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts},
  volume = {36},
  year = {2023}
}

@inproceedings{cui2023breakingcurse,
  abstract = {We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with large state space and number of agents. This is a class of games where each agent has its own function approximation for the state-action value functions that are marginalized to other players' policies. We design new algorithms for learning Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tackle non-stationarity incurred by multiple agents and the use of function approximation; (2) separating learning Markov equilibria and exploration in the Markov games, which allows us to use the full-information no-regret learning oracle instead of the stronger bandit-feedback no-regret learning oracle to learn equilibria as in the tabular setting.},
  author = {Qiwen Cui and Kaiqing Zhang and Simon S. Du},
  booktitle = {Proceedings of Thirty Sixth Conference on Learning Theory},
  file = {:/home/b/documents/inproceedings/cui2023breakingcurse.pdf:pdf},
  openalex = {W4319654001},
  pages = {2651--2652},
  pdf = {https://proceedings.mlr.press/v195/cui23a/cui23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation},
  volume = {195},
  year = {2023}
}

@article{zhang2023modelbasedmarl,
  abstract = {Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, we study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $ ildeO(|S||A||B|(1-γ)^-3ε^-2)$ for finding the Nash equilibrium (NE) value up to some $ε$ error, and the $ε$-NE policies, where $γ$ is the discount factor, and $S$, $A$, $B$ denote the state space, and the action spaces for the two agents. We also show that such a sample complexity is near-minimax optimal by providing an $Ømega(|S|(|A|+|B|)(1-γ)^-3ε^-2)$ lower bound.},
  author = {Zhang, Kaiqing and Kakade, Sham M. and Başar, Tamer and Yang, Lin F.},
  file = {:/home/b/documents/articles/zhang2023modelbasedmarl.pdf:pdf},
  journal = {Journal of Machine Learning Research},
  number = {175},
  openalex = {W3106398159},
  pages = {1--53},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/0cc6ee01c82fc49c28706e0918f57e2d-Paper.pdf},
  title = {Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity},
  url = {https://www.jmlr.org/papers/v24/20-1131.html},
  volume = {24},
  year = {2023}
}

@inproceedings{devin2022modular,
  abstract = {Humans commonly solve complex problems by decomposing them into easier subproblems and then combining the subproblem solutions. This type of compositional reasoning permits reuse of the subproblem solutions when tackling future tasks that share part of the underlying compositional structure. In a continual or lifelong reinforcement learning setting, this ability to decompose knowledge into reusable components would enable agents to quickly learn new RL tasks by leveraging accumulated compositional structures. We explore a particular form of composition based on neural modules and present a set of RL problems that intuitively admit compositional solutions. Empirically, we demonstrate that neural composition indeed captures the underlying structure of this space of problems. We further propose a compositional lifelong RL method that leverages accumulated neural components to accelerate the learning of future tasks while retaining performance on previous tasks via off-line RL over replayed experiences. The algorithm demonstrates zero-shot and forward transfer, avoidance of forgetting, and backward transfer in discrete 2D and robotic manipulation domains.},
  author = {Jorge A. Mendez and Harm van Seijen and Eric Eaton},
  booktitle = {The Tenth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/devin2022modular.pdf:pdf},
  openalex = {W4283810007},
  pdf = {https://openreview.net/pdf?id=5XmLzdslFNN},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Modular Lifelong Reinforcement Learning via Neural Composition},
  url = {https://openreview.net/forum?id=5XmLzdslFNN},
  year = {2022}
}

@inproceedings{hu2022actor,
  abstract = {We show that the simplest actor-critic method -- a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration -- does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like $ε$-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution.},
  author = {Yuzheng Hu and Ziwei Ji and Matus Telgarsky},
  booktitle = {The Tenth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2022actor.pdf:pdf},
  openalex = {W4286897897},
  pdf = {https://openreview.net/pdf?id=vEZyTBRPP6o},
  title = {Actor-critic Is Implicitly Biased Towards High Entropy Optimal Policies},
  url = {https://openreview.net/forum?id=vEZyTBRPP6o},
  year = {2022}
}

@inproceedings{kostrikov2022offline,
  abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
  author = {Ilya Kostrikov and Ashvin Nair and Sergey Levine},
  booktitle = {The Tenth International Conference on Learning Representations},
  doi = {10.48550/arxiv.2110.06169},
  file = {:/home/b/documents/inproceedings/kostrikov2022offline.pdf:pdf},
  openalex = {W3205794883},
  pdf = {https://openreview.net/pdf?id=68n2s9ZJWF8},
  title = {Offline Reinforcement Learning with Implicit Q-Learning},
  url = {https://openreview.net/forum?id=68n2s9ZJWF8},
  year = {2022}
}

@inproceedings{mai2022sample,
  abstract = {In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.},
  author = {Vincent Mai and Kaustubh Mani and Liam Paull},
  booktitle = {The Tenth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/mai2022sample.pdf:pdf},
  note = {Spotlight paper. Proposes a method to improve sample efficiency in deep RL by using uncertainty estimation to guide exploration. By quantifying the uncertainty in the value function, the agent can be directed to explore parts of the state-action space where its knowledge is lowest, leading to more efficient data collection.},
  openalex = {W4221161302},
  pdf = {https://openreview.net/pdf?id=vrW3tvDfOJQ},
  title = {Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation},
  url = {https://openreview.net/forum?id=vrW3tvDfOJQ},
  year = {2022}
}

@inproceedings{mehta2022understanding,
  abstract = {Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization -- one of the most popular algorithms for sim-to-real transfer -- has been demonstrated to be effective in various tasks in robotics and autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is limited. In this paper, we propose a theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction). We provide sharp bounds on the sim-to-real gap -- the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest.},
  author = {Chen, Xiaoyu and Hu, Jiachen and Jin, Chi and Li, Lihong and Wang, Liwei},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/mehta2022understanding.pdf:pdf},
  note = {Spotlight paper. Provides a theoretical and empirical analysis of domain randomization, a widely used technique for transferring policies trained in simulation to the real world. The paper characterizes the conditions under which domain randomization can succeed and provides insights into how to design effective randomization strategies.},
  openalex = {W3203489455},
  pdf = {https://openreview.net/pdf?id=T8vZHIRTrY},
  title = {Understanding Domain Randomization for Sim-to-real Transfer},
  url = {https://openreview.net/forum?id=T8vZHIRTrY},
  year = {2022}
}

@inproceedings{ouyang2022training,
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  file = {:/home/b/documents/inproceedings/ouyang2022training.pdf:pdf},
  note = {The landmark paper detailing the method used to align models like GPT-3 to follow user instructions, leading to ChatGPT. It describes the three-step RLHF process: supervised fine-tuning (SFT), reward model training, and PPO-based policy optimization. InstructGPT models were shown to be vastly preferred by humans over the base GPT-3 models, despite being much smaller.},
  openalex = {W4226278401},
  pages = {27730--27744},
  pdf = {https://neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'22},
  title = {Training language models to follow instructions with human feedback},
  url = {https://proceedings.neurips.cc/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{wen2022multi,
  abstract = {Large sequence model (SM) such as GPT series and BERT has displayed outstanding performance and generalization capabilities on vision, language, and recently reinforcement learning tasks. A natural follow-up question is how to abstract multi-agent decision making into an SM problem and benefit from the prosperous development of SMs. In this paper, we introduce a novel architecture named Multi-Agent Transformer (MAT) that effectively casts cooperative multi-agent reinforcement learning (MARL) into SM problems wherein the task is to map agents' observation sequence to agents' optimal action sequence. Our goal is to build the bridge between MARL and SMs so that the modeling power of modern sequence models can be unleashed for MARL. Central to our MAT is an encoder-decoder architecture which leverages the multi-agent advantage decomposition theorem to transform the joint policy search problem into a sequential decision making process; this renders only linear time complexity for multi-agent problems and, most importantly, endows MAT with monotonic performance improvement guarantee. Unlike prior arts such as Decision Transformer fit only pre-collected offline data, MAT is trained by online trials and errors from the environment in an on-policy fashion. To validate MAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo, Dexterous Hands Manipulation, and Google Research Football benchmarks. Results demonstrate that MAT achieves superior performance and data efficiency compared to strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that MAT is an excellent few-short learner on unseen tasks regardless of changes in the number of agents.},
  author = {Muning Wen and Jakub Grudzien Kuba and Runji Lin and Weinan Zhang and Ying Wen and Jun Wang and Yaodong Yang},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  file = {:/home/b/documents/inproceedings/wen2022multi.pdf:pdf},
  openalex = {W4281622133},
  pages = {16509--16521},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/69413f87e5a34897cd010ca698097d0a-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Multi-Agent Reinforcement Learning is a Sequence Modeling Problem},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/69413f87e5a34897cd010ca698097d0a-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{xu2022accelerated,
  abstract = {Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanishing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efficiency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17x reduction in training time over the best-performing established RL algorithm.},
  author = {Xu, Jie and Makoviychuk, Viktor and Narang, Yashraj and Ramos, Fabio and Matusik, Wojciech and Garg, Animesh and Macklin, Miles},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4224879709},
  title = {Accelerated Policy Learning with Parallel Differentiable Simulation},
  url = {https://openreview.net/forum?id=ZSKRQMvttc},
  year = {2022}
}

@inproceedings{yu2021mappo,
  abstract = {Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning.},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/yu2021mappo.pdf:pdf},
  note = {Builds on the findings of the surprising effectiveness of PPO and proposes Multi-Agent PPO (MAPPO), which incorporates several simple but crucial modifications to the PPO algorithm for the MARL setting. MAPPO became a dominant baseline and state-of-the-art method on many cooperative MARL benchmarks.},
  openalex = {W4286748781},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.pdf},
  publisher = {Curran Associates, Inc.},
  title = {The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games},
  volume = {35},
  year = {2022}
}

@inproceedings{ouyang2022instructgpt,
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  file = {:/home/b/documents/inproceedings/ouyang2022instructgpt.pdf:pdf},
  openalex = {W4226278401},
  pages = {27730--27744},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Training language models to follow instructions with human feedback},
  url = {https://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{kostrikov2022implicit,
  abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. Prior methods for offline RL either constrain the policy to remain close to the behavior policy, or rely on estimates of the value of unseen actions, both of which prevent the learned policy from substantially improving over the best behavior in the data in the presence of sparse rewards or noisy data. In this paper, we propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our approach is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the behavior policy), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available actions. We dub the resulting algorithm implicit Q-learning (IQL). IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. IQL demonstrates state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning, and we also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
  author = {Ilya Kostrikov and Ashvin Nair and Sergey Levine},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kostrikov2022implicit.pdf:pdf},
  openalex = {W3205794883},
  pdf = {https://openreview.net/pdf?id=68n2s9ZJWF8},
  publisher = {OpenReview.net},
  title = {Offline Reinforcement Learning with Implicit Q-Learning},
  url = {https://openreview.net/forum?id=68n2s9ZJWF8},
  year = {2022}
}

@article{cen2022convergencepg,
  abstract = {We consider infinite-horizon discounted Markov decision problems with finite state and action spaces and study the convergence rates of the projected policy gradient method and a general class of policy mirror descent methods, all with direct parametrization in the policy space. We develop a theory of weak gradient-mapping dominance and use it to prove sharper sublinear convergence rate of the projected policy gradient method. With geometrically increasing step sizes, we show that a general class of policy mirror descent methods, including the natural policy gradient method and a projected Q-descent method, all enjoy a linear rate of convergence without relying on entropy or other strongly convex regularization. Our convergence rates are either dimension-free or have better dimension dependence than existing results. We also analyze the convergence rate of an inexact policy mirror descent method and estimate its sample complexity under a simple generative model.},
  author = {Xiao, Lin},
  file = {:/home/b/documents/articles/cen2022convergencepg.pdf:pdf},
  journal = {Journal of Machine Learning Research},
  number = {282},
  pages = {1--36},
  pdf = {https://www.jmlr.org/papers/volume23/22-0056/22-0056.pdf},
  title = {On the Convergence Rates of Policy Gradient Methods},
  url = {http://jmlr.org/papers/v23/22-0056.html},
  volume = {23},
  year = {2022}
}

@inproceedings{chen2021decision,
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Vaughan, Jennifer Wortman},
  file = {:/home/b/documents/inproceedings/chen2021decision.pdf:pdf},
  openalex = {W3169291081},
  pages = {15084--15097},
  pdf = {https://papers.nips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  volume = {34},
  year = {2021}
}

@article{ecoffet2021exploration,
  abstract = {The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states ('detachment') and from failing to first return to a state before exploring from it ('derailment'). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  doi = {10.1038/s41586-020-03157-9},
  file = {:/home/b/documents/articles/ecoffet2021exploration.pdf:pdf},
  journal = {Nature},
  note = {A two-phase algorithm that first explores to find a promising state and then robustifies a policy from that state, solving previously intractable hard-exploration games like Montezuma's Revenge.},
  number = {7847},
  openalex = {W3129322645},
  pages = {580--586},
  pdf = {https://arxiv.org/pdf/2004.12919.pdf},
  title = {First return, then explore},
  volume = {590},
  year = {2021}
}

@inproceedings{gupta2021uneven,
  abstract = {VDN and QMIX are two popular value-based algorithms for cooperative MARL that learn a centralized action value function as a monotonic mixing of per-agent utilities. While this enables easy decentralization of the learned policy, the restricted joint action value function can prevent them from solving tasks that require significant coordination between agents at a given timestep. We show that this problem can be overcome by improving the joint exploration of all agents during training. Specifically, we propose a novel MARL approach called Universal Value Exploration (UneVEn) that learns a set of related tasks simultaneously with a linear decomposition of universal successor features. With the policies of already solved related tasks, the joint exploration process of all agents can be improved to help them achieve better coordination. Empirical results on a set of exploration games, challenging cooperative predator-prey tasks requiring significant coordination among agents, and StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where other state-of-the-art MARL methods fail.},
  author = {Gupta, Tarun and Mahajan, Anuj and Peng, Bei and Boehmer, Wendelin and Whiteson, Shimon},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Meila, Marina and Zhang, Tong},
  file = {:/home/b/documents/inproceedings/gupta2021uneven.pdf:pdf},
  month = {7},
  openalex = {W3171040465},
  pages = {3930--3941},
  pdf = {http://proceedings.mlr.press/v139/gupta21a/gupta21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v139/gupta21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{hafner2020mastering,
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. The DreamerV2 agent is the first world model agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer surpasses the final performance of the top single-GPU agents IQN, Rainbow, and DQN. DreamerV2 also applies to tasks with continuous actions, where it solves 20 tasks across multiple domains with a single set of hyperparameters. Our approach opens the path toward general and data-efficient reinforcement learning.},
  author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hafner2020mastering.pdf:pdf},
  note = {Introduces DreamerV2, which uses discrete representations in world models. First agent based on a learned world model to achieve human-level performance on the Atari benchmark, demonstrating the generality of the latent imagination approach across fundamentally different domains. Surpasses top single-GPU agents like IQN and Rainbow with the same computational budget.},
  openalex = {W3122690883},
  pdf = {https://arxiv.org/pdf/2010.02193.pdf},
  title = {Mastering Atari with Discrete World Models},
  url = {https://arxiv.org/abs/2010.02193},
  year = {2021}
}

@inproceedings{janner2021offline,
  abstract = {Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. We explore how RL can be reframed as "one big sequence modeling" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Importantly, our method is simple: it does not require specialized machinery for enforcing the constraints of sequential decision-making, such as asserting the validity of only a subset of actions at each timestep. Instead, we simply train a Transformer on sequences and use beam search as a planning algorithm. In our experiments, we explore whether this technique can work as a drop-in replacement for existing RL algorithms. We find that this approach matches or exceeds the performance of model-based and model-free methods in offline RL settings, competition-grade MUJOCO continuous control, and Atari.},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/janner2021offline.pdf:pdf},
  note = {Concurrent with Decision Transformer, this work also frames RL as sequence modeling. The Trajectory Transformer uses a discretized state-action space and a beam search planning procedure to generate high-reward trajectories, demonstrating the versatility of sequence models for both behavior cloning and planning.},
  openalex = {W4287126489},
  pages = {1273--1286},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {Offline Reinforcement Learning as One Big Sequence Modeling Problem},
  url = {https://proceedings.neurips.cc/paper/2021/hash/099fe6b0b444c23836c4a5d07346082b-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{kostrikov2021offline,
  abstract = {Many modern approaches to offline Reinforcement Learning (RL) utilize behavior regularization, typically augmenting a model-free actor critic algorithm with a penalty measuring divergence of the policy from the offline data. In this work, we propose an alternative approach to encouraging the learned policy to stay close to the data, namely parameterizing the critic as the log-behavior-policy, which generated the offline data, plus a state-action value offset term, which can be learned using a neural network. Behavior regularization then corresponds to an appropriate regularizer on the offset term. We propose using a gradient penalty regularizer for the offset term and demonstrate its equivalence to Fisher divergence regularization, suggesting connections to the score matching and generative energy-based model literature. We thus term our resulting algorithm Fisher-BRC (Behavior Regularized Critic). On standard offline RL benchmarks, Fisher-BRC achieves both improved performance and faster convergence over existing state-of-the-art methods.},
  author = {Ilya Kostrikov and Rob Fergus and Jonathan Tompson and Ofir Nachum},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kostrikov2021offline.pdf:pdf},
  openalex = {W3166795773},
  pages = {5774--5783},
  pdf = {http://proceedings.mlr.press/v139/kostrikov21a/kostrikov21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Offline Reinforcement Learning with Fisher Divergence Critic Regularization},
  url = {https://proceedings.mlr.press/v139/kostrikov21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{vicol2021unbiased,
  abstract = {Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.},
  author = {Paul Vicol and Luke Metz and Jascha Sohl-Dickstein},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vicol2021unbiased.pdf:pdf},
  openalex = {W4310001898},
  pages = {10553--10563},
  pdf = {http://proceedings.mlr.press/v139/vicol21a/vicol21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies},
  url = {https://proceedings.mlr.press/v139/vicol21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{wang2021coach,
  abstract = {In real-world multi-agent systems, agents with different capabilities may join or leave without altering the team's overarching goals. Coordinating teams with such dynamic composition is challenging since the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks, and demonstrate zero-shot generalization to new team compositions.},
  author = {Liu, Bo and Liu, Qiang and Stone, Peter and Garg, Animesh and Zhu, Yuke and Anandkumar, Anima},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Meila, Marina and Zhang, Tong},
  file = {:/home/b/documents/inproceedings/wang2021coach.pdf:pdf},
  openalex = {W3166318878},
  pages = {6860--6870},
  pdf = {https://proceedings.mlr.press/v139/liu21m/liu21m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coach-Player Multi-agent Reinforcement Learning for Dynamic Team Composition},
  url = {https://proceedings.mlr.press/v139/liu21m.html},
  volume = {139},
  year = {2021}
}

@inproceedings{wang2021rode,
  abstract = {Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy -- the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 10 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents.},
  author = {Wang, Tonghan and Gupta, Tarun and Mahajan, Anuj and Peng, Bei and Whiteson, Shimon and Zhang, Chongjie},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  file = {:/home/b/documents/inproceedings/wang2021rode.pdf:pdf},
  month = {5},
  openalex = {W3125155982},
  pdf = {https://openreview.net/pdf?id=TTUVg6vkNjK},
  publisher = {OpenReview.net},
  title = {RODE: Learning Roles to Decompose Multi-Agent Tasks},
  url = {https://openreview.net/forum?id=TTUVg6vkNjK},
  year = {2021}
}

@inproceedings{zhang2021hierarchical,
  abstract = {We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower-level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods. Code at: https://github.com/jesbu1/hidio.},
  author = {Jesse Zhang and Haonan Yu and Wei Xu},
  booktitle = {9th International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2021hierarchical.pdf:pdf},
  openalex = {W3121879877},
  pdf = {https://openreview.net/pdf?id=r-gPPHEjpmw},
  publisher = {OpenReview.net},
  title = {Hierarchical Reinforcement Learning by Discovering Intrinsic Options},
  url = {https://openreview.net/forum?id=r-gPPHEjpmw},
  year = {2021}
}

@inproceedings{zhang2021near,
  abstract = {We study the reward-free reinforcement learning framework, which is particularly suitable for batch reinforcement learning and scenarios where one needs policies for multiple reward functions. This framework has two phases: in the exploration phase, the agent collects trajectories by interacting with the environment without using any reward signal; in the planning phase, the agent needs to return a near-optimal policy for arbitrary reward functions.},
  author = {Zihan Zhang and Simon S. Du and Xiangyang Ji},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2021near.pdf:pdf},
  note = {Addresses the reward-free RL setting, where an agent first explores an environment without a reward function and then must quickly compute a near-optimal policy for any given reward function. This work provides a new algorithm with a near-optimal sample complexity that scales only logarithmically with the horizon, a significant theoretical improvement over prior methods.},
  openalex = {W3167017381},
  pages = {12402--12412},
  pdf = {http://proceedings.mlr.press/v139/zhang21e/zhang21e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near Optimal Reward-Free Reinforcement Learning},
  url = {https://proceedings.mlr.press/v139/zhang21e.html},
  volume = {139},
  year = {2021}
}

@inproceedings{jin2021pessimism,
  abstract = {We study offline reinforcement learning (RL), which aims to learn an optimal policy based on a dataset collected a priori. Due to the lack of further interactions with the environment, offline RL suffers from the insufficient coverage of the dataset, which eludes most existing theoretical analysis. In this paper, we propose a pessimistic variant of the value iteration algorithm (PEVI), which incorporates an uncertainty quantifier as the penalty function. Such a penalty function simply flips the sign of the bonus function for promoting exploration in online RL, which makes it easily implementable and compatible with general function approximators. Without assuming the sufficient coverage of the dataset, we establish a data-dependent upper bound on the suboptimality of PEVI for general Markov decision processes (MDPs). When specialized to linear MDPs, it matches the information-theoretic lower bound up to multiplicative factors of the dimension and horizon. In other words, pessimism is not only provably efficient but also minimax optimal. In particular, given the dataset, the learned policy serves as the 'best effort' among all policies, as no other policies can do better. Our theoretical analysis identifies the critical role of pessimism in eliminating a notion of spurious correlation, which emerges from the 'irrelevant' trajectories that are less covered by the dataset and not informative for the optimal policy.},
  author = {Ying Jin and Zhuoran Yang and Zhaoran Wang},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jin2021pessimism.pdf:pdf},
  openalex = {W3166645952},
  pages = {5084--5096},
  pdf = {http://proceedings.mlr.press/v139/jin21e/jin21e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Is Pessimism Provably Efficient for Offline RL?},
  volume = {139},
  year = {2021}
}

@article{zhang2021globalconvergencepg,
  abstract = {Policy gradient (PG) methods are a widely used reinforcement learning methodology in many applications such as video games, autonomous driving, and robotics. In spite of its empirical success, a rigorous understanding of the global convergence of PG methods is lacking in the literature. In this work, we close the gap by viewing PG methods from a nonconvex optimization perspective. In particular, we propose a new variant of PG methods for infinite-horizon problems that uses a random rollout horizon for the Monte-Carlo estimation of the policy gradient. This method then yields an unbiased estimate of the policy gradient with bounded variance, which enables the tools from nonconvex optimization to be applied to establish global convergence. Employing this perspective, we first recover the convergence results with rates to the stationary-point policies in the literature. More interestingly, motivated by advances in nonconvex optimization, we modify the proposed PG method by introducing periodically enlarged stepsizes. The modified algorithm is shown to escape saddle points under mild assumptions on the reward and the policy parameterization. Under a further strict saddle points assumption, this result establishes convergence to essentially locally-optimal policies of the underlying problem, and thus bridges the gap in existing literature on the convergence of PG methods. Results from experiments on the inverted pendulum are then provided to corroborate our theory, namely, by slightly reshaping the reward function to satisfy our assumption, unfavorable saddle points can be avoided and better limit points can be attained. Intriguingly, this empirical finding justifies the benefit of reward-reshaping from a nonconvex optimization perspective.},
  author = {Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Başar, Tamer},
  file = {:/home/b/documents/articles/zhang2021globalconvergencepg.pdf:pdf},
  journal = {IEEE Transactions on Automatic Control},
  note = {Also published in SIAM Journal on Control and Optimization 58(6):3586--3612, 2020},
  number = {8},
  openalex = {W3109546547},
  pages = {3584--3599},
  pdf = {https://arxiv.org/pdf/1906.08383},
  title = {Global Convergence of Policy Gradient Methods to (Almost) Locally Optimal Policies},
  volume = {66},
  year = {2021}
}

@inproceedings{lu2021causal,
  abstract = {In causal bandit problems, the action set consists of interventions on variables of a causal graph. Several researchers have recently studied such problems and pointed out their practical applications. However, all existing works rely on a restrictive and impractical assumption that the learner is given full knowledge of the causal graph structure upfront. In this paper, we develop novel causal bandit algorithms without knowing the causal graph. Our algorithms work well for causal trees, causal forests and a general class of causal graphs. The regret guarantees of our algorithms greatly improve upon those of standard multi-armed bandit (MAB) algorithms under mild conditions. Lastly, we prove our mild conditions are necessary: without them one cannot do better than standard MAB algorithms.},
  author = {Yangyi Lu and Amirhossein Meisami and Ambuj Tewari},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arxiv.2106.02988},
  file = {:/home/b/documents/inproceedings/lu2021causal.pdf:pdf},
  openalex = {W3165890783},
  pages = {24817--24828},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf},
  title = {Causal Bandits with Unknown Graph Structure},
  volume = {34},
  year = {2021}
}

@inproceedings{agarwal2020optimistic,
  abstract = {Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.},
  author = {Rishabh Agarwal and Dale Schuurmans and Mohammad Norouzi},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/agarwal2020optimistic.pdf:pdf},
  note = {Challenges the prevailing pessimistic view of offline RL by showing that standard off-policy algorithms can succeed without explicit corrections for distribution shift, provided the dataset is sufficiently large and diverse. Introduces the DQN Replay Dataset for Atari, a large-scale benchmark that spurred research in this area.},
  openalex = {W3009584650},
  pages = {104--114},
  pdf = {http://proceedings.mlr.press/v119/agarwal20c/agarwal20c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Optimistic Perspective on Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v119/agarwal20c.html},
  volume = {119},
  year = {2020}
}

@misc{de2020surprising,
  abstract = {Most recently developed approaches to cooperative multi-agent reinforcement learning in the centralized training with decentralized execution setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning.},
  author = {de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip H. S. and Sun, Mingfei and Whiteson, Shimon},
  file = {:/home/b/documents/misc/de2020surprising.pdf:pdf},
  howpublished = {arXiv preprint arXiv:2011.09533},
  month = {11},
  note = {An influential empirical study showing that independent PPO agents can achieve surprisingly strong performance in cooperative MARL benchmarks like the StarCraft Multi-Agent Challenge, often matching or outperforming more complex, specialized MARL algorithms.},
  openalex = {W3099689767},
  pdf = {http://arxiv.org/pdf/2011.09533},
  title = {Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  url = {https://arxiv.org/abs/2011.09533},
  year = {2020}
}

@inproceedings{dennis2020emergent,
  abstract = {A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.},
  author = {Dennis, Michael D. and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre M. and Russell, Stuart J. and Critch, Andrew and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  openalex = {W3110161557},
  pages = {13049--13061},
  publisher = {Curran Associates, Inc.},
  title = {Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design},
  url = {https://proceedings.neurips.cc/paper/2020/file/985e9a46e10005356bbaf194249f6856-Paper.pdf},
  volume = {33},
  year = {2020}
}

@inproceedings{hafner2020dream,
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  author = {Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hafner2020dream.pdf:pdf},
  note = {Introduces Dreamer, an agent that learns long-horizon behaviors from images purely by planning in the latent space of a learned world model. It efficiently learns actor and critic models within this "dream," propagating value gradients back through imagined trajectories. Dreamer achieved state-of-the-art performance on visual control benchmarks with unprecedented data efficiency.},
  openalex = {W2995298643},
  pdf = {https://openreview.net/pdf?id=S1lOTC4tDS},
  title = {Dream to Control: Learning Behaviors by Latent Imagination},
  url = {https://openreview.net/forum?id=S1lOTC4tDS},
  year = {2020}
}

@inproceedings{kidambi2020morel,
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. The ability to train RL policies offline can greatly expand the applicability of RL, its data efficiency, and its experimental velocity. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the offline dataset; and (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP. This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied offline RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g. generative modeling, uncertainty estimation, planning etc.) to directly translate into advances for offline RL.},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/kidambi2020morel.pdf:pdf},
  openalex = {W3025606523},
  pages = {21810--21823},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf},
  title = {MOReL: Model-Based Offline Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper/2020/hash/f7efa4f864ae9b88d43527f4b14f750f-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{klink2020self,
  abstract = {Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.},
  author = {Pascal Klink and Carlo D'Eramo and Jan R. Peters and Joni K. Pajarinen},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/klink2020self.pdf:pdf},
  openalex = {W4287812298},
  pages = {9216--9227},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Self-Paced Deep Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper/2020/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{kumar2020conservative,
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. We propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic algorithms. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  author = {Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arxiv.2006.04779},
  file = {:/home/b/documents/inproceedings/kumar2020conservative.pdf:pdf},
  note = {A highly influential offline RL algorithm that directly addresses value overestimation. CQL learns a conservative Q-function by adding a regularizer to the Bellman error objective that minimizes the Q-values for out-of-distribution actions while maximizing them for in-distribution actions. This ensures the learned Q-function provides a lower bound on the true policy value, leading to robust performance.},
  openalex = {W3033324992},
  pages = {1179--1191},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf},
  publisher = {Curran Associates Inc.},
  series = {NIPS'20},
  title = {Conservative Q-Learning for Offline Reinforcement Learning},
  url = {https://arxiv.org/abs/2006.04779},
  volume = {33},
  year = {2020}
}

@inproceedings{laskin2020reinforcement,
  abstract = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. In this work, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. Additionally, RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks.},
  author = {Michael Laskin and Kimin Lee and Adam Stooke and Lerrel Pinto and Pieter Abbeel and Aravind Srinivas},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arXiv.2004.14990},
  file = {:/home/b/documents/inproceedings/laskin2020reinforcement.pdf:pdf},
  note = {Performs the first extensive study of data augmentation for RL. It shows that simple augmentations applied to input observations (e.g., random crops, color jitter) can dramatically improve the sample efficiency and generalization of model-free RL algorithms, allowing simple agents to outperform more complex state-of-the-art methods.},
  openalex = {W3021708257},
  pages = {19884--19895},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/e615c82aba461681ade82da2da38004a-Paper.pdf},
  title = {Reinforcement Learning with Augmented Data},
  url = {https://papers.nips.cc/paper/2020/hash/e615c82aba461681ade82da2da38004a-Abstract.html},
  volume = {33},
  year = {2020}
}

@article{schrittwieser2020mastering,
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  doi = {10.1038/s41586-020-03051-4},
  journal = {Nature},
  month = {12},
  number = {7839},
  openalex = {W3118210634},
  pages = {604--609},
  pdf = {https://www.nature.com/articles/s41586-020-03051-4.pdf},
  publisher = {Nature Publishing Group},
  title = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  url = {https://www.nature.com/articles/s41586-020-03051-4},
  volume = {588},
  year = {2020}
}

@inproceedings{stiennon2020learning,
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans.},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F.},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/stiennon2020learning.pdf:pdf},
  note = {One of the first large-scale applications of RLHF to natural language processing. This work showed that fine-tuning a pre-trained language model (GPT-3) using a reward model trained on human preferences for summaries led to significantly higher-quality summaries than those produced by supervised methods alone.},
  openalex = {W4287674181},
  pages = {3008--3021},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
  title = {Learning to summarize from human feedback},
  url = {https://arxiv.org/abs/2009.01325},
  volume = {33},
  year = {2020}
}

@inproceedings{xu2020finitetimepbrl,
  author = {Xu, Yichong and Bubeck, Sébastien and Luo, Hao and Lattimore, Tor},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  title = {Preference-based Reinforcement Learning with Finite-Time Guarantees},
  volume = {33},
  year = {2020}
}

@inproceedings{stiennon2020summarization,
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are usually trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, and train a model to predict the human-preferred summary. We then use this model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR summarization dataset of posts from Reddit, and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models. We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to human evaluators. Our results suggest that significantly improving summarization through human feedback is possible, and that our rough metrics for this task lead to worse performance than we would achieve by more directly optimizing for human preferences.},
  archiveprefix = {arXiv},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  eprint = {2009.01325},
  file = {:/home/b/documents/inproceedings/stiennon2020summarization.pdf:pdf},
  openalex = {W4287674181},
  pages = {3008--3021},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf},
  primaryclass = {cs.CL},
  publisher = {Curran Associates, Inc.},
  title = {Learning to Summarize from Human Feedback},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{kumar2020conservative,
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. The paper proposes conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value.},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  file = {:/home/b/documents/inproceedings/kumar2020conservative.pdf:pdf},
  pages = {1179--1191},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf},
  title = {Conservative Q-Learning for Offline Reinforcement Learning},
  url = {https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{xu2020finitetimepbrl,
  abstract = {Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. This paper presents the first finite-time analysis for general PbRL problems. We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy $ε$ with high probability.},
  author = {Yichong Xu and Ruosong Wang and Lin F. Yang and Aarti Singh and Artur Dubrawski},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/xu2020finitetimepbrl.pdf:pdf},
  openalex = {W3099643187},
  pages = {18784--18794},
  pdf = {https://papers.nips.cc/paper/2020/file/d9d3837ee7981e8c064774da6cdd98bf-Paper.pdf},
  title = {Preference-based Reinforcement Learning with Finite-Time Guarantees},
  volume = {33},
  year = {2020}
}

@inproceedings{burda2019exploration,
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. Most notably, Random Network Distillation establishes the first method to exceed average human performance on Montezuma's Revenge without using demonstrations or having access to the underlying state of the game.},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  booktitle = {7th International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/burda2019exploration.pdf:pdf},
  note = {Achieves state-of-the-art performance on Montezuma's Revenge, the first method to exceed average human performance on this notoriously difficult exploration game without demonstrations},
  openalex = {W2964067469},
  pdf = {https://openreview.net/pdf?id=H1lJJnR5Ym},
  title = {Exploration by Random Network Distillation},
  url = {https://openreview.net/forum?id=H1lJJnR5Ym},
  year = {2019}
}

@inproceedings{janner2019when,
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  author = {Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/janner2019when.pdf:pdf},
  note = {An algorithm that effectively combines data from a learned model with real-world experience, using the model for short-term predictions to accelerate learning while mitigating model bias.},
  openalex = {W4288319859},
  pages = {12498--12509},
  pdf = {https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf},
  title = {When to Trust Your Model: Model-Based Policy Optimization},
  url = {https://proceedings.neurips.cc/paper/2019/hash/5faf461eff3099671ad63c6f3f094f7f-Abstract.html},
  volume = {32},
  year = {2019}
}

@misc{ziegler2019learning,
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  eprint = {1909.08593},
  file = {:/home/b/documents/misc/ziegler2019learning.pdf:pdf},
  openalex = {W2973379954},
  pdf = {https://arxiv.org/pdf/1909.08593.pdf},
  primaryclass = {cs.LG},
  title = {Fine-Tuning Language Models from Human Preferences},
  year = {2019}
}

@inproceedings{foerster2018counterfactual,
  abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
  author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v32i1.11794},
  file = {:/home/b/documents/inproceedings/foerster2018counterfactual.pdf:pdf},
  note = {Addresses the multi-agent credit assignment problem in cooperative settings. COMA uses a centralized critic to compute a counterfactual advantage function for each agent, which compares the global reward to a simulated reward where that agent's action is marginalized out. This provides a clear signal of each agent's contribution to the team's success.},
  number = {1},
  openalex = {W2617547828},
  pages = {2974--2982},
  pdf = {https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf},
  title = {Counterfactual Multi-Agent Policy Gradients},
  url = {https://arxiv.org/abs/1705.08926},
  volume = {32},
  year = {2018}
}

@inproceedings{ha2018world,
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own dream environment generated by its world model, and transfer this policy back into the actual environment.},
  author = {Ha, David and Schmidhuber, Jürgen},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arxiv.1809.01999},
  file = {:/home/b/documents/inproceedings/ha2018world.pdf:pdf},
  note = {Proposes a general framework for model-based RL where an agent learns a compressed spatial and temporal representation of its environment. This ``world model'' can then be used to train a compact policy entirely in a ``dream'' environment generated by the model, dramatically improving sample efficiency.},
  openalex = {W2890208753},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf},
  title = {Recurrent World Models Facilitate Policy Evolution},
  url = {https://worldmodels.github.io},
  volume = {31},
  year = {2018}
}

@inproceedings{haarnoja2018soft,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/haarnoja2018soft.pdf:pdf},
  openalex = {W2781726626},
  pages = {1861--1870},
  pdf = {https://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  url = {https://arxiv.org/abs/1801.01290},
  volume = {80},
  year = {2018}
}

@article{haarnoja2018soft2,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  doi = {10.48550/arxiv.1812.05905},
  file = {:/home/b/documents/articles/haarnoja2018soft2.pdf:pdf},
  journal = {arXiv preprint arXiv:1812.05905},
  month = {12},
  note = {An updated version of the SAC algorithm that introduces a crucial improvement: automatic tuning of the temperature parameter that balances the reward and entropy terms. This removes a sensitive hyperparameter, making the algorithm much easier to use in practice and further improving its stability and performance.},
  openalex = {W2904246096},
  pdf = {https://arxiv.org/pdf/1812.05905.pdf},
  title = {Soft Actor-Critic Algorithms and Applications},
  url = {https://arxiv.org/abs/1812.05905},
  year = {2018}
}

@inproceedings{henderson2018deep,
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. We investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. We investigate these sources of variance in reported results through a representative set of experiments, focusing our investigation on policy gradient (PG) methods in continuous control.},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/AAAI.V32I1.11694},
  file = {:/home/b/documents/inproceedings/henderson2018deep.pdf:pdf},
  number = {1},
  openalex = {W2754517384},
  pages = {3207--3214},
  pdf = {http://arxiv.org/pdf/1709.06560},
  title = {Deep Reinforcement Learning that Matters},
  volume = {32},
  year = {2018}
}

@inproceedings{hessel2018rainbow,
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Gheshlaghi Azar, Mohammad and Silver, David},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v32i1.11796},
  file = {:/home/b/documents/inproceedings/hessel2018rainbow.pdf:pdf},
  number = {1},
  openalex = {W2761873684},
  pages = {3215--3222},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/download/11796/11655},
  publisher = {AAAI Press},
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  volume = {32},
  year = {2018}
}

@inproceedings{kurutach2018model,
  abstract = {Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Empirically, we demonstrate that our approach significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  author = {Kurutach, Thanard and Clavera, Ignasi and Duan, Yan and Tamar, Aviv and Abbeel, Pieter},
  booktitle = {International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kurutach2018model.pdf:pdf},
  note = {Improves model-based policy optimization by using an ensemble of learned dynamics models to quantify model uncertainty and incorporate it into the TRPO algorithm.},
  openalex = {W2785389871},
  pdf = {https://openreview.net/pdf?id=SJJinbWRZ},
  title = {Model-Ensemble Trust-Region Policy Optimization},
  url = {https://openreview.net/forum?id=SJJinbWRZ},
  year = {2018}
}

@inproceedings{rashid2018qmix,
  abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
  author = {Rashid, Tabish and Samvelyan, Mikayel and de Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rashid2018qmix.pdf:pdf},
  openalex = {W4295598622},
  pages = {4295--4304},
  pdf = {http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  volume = {80},
  year = {2018}
}

@book{sutton2018reinforcement,
  abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This significantly expanded and updated new edition presents new topics and updates coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  address = {Cambridge, MA},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  isbn = {9780262039246},
  pages = {548},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Reinforcement Learning: An Introduction},
  url = {http://www.incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@inproceedings{achiam2017constrained,
  abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
  author = {Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Doina Precup and Yee Whye Teh},
  file = {:/home/b/documents/inproceedings/achiam2017constrained.pdf:pdf},
  month = {8},
  openalex = {W4293545785},
  pages = {22--31},
  pdf = {http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Policy Optimization},
  url = {https://proceedings.mlr.press/v70/achiam17a.html},
  volume = {70},
  year = {2017}
}

@inproceedings{bellemare2017distributional,
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach of reinforcement learning which models only the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behavior. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning.},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bellemare2017distributional.pdf:pdf},
  openalex = {W2963423916},
  pages = {449--458},
  pdf = {https://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Distributional Perspective on Reinforcement Learning},
  volume = {70},
  year = {2017}
}

@inproceedings{christiano2017deep,
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces cost of oversight far enough it can be practically applied to state-of-the-art RL. To demonstrate flexibility, we successfully train novel behaviors about an hour of time. These environments are considerably more complex than any have been previously learned from feedback.},
  author = {Paul F. Christiano and Jan Leike and Tom Brown and Miljan Martic and Shane Legg and Dario Amodei},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  file = {:/home/b/documents/inproceedings/christiano2017deep.pdf:pdf},
  openalex = {W4321392130},
  pages = {4299--4307},
  pdf = {http://papers.neurips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Deep Reinforcement Learning from Human Preferences},
  volume = {30},
  year = {2017}
}

@inproceedings{duan2016rl2,
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL², the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP.},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  booktitle = {International Conference on Learning Representations},
  doi = {10.48550/arxiv.1611.02779},
  file = {:/home/b/documents/inproceedings/duan2016rl2.pdf:pdf},
  note = {Frames meta-learning as an RL problem itself, where a recurrent agent learns a learning algorithm that can solve new MDPs efficiently.},
  openalex = {W2578206533},
  pdf = {http://arxiv.org/pdf/1611.02779},
  title = {RL²: Fast Reinforcement Learning via Slow Reinforcement Learning},
  url = {https://openreview.net/forum?id=HkLXCE9lx},
  year = {2017}
}

@inproceedings{finn2017model,
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  file = {:/home/b/documents/inproceedings/finn2017model.pdf:pdf},
  openalex = {W2604763608},
  pages = {1126--1135},
  pdf = {https://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  url = {https://proceedings.mlr.press/v70/finn17a.html},
  volume = {70},
  year = {2017}
}

@inproceedings{lowe2017multi,
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination.},
  address = {Red Hook, NY, USA},
  author = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/lowe2017multi.pdf:pdf},
  note = {A foundational algorithm for deep multi-agent RL that extends DDPG to the multi-agent setting. It introduces the paradigm of Centralized Training with Decentralized Execution (CTDE), where a centralized critic has access to all agents' observations and actions during training, while each actor only uses its local observation for execution. This stabilizes learning in non-stationary multi-agent environments.},
  openalex = {W4299802797},
  pages = {6382--6393},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf},
  publisher = {Curran Associates Inc.},
  title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  url = {https://arxiv.org/abs/1706.02275},
  volume = {30},
  year = {2017}
}

@misc{schulman2017proximal,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a 'surrogate' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  file = {:/home/b/documents/misc/schulman2017proximal.pdf:pdf},
  howpublished = {arXiv preprint arXiv:1707.06347},
  month = {7},
  note = {Introduces a simpler and more scalable alternative to TRPO. PPO achieves the data efficiency and reliable performance of TRPO using only first-order optimization. It uses a clipped surrogate objective function to constrain the policy update, making it much easier to implement and more widely applicable. PPO has become one of the most popular and default RL algorithms.},
  openalex = {W2736601468},
  pdf = {https://arxiv.org/pdf/1707.06347.pdf},
  title = {Proximal Policy Optimization Algorithms},
  url = {https://arxiv.org/abs/1707.06347},
  year = {2017}
}

@article{silver2017mastering,
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature24270},
  file = {:/home/b/documents/articles/silver2017mastering.pdf:pdf},
  journal = {Nature},
  note = {Introduces AlphaGo Zero, which surpassed the original AlphaGo by learning entirely from self-play, without any human data. It used a single, unified neural network for both policy and value functions and a simpler MCTS. This demonstrated that pure reinforcement learning, when combined with powerful search, could discover strategies superior to those developed through millennia of human play.},
  number = {7676},
  openalex = {W2766447205},
  pages = {354--359},
  pdf = {https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf},
  publisher = {Nature Publishing Group},
  title = {Mastering the game of Go without human knowledge},
  url = {https://www.nature.com/articles/nature24270},
  volume = {550},
  year = {2017}
}

@inproceedings{tobin2017domain,
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  address = {Vancouver, BC, Canada},
  author = {Josh Tobin and Rachel Fong and Alex Ray and Jonas Schneider and Wojciech Zaremba and Pieter Abbeel},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2017.8202133},
  file = {:/home/b/documents/inproceedings/tobin2017domain.pdf:pdf},
  month = {9},
  note = {A key technique for sim-to-real transfer in robotics, where training in a simulation with randomized physical and visual properties leads to policies that are robust enough to work on a real robot.},
  openalex = {W2605102758},
  organization = {IEEE},
  pages = {23--30},
  pdf = {http://arxiv.org/pdf/1703.06907},
  title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
  url = {https://ieeexplore.ieee.org/document/8202133/},
  year = {2017}
}

@inproceedings{christiano2017deeprlhf,
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  author = {Christiano, Paul F. and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {:/home/b/documents/inproceedings/christiano2017deeprlhf.pdf:pdf},
  openalex = {W4321392130},
  pages = {4299--4307},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
  title = {Deep Reinforcement Learning from Human Preferences},
  volume = {30},
  year = {2017}
}

@article{wirth2017surveypbrl,
  abstract = {Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.},
  author = {Christian Wirth and Riad Akrour and Gerhard Neumann and Johannes Fürnkranz},
  file = {:/home/b/documents/articles/wirth2017surveypbrl.pdf:pdf},
  journal = {Journal of Machine Learning Research},
  number = {136},
  openalex = {W2785324569},
  pages = {1--46},
  pdf = {https://jmlr.org/papers/volume18/16-634/16-634.pdf},
  title = {A Survey of Preference-Based Reinforcement Learning Methods},
  url = {https://jmlr.org/papers/v18/16-634.html},
  volume = {18},
  year = {2017}
}

@inproceedings{lillicrap2016continuous,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies ``end-to-end'': directly from raw pixel inputs.},
  address = {San Juan, Puerto Rico},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle = {4th International Conference on Learning Representations},
  eprint = {1509.02971},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/lillicrap2016continuous.pdf:pdf},
  month = {5},
  openalex = {W2963864421},
  pdf = {https://arxiv.org/pdf/1509.02971.pdf},
  title = {Continuous control with deep reinforcement learning},
  url = {https://arxiv.org/abs/1509.02971},
  year = {2016}
}

@inproceedings{mnih2016asynchronous,
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers.},
  author = {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  editor = {Maria Florina Balcan and Kilian Q. Weinberger},
  file = {:/home/b/documents/inproceedings/mnih2016asynchronous.pdf:pdf},
  openalex = {W2964043796},
  pages = {1928--1937},
  pdf = {http://proceedings.mlr.press/v48/mniha16.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  url = {https://proceedings.mlr.press/v48/mniha16.html},
  volume = {48},
  year = {2016}
}

@inproceedings{schaul2016prioritized,
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  address = {San Juan, Puerto Rico},
  author = {Tom Schaul and John Quan and Ioannis Antonoglou and David Silver},
  booktitle = {4th International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/schaul2016prioritized.pdf:pdf},
  month = {5},
  openalex = {W2201581102},
  pdf = {https://arxiv.org/pdf/1511.05952},
  title = {Prioritized Experience Replay},
  url = {https://openreview.net/group?id=ICLR.cc/2016},
  year = {2016}
}

@article{silver2016mastering,
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  doi = {10.1038/nature16961},
  file = {:/home/b/documents/articles/silver2016mastering.pdf:pdf},
  journal = {Nature},
  number = {7587},
  openalex = {W2257979135},
  pages = {484--489},
  pdf = {https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf},
  publisher = {Nature Publishing Group},
  title = {Mastering the game of Go with deep neural networks and tree search},
  volume = {529},
  year = {2016}
}

@inproceedings{van2016deep,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  author = {Hado van Hasselt and Arthur Guez and David Silver},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v30i1.10295},
  file = {:/home/b/documents/inproceedings/van2016deep.pdf:pdf},
  number = {1},
  openalex = {W2746553466},
  pages = {2094--2100},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154},
  title = {Deep Reinforcement Learning with Double Q-learning},
  volume = {30},
  year = {2016}
}

@inproceedings{wang2016dueling,
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  file = {:/home/b/documents/inproceedings/wang2016dueling.pdf:pdf},
  month = {6},
  openalex = {W2173564293},
  pages = {1995--2003},
  pdf = {https://proceedings.mlr.press/v48/wangf16.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dueling Network Architectures for Deep Reinforcement Learning},
  url = {https://proceedings.mlr.press/v48/wangf16.html},
  volume = {48},
  year = {2016}
}

@article{mnih2015human,
  abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters.},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  doi = {10.1038/nature14236},
  journal = {Nature},
  month = {2},
  note = {The expanded journal version of the original DQN paper. It introduces a key stabilization technique: the use of a separate, periodically updated target network to provide stable targets for the Q-learning updates. This work demonstrated superhuman performance across a suite of 49 Atari games with a single, general-purpose architecture, solidifying deep RL as a major field of research.},
  number = {7540},
  openalex = {W2145339207},
  pages = {529--533},
  pmid = {25719670},
  publisher = {Nature Publishing Group},
  title = {Human-level control through deep reinforcement learning},
  url = {https://www.nature.com/articles/nature14236},
  volume = {518},
  year = {2015}
}

@inproceedings{schulman2015trust,
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael I. and Moritz, Philipp},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schulman2015trust.pdf:pdf},
  note = {A seminal paper that addresses the instability of policy gradient methods. TRPO constrains policy updates to lie within a "trust region" defined by the KL-divergence between the old and new policies, guaranteeing monotonic policy improvement. While computationally intensive, it provided a robust method for optimizing large, nonlinear policies and set a new standard for performance on continuous control tasks.},
  openalex = {W1771410628},
  pages = {1889--1897},
  pdf = {http://proceedings.mlr.press/v37/schulman15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trust Region Policy Optimization},
  url = {https://arxiv.org/abs/1502.05477},
  volume = {37},
  year = {2015}
}

@inproceedings{mnih2013playing,
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose raw pixels and output value function estimates future rewards. We apply our method to seven Atari 2600 games in the Arcade Learning Environment, with no adjustment to architecture or algorithm. We find that it outperforms all previous approaches on six and surpasses human expert performance on three of them.},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  booktitle = {NIPS Deep Learning Workshop},
  doi = {10.48550/arxiv.1312.5602},
  file = {:/home/b/documents/inproceedings/mnih2013playing.pdf:pdf},
  note = {The paper that ignited the deep reinforcement learning revolution. It introduces the Deep Q-Network (DQN), the first model to successfully learn control policies directly from high-dimensional sensory input (raw pixels) using RL. It combines a deep convolutional network with Q-learning and introduces experience replay to stabilize training.},
  openalex = {W4298857966},
  pdf = {https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {https://arxiv.org/abs/1312.5602},
  year = {2013}
}

@inproceedings{sutton2000policy,
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  author = {Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  booktitle = {Advances in Neural Information Processing Systems 12},
  editor = {Solla, Sara A. and Leen, Todd K. and Müller, Klaus-Robert},
  isbn = {0-262-19450-3},
  note = {Establishes the Policy Gradient Theorem, a cornerstone of modern RL. The theorem provides a general form for the policy gradient that does not involve the derivative of the state distribution, making it practical to estimate. It unifies previous approaches like REINFORCE and actor-critic methods and proves the convergence of a policy iteration algorithm with function approximation to a locally optimal policy.},
  openalex = {W2155027007},
  pages = {1057--1063},
  publisher = {MIT Press},
  series = {NIPS Proceedings},
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  url = {https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
  volume = {12},
  year = {2000}
}

@inproceedings{konda1999actorcritic,
  abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
  address = {Cambridge, MA},
  author = {Konda, Vijay R. and Tsitsiklis, John N.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Solla, S. A. and Leen, T. K. and Müller, K.-R.},
  file = {:/home/b/documents/inproceedings/konda1999actorcritic.pdf:pdf},
  pages = {1008--1014},
  pdf = {https://proceedings.neurips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
  publisher = {MIT Press},
  title = {Actor-Critic Algorithms},
  url = {https://proceedings.neurips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html},
  volume = {12},
  year = {1999}
}

@article{watkins1992qlearning,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  doi = {10.1007/BF00992698},
  journal = {Machine Learning},
  note = {This journal article provides the formal convergence proof for the tabular Q-Learning algorithm introduced in Watkins' thesis. It proves that Q-learning converges to the optimal action-values with probability 1, provided all state-action pairs are visited infinitely often. This rigorous theoretical backing solidified Q-learning's status as a cornerstone of the field.},
  number = {3-4},
  openalex = {W32403112},
  pages = {279--292},
  publisher = {Springer},
  title = {Q-Learning},
  url = {https://link.springer.com/article/10.1007/BF00992698},
  volume = {8},
  year = {1992}
}

@article{williams1992simple,
  author = {Williams, Ronald J.},
  doi = {10.1007/BF00992696},
  file = {:/home/b/documents/articles/williams1992simple.pdf:pdf},
  journal = {Machine Learning},
  note = {Introduces the REINFORCE algorithm, a foundational policy gradient method. It provides a general way to estimate the gradient of the expected reward with respect to the policy parameters using Monte Carlo sampling, establishing the mathematical basis for training stochastic policies via gradient ascent.},
  number = {3-4},
  openalex = {W2119717200},
  pages = {229--256},
  pdf = {https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf},
  publisher = {Springer},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  url = {https://link.springer.com/article/10.1007/BF00992696},
  volume = {8},
  year = {1992}
}

@article{watkins1992qlearning,
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  doi = {10.1007/BF00992698},
  file = {:/home/b/documents/articles/watkins1992qlearning.pdf:pdf},
  issn = {0885-6125},
  journal = {Machine Learning},
  month = {5},
  number = {3-4},
  openalex = {W32403112},
  pages = {279--292},
  pdf = {https://link.springer.com/content/pdf/10.1007/BF00992698.pdf},
  publisher = {Springer},
  title = {Technical Note: Q-learning},
  url = {https://doi.org/10.1007/BF00992698},
  volume = {8},
  year = {1992}
}

@article{williams1992reinforce,
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  author = {Williams, Ronald J.},
  doi = {10.1007/BF00992696},
  journal = {Machine Learning},
  month = {5},
  number = {3-4},
  openalex = {W2119717200},
  pages = {229--256},
  publisher = {Springer},
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  url = {https://link.springer.com/article/10.1007/BF00992696},
  volume = {8},
  year = {1992}
}

@phdthesis{watkins1989learning,
  abstract = {This thesis introduces the notion of reinforcement learning as learning to control a Markov Decision Process by incremental dynamic programming, and describes a range of algorithms for doing this, including Q-learning, for which a sketch of a proof of convergence is given. The work addresses learning from rewards and punishments as incrementally optimising control of a Markov Decision Process and proposes the Q-learning algorithm that can in principle learn optimal control directly without modelling the transition probabilities or expected rewards of the MDP.},
  address = {Cambridge, England},
  author = {Watkins, Christopher John Cornish Hellaby},
  file = {:/home/b/documents/phdthesis/watkins1989learning.pdf:pdf},
  keywords = {reinforcement learning, Q-learning, Markov decision process, dynamic programming, temporal difference learning},
  month = {5},
  note = {Introduces the groundbreaking Q-Learning algorithm. This work provided the first provably convergent, model-free reinforcement learning algorithm for control. Q-Learning elegantly combines temporal difference learning with a value iteration-like update, allowing an agent to learn the optimal action-value function directly from experience. Original electronic version lost; current version scanned from photocopy.},
  openalex = {W3011120880},
  pdf = {https://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf},
  school = {King's College, Cambridge},
  title = {Learning from Delayed Rewards},
  year = {1989}
}

@article{sutton1988learning,
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction---that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions.},
  author = {Sutton, Richard S.},
  doi = {10.1007/BF00115009},
  journal = {Machine Learning},
  note = {The foundational paper for modern reinforcement learning. It formally introduces temporal-difference (TD) learning, a model-free method for prediction. It proves that TD methods are more computationally efficient and learn faster than conventional supervised learning approaches for multi-step prediction problems by updating estimates based on other estimates (bootstrapping).},
  number = {1},
  openalex = {W3041202696},
  pages = {9--44},
  publisher = {Springer},
  title = {Learning to Predict by the Methods of Temporal Differences},
  url = {http://incompleteideas.net/papers/sutton-88-with-erratum.pdf},
  volume = {3},
  year = {1988}
}

@article{sutton1988temporal,
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction---that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions.},
  author = {Sutton, Richard S.},
  doi = {10.1007/BF00115009},
  issn = {0885-6125},
  journal = {Machine Learning},
  month = {8},
  number = {1},
  openalex = {W3041202696},
  pages = {9--44},
  publisher = {Springer},
  title = {Learning to Predict by the Methods of Temporal Differences},
  volume = {3},
  year = {1988}
}

@article{barto1983neuronlike,
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide.},
  author = {Andrew G. Barto and Richard S. Sutton and Charles W. Anderson},
  doi = {10.1109/TSMC.1983.6313077},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  month = {9},
  note = {Introduces the actor-critic architecture with the Associative Search Element (ASE) and the Adaptive Critic Element (ACE). The actor (ASE) learns and selects actions, while the critic (ACE) learns to predict future reinforcement, providing a more informative signal to the actor than the raw environmental reward. This separation of policy and value function is a foundational concept in modern RL.},
  number = {5},
  openalex = {W2091565802},
  pages = {834--846},
  publisher = {IEEE},
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  url = {https://ieeexplore.ieee.org/document/6313077/},
  volume = {SMC-13},
  year = {1983}
}

@book{howard1960dynamic,
  author = {Howard, Ronald A.},
  isbn = {0262080095},
  note = {Devised the policy iteration algorithm for solving Markov Decision Processes (MDPs). This work provided one of the first practical, iterative algorithms for finding optimal policies in MDPs, demonstrating how to systematically improve a policy by alternating between policy evaluation and policy improvement steps. Presents an analytic structure for decision-making systems based on Markov processes as system models with iterative optimization methods.},
  pages = {viii + 136},
  publisher = {MIT Press},
  series = {Technology Press Research Monographs},
  title = {Dynamic Programming and Markov Processes},
  url = {https://mitpress.mit.edu/9780262582260/dynamic-programming-and-markov-processes/},
  year = {1960}
}

@book{howard1960dynamic,
  abstract = {Presents an analytic structure for a decision-making system that is at the same time both general enough to be descriptive and yet computationally feasible. Based on the Markov process as a system model, and uses an iterative technique like dynamic programming as its optimization method. Focuses on analyzing processes of indefinite duration that will make many transitions before termination.},
  address = {Cambridge},
  author = {Howard, Ronald A.},
  isbn = {9780262080095},
  lccn = {60011030},
  note = {Outgrowth of the author's Sc.D. thesis submitted to the Department of Electrical Engineering, M.I.T., in June, 1958},
  pages = {viii + 136},
  publisher = {Technology Press of Massachusetts Institute of Technology},
  series = {Technology Press Research Monographs},
  title = {Dynamic Programming and Markov Processes},
  year = {1960}
}

@book{bellman1957dynamic,
  abstract = {This classic book introduces dynamic programming, presented by the scientist who coined the term and developed the theory in its early stages. The book provides a mathematical theory of multistage decision processes and explores optimal policies using a functional equation approach. It covers topics including inventory equations, bottleneck problems, and game strategies, requiring only a basic mathematical foundation including calculus. The work serves as a mathematical tool for treating complex problems across diverse fields like mathematical economics, logistics, scheduling theory, communication theory, and control processes.},
  author = {Bellman, Richard},
  isbn = {978-0691079516},
  month = {10},
  note = {Introduces the principle of optimality and the functional equation that bears his name, the Bellman equation. This work lays the entire mathematical foundation for solving sequential decision-making problems by breaking them down into smaller, recursive subproblems, forming the basis of modern reinforcement learning's value functions.},
  openalex = {W2341171179},
  pages = {342},
  publisher = {Princeton University Press},
  title = {Dynamic Programming},
  url = {https://press.princeton.edu/books/hardcover/9780691125559/dynamic-programming},
  year = {1957}
}

@book{bellman1957dynamic,
  abstract = {An introduction to the mathematical theory of multistage decision processes, this text takes a 'functional equation' approach to the discovery of optimum policies. Written by a leading developer of such policies, it presents a series of methods, uniqueness and existence theorems, and examples for solving the relevant equations. The text examines existence and uniqueness theorems, the optimal inventory equation, bottleneck problems in multistage production processes, a new formalism in the calculus of variation, strategies behind multistage games, and Markovian decision processes. The book is written at a moderate mathematical level, requiring only a basic foundation in mathematics, including calculus.},
  address = {Princeton, N.J.},
  author = {Richard E. Bellman},
  isbn = {9780691079516},
  keywords = {dynamic programming, optimization, decision processes, operations research, mathematical programming},
  note = {Dover reprint available with ISBN 0486428095},
  pages = {xxv+342},
  publisher = {Princeton University Press},
  title = {Dynamic Programming},
  url = {https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming},
  year = {1957}
}

@techreport{dreyfus1956survey,
  author = {Dreyfus, Stuart E.},
  institution = {RAND Corporation},
  title = {A Survey of Dynamic Programming},
  year = {1956}
}
