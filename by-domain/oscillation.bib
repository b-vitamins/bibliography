@misc{boyer2025learning,
  archiveprefix = {arXiv},
  author = {Boyer, J. and Rusch, T. K. and Rus, D.},
  eprint = {2505.12171},
  note = {Introduces D-LinOSS, an improvement on LinOSS that adds a learnable damping term, allowing the model to dissipate energy (forget) on multiple timescales, increasing expressivity.},
  primaryclass = {cs.LG},
  title = {Learning to Dissipate Energy in Oscillatory State-Space Models},
  url = {https://arxiv.org/pdf/2505.12171},
  year = {2025}
}

@misc{darlow2025continuous,
  archiveprefix = {arXiv},
  author = {Darlow, Luke and Regan, Ciaran and Risi, Sebastian and Seely, Jeffrey and Jones, Llion},
  eprint = {2505.05522},
  note = {Introduces the Continuous Thought Machine (CTM), a model leveraging neural dynamics as core representation with neuron-level temporal processing and neural synchronization. Demonstrates emergent oscillatory behavior and traveling waves similar to biological neural processing.},
  primaryclass = {cs.LG},
  title = {Continuous Thought Machines},
  url = {https://arxiv.org/abs/2505.05522},
  year = {2025}
}

@misc{fanaskov2025binding,
  archiveprefix = {arXiv},
  author = {Fanaskov, Vladimir and Oseledets, Ivan},
  eprint = {2505.03648},
  note = {Proposes a more theoretically coherent Hopfield-Kuramoto framework to ground the coupling of oscillatory and static threshold neurons, providing a Lyapunov function and linking to concepts like LoRA.},
  title = {Binding threshold units with artificial oscillatory neurons},
  url = {https://arxiv.org/pdf/2505.03648},
  year = {2025}
}

@misc{merrill2025mechanistic,
  archiveprefix = {arXiv},
  author = {Merrill, K. and others},
  eprint = {2505.15105},
  note = {TLDR: Performs a direct mechanistic comparison of Transformers and SSMs, finding that they learn fundamentally different algorithms for associative recall despite achieving similar performance.},
  title = {Mechanistic evaluation of Transformers and state space models},
  year = {2025}
}

@inproceedings{miyato2025artificial,
  author = {Miyato, Takeru and Löwe, Sindy and Geiger, Andreas and Welling, Max},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Oral presentation. Introduces a novel neuron based on a generalized Kuramoto model of coupled oscillators on a hypersphere. Demonstrates strong performance in unsupervised object discovery, reasoning, and robustness.},
  openalex = {W4403580139},
  title = {Artificial Kuramoto Oscillatory Neurons},
  url = {https://www.cvlibs.net/publications/Miyato2025ICLR.pdf},
  year = {2025}
}

@misc{muzellec2025enhancing,
  archiveprefix = {arXiv},
  author = {Muzellec, Sabine and Alamia, Andrea and Serre, Thomas and VanRullen, Rufin},
  doi = {10.48550/arXiv.2502.21077},
  eprint = {2502.21077},
  note = {Combines complex-valued representations with Kuramoto dynamics to promote phase synchronization for object binding in visual scenes. Demonstrates improved performance on multi-object tasks through synchrony-based mechanisms inspired by neural synchrony in the brain.},
  primaryclass = {cs.CV},
  title = {Enhancing deep neural networks through complex-valued representations and Kuramoto synchronization dynamics},
  url = {https://arxiv.org/abs/2502.21077},
  year = {2025}
}

@article{rohan2025classifier,
  author = {Rohan, N. R. and others},
  doi = {10.1101/2025.06.27.661914},
  journal = {bioRxiv},
  note = {Proposes a novel Y-shaped architecture to simultaneously model behavior (image classification) and neural dynamics (EEG signal generation) from a single stimulus, as a tool for computational neuroscience.},
  openalex = {W4411969226},
  publisher = {Cold Spring Harbor Laboratory},
  title = {Classifier-guided Deep Oscillatory Neural Networks (cDONN) for simultaneous brain and behaviour modelling},
  url = {https://www.biorxiv.org/content/10.1101/2025.06.27.661914v1.full.pdf},
  year = {2025}
}

@inproceedings{rusch2025oscillatory,
  author = {Rusch, T. Konstantin and Rus, Daniela},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Oral presentation. Proposes LinOSS, a highly stable and provably universal State-Space Model based on forced harmonic oscillators. Outperforms state-of-the-art models like Mamba on very long sequence tasks.},
  openalex = {W4403929263},
  title = {Oscillatory State-Space Models},
  url = {https://arxiv.org/pdf/2410.03943},
  year = {2025}
}

@misc{somvanshi2025s4,
  archiveprefix = {arXiv},
  author = {Somvanshi, S. and others},
  eprint = {2503.18970},
  note = {TLDR: A comprehensive survey tracing the rapid evolution of SSMs from S4 to Mamba, detailing architectural improvements and performance gains across various domains.},
  title = {From S4 to Mamba: A Comprehensive Survey on Structured State Space Models},
  year = {2025}
}

@misc{stolzle2025learning,
  author = {Stölzle, M. and Rusch, T. K. and Patterson, Z. J. and Pérez-Dattari, R. and Stella, F. and Hughes, J.},
  howpublished = {arXiv preprint},
  title = {Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees},
  year = {2025}
}

@article{yao2025brain,
  author = {Yao, R. and others},
  doi = {10.1103/PhysRevE.111.044310},
  journal = {Physical Review E},
  note = {Develops a Hopfield-Kuramoto model where multiple brain wave patterns are stored as attractors using a Hebbian-like rule for connectivity strength, providing a physics-based generative model for fMRI patterns.},
  openalex = {W4409499304},
  pages = {044310},
  publisher = {American Physical Society},
  title = {Brain wave dynamics in a Hopfield-Kuramoto model with heterogeneous connectivity strength},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.111.044310},
  volume = {111},
  year = {2025}
}

@article{bittar2024emergence,
  author = {Bittar, A. and Garner, P. N.},
  journal = {Frontiers in Neuroscience},
  title = {Emergence of neural oscillations in a scalable, end-to-end trainable, and physiologically inspired speech recognition architecture},
  year = {2024}
}

@article{clark2024theory,
  author = {Clark, David G. and Abbott, L. F.},
  doi = {10.1103/PhysRevX.14.021001},
  journal = {Physical Review X},
  note = {Explores networks where synapses are also dynamic variables, revealing novel dynamic regimes like "freezable chaos" and proposing a new physical mechanism for working memory based on synaptic stability.},
  openalex = {W4393386271},
  pages = {021001},
  publisher = {American Physical Society},
  title = {Theory of Coupled Neuronal-Synaptic Dynamics},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.14.021001},
  volume = {14},
  year = {2024}
}

@misc{ding2024remamba,
  archiveprefix = {arXiv},
  author = {Ding, Jiayu and others},
  eprint = {2408.15496},
  note = {TLDR: Addresses Mamba's limitations in long-context tasks by proposing a two-stage re-forward process that enhances its long-sequence comprehension with minimal overhead.},
  title = {ReMamba: Equip Mamba with Effective Long-Sequence Modeling},
  year = {2024}
}

@article{fan2024brain,
  author = {Fan, Z. and Liu, Z. and Wang, R. and Li, C. and Wang, Y.},
  journal = {Physical Review E},
  title = {Brain wave dynamics in a Hopfield-Kuramoto model},
  year = {2024}
}

@misc{huang2024freezing,
  author = {Huang, W. and Huang, H.},
  howpublished = {arXiv preprint},
  title = {Freezing chaos without synaptic plasticity},
  year = {2024}
}

@misc{li2024videomamba,
  archiveprefix = {arXiv},
  author = {Li, K. and others},
  eprint = {2403.06977},
  note = {TLDR: Adapts the Mamba architecture to the video domain by introducing a bidirectional state-space model to efficiently capture spatial and temporal information.},
  title = {VideoMamba: State Space Model for Efficient Video Understanding},
  year = {2024}
}

@inproceedings{nguyen2024kuramotognn,
  author = {Nguyen, D. Q. and Nguyen, K. and Nguyen, D. V. and Nguyen, T. D.},
  booktitle = {Proceedings of Machine Learning Research},
  title = {KuramotoGNN: Reducing Over-smoothing via a Kuramoto Model-based Approach},
  year = {2024}
}

@misc{rohan2024deep,
  archiveprefix = {arXiv},
  author = {Rohan, Nurani Rajagopal and C, Vigneswaran and Ghosh, Sayan and Rajendran, Kishore and A, Gaurav and Chakravarthy, V. Srinivasa},
  eprint = {2405.03725},
  note = {Presents a trainable deep network using nonlinear Hopf oscillators and complex-valued weights to bridge the gap between AI performance and biologically plausible brain dynamics.},
  openalex = {W4396813460},
  primaryclass = {cs.NE},
  title = {Deep Oscillatory Neural Network},
  url = {https://arxiv.org/html/2405.03725v1},
  year = {2024}
}

@article{rudner2024design,
  author = {Rudner, T. and Porod, W. and Csaba, G.},
  journal = {Frontiers in Neuroscience},
  note = {TLDR: Demonstrates the use of backpropagation through time to design the coupling weights in networks of ring oscillators, enabling the creation of high-performance oscillatory associative memories.},
  title = {Design of oscillatory neural networks by machine learning},
  year = {2024}
}

@article{zang2024structural,
  author = {Zang, Jie and Liu, Shenquan and Helson, Pascal and Kumar, Arvind},
  doi = {10.7554/eLife.88777},
  journal = {eLife},
  note = {Uses dynamical systems theory to prove that generating oscillations in a threshold-linear network requires specific structural conditions, including cycles with odd numbers of inhibitory nodes.},
  openalex = {W4392761779},
  pages = {e88777},
  title = {Structural constraints on the emergence of oscillations in multi-population neural networks},
  url = {https://elifesciences.org/articles/88777},
  volume = {13},
  year = {2024}
}

@misc{bassey2023survey,
  archiveprefix = {arXiv},
  author = {Bassey, J. and Qian, L. and Li, X.},
  eprint = {2312.06087},
  note = {TLDR: A comprehensive survey of CVNN theory and applications, covering complex activation functions, Wirtinger calculus for learning, and the benefits for wave-like data.},
  title = {A survey of complex-valued neural networks},
  year = {2023}
}

@inproceedings{dao2023flashattention,
  author = {Dao, Tri},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {TLDR: Presents a highly optimized implementation of the attention mechanism that is significantly faster and more memory-efficient, pushing the practical limits of sequence length for Transformers.},
  title = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  year = {2023}
}

@misc{ding2023longnet,
  archiveprefix = {arXiv},
  author = {Ding, Jiayu and others},
  eprint = {2307.02486},
  note = {TLDR: Proposes "dilated attention," a mechanism that expands the attentive field exponentially with distance, allowing Transformer models to scale to billion-token sequences with linear complexity.},
  title = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  year = {2023}
}

@misc{gu2023mamba,
  archiveprefix = {arXiv},
  author = {Gu, Albert and Dao, Tri},
  eprint = {2312.00752},
  note = {TLDR: Introduced Mamba, which makes SSM parameters input-dependent via a selection mechanism, allowing the model to selectively process information and achieve Transformer-level performance with linear-time complexity.},
  title = {Mamba: Linear-time sequence modeling with selective state spaces},
  year = {2023}
}

@misc{he2023robust,
  author = {He, Yixuan and Reinert, Gesine and Wipf, David and Cucuringu, Mihai},
  howpublished = {arXiv preprint},
  title = {Robust Angular Synchronization via Directed Graph Neural Networks},
  year = {2023}
}

@inproceedings{lanthaler2023neural,
  author = {Lanthaler, Samuel and Rusch, T. Konstantin and Mishra, Siddhartha},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Provides a foundational theoretical result proving that a general, abstract class of neural oscillators are universal approximators for continuous and causal operators, justifying the field's pursuit.},
  openalex = {W4376654394},
  pages = {46786--46806},
  title = {Neural Oscillators are Universal},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/923285deb805c3e14e1aeebc9854d644-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{poli2023hyenadna,
  author = {Poli, Michael and others},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {TLDR: Introduces a genomic foundation model based on the Hyena (long convolution) architecture that can process context lengths of up to 1 million tokens, setting a new state of the art in genomics.},
  title = {HyenaDNA: Long-range genomic sequence modeling at single nucleotide resolution},
  year = {2023}
}

@misc{gu2022parameterization,
  archiveprefix = {arXiv},
  author = {Gu, Albert and Gupta, Ankit and Goel, Karan and Ré, Christopher},
  eprint = {2206.11893},
  note = {TLDR: Introduced S4D, a simplified and more efficient version of S4 that uses a diagonal state matrix, demonstrating that a diagonal approximation of the HiPPO matrix retains strong performance.},
  title = {On the parameterization and initialization of diagonal state space models},
  year = {2022}
}

@article{kazis2022survey,
  author = {Kazis, Panos and others},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  note = {TLDR: A comprehensive survey providing a unified mathematical framework for deep learning on non-Euclidean domains, including graph neural networks and manifold-based models.},
  title = {A survey on geometric deep learning on graphs and manifolds},
  year = {2022}
}

@inproceedings{rusch2022graph,
  author = {Rusch, T. K. and Chamberlain, B. P. and Rowbottom, J. and Mishra, S. and Bronstein, M. M.},
  booktitle = {International Conference on Machine Learning (ICML)},
  title = {Graph-coupled oscillator networks},
  year = {2022}
}

@misc{smith2022simplified,
  archiveprefix = {arXiv},
  author = {Smith, Jimmy T. and Warrington, Andrew and Linderman, Scott W.},
  eprint = {2208.04933},
  note = {TLDR: Proposed the S5 layer, which streamlines S4 by using a single MIMO SSM and an efficient parallel scan, simplifying the implementation while matching S4's performance.},
  title = {Simplified state space layers for sequence modeling},
  year = {2022}
}

@article{abernot2021digital,
  author = {Abernot, Madeleine and Gil, Thierry and Jiménez, Manuel and Núñez, Juan and Avellido, María J. and Linares-Barranco, Bernabé and Gonos, Théophile and Hardelin, Tanguy and Todri-Sanial, Aida},
  doi = {10.3389/fnins.2021.713054},
  journal = {Frontiers in Neuroscience},
  note = {Addresses the digital implementation of oscillatory neural networks for practical image recognition applications.},
  openalex = {W3161228231},
  title = {Digital Implementation of Oscillatory Neural Network for Image Recognition Applications},
  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.713054/full},
  volume = {15},
  year = {2021}
}

@inproceedings{chamberlain2021grand,
  author = {Chamberlain, B. P. and others},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {TLDR: Connects GNNs to dynamical systems by framing message passing as a diffusion process governed by a graph-based differential equation, providing a deeper theoretical understanding of GNNs.},
  title = {GRAND: Graph neural diffusion},
  year = {2021}
}

@article{elhage2021mathematical,
  author = {Elhage, Nelson and others},
  journal = {Transformer Circuits Thread, Anthropic},
  note = {TLDR: Provides a detailed mathematical framework for analyzing algorithms within Transformers, notably identifying key circuits like "induction heads" used for in-context learning.},
  title = {A mathematical framework for transformer circuits},
  year = {2021}
}

@inproceedings{gu2021efficiently,
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  booktitle = {International Conference on Learning Representations},
  note = {TLDR: The seminal paper introducing the S4 model, which combines HiPPO theory with an efficient convolutional computation to set a new state of the art on long-range dependency benchmarks.},
  title = {Efficiently modeling long sequences with structured state spaces},
  year = {2021}
}

@inproceedings{tay2021long,
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and others},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {TLDR: Introduced the Long Range Arena (LRA) benchmark, a suite of long-context tasks that systematically evaluated efficient Transformers and catalyzed the development of SSMs.},
  title = {Long range arena: A benchmark for efficient transformers},
  year = {2021}
}

@inproceedings{yildiz2021continuous,
  author = {Yildiz, Cagatay and Heinonen, Markus and Lahdesmaki, Harri},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {TLDR: Applies the continuous-time modeling paradigm to reinforcement learning using Bayesian neural ODEs, avoiding time-discretization errors and improving robustness.},
  title = {Continuous-time model-based reinforcement learning},
  year = {2021}
}

@article{csaba2020coupled,
  author = {Csaba, G. and Porod, W.},
  journal = {Applied Physics Reviews},
  title = {Coupled oscillators for computing: A review and perspective},
  year = {2020}
}

@misc{gu2020hippo,
  archiveprefix = {arXiv},
  author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Ré, Christopher},
  eprint = {2008.07669},
  note = {TLDR: Formalized the HiPPO framework for constructing state-space matrices that optimally compress and reconstruct continuous signals, laying the direct mathematical groundwork for S4.},
  title = {Hippo: A framework for online memory and function approximation},
  year = {2020}
}

@inproceedings{kidger2020neural,
  author = {Kidger, Patrick and Morrill, James and Lyons, Terry},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {TLDR: Extends Neural ODEs to Neural Controlled Differential Equations (NCDEs), a more powerful approach for handling irregularly-sampled time-series by treating the input as a continuous control path.},
  title = {Neural controlled differential equations for irregularly-sampled data},
  year = {2020}
}

@article{olah2020zoom,
  author = {Olah, Chris and others},
  journal = {Distill},
  note = {TLDR: A foundational article in mechanistic interpretability that introduces the "circuits" perspective, arguing for analyzing the algorithms implemented by small, interconnected groups of neurons.},
  title = {Zoom in: An introduction to circuits},
  year = {2020}
}

@inproceedings{pareja2020evolvegcn,
  author = {Pareja, A. and others},
  booktitle = {AAAI Conference on Artificial Intelligence},
  note = {TLDR: Proposes a dynamic GNN that adapts its parameters over time using an RNN, allowing the model to evolve as the graph structure changes.},
  title = {EvolveGCN: Evolving graph convolutional networks for dynamic graphs},
  year = {2020}
}

@misc{ramsauer2020hopfield,
  archiveprefix = {arXiv},
  author = {Ramsauer, Hubert and others},
  eprint = {2008.02217},
  note = {TLDR: Revealed a surprising equivalence between the attention mechanism in Transformers and the update rule of modern Hopfield networks, reframing attention as memory retrieval.},
  title = {Hopfield networks is all you need},
  year = {2020}
}

@inproceedings{voelker2019legendre,
  author = {Voelker, Aaron and Kajić, Ivana and Eliasmith, Chris},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {TLDR: Introduced the Legendre Memory Unit (LMU), a memory cell for RNNs based on a linear ODE that projects the input history onto Legendre polynomials, serving as a precursor to HiPPO.},
  title = {Legendre memory units: Continuous-time representation in recurrent neural networks},
  year = {2019}
}

@inproceedings{chen2018neural,
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {TLDR: This groundbreaking paper introduced the Neural ODE framework, treating deep networks as continuous-depth models trained with constant memory cost, providing a new paradigm for time-series modeling.},
  title = {Neural ordinary differential equations},
  year = {2018}
}

@inproceedings{trabelsi2018deep,
  author = {Trabelsi, C. and others},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {TLDR: A foundational paper on modern deep complex-valued networks, introducing complex versions of standard components like convolutions and batch normalization.},
  title = {Deep complex networks},
  year = {2018}
}

@article{ashwin2016mathematical,
  author = {Ashwin, Peter and Coombes, Stephen and Nicks, Rachel},
  doi = {10.1186/s13408-015-0033-6},
  journal = {Journal of Mathematical Neuroscience},
  note = {Provides mathematical frameworks for understanding oscillatory dynamics in neural networks from a neuroscience perspective.},
  openalex = {W2220951078},
  pages = {2},
  title = {Mathematical Frameworks for Oscillatory Network Dynamics in Neuroscience},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC4703605/},
  volume = {6},
  year = {2016}
}

@article{krotov2016dense,
  author = {Krotov, D. and Hopfield, J. J.},
  journal = {Neural Computation},
  note = {TLDR: Introduced the "modern Hopfield network" with higher-order energy functions, dramatically increasing memory storage capacity beyond the linear limits of the original model.},
  title = {Dense associative memory for pattern recognition},
  year = {2016}
}

@article{lukosevicius2009reservoir,
  author = {Lukoševičius, M. and Jaeger, H.},
  journal = {Computer Science Review},
  note = {TLDR: A comprehensive and highly-cited review of the reservoir computing paradigm, providing a practical guide to the theory, design, and application of ESNs and LSMs.},
  title = {Reservoir computing approaches to recurrent neural network training},
  year = {2009}
}

@article{acebron2005kuramoto,
  author = {Acebrón, J. A. and others},
  journal = {Reviews of Modern Physics},
  note = {TLDR: A comprehensive review treating the Kuramoto model from a statistical physics perspective, covering its equilibrium and non-equilibrium properties and diverse applications.},
  title = {The Kuramoto model: A simple paradigm for synchronization phenomena},
  year = {2005}
}

@article{maass2002real,
  author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
  journal = {Neural Computation},
  note = {TLDR: Introduced the Liquid State Machine (LSM), a spiking neuron-based reservoir model that frames computation as the interpretation of transient dynamics in a recurrent circuit.},
  title = {Real-time computing without stable states: a new framework for neural computation based on perturbations},
  year = {2002}
}

@techreport{jaeger2001echo,
  author = {Jaeger, Herbert},
  institution = {GMD -- German National Research Center for Information Technology},
  note = {TLDR: The foundational report introducing the Echo State Network (ESN), a reservoir computing model where only a linear readout is trained, avoiding the difficulties of training recurrent weights.},
  number = {148},
  title = {The "echo state" approach to analysing and training recurrent neural networks},
  year = {2001}
}

@article{strogatz2000kuramoto,
  author = {Strogatz, S. H.},
  journal = {Physica D: Nonlinear Phenomena},
  note = {TLDR: A landmark review that contextualizes Kuramoto's work and details subsequent mathematical developments, providing a rigorous yet accessible overview of the field.},
  title = {From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators},
  year = {2000}
}

@article{singer1999neuronal,
  author = {Singer, W.},
  journal = {Neuron},
  note = {TLDR: A key review article summarizing the experimental evidence for the binding-by-synchrony hypothesis, particularly the role of gamma-band oscillations in the visual cortex.},
  title = {Neuronal synchrony: a versatile code for the definition of relations?},
  year = {1999}
}

@article{singer1995visual,
  author = {Singer, W. and Gray, C. M.},
  journal = {Annual Review of Neuroscience},
  title = {Visual feature integration and the temporal correlation hypothesis},
  year = {1995}
}

@article{gray1989stimulus,
  author = {Gray, C. M. and Singer, W.},
  journal = {Proceedings of the National Academy of Sciences},
  title = {Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex},
  year = {1989}
}

@article{eckhorn1988coherent,
  author = {Eckhorn, R. and Bauer, R. and Jordan, W. and Brosch, M. and Kruse, W. and Munk, M. and Reitboeck, H. J.},
  journal = {Biological Cybernetics},
  title = {Coherent oscillations: A mechanism of feature linking in the visual cortex?},
  year = {1988}
}

@article{hopfield1982neural,
  author = {Hopfield, J. J.},
  journal = {Proceedings of the National Academy of Sciences},
  note = {TLDR: The seminal paper introducing the Hopfield network, which established a link between spin glass models and neural computation, demonstrating emergent content-addressable memory.},
  title = {Neural networks and physical systems with emergent collective computational abilities},
  year = {1982}
}

@techreport{vondermalsburg1981correlation,
  author = {von der Malsburg, C.},
  institution = {Max-Planck-Institute for Biophysical Chemistry},
  note = {TLDR: The foundational report proposing that the brain solves the "binding problem" by using the temporal correlation (synchronization) of neural firing to dynamically group features.},
  number = {81-2},
  title = {The Correlation Theory of Brain Function},
  type = {Internal Report},
  year = {1981}
}

@article{amari1977dynamics,
  author = {Amari, S.},
  journal = {Biological Cybernetics},
  title = {Dynamics of pattern formation in lateral-inhibition type neural fields},
  year = {1977}
}

@article{grossberg1976adaptive,
  author = {Grossberg, S.},
  journal = {Biological Cybernetics},
  title = {Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors},
  year = {1976}
}

@inproceedings{kuramoto1975self,
  author = {Kuramoto, Y.},
  booktitle = {International Symposium on Mathematical Problems in Theoretical Physics},
  note = {TLDR: The foundational paper that introduced the now-famous Kuramoto model, presenting a mathematically tractable framework for spontaneous synchronization in populations of coupled oscillators.},
  title = {Self-entrainment of a population of coupled non-linear oscillators},
  year = {1975}
}

@article{milner1974model,
  author = {Milner, P. M.},
  journal = {Psychological Review},
  title = {A model for visual shape recognition},
  year = {1974}
}
