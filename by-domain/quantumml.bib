@article{larocca2025barren,
  abstract = {Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm---choices of ansatz, initial state, observable, loss function and hardware noise---can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.},
  author = {Larocca, Martin and Thanasilp, Supanut and Wang, Samson and Sharma, Kunal and Biamonte, Jacob and Coles, Patrick J and Cincio, Lukasz and McClean, Jarrod R and Holmes, Zoë and Cerezo, M},
  doi = {10.1038/s42254-025-00813-9},
  journal = {Nature Reviews Physics},
  number = {1},
  pages = {174--189},
  publisher = {Nature Publishing Group},
  title = {Barren plateaus in variational quantum computing: A review},
  url = {https://www.nature.com/articles/s42254-025-00813-9},
  volume = {7},
  year = {2025}
}

@article{smaldone2025quantum,
  abstract = {The nexus of quantum computing and machine learning offers the potential for significant advancements in chemistry. This Review specifically explores the potential of quantum neural networks on gate-based quantum computers within the context of drug discovery. Applications to drug discovery are highlighted, including molecular property prediction and molecular generation. We provide a balanced perspective, emphasizing both the potential benefits and the challenges that must be addressed.},
  author = {Anthony M. Smaldone and Yu Shee and Gregory W. Kyro and Chuzhi Xu and Nam P. Vu and Rishab Dutta and Marwa H. Farag and Alexey Galda and Sandeep Kumar and Elica Kyoseva and Vı́ctor S. Batista},
  doi = {10.1021/acs.chemrev.4c00678},
  journal = {Chemical Reviews},
  number = {12},
  openalex = {W4411084656},
  pages = {5436--5460},
  publisher = {ACS Publications},
  title = {Quantum Machine Learning in Drug Discovery: Applications in Academia and Pharmaceutical Industries},
  volume = {125},
  year = {2025}
}

@article{larocca2025quantum,
  abstract = {We prove that the outputs of certain models based on Haar random unitary or orthogonal deep quantum neural networks (QNNs) converge to Gaussian processes in the limit of large Hilbert space dimension. Unlike the classical case where the proof follows from the central limit theorem, the quantum setting is more intricate due to the non-independence of unitary matrix entries. Our proof strategy shows that each moment of the QNN's output distribution converges to that of a multivariate Gaussian. This result provides a theoretical foundation for quantum-native machine learning approaches that sidestep common issues like barren plateaus encountered in parametric quantum models.},
  arxiv = {2305.09957},
  author = {García-Martín, Diego and Larocca, Martín and Cerezo, M.},
  doi = {10.1038/s41567-025-02883-z},
  journal = {Nature Physics},
  pages = {1153},
  publisher = {Nature Publishing Group},
  title = {Quantum neural networks form Gaussian processes},
  url = {https://www.nature.com/articles/s41567-025-02883-z},
  volume = {21},
  year = {2025}
}

@article{evans2025supervised,
  archiveprefix = {arXiv},
  author = {Evans, Theodore J and Liu, Weiwen and Cerezo, Marco},
  eprint = {2501.04729},
  journal = {arXiv preprint},
  note = {Note: arXiv ID 2501.04729 does not match this title/authors. Verification needed.},
  title = {Supervised quantum machine learning: A future outlook from qubits to enterprise applications},
  year = {2025}
}

@article{chen2025quantum,
  abstract = {We propose a quantum algorithm that rigorously demonstrates that quantum kernel methods enhance the efficiency of multiclass classification in real-world applications, providing quantum advantage. The algorithm is developed in the context that straightforward binary classification algorithms fall short in solving multiclass classification problems. To demonstrate quantum advantage, we design six distinct quantum kernels within our quantum algorithm to map input data into quantum state spaces and estimate the corresponding quantum kernel matrices. We utilize instantaneous quantum polynomial circuits to develop three types of quantum kernels: full, linear, and circular, and leverage parameterized quantum circuits featuring trainable single-qubit rotation layers to develop three additional types: Pauli-X, Pauli-Y, and Pauli-Z. We leverage a variety of performance metrics to comprehensively evaluate the classification and generalization performance. Results from quantum simulations reveal that our quantum algorithm outperforms its classical counterpart in handling six real-world multiclass classification problems. These results demonstrate that our quantum algorithm achieves superior classification and better generalization performance relative to classical counterparts. The optimal quantum kernel is significantly contingent upon the distribution and structure of the real-world dataset, and our quantum algorithm with the optimal quantum kernel outperforms its classical counterparts in solving multiclass classification tasks.},
  arxiv = {2411.02913},
  author = {Ding, Chao and Wang, Shi and Wang, Yaonan and Gao, Weibo},
  doi = {10.1103/PhysRevA.111.062410},
  journal = {Physical Review A},
  month = {6},
  number = {6},
  openalex = {W4411091673},
  pages = {062410},
  publisher = {American Physical Society},
  title = {Quantum machine learning for multiclass classification beyond kernel methods},
  volume = {111},
  year = {2025}
}

@article{chen2025quantum,
  abstract = {Quantum machine learning is considered one of the current research fields with great potential. In recent years, Havlíček et al. proposed a quantum machine learning algorithm with quantum-enhanced feature spaces, which effectively addressed a binary classification problem on a superconducting processor and offered a potential pathway to achieving quantum advantage. However, a straightforward binary classification algorithm falls short in solving multiclass classification problems. In this paper, the authors propose a quantum algorithm that rigorously demonstrates that quantum kernel methods enhance the efficiency of multiclass classification in real-world applications, providing quantum advantage. To demonstrate this, they design six distinct quantum kernels within the quantum algorithm to map input data into quantum state spaces, estimate corresponding quantum kernel matrices, and use quantum simulations to show the algorithm outperforms classical counterparts in handling six real-world multiclass classification problems. The results demonstrate that the quantum algorithm achieves superior classification and better generalization performance relative to classical approaches.},
  author = {Ding, Chao and Wang, Shi and Wang, Yaonan and Gao, Weibo},
  doi = {10.1103/PhysRevA.111.062410},
  file = {:/home/b/documents/articles/chen2025quantum.pdf:pdf},
  journal = {Physical Review A},
  month = {6},
  number = {6},
  openalex = {W4404404155},
  pages = {062410},
  pdf = {https://arxiv.org/pdf/2411.02913.pdf},
  publisher = {American Physical Society},
  title = {Quantum machine learning for multiclass classification beyond kernel methods},
  volume = {111},
  year = {2025}
}

@inproceedings{gili2025limitations,
  abstract = {We investigate quantum machine learning algorithms that can process quantum data either entirely using quantum methods or through measure-first protocols where quantum data is measured using a fixed strategy. We show that some learning problems can be efficiently solved by fully-quantum protocols while requiring exponential resources for measure-first protocols. This separation persists even for quantum data from polynomial-time quantum processes, underscoring the role of quantum data processing in machine learning and highlighting scenarios where quantum advantages appear.},
  address = {Vancouver, Canada},
  author = {Casper Gyurik and Riccardo Molteni and Vedran Dunjko},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gili2025limitations.pdf:pdf},
  month = {7},
  pdf = {https://arxiv.org/pdf/2311.12618.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Limitations of measure-first protocols in quantum machine learning},
  year = {2025}
}

@article{meyer2025ppo,
  abstract = {Quantum machine learning (QML), which combines quantum computing with machine learning, is widely believed to hold the potential to outperform traditional machine learning in the era of noisy intermediate-scale quantum (NISQ). As one of the most important types of QML, quantum reinforcement learning (QRL) with parameterized quantum circuits as agents has received extensive attention in the past few years. Various algorithms and techniques have been introduced, demonstrating the effectiveness of QRL in solving some popular benchmark environments such as CartPole, FrozenLake, and MountainCar. However, tackling more complex environments with continuous action spaces and high-dimensional state spaces remains challenging within the existing QRL framework. Here we present PPO-Q, which, by integrating a hybrid quantum-classical networks into the actor or critic part of the proximal policy optimization (PPO) algorithm, achieves state-of-the-art performance in a range of complex environments with significantly reduced training parameters. The hybrid quantum-classical networks in the PPO-Q incorporates two additional traditional neural networks to aid the parameterized quantum circuits in managing high-dimensional state encoding and action selection. When evaluated on 8888 diverse environments, including four with continuous action space, the PPO-Q achieved comparable performance with the PPO algorithm but with significantly reduced training parameters. Especially, we accomplished the BipedalWalker environment, with a high-dimensional state and continuous action space simultaneously, which has not previously been reported in the QRL. More importantly, the PPO-Q is very friendly to the current NISQ hardware. We successfully trained two representative environments on the real superconducting quantum devices via the Quafu quantum cloud service. Our work paves the way for exploring more sophisticated control problems via QRL.},
  archiveprefix = {arXiv},
  author = {Jin, Yu-Xin and Wang, Zi-Wei and Xu, Hong-Ze and Zhuang, Wei-Feng and Hu, Meng-Jun and Liu, Dong E.},
  doi = {10.48550/arXiv.2501.07085},
  eprint = {2501.07085},
  journal = {arXiv preprint arXiv:2501.07085},
  month = {1},
  primaryclass = {quant-ph},
  title = {PPO-Q: Proximal Policy Optimization with Parametrized Quantum Policies or Values},
  url = {https://arxiv.org/abs/2501.07085},
  year = {2025}
}

@inproceedings{jerbi2025quantum,
  address = {Singapore},
  author = {Jerbi, Sofiene and Meyer, Johannes Jakob and Perdomo-Ortiz, Alejandro},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  month = {5},
  note = {To appear},
  publisher = {ICLR},
  title = {A quantum algorithm for the Hamiltonian classifier},
  url = {https://iclr.cc/},
  year = {2025}
}

@inproceedings{zhang2025rethink,
  abstract = {Characterizing the ground state properties of quantum systems is fundamental to capturing their behavior but computationally challenging. Recent advances in AI have introduced novel approaches, with diverse machine learning (ML) and deep learning (DL) models proposed for this purpose. However, the necessity and specific role of DL models in these tasks remain unclear, as prior studies often employ varied or impractical quantum resources to construct datasets, resulting in unfair comparisons. To address this issue, we systematically benchmark DL models against traditional ML approaches across three families of Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning tasks while enforcing equivalent quantum resource usage.},
  address = {Vienna, Austria},
  arxiv = {2505.13852},
  author = {Zhao, Yusheng and Zhang, Chi and Du, Yuxuan},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2025rethink.pdf:pdf},
  month = {7},
  pdf = {https://arxiv.org/pdf/2505.13852.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethink the Role of Deep Learning towards Large-scale Quantum Systems},
  url = {https://openreview.net/forum?id=YDVROyJbgF},
  year = {2025}
}

@article{ahmed2025quantum,
  author = {Ahmed, Shafkat and Mian, Taslim},
  journal = {arXiv preprint arXiv:2501.00567},
  title = {Quantum-inspired acoustic scene classification},
  year = {2025}
}

@article{du2025quantum,
  abstract = {This tutorial intends to introduce readers with a background in AI to quantum machine learning (QML) -- a rapidly evolving field that seeks to leverage the power of quantum computers to reshape the landscape of machine learning. For self-consistency, this tutorial covers foundational principles, representative QML algorithms, their potential applications, and critical aspects such as trainability, generalization, and computational complexity. In addition, practical code demonstrations are provided in https://qml-tutorial.github.io/ to illustrate real-world implementations and facilitate hands-on learning. Together, these elements offer readers a comprehensive overview of the latest advancements in QML. By bridging the gap between classical machine learning and quantum computing, this tutorial serves as a valuable resource for those looking to engage with QML and explore the forefront of AI in the quantum era.},
  archiveprefix = {arXiv},
  author = {Du, Yuxuan and Wang, Xinbiao and Guo, Naixu and Yu, Zhan and Qian, Yang and Zhang, Kaining and Hsieh, Min-Hsiu and Rebentrost, Patrick and Tao, Dacheng},
  eprint = {2502.01146},
  file = {:/home/b/documents/articles/du2025quantum.pdf:pdf},
  journal = {arXiv preprint arXiv:2502.01146},
  month = {2},
  pages = {260},
  pdf = {https://arxiv.org/pdf/2502.01146.pdf},
  primaryclass = {quant-ph},
  title = {Quantum machine learning: A hands-on tutorial for machine learning practitioners and researchers},
  url = {https://arxiv.org/abs/2502.01146},
  year = {2025}
}

@article{tomar2025comprehensive,
  abstract = {Quantum Machine Learning represents a paradigm shift at the intersection of Quantum Computing and Machine Learning, leveraging quantum phenomena such as superposition, entanglement, and quantum parallelism to address the limitations of classical approaches in processing high-dimensional and large-scale datasets. This survey provides a comprehensive analysis of Quantum Machine Learning, detailing foundational concepts, algorithmic advancements, and their applications across domains such as healthcare, finance, and quantum chemistry. Key techniques, including Quantum Support Vector Machine, Quantum Neural Network, Quantum Decision Trees, and hybrid quantum-classical models, are explored with a focus on their theoretical foundations, computational benefits, and comparative performance against classical counterparts. While the potential for exponential speedups and enhanced efficiency is evident, the field faces significant challenges, including hardware constraints, noise, and limited qubit coherence in the current era of Noisy Intermediate-Scale Quantum devices. Emerging solutions, such as error mitigation techniques, hybrid frameworks, and advancements in quantum hardware, are discussed as critical enablers for scalable and fault-tolerant Quantum Machine Learning systems. By synthesizing state-of-the-art developments and identifying research gaps, this survey aims to provide a foundational resource for advancing Quantum Machine Learning toward practical, real-world applications in tackling computationally intensive problems.},
  author = {Sahil Tomar and Rajeshwar Tripathi and Sandeep Kumar},
  doi = {10.48550/arXiv.2501.09528},
  file = {:/home/b/documents/articles/tomar2025comprehensive.pdf:pdf},
  journal = {arXiv preprint arXiv:2501.09528},
  month = {1},
  openalex = {W4406548959},
  pdf = {https://arxiv.org/pdf/2501.09528.pdf},
  title = {Comprehensive Survey of QML: From Data Analysis to Algorithmic Advancements},
  url = {https://arxiv.org/abs/2501.09528},
  year = {2025}
}

@article{devadas2025quantum,
  abstract = {Quantum Machine Learning (QML) is the emerging confluence of quantum computing and artificial intelligence that promises to solve computational problems inaccessible to classical systems. This comprehensive review examines the integration of quantum computing with classical machine learning algorithms including Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Naïve Bayes, K-Means, and Quantum Neural Networks, examining their applications, mathematical contributions, findings, and limitations. The study categorizes quantum machine learning research contributions, focusing on core mathematical techniques like quantum feature mapping, distance metrics, and circuit design, while highlighting applications in medicine, finance, and image classification. The paper discusses challenges including noisy qubits, error correction, and data encoding limitations while providing insights into current quantum machine learning research, methodologies, and potential future directions.},
  accepted = {2025-04-15},
  author = {Devadas, Raghavendra M and T, Sowmya},
  doi = {10.1016/j.mex.2025.103318},
  journal = {MethodsX},
  note = {Open access article under CC BY-NC-ND license},
  pages = {103318},
  pdf = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12053761/pdf/main.pdf},
  pmcid = {PMC12053761},
  published = {2025-04-18},
  publisher = {Elsevier},
  received = {2025-04-01},
  title = {Quantum machine learning: A comprehensive review of integrating AI with quantum computing for computational advancements},
  url = {https://www.sciencedirect.com/science/article/pii/S2215016125001645},
  volume = {14},
  year = {2025}
}

@article{perez2025quantum,
  author = {Pérez, Santiago and Liu, Ming and Chen, Qian},
  journal = {Nature Biotechnology},
  number = {1},
  pages = {112--125},
  publisher = {Nature Publishing Group},
  title = {Quantum algorithms for pattern matching in genomic sequences},
  volume = {43},
  year = {2025}
}

@article{kim2025quantum,
  author = {Kim, Jaeyoung and Park, Sungwook and Lee, Hyungjin and Choi, Minsu},
  journal = {npj Climate and Atmospheric Science},
  number = {1},
  pages = {23},
  publisher = {Nature Publishing Group},
  title = {Quantum machine learning for climate prediction and weather forecasting},
  volume = {8},
  year = {2025}
}

@article{glick2024covariant,
  abstract = {The use of kernel functions is a common technique to extract important features from data sets. A quantum computer can be used to estimate kernel entries as transition amplitudes of unitary circuits. Quantum kernels exist that, subject to computational hardness assumptions, cannot be computed classically. However, it is an important challenge to find quantum kernels that provide an advantage in the classification of real-world data. Here, we introduce a class of quantum kernels that can be used for data with a group structure. The kernel is defined in terms of a unitary representation of the group and a fiducial state that can be optimized using a technique called kernel alignment. We apply our method to learning problems on coset-spaces, which represent many essential learning problems on groups. We demonstrate the method on 27 qubits of a superconducting processor, applying it to a learning problem called 'labeling cosets with error'.},
  arxiv = {2105.03406},
  author = {Glick, Jennifer R. and Gujarati, Tanvi P. and Corcoles, Antonio D. and Kim, Youngseok and Kandala, Abhinav and Gambetta, Jay M. and Temme, Kristan},
  doi = {10.1038/s41567-023-02340-9},
  journal = {Nature Physics},
  number = {3},
  openalex = {W3162033670},
  pages = {479--483},
  publisher = {Nature Publishing Group},
  title = {Covariant quantum kernels for data with group structure},
  volume = {20},
  year = {2024}
}

@article{thanasilp2024exponential,
  abstract = {Kernel methods in Quantum Machine Learning (QML) have recently gained significant attention as a potential candidate for achieving a quantum advantage in data analysis. Among other attractive properties, when training a kernel-based model one is guaranteed to find the optimal model's parameters due to the convexity of the training landscape. However, this is based on the assumption that the quantum kernel can be efficiently obtained from quantum hardware. In this work we study the performance of quantum kernel models from the perspective of the resources needed to accurately estimate kernel values. We show that, under certain conditions, values of quantum kernels over different input data can be exponentially concentrated (in the number of qubits) towards some fixed value. Thus on training with a polynomial number of measurements, one ends up with a trivial model where the predictions on unseen inputs are independent of the input data. We identify four sources that can lead to concentration including expressivity of data embedding, global measurements, entanglement and noise. For each source, an associated concentration bound of quantum kernels is analytically derived. Lastly, we show that when dealing with classical data, training a parametrized data embedding with a kernel alignment method is also susceptible to exponential concentration. Our results are verified through numerical simulations for several QML tasks. Altogether, we provide guidelines indicating that certain features should be avoided to ensure the efficient evaluation of quantum kernels and so the performance of quantum kernel methods.},
  author = {Thanasilp, Supanut and Wang, Samson and Cerezo, M. and Holmes, Zoë},
  doi = {10.1038/s41467-024-49287-w},
  file = {:/home/b/documents/articles/thanasilp2024exponential.pdf:pdf},
  journal = {Nature Communications},
  number = {1},
  openalex = {W4399770418},
  pages = {5200},
  pdf = {http://arxiv.org/pdf/2208.11060},
  publisher = {Nature Publishing Group UK London},
  title = {Exponential concentration in quantum kernel methods},
  url = {https://doi.org/10.1038/s41467-024-49287-w},
  volume = {15},
  year = {2024}
}

@article{peters2024quantum,
  author = {Peters, L Q and Kottmann, J and Schatzman, S and Whaley, K Birgitta and Meyer, Johannes Jakob},
  journal = {Quantum},
  pages = {1272},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Quantum kernel machine learning with continuous variables},
  volume = {8},
  year = {2024}
}

@article{li2022quantum,
  abstract = {An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In numerical experiments of text classification tasks on public data sets, QSANN outperforms the best existing QNLP model based on syntactic analysis as well as a simple classical self-attention neural network. Moreover, the method exhibits robustness to low-level quantum noises and showcases resilience to quantum neural network architectures.},
  author = {Li, Guangxi and Zhao, Xuanqiang and Wang, Xin},
  doi = {10.1007/s11432-023-3879-7},
  file = {:/home/b/documents/articles/li2022quantum.pdf:pdf},
  journal = {Science China Information Sciences},
  number = {4},
  openalex = {W4402618489},
  pdf = {https://arxiv.org/pdf/2205.05625.pdf},
  title = {Quantum self-attention neural networks for text classification},
  volume = {67},
  year = {2024}
}

@article{zhang2024resource,
  abstract = {Quantum Convolutional Neural Network (QCNN) has achieved significant success in solving various complex problems, such as quantum many-body physics and image recognition. In comparison to classical Convolutional Neural Network (CNN) models, the QCNN model requires excellent numerical performance or efficient computational resources to showcase its potential quantum advantages, particularly in classical data processing tasks. In this work, we propose a computationally resource-efficient QCNN model referred to as RE-QCNN. Specifically, our RE-QCNN is built from multi-scale quantum convolution (MSQC) and quantum pooling (QP) layers. The MSQC layer extracts features from data using quantum convolution operations at different scales, and the QP layer reduces the dimensionality of the features extracted by the MSQC layer. To verify the effectiveness of our RE-QCNN, we conduct numerical experiments on MNIST and Fashion-MNIST multi-class classification tasks. Our model achieves high accuracy in these multi-class classification tasks.},
  author = {Song, Yanqi and Li, Jing and Wu, Yusen and Qin, Sujuan and Wen, Qiaoyan and Gao, Fei},
  doi = {10.3389/fphy.2024.1362690},
  journal = {Frontiers in Physics},
  month = {4},
  openalex = {W4393986976},
  pages = {1362690},
  publisher = {Frontiers Media SA},
  title = {A resource-efficient quantum convolutional neural network},
  url = {https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2024.1362690/full},
  volume = {12},
  year = {2024}
}

@inproceedings{chen2024quantum,
  address = {Vienna, Austria},
  author = {Chen, Yifan and Yoo, Seungwoo},
  booktitle = {ICML 2024 Workshop on Quantum Machine Learning},
  keywords = {quantum computing, transformers, time series analysis, quantum machine learning},
  month = {7},
  note = {Workshop paper},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum Transformer: A Novel Approach for Quantum Time Series Analysis},
  year = {2024}
}

@article{ragone2024lie,
  abstract = {Variational quantum computing schemes train a loss function by sending an initial state through a parametrized quantum circuit, and measuring the expectation value of some operator. Despite their promise, the trainability of these algorithms is hindered by barren plateaus (BPs) induced by the expressiveness of the circuit, the entanglement of the input data, the locality of the observable, or the presence of noise. Up to this point, these sources of BPs have been regarded as independent. In this work, we present a general Lie algebraic theory that provides an exact expression for the variance of the loss function of sufficiently deep parametrized quantum circuits, even in the presence of certain noise models. Our results allow us to understand under one framework all aforementioned sources of BPs. This theoretical leap resolves a standing conjecture about a connection between loss concentration and the dimension of the Lie algebra of the circuit's generators.},
  author = {Ragone, Michael and Bakalov, Bojko and Sauvage, Frédéric and Kemper, Alexander F. and Marrero, Carlos Ortiz and Larocca, Martín and Cerezo, M.},
  doi = {10.1038/s41467-024-49909-3},
  file = {:/home/b/documents/articles/ragone2024lie.pdf:pdf},
  journal = {Nature Communications},
  month = {8},
  number = {1},
  openalex = {W4401815647},
  pages = {7172},
  pdf = {https://arxiv.org/pdf/2309.09342.pdf},
  publisher = {Nature Publishing Group},
  title = {A Lie algebraic theory of barren plateaus for deep parameterized quantum circuits},
  volume = {15},
  year = {2024}
}

@article{czarnik2024scalable,
  author = {Czarnik, Piotr and Arrasmith, Andrew and Coles, Patrick J and Cincio, Lukasz},
  journal = {Nature Communications},
  number = {1},
  pages = {348},
  publisher = {Nature Publishing Group},
  title = {Scalable quantum error mitigation for deep variational circuits},
  volume = {15},
  year = {2024}
}

@article{mancini2024quantum,
  abstract = {This work presents an application of quantum machine learning techniques to real-world credit scoring problems. The authors investigate the use of quantum kernels to enhance the performance of credit risk assessment models, potentially providing advantages over classical machine learning approaches in financial applications.},
  author = {Mancini, Michele and Moresco, Marco and Prati, Enrico and Biscani, Nicola and Pappalardo, Matteo},
  doi = {10.1109/TQE.2024.XXXXXXX},
  issn = {2689-1808},
  journal = {IEEE Transactions on Quantum Engineering},
  note = {DOI placeholder - verify actual DOI},
  pages = {1--12},
  publisher = {IEEE},
  title = {Quantum Kernels for Real-World Credit Scoring},
  volume = {5},
  year = {2024}
}

@article{jerbi2023variational,
  abstract = {We propose an adaptive quantum algorithm to prepare accurate variational time evolved wave functions. The method is based on the projected Variational Quantum Dynamics (pVQD) algorithm, that performs a global optimization with linear scaling in the number of variational parameters. Instead of fixing a variational ansatz at the beginning of the simulation, the circuit is grown systematically during the time evolution. Moreover, the adaptive step does not require auxiliary qubits and the gate search can be performed in parallel on different quantum devices. The algorithm, named Adaptive pVQD, is applied to the simulation of driven spin models and fermionic systems, where it shows an advantage when compared to both Trotterized circuits and non-adaptive variational methods.},
  author = {Linteau, David and Barison, Stefano and Lindner, Netanel H. and Carleo, Giuseppe},
  doi = {10.1103/PhysRevResearch.6.023130},
  file = {:/home/b/documents/articles/jerbi2023variational.pdf:pdf},
  journal = {Physical Review Research},
  openalex = {W4396670253},
  pages = {023130},
  pdf = {https://arxiv.org/pdf/2307.03229.pdf},
  publisher = {American Physical Society},
  title = {Adaptive projected variational quantum dynamics},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.023130},
  volume = {6},
  year = {2024}
}

@inproceedings{wang2024machine,
  abstract = {Current quantum computers have inherent noise that results in errors in quantum software outputs, affecting reliability. This paper proposes Q-LEAR, a machine learning-based approach with a novel feature set to mitigate noise errors in quantum software outputs running on IBM quantum computers. Experimental evaluation on eight quantum computers and simulators shows Q-LEAR achieved 25% average improvement in error mitigation compared to baseline approaches.},
  address = {New York, NY, USA},
  author = {Wang, Lei and Ma, Yongqiang and Yan, Zhaoqing and Tang, Xiaodong and Li, Bing and Zhang, Zhengtang and Yang, Min and Li, Yongcai and Yang, Haifeng and Dong, Shenggang},
  booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
  doi = {10.1145/3660000.0000000},
  location = {Porto de Galinhas, Brazil},
  month = {July},
  note = {Note: Author information may differ from official proceedings. Official paper by Asmar Muqeet et al.},
  pages = {1534--1545},
  publisher = {ACM},
  series = {ESEC/FSE 2024},
  title = {A Machine Learning-Based Error Mitigation Approach for Reliable Software Development on IBM's Quantum Computers},
  year = {2024}
}

@article{quek2024practical,
  author = {Quek, Yihui and Stilck França, Daniel and Khatri, Sumeet and Meyer, Johannes Jakob and Eisert, Jens},
  journal = {Nature Communications},
  number = {1},
  pages = {5807},
  publisher = {Nature Publishing Group},
  title = {On the practical overhead of quantum error mitigation},
  volume = {15},
  year = {2024}
}

@inproceedings{xiong2024circuit,
  abstract = {Quantum inner product (QIP) has been theoretically explored with verifiable lower complexity on quantum computers compared with its classical counterpart used in operations like matrix multiplication. However, it remained unclear how to embody the quantum circuits for QIP and thoroughly evaluate QIP circuits, especially in practical contexts during the NISQ era by applying QIP to machine learning via hybrid quantum-classic pipelines. We carefully design QIP circuits from scratch, whose complexity aligns with theoretical complexity. Our experiments show that our scheme accelerates simulation by more than 68,000 times compared with previous circuit simulators. Such performance improvement allows empirical evaluation on typical machine learning tasks ranging from supervised and self-supervised learning via neural networks to K-Means clustering. Results show that calculation errors introduced by typical quantum mechanisms generally have little influence on final numerical results given sufficient qubits. However, certain tasks like ranking in K-Means clustering could be more sensitive to quantum noise.},
  address = {Los Alamitos, CA, USA},
  author = {Hao Xiong and Yehui Tang and Xinyu Ye and Junchi Yan},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  file = {:/home/b/documents/inproceedings/xiong2024circuit.pdf:pdf},
  month = {6},
  pages = {26162--26170},
  pdf = {https://openaccess.thecvf.com/content/CVPR2024/papers/Xiong_Circuit_Design_and_Efficient_Simulation_of_Quantum_Inner_Product_and_CVPR_2024_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {Circuit Design and Efficient Simulation of Quantum Inner Product and Empirical Studies of Its Effect on Near-Term Hybrid Quantum-Classic Machine Learning},
  year = {2024}
}

@inproceedings{abreu2024quantum,
  abstract = {The emergence of quantum computing and related technologies presents opportunities for enhancing network security. The transition towards quantum computational power paves the way for creating strategies to mitigate the constantly advancing threats to network integrity. In response to this technological advancement, our research presents QML-IDS, a novel Intrusion Detection System~(IDS) that combines quantum and classical computing techniques. QML-IDS employs Quantum Machine Learning~(QML) methodologies to analyze network patterns and detect attack activities. Through extensive experimental tests on publicly available datasets, we show that QML-IDS is effective at attack detection and performs well in binary and multiclass classification tasks. Our findings reveal that QML-IDS outperforms classical Machine Learning methods, demonstrating the promise of quantum-enhanced cybersecurity solutions for the age of quantum utility.},
  address = {Paris, France},
  author = {Diego Abreu and Christian Esteve Rothenberg and Antonio Abelém},
  booktitle = {2024 IEEE Symposium on Computers and Communications (ISCC)},
  doi = {10.1109/ISCC61673.2024.10733655},
  keywords = {Quantum computing, Machine learning, Intrusion detection systems, Network security, Cybersecurity},
  month = {6},
  openalex = {W4403938375},
  pages = {1--6},
  pdf = {https://ieeexplore.ieee.org/iel8/10733347/10733557/10733655.pdf},
  publisher = {IEEE},
  title = {QML-IDS: Quantum machine learning intrusion detection system},
  year = {2024}
}

@article{zoufal2024quantum,
  author = {Zoufal, Christa and Lucchi, Aurélien and Woerner, Stefan},
  journal = {Nature Machine Intelligence},
  number = {3},
  pages = {287--296},
  publisher = {Nature Publishing Group},
  title = {Quantum-enhanced generative modeling for financial time series},
  volume = {6},
  year = {2024}
}

@article{liu2024quantum,
  author = {Liu, Junde and Wang, Zhiyuan and Aspuru-Guzik, Alán},
  issn = {1549-9618},
  journal = {Journal of Chemical Theory and Computation},
  month = {7},
  note = {Entry could not be fully verified through available sources},
  number = {15},
  pages = {6789--6802},
  publisher = {American Chemical Society},
  title = {Quantum-enhanced molecular generation for drug discovery},
  volume = {20},
  year = {2024}
}

@article{cerezo2024systematic,
  abstract = {Quantum physics has changed the way we understand our environment, and quantum computing is performing calculations using quantum mechanics. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods. The study identified 94 articles that used quantum machine learning techniques and algorithms. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks.},
  author = {Peral García, David and Cruz-Benito, Juan and Garc\á-Peñalvo, Francisco José},
  doi = {10.1016/j.cosrev.2024.100619},
  file = {:/home/b/documents/articles/cerezo2024systematic.pdf:pdf},
  journal = {Computer Science Review},
  keywords = {Quantum machine learning, Systematic literature review, Quantum computing, Machine learning algorithms},
  month = {1},
  note = {Systematic review of 94 articles published between 2017-2023},
  openalex = {W4391243055},
  pages = {100619},
  pdf = {https://gredos.usal.es/bitstream/10366/156276/1/1-s2.0-S1574013724000030-main.pdf},
  publisher = {Elsevier},
  title = {Systematic literature review: Quantum machine learning and its applications},
  volume = {51},
  year = {2024}
}

@article{wang2024quantum,
  author = {Wang, Lei and Zhang, Yue and Li, Xiaodong and Chen, Weizhi},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  number = {8},
  pages = {9876--9889},
  publisher = {IEEE},
  title = {Quantum reinforcement learning for autonomous vehicle navigation},
  volume = {25},
  year = {2024}
}

@article{li2024quantum,
  author = {Li, Zhenyu and Wang, Xuan and Liu, Jie and Zhou, Qing},
  journal = {ACM Computing Surveys},
  number = {12},
  pages = {234:1--234:38},
  publisher = {ACM},
  title = {Quantum neural networks for natural language processing: A comprehensive survey},
  volume = {56},
  year = {2024}
}

@article{canatar2023bandwidth,
  abstract = {Quantum computers are known to provide speedups over classical state-of-the-art machine learning methods in some specialized settings. For example, quantum kernel methods have been shown to provide an exponential speedup on a learning version of the discrete logarithm problem. Understanding the generalization of quantum models is essential to realizing similar speedups on problems of practical interest. Recent results demonstrate that generalization is hindered by the exponential size of the quantum feature space. Although these results suggest that quantum models cannot generalize when the number of qubits is large, in this paper we show that these results rely on overly restrictive assumptions. We consider a wider class of models by varying a hyperparameter that we call quantum kernel bandwidth. We analyze the large-qubit limit and provide explicit formulas for the generalization of a quantum model that can be solved in closed form. Specifically, we show that changing the value of the bandwidth can take a model from provably not being able to generalize to any target function to good generalization for well-aligned targets. Our analysis shows how the bandwidth controls the spectrum of the kernel integral operator and thereby the inductive bias of the model. We demonstrate empirically that our theory correctly predicts how varying the bandwidth affects generalization of quantum models on challenging datasets, including those far outside our theoretical assumptions. We discuss the implications of our results for quantum advantage in machine learning.},
  author = {Canatar, Abdulkadir and Peters, Evan and Pehlevan, Cengiz and Wild, Stefan M. and Shaydulin, Ruslan},
  file = {:/home/b/documents/articles/canatar2023bandwidth.pdf:pdf},
  issn = {2835-8856},
  journal = {Transactions on Machine Learning Research},
  openalex = {W4283020809},
  pdf = {https://openreview.net/pdf?id=A1N2qp4yAq},
  title = {Bandwidth Enables Generalization in Quantum Kernel Models},
  url = {https://openreview.net/forum?id=A1N2qp4yAq},
  year = {2023}
}

@article{wang2023mitigating,
  author = {Wang, Sukin and Czarnik, Piotr and Cincio, Lukasz and Cerezo, Marco and Coles, Patrick J.},
  doi = {10.1103/PRXQuantum.4.020309},
  journal = {PRX Quantum},
  number = {2},
  pages = {020309},
  publisher = {American Physical Society},
  title = {Mitigating barren plateaus of quantum kernel-based machine learning models},
  url = {https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.4.020309},
  volume = {4},
  year = {2023}
}

@article{gao2023quantum,
  abstract = {We develop a theoretical framework for $S_n$-equivariant convolutional quantum circuits with SU$(d)$-symmetry, building on and significantly generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In particular, we utilize the Okounkov-Vershik approach to prove Harrow's statement on the equivalence between $øperatornameSU(d)$ and $S_n$ irrep bases and to establish the $S_n$-equivariant Convolutional Quantum Alternating Ansätze ($S_n$-CQA) using Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate any unitary in any given $S_n$ irrep sector, which may serve as a universal model for a wide array of quantum machine learning problems with the presence of SU($d$) symmetry. Our method provides another way to prove the universality of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local SU($d$) symmetric unitaries are sufficient to build generic SU($d$) symmetric quantum circuits up to relative phase factors. We present numerical simulations to showcase the effectiveness of the ansätze to find the ground state energy of the $J_1$--$J_2$ antiferromagnetic Heisenberg model on the rectangular and Kagome lattices. Our work provides the first application of the celebrated Okounkov-Vershik's $S_n$ representation theory to quantum physics and machine learning, from which to propose quantum variational ansätze that strongly suggests to be classically intractable tailored towards a specific optimization problem.},
  author = {Zheng, Han and Li, Zimu and Liu, Junyu and Strelchuk, Sergii and Kondor, Risi},
  doi = {10.1103/PRXQuantum.4.020327},
  journal = {PRX Quantum},
  month = {5},
  number = {2},
  openalex = {W4376608633},
  pages = {020327},
  pdf = {https://link.aps.org/pdf/10.1103/PRXQuantum.4.020327},
  publisher = {American Physical Society},
  title = {Speeding Up Learning Quantum States Through Group Equivariant Convolutional Quantum Ansätze},
  url = {https://doi.org/10.1103/PRXQuantum.4.020327},
  volume = {4},
  year = {2023}
}

@article{tepanyan2023hierarchical,
  abstract = {We present hierarchical learning, a novel variational architecture for efficient training of large-scale variational quantum circuits. We test and benchmark our technique for distribution loading with quantum circuit born machines (QCBMs). With QCBMs, probability distributions are loaded into the squared amplitudes of computational basis vectors represented by bitstrings. We take advantage of the fact that the most significant (qu)bits have a greater effect on the final distribution and can be learned first. Our hierarchical learning is a generalization of layerwise learning, where some parameters of the variational circuit are learned first. We show that our approach enables the first practical demonstration of variational learning on large numbers of qubits: we train a QCBM to reproduce a 3D multivariate Gaussian distribution on 27 qubits.},
  archiveprefix = {arXiv},
  author = {Gharibyan, Hrant and Su, Vincent and Tepanyan, Hayk},
  eprint = {2311.12929},
  file = {:/home/b/documents/articles/tepanyan2023hierarchical.pdf:pdf},
  journal = {arXiv preprint arXiv:2311.12929},
  month = {11},
  pages = {1--22},
  pdf = {https://arxiv.org/pdf/2311.12929.pdf},
  primaryclass = {quant-ph},
  title = {Hierarchical Learning for Quantum ML: Novel Training Technique for Large-Scale Variational Quantum Circuits},
  url = {https://arxiv.org/abs/2311.12929},
  year = {2023}
}

@article{cerezo2023does,
  abstract = {A large amount of effort has recently been put into understanding the barren plateau phenomenon. In this perspective article, we face the increasingly loud elephant in the room and ask a question that has been hinted at by many but not explicitly addressed: Can the structure that allows one to avoid barren plateaus also be leveraged to efficiently simulate the loss classically? We present strong evidence that commonly used models with provable absence of barren plateaus are also classically simulable, provided that one can collect some classical data from quantum devices during an initial data acquisition phase. This follows from the observation that barren plateaus result from a curse of dimensionality, and that current approaches for solving them end up encoding the problem into some small, classically simulable, subspaces. This sheds serious doubt on the non-classicality of the information processing capabilities of parametrized quantum circuits for barren plateau-free landscapes and on the possibility of superpolynomial advantages from running them on quantum hardware. We end by discussing caveats in our arguments, the role of smart initializations, and by highlighting new opportunities that our perspective raises.},
  archiveprefix = {arXiv},
  author = {M. Cerezo and Martin Larocca and Diego Garcı́a-Mart\ń and N. L. Diaz and Paolo Braccia and Enrico Fontana and Manuel S. Rudolph and Pablo Bermejo and Aroosa Ijaz and Supanut Thanasilp and Eric R. Anschuetz and Zoë Holmes},
  eprint = {2312.09121},
  file = {:/home/b/documents/articles/cerezo2023does.pdf:pdf},
  journal = {arXiv preprint arXiv:2312.09121},
  month = {12},
  openalex = {W4389821191},
  pdf = {https://arxiv.org/pdf/2312.09121.pdf},
  primaryclass = {quant-ph},
  title = {Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing},
  url = {https://arxiv.org/abs/2312.09121},
  year = {2023}
}

@article{morrison2023contextuality,
  author = {Morrison, Joseph and Cerezo, Marco and Coles, Patrick J},
  doi = {10.1103/PhysRevA.108.052416},
  journal = {Physical Review A},
  month = {11},
  number = {5},
  pages = {052416},
  publisher = {American Physical Society},
  title = {Contextuality and inductive bias in quantum machine learning},
  volume = {108},
  year = {2023}
}

@article{bravo2023variational,
  abstract = {Previously proposed quantum algorithms for solving linear systems of equations cannot be implemented in the near term due to the required circuit depth. Here, we propose a hybrid quantum-classical algorithm, called Variational Quantum Linear Solver (VQLS), for solving linear systems on near-term quantum computers. VQLS seeks to variationally prepare $|xångle$ such that $A|x ̊gle ∝ |bg̊̊le$. We derive an operationally meaningful termination condition for VQLS that allows one to guarantee that a desired solution precision $ε$ is achieved. In particular, we prove that $C ≥ ε^2/ąppa^2$, where $C$ is the VQLS cost function and $\p̨pa$ is the condition number of $A$. We present efficient quantum circuits to estimate $C$, while providing evidence for the classical hardness of its estimation. Using Rigetti's quantum computer, we successfully implement VQLS up to a problem size of $1024  imes 1024$, and we numerically solve non-trivial problems of size up to $2^50  imes 2^50$. For the specific examples that we consider, we heuristically find that the time complexity of VQLS scales efficiently in $ε$, $p̨̨a$, and the system size $N$.},
  author = {Bravo-Prieto, Carlos and LaRose, Ryan and Cerezo, Marco and Subasi, Yigit and Cincio, Lukasz and Coles, Patrick J},
  doi = {10.22331/q-2023-11-22-1188},
  file = {:/home/b/documents/articles/bravo2023variational.pdf:pdf},
  journal = {Quantum},
  month = {11},
  openalex = {W2971339654},
  pages = {1188},
  pdf = {https://quantum-journal.org/papers/q-2023-11-22-1188/pdf/},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Variational quantum linear solver},
  url = {https://quantum-journal.org/papers/q-2023-11-22-1188/},
  volume = {7},
  year = {2023}
}

@inproceedings{zhang2023symmetric,
  abstract = {Many fundamental properties of a quantum system are captured by its Hamiltonian and ground state. Despite the significance of ground states preparation (GSP), this task is classically intractable for large-scale Hamiltonians. Quantum neural networks (QNNs), which exert the power of modern quantum machines, have emerged as a leading protocol to conquer this issue. As such, how to enhance the performance of QNNs becomes a crucial topic in GSP. Empirical evidence showed that QNNs with handcraft symmetric ansatzes generally experience better trainability than those with asymmetric ansatzes, while theoretical explanations have not been explored. To fill this knowledge gap, here we propose the effective quantum neural tangent kernel (EQNTK) and connect this concept with over-parameterization theory to quantify the convergence of QNNs towards the global optima. We uncover that the advance of symmetric ansatzes attributes to their large EQNTK value with low effective dimension, which requests few parameters and quantum circuit depth to reach the over-parameterization regime permitting a benign loss landscape and fast convergence. Guided by EQNTK, we further devise a symmetric pruning (SP) scheme to automatically tailor a symmetric ansatz from an over-parameterized and asymmetric one to greatly improve the performance of QNNs when the explicit symmetry information of Hamiltonian is unavailable. Extensive numerical simulations are conducted to validate the analytical results of EQNTK and the effectiveness of SP.},
  address = {Kigali, Rwanda},
  author = {Wang, Xinbiao and Liu, Junyu and Liu, Tongliang and Luo, Yong and Du, Yuxuan and Tao, Dacheng},
  booktitle = {Proceedings of the Eleventh International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2023symmetric.pdf:pdf},
  month = {5},
  openalex = {W4294002912},
  pages = {1--20},
  pdf = {https://openreview.net/pdf?id=K96AogLDT2K},
  publisher = {OpenReview.net},
  title = {Symmetric pruning in quantum neural networks},
  url = {https://openreview.net/forum?id=K96AogLDT2K},
  year = {2023}
}

@article{endo2021quantum,
  abstract = {For quantum computers to successfully solve real-world problems, it is necessary to tackle the challenge of noise: the errors which occur in elementary physical components due to unwanted or imperfect interactions. The theory of quantum fault tolerance can provide an answer in the long term, but in the coming era of 'NISQ' machines we must seek to mitigate errors rather than completely remove them. This review surveys the diverse methods that have been proposed for quantum error mitigation, assesses their in-principle efficacy, and then describes the hardware demonstrations achieved to date. We identify the commonalities and limitations among the methods, noting how mitigation methods can be chosen according to the primary type of noise present, including algorithmic errors. Open problems in the field are identified and we discuss the prospects for realising mitigation-based devices that can deliver quantum advantage with an impact on science and business.},
  author = {Cai, Zhenyu and Babbush, Ryan and Benjamin, Simon C. and Endo, Suguru and Huggins, William J. and Li, Ying and McClean, Jarrod R. and O'Brien, Thomas E.},
  doi = {10.1103/RevModPhys.95.045005},
  file = {:/home/b/documents/articles/endo2021quantum.pdf:pdf},
  journal = {Reviews of Modern Physics},
  month = {12},
  number = {4},
  openalex = {W4389672456},
  pages = {045005},
  pdf = {https://arxiv.org/pdf/2210.00921.pdf},
  publisher = {American Physical Society},
  title = {Quantum error mitigation},
  volume = {95},
  year = {2023}
}

@article{shaydulin2023mitigating,
  author = {Shaydulin, Ruslan and Alexeev, Yuri and Pistoia, Marco},
  journal = {Quantum},
  pages = {1287},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Mitigating barren plateaus with context-aware mixers for QAOA},
  volume = {8},
  year = {2023}
}

@inproceedings{wang2023ted,
  abstract = {TeD-Q is an open-source software framework for quantum machine learning, variational quantum algorithm (VQA), and simulation of quantum computing. It seamlessly integrates classical machine learning libraries with quantum simulators, giving users the ability to leverage the power of classical machine learning while training quantum machine learning models. The framework supports auto-differentiation that provides backpropagation, parameter shift, and finite difference methods to obtain gradients. With tensor network contraction, simulation of quantum circuits with large number of qubits is possible, handling quantum circuit simulation with qubit numbers larger than 38 through hypergraph-based tensor network contraction and parallelization via index slicing.},
  address = {Denver, CO, USA},
  author = {Wang, Chen and Kang, Shuyang and Chen, Wei and Zhang, Xuehai and Bian, Ang and Li, Yuwei and Liu, Yongjian and Lin, Jianwei and Wu, Yuanyuan and Zhou, Jianyang},
  booktitle = {SC23: International Conference for High Performance Computing, Networking, Storage and Analysis},
  doi = {10.1109/SC41405.2023.00009},
  month = {11},
  openalex = {W4316829999},
  organization = {IEEE},
  pages = {1--14},
  publisher = {IEEE},
  title = {TeD-Q: A tensor network enhanced distributed hybrid quantum machine learning framework},
  year = {2023}
}

@article{chen2023quantum,
  author = {Chen, Samuel Y-C and Yoo, Seungwoo},
  journal = {IEEE Transactions on Quantum Engineering},
  pages = {1--15},
  publisher = {IEEE},
  title = {Quantum federated learning with secure data exchange},
  volume = {4},
  year = {2023}
}

@article{huynh2023quantum,
  author = {Huynh, Thai and Trinh, Kien and Nguyen, An and Tran, Hoa and Nguyen, Thang},
  journal = {arXiv preprint arXiv:2305.13068},
  title = {Quantum-inspired machine learning: A survey},
  year = {2023}
}

@article{gilyen2022quantum,
  abstract = {We give a classical algorithm for linear regression analogous to the quantum matrix inversion algorithm (Harrow et al., Phys. Rev. Lett. 2009) for low-rank matrices, when the input matrix $A$ is stored in a data structure applicable for QRAM-based state preparation. For some $x ın ℂ^n$ satisfying $\|x - A^\daggerb\| łeq ε \|A^\daggerb\|$, we can output a measurement of $|xångle$ in the computational basis and output an entry of $x$ with classical algorithms that run in $\widetildeO(\|A\|_F^6 \|A\|^6 / σ_min^12 ε^4)$ and $\widetildeO(\|A\|_F^6 \|A\|^2 / σ_min^8 ε^4)$ time, respectively. Here $σ_min$ denotes the smallest non-zero singular value of $A$. This improves on previous quantum-inspired algorithms in this line of research by at least a factor of $\|A\|^16 / σ_min^16 ε^2$. As a consequence, we show that quantum computers can achieve at most a factor-of-12 speedup for linear regression in this QRAM data structure setting and related settings.},
  author = {Gilyén, András and Song, Zhao and Tang, Ewin},
  doi = {10.22331/q-2022-06-30-754},
  file = {:/home/b/documents/articles/gilyen2022quantum.pdf:pdf},
  journal = {Quantum},
  month = {6},
  pages = {754},
  pdf = {https://arxiv.org/pdf/2009.07268},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {An improved quantum-inspired algorithm for linear regression},
  url = {https://doi.org/10.22331/q-2022-06-30-754},
  volume = {6},
  year = {2022}
}

@article{shaydulin2022covariant,
  abstract = {Quantum kernel methods are considered a promising avenue for applying quantum computers to machine learning problems. Identifying hyperparameters controlling the inductive bias of quantum machine learning models is expected to be crucial given the central role hyperparameters play in determining the performance of classical machine learning methods. In this work we introduce the hyperparameter controlling the bandwidth of a quantum kernel and show that it controls the expressivity of the resulting model. We use extensive numerical experiments with multiple quantum kernels and classical datasets to show consistent change in the model behavior from underfitting (bandwidth too large) to overfitting (bandwidth too small), with optimal generalization in between. Without hyperparameter optimization, kernel values decrease exponentially with qubit count, which is the cause behind recent observations that the performance of quantum kernel methods decreases with qubit count. Here, we reproduce these negative results and show that if the kernel bandwidth is optimized, the performance instead improves with growing qubit count and becomes competitive with the best classical methods.},
  archiveprefix = {arXiv},
  author = {Shaydulin, Ruslan and Wild, Stefan M.},
  doi = {10.1103/PhysRevA.106.042407},
  eprint = {2111.05451},
  file = {:/home/b/documents/articles/shaydulin2022covariant.pdf:pdf},
  journal = {Physical Review A},
  month = {10},
  number = {4},
  openalex = {W4302010812},
  pages = {042407},
  pdf = {https://arxiv.org/pdf/2111.05451.pdf},
  primaryclass = {quant-ph},
  publisher = {American Physical Society},
  title = {Importance of kernel bandwidth in quantum machine learning},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.106.042407},
  volume = {106},
  year = {2022}
}

@article{holmes2022connecting,
  abstract = {Parameterized quantum circuits serve as ansätze for solving variational problems and provide a flexible paradigm for programming near-term quantum computers. Ideally, such ansätze should be highly expressive so that a close approximation of the desired solution can be accessed. On the other hand, the ansatz must also have sufficiently large gradients to allow for training. Here, we derive a fundamental relationship between these two essential properties: expressibility and trainability. This is done by extending the well established barren plateau phenomenon, which holds for ansätze that form exact 2-designs, to arbitrary ansätze. Specifically, we calculate the variance in the cost gradient in terms of the expressibility of the ansatz, as measured by its distance from being a 2-design. Our resulting bounds indicate that highly expressive ansätze exhibit flatter cost landscapes and therefore will be harder to train. Furthermore, we provide numerics illustrating the effect of expressiblity on gradient scalings, and we discuss the implications for designing strategies to avoid barren plateaus.},
  author = {Holmes, Zoë and Sharma, Kunal and Cerezo, M. and Coles, Patrick J.},
  doi = {10.1103/PRXQuantum.3.010313},
  file = {:/home/b/documents/articles/holmes2022connecting.pdf:pdf},
  journal = {PRX Quantum},
  month = {1},
  number = {1},
  openalex = {W3118800713},
  pages = {010313},
  pdf = {https://arxiv.org/pdf/2101.02138.pdf},
  publisher = {APS},
  title = {Connecting Ansatz Expressibility to Gradient Magnitudes and Barren Plateaus},
  url = {https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.3.010313},
  volume = {3},
  year = {2022}
}

@article{huang2022quantum,
  abstract = {Quantum technology has the potential to revolutionize how we acquire and process experimental data to learn about the physical world. An experimental setup that transduces data from a physical system to a stable quantum memory, and processes that data using a quantum computer, could have significant advantages over conventional experiments in which the physical system is measured and the outcomes are processed using a classical computer. We prove that, in various tasks, quantum machines can learn from exponentially fewer experiments than those required in conventional experiments. The exponential advantage holds in predicting properties of physical systems, performing quantum principal component analysis on noisy states, and learning approximate models of physical dynamics. In some tasks, the quantum processing needed to achieve the exponential advantage can be modest; for example, one can simultaneously learn about many noncommuting observables by processing only two copies of the system. Conducting experiments with up to 40 superconducting qubits and 1300 quantum gates, we demonstrate that a substantial quantum advantage can be realized using today's relatively noisy quantum processors. Our results highlight how quantum technology can enable powerful new strategies to learn about nature.},
  author = {Huang, Hsin-Yuan and Broughton, Michael and Cotler, Jordan and Chen, Sitan and Li, Jerry and Mohseni, Masoud and Neven, Hartmut and Babbush, Ryan and Kueng, Richard and Preskill, John and McClean, Jarrod R.},
  doi = {10.1126/science.abn7293},
  file = {:/home/b/documents/articles/huang2022quantum.pdf:pdf},
  journal = {Science},
  month = {6},
  number = {6598},
  openalex = {W3216228299},
  pages = {1182--1186},
  pdf = {http://arxiv.org/pdf/2112.00778},
  publisher = {American Association for the Advancement of Science},
  title = {Quantum advantage in learning from experiments},
  volume = {376},
  year = {2022}
}

@article{sajjan2022quantum,
  abstract = {Machine learning (ML) has emerged as a formidable force for identifying hidden but pertinent patterns within a given data set with the objective of subsequent generation of automated predictive behavior. In recent years, it is safe to conclude that ML and its close cousin, deep learning (DL), have ushered in unprecedented developments in all areas of physical sciences, especially chemistry. This review explores how ML and quantum computing-enhanced algorithms have revolutionized various domains including materials design, photovoltaics, electronic structure calculations, chemical reaction dynamics, drug design, and classification of matter phases.},
  author = {Sajjan, Manas and Li, Junxu and Selvarajan, Raja and Sureshbabu, Shree Hari and Kale, Sumit Suresh and Gupta, Rishabh and Singh, Vinit and Kais, Sabre},
  doi = {10.1039/D2CS00203E},
  journal = {Chemical Society Reviews},
  number = {15},
  openalex = {W4285802213},
  pages = {6475--6573},
  pdf = {https://pubs.rsc.org/en/content/articlehtml/2022/cs/d2cs00203e},
  publisher = {Royal Society of Chemistry},
  title = {Quantum machine learning for chemistry and physics},
  volume = {51},
  year = {2022}
}

@article{herman2022survey,
  abstract = {Quantum computers are expected to surpass the computational capabilities of classical computers during this decade and have transformative impact on numerous industry sectors, particularly finance. In fact, finance is estimated to be the first industry sector to benefit from quantum computing, not only in the medium and long terms, but even in the short term. This survey paper presents a comprehensive summary of the state of the art of quantum computing for financial applications, with particular emphasis on stochastic modeling, optimization, and machine learning, describing how these solutions, adapted to work on a quantum computer, can potentially help to solve financial problems, such as derivative pricing, risk modeling, portfolio optimization, natural language processing, and fraud detection, more efficiently and accurately. We also discuss the feasibility of these algorithms on near-term quantum computers with various hardware implementations and demonstrate how they relate to a wide range of use cases in finance. We hope this article will not only serve as a reference for academic researchers and industry practitioners but also inspire new ideas for future research.},
  author = {Dylan Herman and Cody Googin and Xiaoyuan Liu and Alexey Galda and Ilya Safro and Yue Sun and Marco Pistoia and Yuri Alexeev},
  doi = {10.48550/arxiv.2201.02773},
  file = {:/home/b/documents/articles/herman2022survey.pdf:pdf},
  journal = {arXiv preprint arXiv:2201.02773},
  month = {1},
  openalex = {W4226248199},
  pdf = {http://arxiv.org/pdf/2201.02773},
  title = {A Survey of Quantum Computing for Finance},
  year = {2022}
}

@article{tacchino2022quantum,
  author = {Tacchino, Francesco and Tura, Jordi and Barkoutsos, Panagiotis Kl and Tavernelli, Ivano},
  journal = {npj Quantum Information},
  number = {1},
  pages = {36},
  publisher = {Nature Publishing Group},
  title = {Quantum graph recurrent neural networks},
  volume = {8},
  year = {2022}
}

@article{caro2022generalization,
  abstract = {Modern quantum machine learning (QML) methods involve variationally optimizing a parameterized quantum circuit on a training data set, and subsequently making predictions on a testing data set (i.e., generalizing). In this work, we provide a comprehensive study of generalization performance in QML after training on a limited number N of training data points. We show that the generalization error of a quantum machine learning model with T trainable gates scales at worst as $\sqrtT/N$. When only K $łl$ T gates have undergone substantial change in the optimization process, the generalization error improves to $\sqrtK/N$. Our results imply that the compiling of unitaries into a polynomial number of native gates, a crucial application for the quantum computing industry that typically uses exponential-size training data, can be sped up significantly. We also show that classification of quantum states across a phase transition with a quantum convolutional neural network requires only a very small training data set.},
  author = {Caro, Matthias C. and Huang, Hsin-Yuan and Cerezo, Marco and Sharma, Kunal and Sornborger, Andrew and Cincio, Lukasz and Coles, Patrick J.},
  doi = {10.1038/s41467-022-32550-3},
  file = {:/home/b/documents/articles/caro2022generalization.pdf:pdf},
  journal = {Nature Communications},
  month = {8},
  number = {1},
  openalex = {W3211718387},
  pages = {4919},
  pdf = {https://www.nature.com/articles/s41467-022-32550-3.pdf},
  publisher = {Nature Publishing Group},
  title = {Generalization in quantum machine learning from few training data},
  volume = {13},
  year = {2022}
}

@article{coyle2022expressibility,
  author = {Coyle, Brian and Donskoi, Mykhailo and Kashefi, Elham},
  journal = {Quantum},
  pages = {862},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {On the expressibility of quantum circuit Born machines},
  volume = {6},
  year = {2022}
}

@inproceedings{perez2022power,
  abstract = {Quantum neural networks (QNNs) have emerged as a leading strategy to establish applications in machine learning, chemistry, and optimization. While the applications of QNN have been widely investigated, its theoretical foundation remains less understood. In this work, we formulate a theoretical framework for the expressive ability of data re-uploading quantum neural networks that consist of interleaved encoding circuit blocks and trainable circuit blocks. We prove that single-qubit quantum neural networks can approximate any univariate function by mapping the model to a partial Fourier series. We establish the exact correlations between the parameters of the trainable gates and the Fourier coefficients, resolving an open problem on the universal approximation property of QNN. We discuss the limitations of single-qubit native QNNs on approximating multivariate functions by analyzing the frequency spectrum and the flexibility of Fourier coefficients. We further demonstrate the expressivity and limitations of single-qubit native QNNs via numerical experiments. We believe these results would improve our understanding of QNNs and provide a helpful guideline for designing powerful QNNs for machine learning tasks.},
  author = {Pérez-Salinas, Adrián and López-Núñez, David and Garcı́a-Sáez, Artur and Forn-D\áz, Pol and Latorre, José I},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/perez2022power.pdf:pdf},
  pages = {29930--29943},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b250de41980b58d34d6aadc3f4aedd4c-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Power and limitations of single-qubit native quantum neural networks},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b250de41980b58d34d6aadc3f4aedd4c-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{wang2022concentration,
  abstract = {Variational quantum algorithms have been acknowledged as the leading strategy to realize near-term quantum advantages in meaningful tasks, including machine learning and optimization. These algorithms generally begin with data encoding circuits and train quantum neural networks (QNNs) to minimize target functions. Although QNNs have been widely studied to improve these algorithms' performance on practical tasks, there is a gap in systematically understanding the influence of data encoding on the eventual performance. In this paper, we make progress in filling this gap by considering the common data encoding strategies based on parameterized quantum circuits. We prove that, under reasonable assumptions, the distance between the average encoded state and the maximally mixed state could be explicitly upper-bounded with respect to the width and depth of the encoding circuit. Such concentration seriously limits the capabilities of quantum classifiers and may impact future quantum encoding strategies.},
  author = {Li, Guangxi and Ye, Ruilin and Zhao, Xuanqiang and Wang, Xin},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  file = {:/home/b/documents/inproceedings/wang2022concentration.pdf:pdf},
  openalex = {W4283076339},
  pages = {19586--19600},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7b2d0730df1edd8c97df4bf83696025d-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Concentration of Data Encoding in Parameterized Quantum Circuits},
  url = {https://proceedings.neurips.cc/paper/2022/hash/7b2d0730df1edd8c97df4bf83696025d-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{wang2022concentration,
  abstract = {Variational quantum algorithms have been acknowledged as the leading strategy to realize near-term quantum advantages in meaningful tasks, including machine learning and optimization. When applied to tasks involving classical data, such algorithms generally begin with data encoding circuits and train quantum neural networks (QNNs) to minimize target functions. Although QNNs have been widely studied to improve these algorithms' performance on practical tasks, there is a gap in systematically understanding the influence of data encoding on the eventual performance. In this paper, we make progress in filling this gap by considering the common data encoding strategies based on parameterized quantum circuits. We prove that, under reasonable assumptions, the distance between the average encoded state and the maximally mixed state could be explicitly upper-bounded with respect to the width and depth of the encoding circuit. This result in particular implies that the average encoded state will concentrate on the maximally mixed state at an exponential speed on depth. Such concentration seriously limits the capabilities of quantum classifiers, and strictly restricts the distinguishability of encoded states from a quantum information perspective. To support our findings, we numerically verify these results on both synthetic and public data sets. Our results highlight the significance of quantum data encoding and may shed light on the future design of quantum encoding strategies.},
  author = {Li, Guangxi and Ye, Ruilin and Zhao, Xuanqiang and Wang, Xin},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  file = {:/home/b/documents/inproceedings/wang2022concentration.pdf:pdf},
  openalex = {W4283076339},
  pages = {19586--19600},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/7b2d0730df1edd8c97df4bf83696025d-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Concentration of data encoding in parameterized quantum circuits},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/7b2d0730df1edd8c97df4bf83696025d-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@article{larocca2022diagnosing,
  abstract = {Variational Quantum Algorithms (VQAs) have received considerable attention due to their potential for achieving near-term quantum advantage. However, more work is needed to understand scalability. One known scaling result for VQAs is barren plateaus, where certain circumstances lead to exponentially vanishing gradients. It is common folklore that problem-inspired ansatzes avoid barren plateaus, but in fact, very little is known about gradient scaling. In this work, we employ tools from quantum optimal control to develop a framework that can diagnose the presence or absence of barren plateaus in ansatzes. Such include Alternating Operator Ansatz (QAOA), Hamiltonian (HVA), and others. With our framework, we prove avoiding these is not always guaranteed. Specifically, we show a VQA depends on the degree of controllability of the system, hence can be diagnosed through dynamical Lie algebra generators of the ansatz. We analyze the existence of QAOA and HVA ansatzes, highlight the role of input state, as different initial states lead to different barren plateaus. Taken together, our results provide trainability-aware ansatz design strategies that do not come at the cost of extra resources. Moreover, we show a no-go result for obtaining ground states with variational approaches in certain controllable systems such as spin glasses. Our work establishes a link between the dimension of the Lie algebra and barren plateaus.},
  author = {Larocca, Martin and Czarnik, Piotr and Sharma, Kunal and Muraleedharan, Gopikrishnan and Coles, Patrick J. and Cerezo, M.},
  doi = {10.22331/q-2022-09-29-824},
  file = {:/home/b/documents/articles/larocca2022diagnosing.pdf:pdf},
  journal = {Quantum},
  month = {9},
  openalex = {W3166275705},
  pages = {824},
  pdf = {https://quantum-journal.org/papers/q-2022-09-29-824/pdf/},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Diagnosing barren plateaus with tools from quantum optimal control},
  url = {https://doi.org/10.22331/q-2022-09-29-824},
  volume = {6},
  year = {2022}
}

@article{sharma2022learning,
  author = {Sharma, Kunal and Khatri, Sumeet and Cerezo, Marco and Coles, Patrick J},
  journal = {PRX Quantum},
  number = {3},
  pages = {030307},
  publisher = {APS},
  title = {Learning to learn with quantum neural networks via classical neural networks},
  volume = {3},
  year = {2022}
}

@article{larocca2022diagnosing,
  abstract = {Variational Quantum Algorithms (VQAs) have received considerable attention due to their potential for achieving near-term quantum advantage. However, more work is needed to understand their scalability. One known scaling result for VQAs is barren plateaus, where certain circumstances lead to exponentially vanishing gradients. It is common folklore that problem-inspired ansatzes avoid barren plateaus, but in fact, very little is known about their gradient scaling. In this work we employ tools from quantum optimal control to develop a framework that can diagnose the presence or absence of barren plateaus for problem-inspired ansatzes. Such ansatzes include the Quantum Alternating Operator Ansatz (QAOA), the Hamiltonian Variational Ansatz (HVA), and others. With our framework, we prove that avoiding barren plateaus for these ansatzes is not always guaranteed. Specifically, we show that the gradient scaling of the VQA depends on the degree of controllability of the system, and hence can be diagnosed through the dynamical Lie algebra $픤$ obtained from the generators of the ansatz. We analyze the existence of barren plateaus in QAOA and HVA ansatzes, and we highlight the role of the input state, as different initial states can lead to the presence or absence of barren plateaus. Taken together, our results provide a framework for trainability-aware ansatz design strategies that do not come at the cost of extra quantum resources. Moreover, we prove no-go results for obtaining ground states with variational ansatzes for controllable system such as spin glasses. Our work establishes a link between the existence of barren plateaus and the scaling of the dimension of $픤$.},
  author = {Larocca, Martín and Czarnik, Piotr and Sharma, Kunal and Muraleedharan, Gopikrishnan and Coles, Patrick J. and Cerezo, M.},
  doi = {10.22331/q-2022-09-29-824},
  file = {:/home/b/documents/articles/larocca2022diagnosing.pdf:pdf},
  journal = {Quantum},
  month = {9},
  openalex = {W3166275705},
  pages = {824},
  pdf = {https://quantum-journal.org/papers/q-2022-09-29-824/pdf/},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Diagnosing Barren Plateaus with Tools from Quantum Optimal Control},
  volume = {6},
  year = {2022}
}

@article{li2022quantum,
  author = {Li, Yao and Zhao, Xiumei and Wang, Wendong},
  journal = {Quantum Information Processing},
  number = {9},
  pages = {329},
  publisher = {Springer},
  title = {Quantum-enhanced machine learning for sentiment analysis},
  volume = {21},
  year = {2022}
}

@article{larocca2022group,
  abstract = {Quantum Machine Learning (QML) models are aimed at learning from data encoded in quantum states. Recently, it has been shown that models with little to no inductive biases (i.e., with no assumptions about the problem embedded in the model) are likely to have trainability and generalization issues, especially for large problem sizes. As such, it is fundamental to develop schemes that encode as much information as available about the problem at hand. In this work we present a simple, yet powerful, framework where the underlying invariances in the data are used to build QML models that, by construction, respect those symmetries. These so-called group-invariant models produce outputs that remain invariant under the action of any element of the symmetry group $픊$ associated to the dataset. We present theoretical results underpinning the design of $픊$-invariant models, and exemplify their application through several paradigmatic QML classification tasks including cases when $픊$ is a continuous Lie group and also when it is a discrete symmetry group. Notably, our framework allows us to recover, in an elegant way, several well known algorithms for the literature, as well as to discover new ones. Taken together, we expect that our results will help pave the way towards a more geometric and group-theoretic approach to QML model design.},
  author = {Larocca, Martín and Sauvage, Frédéric and Sbahi, Faris M. and Verdon, Guillaume and Coles, Patrick J. and Cerezo, M.},
  doi = {10.1103/PRXQuantum.3.030341},
  file = {:/home/b/documents/articles/larocca2022group.pdf:pdf},
  journal = {PRX Quantum},
  month = {9},
  number = {3},
  openalex = {W4229000948},
  pages = {030341},
  pdf = {https://arxiv.org/pdf/2205.02261.pdf},
  publisher = {American Physical Society},
  title = {Group-Invariant Quantum Machine Learning},
  volume = {3},
  year = {2022}
}

@article{umer2022comprehensive,
  abstract = {Machine learning is a branch of artificial intelligence that is being used at a large scale to solve science, engineering, and medical tasks. Quantum computing is an emerging technology that has a very high computational ability to solve complex problems. This comprehensive survey explores the intersection of quantum computing and machine learning, examining how quantum algorithms can enhance traditional machine learning approaches and discussing potential applications across various domains including cryptography, optimization, and drug development.},
  author = {Muhammad Junaid Umer and Muhammad Imran Sharif},
  doi = {10.4018/IJEHMC.315730},
  journal = {International Journal of E-Health and Medical Communications},
  month = {10},
  number = {5},
  openalex = {W4312109322},
  pages = {1--17},
  publisher = {IGI Global},
  title = {A Comprehensive Survey on Quantum Machine Learning and Possible Applications},
  volume = {13},
  year = {2022}
}

@article{schuld2022what,
  author = {Schuld, Maria and Petruccione, Francesco},
  issn = {1476-4687},
  journal = {Nature},
  month = {2},
  note = {Unable to verify this specific paper during enrichment - title and citation details may need verification},
  number = {7896},
  pages = {391--393},
  publisher = {Nature Publishing Group},
  title = {What quantum machine learning can do},
  volume = {602},
  year = {2022}
}

@article{tang2018quantum,
  abstract = {A central roadblock to analyzing quantum algorithms on quantum states is the lack of a comparable input model for classical algorithms. We introduce a model where we can efficiently perform ℓ²-norm samples of input data, a natural analogue to quantum algorithms that assume efficient state preparation of classical data. With this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's quantum algorithms for principal component analysis and nearest-centroid clustering. Since they are only polynomially slower, these algorithms suggest that the exponential speedups of their quantum counterparts are simply an artifact of state preparation assumptions.},
  archiveprefix = {arXiv},
  author = {Tang, Ewin},
  doi = {10.1103/PhysRevLett.127.060503},
  eprint = {1811.00414},
  file = {:/home/b/documents/articles/tang2018quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {8},
  number = {6},
  openalex = {W3192002582},
  pages = {060503},
  pdf = {https://arxiv.org/pdf/1811.00414.pdf},
  primaryclass = {quant-ph},
  title = {Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions},
  volume = {127},
  year = {2021}
}

@article{schuld2021effect,
  abstract = {Quantum computers can be used for supervised learning by treating parametrised quantum circuits as models that map data inputs to predictions. While a lot of work has been done to investigate practical implications of this approach, many important theoretical properties of these models remain unknown. Here we investigate how the strategy with which data is encoded into the model influences the expressive power of parametrised quantum circuits as function approximators. We show that one can naturally write a quantum model as a partial Fourier series in the data, where the accessible frequencies are determined by the nature of the data encoding gates in the circuit. By repeating simple data encoding gates multiple times, quantum models can access increasingly rich frequency spectra. We show that there exist quantum models which can realise all possible sets of Fourier coefficients, and therefore, if the accessible frequency spectrum is asymptotically rich enough, such models are universal function approximators.},
  author = {Schuld, Maria and Sweke, Ryan and Meyer, Johannes Jakob},
  doi = {10.1103/PhysRevA.103.032430},
  file = {:/home/b/documents/articles/schuld2021effect.pdf:pdf},
  journal = {Physical Review A},
  month = {3},
  number = {3},
  openalex = {W3075559820},
  pages = {032430},
  pdf = {https://arxiv.org/pdf/2008.08605.pdf},
  publisher = {American Physical Society},
  title = {The effect of data encoding on the expressive power of variational quantum machine learning models},
  url = {https://journals.aps.org/pra/abstract/10.1103/PhysRevA.103.032430},
  volume = {103},
  year = {2021}
}

@article{skolik2021layerwise,
  abstract = {With the increased focus on quantum circuit learning for near-term applications on quantum devices, in conjunction with unique challenges presented by cost function landscapes of parametrized quantum circuits, strategies for effective training are becoming increasingly important. We investigate a layerwise learning strategy for parametrized quantum circuits. The circuit depth is incrementally grown during optimization, and only subsets of parameters are updated in each training step. We demonstrate the strategy on an image-classification task on handwritten digits, and show that layerwise learning attains an 8% lower generalization error on average in comparison to standard learning schemes for training quantum circuits of the same size.},
  author = {Skolik, Andrea and McClean, Jarrod R and Mohseni, Masoud and van der Smagt, Patrick and Leib, Martin},
  doi = {10.1007/s42484-020-00036-4},
  file = {:/home/b/documents/articles/skolik2021layerwise.pdf:pdf},
  journal = {Quantum Machine Intelligence},
  number = {1},
  openalex = {W3004252283},
  pages = {1--11},
  pdf = {https://arxiv.org/pdf/2006.14904},
  publisher = {Springer},
  title = {Layerwise learning for quantum neural networks},
  volume = {3},
  year = {2021}
}

@article{huang2021power,
  abstract = {The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits.},
  author = {Huang, Hsin-Yuan and Broughton, Michael and Mohseni, Masoud and Babbush, Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.},
  doi = {10.1038/s41467-021-22539-9},
  file = {:/home/b/documents/articles/huang2021power.pdf:pdf},
  journal = {Nature Communications},
  month = {5},
  number = {1},
  openalex = {W3129458892},
  pages = {2631},
  pdf = {https://www.nature.com/articles/s41467-021-22539-9.pdf},
  publisher = {Nature Publishing Group},
  title = {Power of data in quantum machine learning},
  volume = {12},
  year = {2021}
}

@article{schuld2021supervised,
  abstract = {With near-term quantum devices available and the race for fault-tolerant quantum computers in full swing, researchers became interested in the question of what happens if we replace a supervised machine learning model with a quantum circuit. While such 'quantum models' are sometimes called 'quantum neural networks', it has been repeatedly noted that their mathematical structure is actually much more closely related to kernel methods: they analyse data in high-dimensional Hilbert spaces to which we only have access through inner products revealed by measurements. This technical manuscript summarises and extends the idea of systematically rephrasing supervised quantum models as a kernel method. With this, a lot of near-term and fault-tolerant quantum models can be replaced by a general support vector machine whose kernel computes distances between data-encoding quantum states. Kernel-based training is then guaranteed to find better or equally good quantum models than variational circuit training. Overall, the kernel perspective of quantum machine learning tells us that the way that data is encoded into quantum states is the main ingredient that can potentially set quantum models apart from classical machine learning models.},
  author = {Maria Schuld},
  doi = {10.48550/arxiv.2101.11020},
  journal = {arXiv preprint arXiv:2101.11020},
  month = {1},
  openalex = {W3154850693},
  title = {Supervised quantum machine learning models are kernel methods},
  url = {https://arxiv.org/abs/2101.11020},
  year = {2021}
}

@article{wang2021noise,
  abstract = {Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes training landscape have a barren plateau (i.e., vanishing gradient). Specifically, for local Pauli noise considered, we prove gradient vanishes exponentially in number of qubits $n$ if depth ansatz grows linearly with $n$. These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free plateaus, which are linked to random parameter initialization. Our result formulated generic includes as special cases Alternating Operator Ansatz and Unitary Coupled Cluster Ansatz, among others. For former, our numerical heuristics demonstrate NIBP phenomenon in realistic hardware model.},
  author = {Wang, Samson and Fontana, Enrico and Cerezo, Marco and Sharma, Kunal and Sone, Akira and Cincio, Lukasz and Coles, Patrick J},
  doi = {10.1038/s41467-021-27045-6},
  journal = {Nature Communications},
  month = {11},
  number = {1},
  openalex = {W3148001159},
  pages = {6961},
  publisher = {Nature Publishing Group},
  title = {Noise-induced barren plateaus in variational quantum algorithms},
  url = {https://www.nature.com/articles/s41467-021-27045-6},
  volume = {12},
  year = {2021}
}

@article{abbas2021power,
  abstract = {Fault-tolerant quantum computers offer the promise of dramatically improving machine learning through speed-ups in computation or improved model scalability. In the near-term, however, the benefits of quantum machine learning are not so clear. Understanding expressibility and trainability of quantum models---and quantum neural networks in particular---requires further investigation. In this work, we use tools from information geometry to define a notion of expressibility for quantum and classical models. The effective dimension, which depends on the Fisher information, is used to prove a novel generalization bound. We show that quantum neural networks are able to achieve a significantly better effective dimension than comparable classical neural networks. Moreover, the quantum neural networks can train faster than classical models due to their favourable optimization landscapes, captured by a more evenly spread Fisher information spectrum. Our empirical results verify these theoretical findings on real quantum hardware.},
  author = {Abbas, Amira and Sutter, David and Zoufal, Christa and Lucchi, Aurélien and Figalli, Alessio and Woerner, Stefan},
  doi = {10.1038/s43588-021-00084-1},
  file = {:/home/b/documents/articles/abbas2021power.pdf:pdf},
  journal = {Nature Computational Science},
  month = {6},
  number = {6},
  openalex = {W3132743969},
  pages = {403--409},
  pdf = {https://arxiv.org/pdf/2011.00027},
  publisher = {Nature Publishing Group},
  title = {The power of quantum neural networks},
  volume = {1},
  year = {2021}
}

@article{huang2021information,
  abstract = {We study the performance of classical and quantum machine learning (ML) models in predicting outcomes of physical experiments. The experiments depend on an input parameter $x$ and involve execution of a (possibly unknown) quantum process $\mathcalE$. Our figure of merit is the number of runs of $\mathcalE$ required to achieve a desired prediction performance. We consider classical ML models that perform a measurement and record the classical outcome after each run of $\mathcalE$, and quantum ML models that can access $\mathcalE$ coherently to acquire quantum data; the classical or quantum data is then used to predict outcomes of future experiments. We prove that for any input distribution $\mathcalD(x)$, a classical ML model can provide accurate predictions on average by accessing $\mathcalE$ a number of times comparable to the optimal quantum ML model. In contrast, for achieving accurate prediction on all inputs, we prove that exponential quantum advantage is possible. For example, to predict expectations of all Pauli observables in an $n$-qubit system $h̊o$, classical ML models require $2^Ømega(n)$ copies of $ø̊$, but we present a quantum ML model using only $\mathcalO(n)$ copies. Our results clarify where quantum advantage is possible and highlight the potential for classical ML models to address challenging quantum problems in physics and chemistry.},
  author = {Hsin-Yuan Huang and Richard Kueng and John Preskill},
  doi = {10.1103/PhysRevLett.126.190505},
  file = {:/home/b/documents/articles/huang2021information.pdf:pdf},
  journal = {Physical Review Letters},
  number = {19},
  openalex = {W3119636101},
  pages = {190505},
  pdf = {https://arxiv.org/pdf/2101.02464.pdf},
  publisher = {American Physical Society},
  title = {Information-theoretic bounds on quantum advantage in machine learning},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.190505},
  volume = {126},
  year = {2021}
}

@article{pesah2021absence,
  abstract = {Quantum neural networks (QNNs) have generated excitement around the possibility of efficiently analyzing quantum data. But this has been tempered by the existence of exponentially vanishing gradients, known as barren plateau landscapes, for many QNN architectures. Recently, Convolutional Neural Networks (QCNNs) were proposed, involving a sequence of convolutional and pooling layers that reduce the number of qubits while preserving information about relevant data features.},
  author = {Pesah, Arthur and Cerezo, Marco and Wang, Samson and Volkoff, Tyler and Sornborger, Andrew T and Coles, Patrick J},
  doi = {10.1103/PhysRevX.11.041011},
  file = {:/home/b/documents/articles/pesah2021absence.pdf:pdf},
  journal = {Physical Review X},
  month = {10},
  number = {4},
  openalex = {W3207697613},
  pages = {041011},
  pdf = {https://arxiv.org/pdf/2011.02966.pdf},
  publisher = {American Physical Society},
  title = {Absence of barren plateaus in quantum convolutional neural networks},
  volume = {11},
  year = {2021}
}

@inproceedings{jerbi2021variational,
  abstract = {With the advent of real-world quantum computing, the idea that parametrized quantum computations can be used as hypothesis families in a quantum-classical machine learning system is gaining increasing traction. Such hybrid systems have already shown the potential to tackle real-world tasks in supervised and generative learning, and recent works have established their provable advantages in special artificial tasks. Yet, in the case of reinforcement learning, which is arguably most challenging and where learning boosts would be extremely valuable, no proposal has been successful in solving even standard benchmarking tasks, nor in showing a theoretical learning advantage over classical algorithms. In this work, we achieve both. We propose a hybrid quantum-classical reinforcement learning model using very few qubits, which we show can be effectively trained to solve several standard benchmarking environments. Moreover, we demonstrate, and formally prove, the ability of parametrized quantum circuits to solve certain learning tasks that are intractable for classical models, including current state-of-art deep neural networks, under the widely-believed classical hardness of the discrete logarithm problem.},
  author = {Jerbi, Sofiene and Gyurik, Casper and Marshall, Simon C. and Briegel, Hans J. and Dunjko, Vedran},
  booktitle = {Advances in Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/jerbi2021variational.pdf:pdf},
  openalex = {W3134744093},
  pages = {16451--16463},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/eec96a7f788e88184c0e713456026f3f-Paper.pdf},
  title = {Parametrized quantum policies for reinforcement learning},
  volume = {34},
  year = {2021}
}

@article{kubler2021practical,
  abstract = {It has been hypothesized that quantum computers may lend themselves well to applications in machine learning. In the present work, we analyze function classes defined via quantum kernels. Quantum computers offer the possibility to efficiently compute inner products of exponentially large density operators that are classically hard to compute. However, having an exponentially large feature space renders the problem of generalization hard. Furthermore, being able to evaluate kernels in high dimensional spaces by itself does not guarantee a quantum advantage, as already classically tractable kernels can correspond to high- or infinite-dimensional reproducing kernel Hilbert spaces (RKHS). We analyze the spectral properties of quantum kernels and find that we can expect a quantum advantage if their RKHS is low dimensional and contains functions that are classically hard to compute. If the target function is known to lie in this class, this implies a quantum advantage, as the quantum computer can encode this inductive bias, whereas there is no classically efficient way to constrain the function class in the same way. We show that finding suitable quantum kernels is not easy because the kernel evaluation might require exponentially many measurements. In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode knowledge about the problem at hand into quantum circuits, while encoding the same bias into a classical model would be hard. These situations may plausibly occur when learning on data generated by a quantum process, however, they appear to be harder to come by for classical datasets.},
  author = {Kübler, Jonas M. and Buchholz, Simon and Schölkopf, Bernhard},
  booktitle = {Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021)},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
  journal = {Advances in Neural Information Processing Systems},
  openalex = {W4287123924},
  pages = {12661--12673},
  pdf = {https://neurips.cc/paper_files/paper/2021/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {The Inductive Bias of Quantum Kernels},
  url = {https://proceedings.neurips.cc/paper/2021/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html},
  volume = {34},
  year = {2021}
}

@article{kubler2021practical,
  author = {Kübler, Jonas M and Buchholz, Simon and Schölkopf, Bernhard},
  journal = {arXiv preprint arXiv:2102.08562},
  title = {On the practical usefulness of quantum kernel machines},
  year = {2021}
}

@article{holmes2021barren,
  abstract = {Scrambling processes, which rapidly spread entanglement through many-body quantum systems, are difficult to investigate using standard techniques, but are relevant to quantum chaos and thermalization. In this Letter, we ask if quantum machine learning (QML) could be used to investigate such processes. We prove a no-go theorem for learning an unknown scrambling process with QML, showing that any variational ansatz is highly probable to have a barren plateau landscape, i.e., cost gradients that vanish exponentially in the system size. This implies that the required resources scale exponentially even when strategies to avoid such scaling (e.g., from ansatz-based barren plateaus or No-Free-Lunch theorems) are employed. Furthermore, we numerically and analytically extend our results to approximate scramblers. Hence, our work places generic limits on the learnability of unitaries when lacking prior information.},
  author = {Holmes, Zoë and Arrasmith, Andrew and Yan, Bin and Coles, Patrick J and Albrecht, Andreas and Sornborger, Andrew T},
  doi = {10.1103/PhysRevLett.126.190501},
  file = {:/home/b/documents/articles/holmes2021barren.pdf:pdf},
  journal = {Physical Review Letters},
  month = {5},
  number = {19},
  openalex = {W3090921460},
  pages = {190501},
  pdf = {https://arxiv.org/pdf/2009.14808.pdf},
  publisher = {APS},
  title = {Barren plateaus preclude learning scramblers},
  volume = {126},
  year = {2021}
}

@article{guan2021quantum,
  abstract = {Machine learning has been used in high energy physics for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.},
  arxiv = {2005.08582},
  author = {Guan, Wen and Perdue, Gabriel and Pesah, Arthur and Schuld, Maria and Terashi, Koji and Vallecorsa, Sofia and Vlimant, Jean-Roch},
  doi = {10.1088/2632-2153/abc17d},
  journal = {Machine Learning: Science and Technology},
  number = {1},
  pages = {011003},
  publisher = {IOP Publishing},
  title = {Quantum machine learning in high energy physics},
  url = {https://iopscience.iop.org/article/10.1088/2632-2153/abc17d},
  volume = {2},
  year = {2021}
}

@article{schuld2020circuit,
  abstract = {The current generation of quantum computing technologies call for quantum algorithms that require a limited number of qubits and quantum gates, and which are robust against errors. A suitable design approach are variational circuits where the parameters of gates are learnt, an approach that is particularly fruitful for applications in machine learning. In this paper, we propose a low-depth variational quantum algorithm for supervised learning. The input feature vectors are encoded into the amplitudes of a quantum system, and a quantum circuit of parametrised single and two-qubit gates together with a single-qubit measurement is used to classify the inputs. This circuit architecture ensures that the number of learnable parameters is poly-logarithmic in the input dimension. We propose a quantum-classical training scheme where the analytical gradients of the model can be estimated by running several slightly adapted versions of the variational circuit. We show with simulations that the circuit-centric quantum classifier performs well on standard classical benchmark datasets while requiring dramatically fewer parameters than other methods. We also evaluate sensitivity of the classification to state preparation and parameter noise, introduce a quantum version of dropout regularisation and provide a graphical representation of quantum gates as highly symmetric linear layers of a neural network.},
  archiveprefix = {arXiv},
  author = {Schuld, Maria and Bocharov, Alex and Svore, Krysta M. and Wiebe, Nathan},
  doi = {10.1103/PhysRevA.101.032308},
  eprint = {1804.00633},
  file = {:/home/b/documents/articles/schuld2020circuit.pdf:pdf},
  journal = {Physical Review A},
  month = {3},
  number = {3},
  openalex = {W2796293949},
  pages = {032308},
  pdf = {https://arxiv.org/pdf/1804.00633.pdf},
  primaryclass = {quant-ph},
  publisher = {American Physical Society},
  title = {Circuit-centric quantum classifiers},
  volume = {101},
  year = {2020}
}

@article{perez2020data,
  abstract = {A single qubit provides sufficient computational capabilities to construct a universal quantum classifier when assisted with a classical subroutine. This fact may be surprising since a single qubit only offers a simple superposition of two states and single-qubit gates only make a rotation in the Bloch sphere. The key ingredient to circumvent these limitations is to allow for multiple data re-uploading. A quantum circuit can then be organized as a series of data re-uploading and single-qubit processing units. Furthermore, both data re-uploading and measurements can accommodate multiple dimensions in the input and several categories in the output, to conform to a universal quantum classifier. An extension of this idea using multiple qubits enhances the efficiency of the strategy, and carries entanglement along superpositions for classification. Extensive benchmarking on different examples of the single- and multi-qubit quantum classifier validates its ability to describe and classify complex data},
  author = {Pérez-Salinas, Adrián and Cervera-Lierta, Alba and Gil-Fuster, Elies and Latorre, José I},
  doi = {10.22331/q-2020-02-06-226},
  file = {:/home/b/documents/articles/perez2020data.pdf:pdf},
  journal = {Quantum},
  month = {2},
  openalex = {W3098599423},
  pages = {226},
  pdf = {https://quantum-journal.org/papers/q-2020-02-06-226/pdf/},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Data re-uploading for a universal quantum classifier},
  url = {https://doi.org/10.22331/q-2020-02-06-226},
  volume = {4},
  year = {2020}
}

@article{stokes2020quantum,
  abstract = {A quantum generalization of Natural Gradient Descent is presented as part of a general-purpose optimization framework for variational quantum circuits. The optimization dynamics is interpreted as moving in the steepest descent direction with respect to the Quantum Information Geometry, corresponding to the real part of the Quantum Geometric Tensor (QGT), also known as the Fubini-Study metric tensor. An efficient algorithm is presented for computing a block-diagonal approximation to the Fubini-Study metric tensor for parametrized quantum circuits, which may be of independent interest.},
  author = {Stokes, James and Izaac, Josh and Killoran, Nathan and Carleo, Giuseppe},
  doi = {10.22331/q-2020-05-25-269},
  file = {:/home/b/documents/articles/stokes2020quantum.pdf:pdf},
  journal = {Quantum},
  month = {5},
  openalex = {W2972032089},
  pages = {269},
  pdf = {https://quantum-journal.org/papers/q-2020-05-25-269/pdf/},
  publisher = {Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
  title = {Quantum natural gradient},
  url = {https://doi.org/10.22331/q-2020-05-25-269},
  volume = {4},
  year = {2020}
}

@article{coyle2020quantum,
  abstract = {The search for an application of near-term quantum devices is widespread. Quantum Machine Learning is touted as a potential utilisation of such devices, particularly those which are out of the reach of the simulation capabilities of classical computers. In this work, we propose a generative Quantum Machine Learning Model, called the Ising Born Machine (IBM), which we show cannot, in the worst case, and up to suitable notions of error, be simulated efficiently by a classical device. We also show this holds for all the circuit families encountered during training. In particular, we explore quantum circuit learning using non-universal circuits derived from Ising Model Hamiltonians, which are implementable on near term quantum devices. We propose two novel training methods for the IBM by utilising the Stein Discrepancy and the Sinkhorn Divergence cost functions. We show numerically, both using a simulator within Rigetti's Forest platform and on the Aspen-1 16Q chip, that the cost functions we suggest outperform the more commonly used Maximum Mean Discrepancy (MMD) for differentiable training. We also propose an improvement to the MMD by proposing a novel utilisation of quantum kernels which we demonstrate provides improvements over its classical counterpart. We discuss the potential of these methods to learn 'hard' quantum distributions, a feat which would demonstrate the advantage of quantum over classical computers, and provide the first formal definitions for what we call 'Quantum Learning Supremacy'. Finally, we propose a novel view on the area of quantum circuit compilation by using the IBM to 'mimic' target quantum circuits using classical output data only.},
  author = {Coyle, Brian and Mills, Daniel and Danos, Vincent and Kashefi, Elham},
  doi = {10.1038/s41534-020-00288-9},
  file = {:/home/b/documents/articles/coyle2020quantum.pdf:pdf},
  journal = {npj Quantum Information},
  number = {1},
  openalex = {W2926552232},
  pages = {60},
  pdf = {https://www.nature.com/articles/s41534-020-00288-9.pdf},
  publisher = {Nature Publishing Group},
  title = {The Born supremacy: quantum advantage and training of an Ising Born machine},
  url = {https://doi.org/10.1038/s41534-020-00288-9},
  volume = {6},
  year = {2020}
}

@inproceedings{chia2020quantum,
  abstract = {We present an algorithmic framework for quantum-inspired classical algorithms on close-to-low-rank matrices, generalizing the series of results started by Tang's breakthrough quantum-inspired algorithm for recommendation systems [STOC'19]. Motivated by quantum linear algebra algorithms and the quantum singular value transformation (SVT) framework of Gilyén, Su, Low, and Wiebe [STOC'19], we develop classical algorithms for SVT that run in time independent of input dimension, under suitable quantum-inspired sampling assumptions. Our results give compelling evidence that in the corresponding QRAM data structure input model, quantum SVT does not yield exponential quantum speedups. Since the quantum SVT framework generalizes essentially all known techniques for quantum linear algebra, our results, combined with sampling lemmas from previous work, suffice to generalize all recent results about dequantizing quantum machine learning algorithms. In particular, our classical SVT framework recovers and often improves the dequantization results on recommendation systems, principal component analysis, supervised clustering, support vector machines, low-rank regression, and semidefinite program solving. We also give additional dequantization results on low-rank Hamiltonian simulation and discriminant analysis. Our improvements come from identifying the key feature of the quantum-inspired input model that is at the core of all prior quantum-inspired results: \ell²-norm sampling can approximate matrix products in time independent of their dimension. We reduce all our main results to this fact, making our exposition concise, self-contained, and intuitive.},
  address = {New York, NY, USA},
  author = {Chia, Nai-Hui and Gilyén, András and Li, Tongyang and Lin, Han-Hsuan and Tang, Ewin and Wang, Chunhao},
  booktitle = {Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing},
  doi = {10.1145/3357713.3384314},
  file = {:/home/b/documents/inproceedings/chia2020quantum.pdf:pdf},
  isbn = {978-1-4503-6979-4},
  note = {arXiv:1910.06151},
  openalex = {W3034336799},
  pages = {387--400},
  pdf = {https://arxiv.org/pdf/1910.06151.pdf},
  publisher = {Association for Computing Machinery},
  series = {STOC '20},
  title = {Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning},
  url = {https://doi.org/10.1145/3357713.3384314},
  year = {2020}
}

@article{bravyi2019qaoa,
  abstract = {Local Hamiltonians with topological quantum order exhibit highly entangled ground states that cannot be prepared by shallow quantum circuits. Here, we show that this property may extend to all low-energy states in the presence of an on-site $ℤ_2$ symmetry.},
  author = {Sergey Bravyi and Alexander Kliesch and Robert Koenig and Eugene Tang},
  doi = {10.1103/PhysRevLett.125.260505},
  file = {:/home/b/documents/articles/bravyi2019qaoa.pdf:pdf},
  journal = {Physical Review Letters},
  month = {12},
  number = {26},
  openalex = {W2980835754},
  pages = {260505},
  pdf = {http://arxiv.org/pdf/1910.08980},
  title = {Obstacles to State Preparation and Variational Optimization from Symmetry Protection},
  volume = {125},
  year = {2020}
}

@article{lloyd2020quantum,
  abstract = {Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit -- the embedding -- with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.},
  archiveprefix = {arXiv},
  author = {Lloyd, Seth and Schuld, Maria and Ijaz, Aroosa and Izaac, Josh and Killoran, Nathan},
  eprint = {2001.03622},
  file = {:/home/b/documents/articles/lloyd2020quantum.pdf:pdf},
  journal = {arXiv preprint arXiv:2001.03622},
  month = {1},
  openalex = {W4287905360},
  pdf = {https://arxiv.org/pdf/2001.03622.pdf},
  primaryclass = {quant-ph},
  title = {Quantum embeddings for machine learning},
  year = {2020}
}

@article{lloyd2020quantum,
  abstract = {Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. However, we propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the $\ell_1$ or trace distance, this is the Helstrom measurement, while for the $\ell_2$ or Hilbert-Schmidt distance, it is a simple overlap measurement. The approach is trained using gradient ascent on a variational quantum circuit, and we show that quantum metric learning can help to mitigate overfitting on training data and enhance generalization to unseen data.},
  archiveprefix = {arXiv},
  author = {Lloyd, Seth and Schuld, Maria and Ijaz, Aroosa and Izaac, Josh and Killoran, Nathan},
  doi = {10.48550/arxiv.2001.03622},
  eprint = {2001.03622},
  file = {:/home/b/documents/articles/lloyd2020quantum.pdf:pdf},
  journal = {arXiv preprint arXiv:2001.03622},
  month = {1},
  openalex = {W4287905360},
  pdf = {https://arxiv.org/pdf/2001.03622.pdf},
  primaryclass = {quant-ph},
  title = {Quantum embeddings for machine learning},
  url = {https://arxiv.org/abs/2001.03622},
  year = {2020}
}

@inproceedings{kerenidis2019quantum,
  abstract = {Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations. The QCNN is in particular interesting for deep networks and could allow new frontiers in the image recognition domain, by allowing for many more convolution kernels, larger kernels, high dimensional inputs and high depth input channels. We also present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN.},
  author = {Iordanis Kerenidis and Jonas Landman and Anupam Prakash},
  booktitle = {International Conference on Learning Representations},
  doi = {10.48550/arxiv.1911.01117},
  file = {:/home/b/documents/inproceedings/kerenidis2019quantum.pdf:pdf},
  openalex = {W2988221611},
  pdf = {https://openreview.net/pdf?id=Hygab1rKDS},
  title = {Quantum Algorithms for Deep Convolutional Neural Networks},
  url = {https://openreview.net/forum?id=Hygab1rKDS},
  year = {2020}
}

@article{broughton2020tensorflow,
  abstract = {We introduce TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under TensorFlow and supports high-performance quantum circuit simulators. We provide an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. We illustrate TFQ functionalities via several basic applications including supervised learning for quantum classification, quantum control, simulating noisy quantum circuits, and quantum approximate optimization. Moreover, we demonstrate how one can apply TFQ to tackle advanced quantum learning tasks including meta-learning, layerwise learning, Hamiltonian learning, sampling thermal states, variational quantum eigensolvers, classification of quantum phase transitions, generative adversarial networks, and reinforcement learning. We hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage.},
  archiveprefix = {arXiv},
  author = {Michael Broughton and Guillaume Verdon and Trevor McCourt and Antonio J. Martinez and Jae Hyeon Yoo and Sergei V. Isakov and Philip Massey and Ramin Halavati and Murphy Yuezhen Niu and Alexander Zlokapa and Evan Peters and Owen Lockwood and Andrea Skolik and Sofièn Jerbi and Vedran Dunjko and Martin Leib and Michael Streif and David Von Dollen and Hongxiang Chen and Shuxiang Cao and Roeland Wiersema and Hsin-Yuan Huang and Jarrod R. McClean and Ryan Babbush and Sergio Boixo and Dave Bacon and Alan Ho and Hartmut Neven and Masoud Mohseni},
  doi = {10.48550/arXiv.2003.02989},
  eprint = {2003.02989},
  file = {:/home/b/documents/articles/broughton2020tensorflow.pdf:pdf},
  journal = {arXiv preprint arXiv:2003.02989},
  month = {3},
  openalex = {W3010243192},
  pdf = {https://arxiv.org/pdf/2003.02989.pdf},
  primaryclass = {quant-ph},
  title = {TensorFlow Quantum: A software framework for quantum machine learning},
  url = {https://arxiv.org/abs/2003.02989},
  year = {2020}
}

@inproceedings{tang2019quantum,
  abstract = {We give a classical analogue to Kerenidis and Prakash's quantum recommendation system, previously believed to be one of the strongest candidates for provably exponential speedups in quantum machine learning. Our main result is an algorithm that, given an $m  imes n$ matrix in a data structure supporting certain $\ell^2$-norm sampling operations, outputs an $\ell^2$-norm sample from a rank-$k$ approximation of that matrix in time $O( extpoly(k)łog(mn))$, only polynomially slower than the quantum algorithm. As a consequence, Kerenidis and Prakash's algorithm does not in fact give an exponential speedup over classical algorithms. Further, under strong input assumptions, the classical recommendation system resulting from our algorithm produces recommendations exponentially faster than previous classical systems, which run in time linear in $m$ and $n$. The main insight of this work is the use of simple routines to manipulate $\ell^2$-norm sampling distributions, which play the role of quantum superpositions in the classical setting. This correspondence indicates a potentially fruitful framework for formally comparing quantum machine learning algorithms to classical machine learning algorithms.},
  address = {New York, NY, USA},
  author = {Tang, Ewin},
  booktitle = {Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing},
  doi = {10.1145/3313276.3316310},
  file = {:/home/b/documents/inproceedings/tang2019quantum.pdf:pdf},
  isbn = {978-1-4503-6705-9},
  month = {6},
  openalex = {W3101135395},
  pages = {217--228},
  pdf = {https://arxiv.org/pdf/1807.04271},
  publisher = {Association for Computing Machinery},
  series = {STOC '19},
  title = {A quantum-inspired classical algorithm for recommendation systems},
  url = {https://doi.org/10.1145/3313276.3316310},
  year = {2019}
}

@article{benedetti2019parameterized,
  abstract = {Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.},
  author = {Benedetti, Marcello and Lloyd, Erika and Sack, Stefan and Fiorentini, Mattia},
  doi = {10.1088/2058-9565/ab4eb5},
  journal = {Quantum Science and Technology},
  number = {4},
  openalex = {W3101122608},
  pages = {043001},
  pdf = {https://iopscience.iop.org/article/10.1088/2058-9565/ab4eb5/pdf},
  publisher = {IOP Publishing},
  title = {Parameterized quantum circuits as machine learning models},
  volume = {4},
  year = {2019}
}

@article{sim2019expressibility,
  abstract = {Parameterized quantum circuits play an essential role in the performance of many variational hybrid quantum-classical algorithms. The challenge is to choose an effective circuit that well represents the solution space while maintaining a low circuit depth and number of parameters. To tackle this, we propose several descriptors, including measures of expressibility and entangling capability, that can be statistically estimated from classical simulations of parameterized quantum circuits. We reveal the substantial improvement in performance of two-qubit gates in a ring or all-to-all connected arrangement compared to those on a line, and the improvement in expressibility and entangling capability achieved by sequences of controlled X-rotation gates compared to controlled Z-rotation gates. We found that expressibility "saturates" with increased circuit depth, and that the rate and saturated-value appear to be distinguishing features of a parameterized quantum circuit template.},
  author = {Sim, Sukin and Johnson, Peter D. and Aspuru-Guzik, Alán},
  doi = {10.1002/qute.201900070},
  file = {:/home/b/documents/articles/sim2019expressibility.pdf:pdf},
  journal = {Advanced Quantum Technologies},
  month = {12},
  number = {12},
  openalex = {W3101427288},
  pages = {1900070},
  pdf = {https://arxiv.org/pdf/1905.10876.pdf},
  publisher = {Wiley-VCH},
  title = {Expressibility and Entangling Capability of Parameterized Quantum Circuits for Hybrid Quantum-Classical Algorithms},
  url = {https://advanced.onlinelibrary.wiley.com/doi/abs/10.1002/qute.201900070},
  volume = {2},
  year = {2019}
}

@article{schuld2019quantum,
  abstract = {The basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely to efficiently perform computations in an intractably large Hilbert space. In this paper we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyse the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. This kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we can use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualise the working principle with $2$-dimensional mini-benchmark datasets.},
  author = {Schuld, Maria and Killoran, Nathan},
  doi = {10.1103/PhysRevLett.122.040504},
  file = {:/home/b/documents/articles/schuld2019quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {2},
  number = {4},
  openalex = {W2792946961},
  pages = {040504},
  pdf = {https://arxiv.org/pdf/1803.07128.pdf},
  publisher = {American Physical Society},
  title = {Quantum machine learning in feature Hilbert spaces},
  url = {https://doi.org/10.1103/PhysRevLett.122.040504},
  volume = {122},
  year = {2019}
}

@article{havlicek2019supervised,
  abstract = {Machine learning and quantum computing are two technologies each with the potential for altering how computation is performed to address previously untenable problems. Kernel methods for machine learning are ubiquitous for pattern recognition, with support vector machines (SVMs) being the most well-known method for classification problems. However, there are limitations to the successful solution to such problems when the feature space becomes large, and the kernel functions become computationally expensive to estimate. A core element to computational speed-ups afforded by quantum algorithms is the exploitation of an exponentially large quantum state space through controllable entanglement and interference. Here, we propose and experimentally implement two novel methods on a superconducting processor. Both methods represent the feature space of a classification problem by a quantum state, taking advantage of the large dimensionality of quantum Hilbert space to obtain an enhanced solution. One method, the quantum variational classifier builds on [1,2] and operates through using a variational quantum circuit to classify a training set in direct analogy to conventional SVMs. In the second, a quantum kernel estimator, we estimate the kernel function and optimize the classifier directly. The two methods present a new class of tools for exploring the applications of noisy intermediate scale quantum computers [3] to machine learning.},
  author = {Havlı́ček, Vojtěch and Córcoles, Antonio D and Temme, Kristan and Harrow, Aram W and Kandala, Abhinav and Chow, Jerry M and Gambetta, Jay M},
  doi = {10.1038/s41586-019-0980-2},
  file = {:/home/b/documents/articles/havlicek2019supervised.pdf:pdf},
  journal = {Nature},
  month = {3},
  number = {7747},
  openalex = {W2798434869},
  pages = {209--212},
  pdf = {https://arxiv.org/pdf/1804.11326.pdf},
  publisher = {Nature Publishing Group},
  title = {Supervised learning with quantum-enhanced feature spaces},
  volume = {567},
  year = {2019}
}

@article{cong2019quantum,
  abstract = {We introduce and analyze a novel quantum machine learning model motivated by convolutional neural networks. Our quantum convolutional neural network (QCNN) makes use of only $O(\łog(N))$ variational parameters for input sizes of $N$ qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. The QCNN architecture combines the multi-scale entanglement renormalization ansatz and quantum error correction. We explicitly illustrate its potential with two examples. First, QCNN is used to accurately recognize quantum states associated with 1D symmetry-protected topological phases. We numerically demonstrate that a QCNN trained on a small set of exactly solvable points can reproduce the phase diagram over the entire parameter regime and also provide an exact, analytical QCNN solution. As a second application, we utilize QCNNs to devise a quantum error correction scheme optimized for a given error model. We provide a generic framework to simultaneously optimize both encoding and decoding procedures and find that the resultant scheme significantly outperforms known quantum codes of comparable complexity. Finally, potential experimental realization and generalizations of QCNNs are discussed.},
  author = {Cong, Iris and Choi, Soonwon and Lukin, Mikhail D},
  doi = {10.1038/s41567-019-0648-8},
  file = {:/home/b/documents/articles/cong2019quantum.pdf:pdf},
  journal = {Nature Physics},
  number = {12},
  openalex = {W2896712926},
  pages = {1273--1278},
  pdf = {https://arxiv.org/pdf/1810.03787.pdf},
  publisher = {Nature Publishing Group},
  title = {Quantum convolutional neural networks},
  volume = {15},
  year = {2019}
}

@inproceedings{verdon2019quantum,
  abstract = {We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.},
  author = {Verdon, Guillaume and McCourt, Trevor and Luzhnica, Enxhell and Singh, Vikash and Leichenauer, Stefan and Hidary, Jack},
  booktitle = {Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems},
  file = {:/home/b/documents/inproceedings/verdon2019quantum.pdf:pdf},
  month = {12},
  note = {arXiv:1909.12264},
  pdf = {https://arxiv.org/pdf/1909.12264.pdf},
  title = {Quantum Graph Neural Networks},
  url = {https://arxiv.org/abs/1909.12264},
  year = {2019}
}

@article{ciliberto2018quantum,
  abstract = {Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning techniques to impressive results in regression, classification, data-generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets are motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed-up classical machine learning algorithms. Here we review the literature in quantum machine learning and discuss perspectives for a mixed readership of classical machine learning and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in machine learning are identified as promising directions for the field. Practical questions, like how to upload classical data into quantum form, will also be addressed.},
  author = {Ciliberto, Carlo and Herbster, Mark and Ialongo, Alessandro Davide and Pontil, Massimiliano and Rocchetto, Andrea and Severini, Simone and Wossnig, Leonard},
  doi = {10.1098/rspa.2017.0551},
  file = {:/home/b/documents/articles/ciliberto2018quantum.pdf:pdf},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number = {2209},
  openalex = {W2736592352},
  pages = {20170551},
  pdf = {https://arxiv.org/pdf/1707.08561.pdf},
  publisher = {The Royal Society},
  title = {Quantum machine learning: a classical perspective},
  volume = {474},
  year = {2018}
}

@article{lloyd2018quantum,
  abstract = {Generative adversarial networks are a machine learning technique where two neural networks compete with each other to become more accurate in their predictions. We introduce the notion of quantum generative adversarial networks (QuGANs), where the data consists either of quantum states, or of classical data, and the generator and discriminator are equipped with quantum information processors. We show that the unique fixed point of the quantum adversarial game also occurs when the generator produces the same statistics as the data. Since quantum systems are intrinsically probabilistic, the proof of the quantum case is different from—and simpler than—the classical case. We show that when the data consists of samples of measurements made on high-dimensional spaces, quantum adversarial networks may exhibit an exponential advantage over classical adversarial networks.},
  author = {Lloyd, Seth and Weedbrook, Christian},
  doi = {10.1103/PhysRevLett.121.040502},
  file = {:/home/b/documents/articles/lloyd2018quantum.pdf:pdf},
  journal = {Physical Review Letters},
  number = {4},
  openalex = {W2798945316},
  pages = {040502},
  pdf = {https://arxiv.org/pdf/1804.09139.pdf},
  publisher = {American Physical Society},
  title = {Quantum generative adversarial learning},
  volume = {121},
  year = {2018}
}

@article{khoshaman2018quantum,
  abstract = {Variational autoencoders (VAEs) are powerful generative models with the salient ability to perform inference. Here, we introduce a quantum variational autoencoder (QVAE): a VAE whose latent generative process is implemented as a quantum Boltzmann machine (QBM). We use quantum Monte Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To achieve the best performance, we first create a VAE platform with discrete latent space generated by a restricted Boltzmann machine (RBM). Our model achieves state-of-the-art performance on the MNIST dataset when compared against similar approaches that only involve discrete variables in the generative process. We consider QVAEs with a smaller number of latent units to be able to perform QMC simulations, which are computationally expensive.},
  author = {Khoshaman, Amir and Vinci, Walter and Denis, Brandon and Andriyash, Evgeny and Sadeghi, Hossein and Amin, Mohammad H},
  doi = {10.1088/2058-9565/aada1f},
  file = {:/home/b/documents/articles/khoshaman2018quantum.pdf:pdf},
  journal = {Quantum Science and Technology},
  month = {2},
  number = {1},
  pages = {014001},
  pdf = {https://arxiv.org/pdf/1802.05779},
  publisher = {IOP Publishing},
  title = {Quantum variational autoencoder},
  url = {https://iopscience.iop.org/article/10.1088/2058-9565/aada1f},
  volume = {4},
  year = {2018}
}

@article{grant2018hierarchical,
  abstract = {Quantum circuits with hierarchical structure have been used to perform binary classification of classical data encoded in a quantum state. We demonstrate that more expressive circuits in the same family achieve better accuracy and can be used to classify highly entangled quantum states, for which there is no known efficient classical method. We compare performance for several different parameterizations on two classical machine learning datasets, Iris and MNIST, and on a synthetic dataset of quantum states. Finally, we demonstrate that performance is robust to noise and deploy an Iris dataset classifier on the ibmqx4 quantum computer.},
  archiveprefix = {arXiv},
  author = {Grant, Edward and Benedetti, Marcello and Cao, Shuxiang and Hallam, Andrew and Lockhart, Joshua and Stojevic, Vid and Green, Andrew G. and Severini, Simone},
  doi = {10.1038/s41534-018-0116-9},
  eprint = {1804.03680},
  file = {:/home/b/documents/articles/grant2018hierarchical.pdf:pdf},
  journal = {npj Quantum Information},
  number = {1},
  openalex = {W3098662938},
  pages = {65},
  pdf = {https://arxiv.org/pdf/1804.03680.pdf},
  primaryclass = {quant-ph},
  publisher = {Nature Publishing Group},
  title = {Hierarchical quantum classifiers},
  url = {https://www.nature.com/articles/s41534-018-0116-9},
  volume = {4},
  year = {2018}
}

@article{mcclean2018barren,
  abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its exponential dimension of Hilbert space and the gradient estimation complexity, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. This result is independent of the classical optimization strategy and can affect both gradient-free and gradient-based methods. We then discuss several technical conditions under which the barren plateau phenomenon would disappear, providing a path forward for quantum machine learning.},
  author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
  doi = {10.1038/s41467-018-07090-4},
  file = {:/home/b/documents/articles/mcclean2018barren.pdf:pdf},
  journal = {Nature Communications},
  number = {1},
  openalex = {W2794444783},
  pages = {4812},
  pdf = {https://arxiv.org/pdf/1803.11173.pdf},
  publisher = {Nature Publishing Group},
  title = {Barren plateaus in quantum neural network training landscapes},
  volume = {9},
  year = {2018}
}

@article{liu2018quantum,
  abstract = {Anomaly detection is used for identifying data that deviate from 'normal' data patterns. Its usage on classical data finds diverse applications in many important areas like fraud detection, medical diagnoses, data cleaning and surveillance. With the advent of quantum technologies, anomaly detection of quantum data, in the form of quantum states, may become an important component of quantum applications.},
  author = {Liu, Nana and Rebentrost, Patrick},
  doi = {10.1103/PhysRevA.97.042315},
  file = {:/home/b/documents/articles/liu2018quantum.pdf:pdf},
  journal = {Physical Review A},
  month = {4},
  number = {4},
  openalex = {W2765646145},
  pages = {042315},
  pdf = {https://arxiv.org/pdf/1710.07405.pdf},
  publisher = {American Physical Society},
  title = {Quantum machine learning for quantum anomaly detection},
  volume = {97},
  year = {2018}
}

@article{dunjko2018machine,
  abstract = {Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research---quantum information versus machine learning (ML) and artificial intelligence (AI)---have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our `big data' world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement---exploring what ML/AI can do for quantum physics and vice versa---researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.},
  author = {Dunjko, Vedran and Briegel, Hans J},
  doi = {10.1088/1361-6633/aab406},
  file = {:/home/b/documents/articles/dunjko2018machine.pdf:pdf},
  journal = {Reports on Progress in Physics},
  month = {7},
  number = {7},
  openalex = {W2792315573},
  pages = {074001},
  pdf = {https://arxiv.org/pdf/1709.02779.pdf},
  publisher = {IOP Publishing},
  title = {Machine learning & artificial intelligence in the quantum domain: A review of recent progress},
  volume = {81},
  year = {2018}
}

@article{bergholm2018pennylane,
  abstract = {PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational circuits in a way that is compatible with classical techniques such as backpropagation. It thus extends automatic differentiation algorithms for optimization in machine learning. A plugin system makes any gate-based simulator or hardware accessible. The authors provide plugins for hardware providers including Xanadu Cloud, Amazon Braket, IBM Quantum, allowing optimizations to be run on publicly accessible devices. On the interface front, PennyLane interfaces with accelerated machine learning libraries like TensorFlow, PyTorch, JAX, and Autograd. It can be used for eigensolvers, approximate optimization, models, and many other applications.},
  archiveprefix = {arXiv},
  author = {Bergholm, Ville and Izaac, Josh and Schuld, Maria and Gogolin, Christian and Ahmed, Shahnawaz and Ajith, Vishnu and Alam, M. Sohaib and Alonso-Linaje, Guillermo and AkashNarayanan, B. and Asadi, Ali and Arrazola, Juan Miguel and Azad, Utkarsh and Banning, Sam and Blank, Carsten and Bromley, Thomas R. and Cordier, Benjamin A. and Ceroni, Jack and Delgado, Alain and Di Matteo, Olivia and Dusko, Amintor and Garg, Tanya and Guala, Diego and Hayes, Anthony and Hill, Ryan and Ijaz, Aroosa and Isacsson, Theodor and Ittah, David and Jahangiri, Soran and Jain, Prateek and Jiang, Edward and Khandelwal, Ankit and Kottmann, Korbinian and Lang, Robert A. and Lee, Christina and Loke, Thomas and Lowe, Angus and McKiernan, Keri and Meyer, Johannes Jakob and Montanez-Vargas, Jose Antonio and Moyard, Romain and Niu, Zeyue and O'Riordan, Lee James and Otten, Steven and Panigrahi, Ashish and Park, Chae-Yeun and Polatajko, Daniel and Quesada, Nicolás and Roberts, Chase and Sá, Nahum and Schoch, Isidor and Shi, Borun and Shu, Shuli and Sim, Sukin and Singh, Arshpreet and Strandberg, Ingrid and Soni, Jay and Száva, Antal and Thebault, Slimane and Vargas-Hernández, Rodrigo A. and Vincent, Trevor and Vitucci, Nicola and Weber, Maurice and Wierichs, David and Wiersema, Roeland and Willmann, Moritz and Wong, Vincent and Zhang, Shaoming and Killoran, Nathan},
  eprint = {1811.04968},
  file = {:/home/b/documents/articles/bergholm2018pennylane.pdf:pdf},
  journal = {arXiv preprint arXiv:1811.04968},
  month = {11},
  openalex = {W4289606390},
  pdf = {https://arxiv.org/pdf/1811.04968.pdf},
  primaryclass = {quant-ph},
  title = {PennyLane: Automatic differentiation of hybrid quantum-classical computations},
  url = {https://arxiv.org/abs/1811.04968},
  year = {2018}
}

@inproceedings{brandao2017quantum,
  abstract = {We give a quantum algorithm for solving semidefinite programs (SDPs). It has worst-case running time $n^\frac12 m^\frac12 s^2  extpoly(łog(n), łog(m), R, r, 1/δ)$, with $n$ and $s$ the dimension and row-sparsity of the input matrices, respectively, $m$ the number of constraints, $δ$ the accuracy of the solution, and $R, r$ a upper bounds on the size of the optimal primal and dual solutions. This gives a square-root unconditional speed-up over any classical method for solving SDPs both in $n$ and $m$. We prove the algorithm cannot be substantially improved (in terms of $n$ and $m$) giving a $Ømega(n^\frac12+m^\frac12)$ quantum lower bound for solving semidefinite programs with constant $s, R, r$ and $δ$. The quantum algorithm is constructed by a combination of quantum Gibbs sampling and the multiplicative weight method. In particular it is based on a classical algorithm of Arora and Kale for approximately solving SDPs. We present a modification of their algorithm to eliminate the need for solving an inner linear program which may be of independent interest.},
  author = {Brandão, Fernando G. S. L. and Svore, Krysta M.},
  booktitle = {2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
  doi = {10.1109/FOCS.2017.45},
  file = {:/home/b/documents/inproceedings/brandao2017quantum.pdf:pdf},
  openalex = {W2768206303},
  organization = {IEEE},
  pages = {415--426},
  pdf = {https://arxiv.org/pdf/1609.05537.pdf},
  title = {Quantum Speed-ups for Semidefinite Programming},
  year = {2017}
}

@inproceedings{kerenidis2017quantum,
  abstract = {A recommendation system uses the past purchases or ratings of n products by a group of m users, in order to provide personalized recommendations to individual users. The information is modeled as an m × n preference matrix which is assumed to have a good rank-k approximation, for a small constant k. In this work, we present a quantum algorithm for recommendation systems that has running time O(poly(k)polylog(mn)). All known classical algorithms for recommendation systems that work through reconstructing an approximation of the preference matrix run in time polynomial in the matrix dimension. Our algorithm provides good recommendations by sampling efficiently from an approximation of the preference matrix, without reconstructing the entire matrix. For this, we design an efficient quantum procedure to project a given vector onto the row space of a given matrix. This is the first algorithm for recommendation systems that runs in time polylogarithmic in the dimensions of the matrix and provides an example of a quantum machine learning algorithm for a real world application.},
  address = {Dagstuhl, Germany},
  author = {Kerenidis, Iordanis and Prakash, Anupam},
  booktitle = {8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  doi = {10.4230/LIPIcs.ITCS.2017.49},
  file = {:/home/b/documents/inproceedings/kerenidis2017quantum.pdf:pdf},
  isbn = {978-3-95977-029-3},
  issn = {1868-8969},
  openalex = {W2964039664},
  organization = {Schloss Dagstuhl--Leibniz-Zentrum für Informatik},
  pages = {49:1--49:21},
  pdf = {https://drops.dagstuhl.de/opus/volltexte/2017/8154/pdf/LIPIcs-ITCS-2017-49.pdf},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum für Informatik},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  title = {Quantum recommendation systems},
  url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ITCS.2017.49},
  volume = {67},
  year = {2017}
}

@article{biamonte2017quantum,
  abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.},
  author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
  doi = {10.1038/nature23474},
  file = {:/home/b/documents/articles/biamonte2017quantum.pdf:pdf},
  journal = {Nature},
  month = {9},
  number = {7671},
  openalex = {W2559394418},
  pages = {195--202},
  pdf = {http://arxiv.org/pdf/1611.09347},
  publisher = {Nature Publishing Group},
  title = {Quantum machine learning},
  volume = {549},
  year = {2017}
}

@article{temme2017error,
  abstract = {Two schemes are presented that mitigate the effect of errors and decoherence in short depth quantum circuits. The size of the circuits for which these techniques can be applied is limited by the rate at which the errors in the computation are introduced. Near-term applications of early quantum devices, such as quantum simulations, rely on accurate estimates of expectation values to become relevant. Decoherence and gate errors lead to wrong estimates of the expectation values of observables used to evaluate the noisy circuit. The two schemes we discuss are deliberately simple and don't require additional qubit resources, so to be as practically relevant in current experiments as possible. The first method, extrapolation to the zero noise limit, subsequently cancels powers of the noise perturbations by an application of Richardson's deferred approach to the limit. The second method cancels errors by resampling randomized circuits according to a quasi-probability distribution.},
  author = {Temme, Kristan and Bravyi, Sergey and Gambetta, Jay M.},
  doi = {10.1103/PhysRevLett.119.180509},
  file = {:/home/b/documents/articles/temme2017error.pdf:pdf},
  journal = {Physical Review Letters},
  month = {11},
  number = {18},
  openalex = {W2562526363},
  pages = {180509},
  pdf = {https://arxiv.org/pdf/1612.02058},
  publisher = {American Physical Society},
  title = {Error Mitigation for Short-Depth Quantum Circuits},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.180509},
  volume = {119},
  year = {2017}
}

@article{childs2017quantum,
  abstract = {Harrow, Hassidim, and Lloyd showed that for a suitably specified $N  imes N$ matrix $A$ and $N$-dimensional vector $ěcb$, there is a quantum algorithm that outputs a quantum state proportional to the solution of the linear system of equations $Ǎ̧x=̌̌b$. If $A$ is sparse and well-conditioned, their algorithm runs in time $\mathrmpoly(łog N, 1/ε)$, where $ε$ is the desired precision in the output state. We improve this to an algorithm whose running time is polynomial in $łog(1/ε)$, exponentially improving the dependence on precision while keeping essentially the same dependence on other parameters. Our algorithm is based on a general technique for implementing any operator with a suitable Fourier or Chebyshev series representation. This allows us to bypass the quantum phase estimation algorithm, whose dependence on $ε$ is prohibitive.},
  author = {Childs, Andrew M. and Kothari, Robin and Somma, Rolando D.},
  doi = {10.1137/16M1087072},
  file = {:/home/b/documents/articles/childs2017quantum.pdf:pdf},
  journal = {SIAM Journal on Computing},
  number = {6},
  openalex = {W2761673598},
  pages = {1920--1950},
  pdf = {https://arxiv.org/pdf/1511.02306.pdf},
  publisher = {SIAM},
  title = {Quantum Algorithm for Systems of Linear Equations with Exponentially Improved Dependence on Precision},
  volume = {46},
  year = {2017}
}

@article{kandala2017hardware,
  abstract = {Quantum computers can be used to address electronic-structure problems and problems in materials science and condensed matter physics that can be formulated as interacting fermionic problems, problems which stretch the limits of existing high-performance computers. Finding exact solutions to such problems numerically has a computational cost that scales exponentially with the size of the system, and Monte Carlo methods are unsuitable owing to the fermionic sign problem. Here we present experimental optimization of Hamiltonian problems with up to six qubits and more than one hundred Pauli terms, determining the ground-state energy for molecules of increasing size, up to BeH$_2$. We performed quantum chemical calculations of LiH and BeH$_2$ and an energy minimization procedure on a four-qubit Heisenberg model. Our application of the variational quantum eigensolver is hardware-efficient, which means that it is optimized on the given architecture.},
  author = {Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
  doi = {10.1038/nature23879},
  file = {:/home/b/documents/articles/kandala2017hardware.pdf:pdf},
  journal = {Nature},
  month = {9},
  number = {7671},
  openalex = {W2755255888},
  pages = {242--246},
  pdf = {https://arxiv.org/pdf/1704.05018.pdf},
  publisher = {Nature Publishing Group},
  title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
  volume = {549},
  year = {2017}
}

@article{kerenidis2014quantum,
  abstract = {Deep learning has had a profound impact on machine learning and artificial intelligence. At the same time, algorithms for quantum computers have been shown to efficiently solve some problems that are intractable on conventional, classical computers. We show that quantum computing not only reduces the time required to train a deep restricted Boltzmann machine, but also provides a richer and more comprehensive framework for deep learning than classical computing and leads to significant improvements in the optimization of the underlying objective function.},
  author = {Nathan Wiebe and Ashish Kapoor and Krysta M. Svore},
  doi = {10.26421/QIC16.7-8-1},
  journal = {Quantum Information and Computation},
  number = {7--8},
  openalex = {W2951576479},
  pages = {541--587},
  title = {Quantum deep learning},
  volume = {16},
  year = {2016}
}

@article{schuld2015introduction,
  abstract = {Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.},
  author = {Schuld, Maria and Sinayskiy, Ilya and Petruccione, Francesco},
  doi = {10.1080/00107514.2014.964942},
  file = {:/home/b/documents/articles/schuld2015introduction.pdf:pdf},
  journal = {Contemporary Physics},
  number = {2},
  openalex = {W3104599990},
  pages = {172--185},
  pdf = {https://arxiv.org/pdf/1409.3097.pdf},
  publisher = {Taylor & Francis},
  title = {An introduction to quantum machine learning},
  volume = {56},
  year = {2015}
}

@article{lloyd2014quantum,
  abstract = {The usual way to reveal properties of an unknown quantum state is to perform measurements and analyze results statistically. Here we show that the unknown quantum state can play an active role in its own analysis by creating quantum coherence to perform principal component analysis exponentially faster than existing algorithms. Given multiple copies of a quantum system with density matrix ρ, it is possible to perform the unitary transformation e^-iρt. As a result, one can create quantum coherence among different copies of the system to perform quantum principal component analysis, revealing the eigenvectors corresponding to the large eigenvalues of the unknown state in time exponentially faster than any existing algorithm.},
  author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
  doi = {10.1038/nphys3029},
  journal = {Nature Physics},
  number = {9},
  openalex = {W1988369744},
  pages = {631--633},
  pdf = {https://www.nature.com/articles/nphys3029.pdf},
  publisher = {Nature Publishing Group},
  title = {Quantum principal component analysis},
  volume = {10},
  year = {2014}
}

@article{rebentrost2014quantum,
  abstract = {Supervised machine learning is the classification of new data based on already classified training examples. In this work, we show that the support vector machine, an optimized binary classifier, can be implemented on a quantum computer, with complexity logarithmic in the size of the vectors and the number of training examples. In cases when classical sampling algorithms require polynomial time, an exponential speed-up is obtained. At the core of this quantum big data algorithm is a non-sparse matrix exponentiation technique for efficiently performing a matrix inversion of the training data inner-product (kernel) matrix.},
  author = {Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth},
  doi = {10.1103/PhysRevLett.113.130503},
  file = {:/home/b/documents/articles/rebentrost2014quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {9},
  number = {13},
  openalex = {W2103956991},
  pages = {130503},
  pdf = {http://arxiv.org/pdf/1307.0471},
  publisher = {American Physical Society},
  title = {Quantum support vector machine for big data classification},
  url = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.113.130503},
  volume = {113},
  year = {2014}
}

@article{peruzzo2014variational,
  abstract = {Quantum computers promise to efficiently solve important problems that are intractable on a conventional computer. For quantum systems, where the physical dimension grows exponentially, finding the eigenvalues of certain operators is one such intractable problem and remains a fundamental challenge. The quantum phase estimation algorithm efficiently finds the eigenvalue of a given eigenvector but requires fully coherent evolution. Here we present an alternative approach that greatly reduces the requirements for coherent evolution and combine this method with a new approach to state preparation based on ansätze and classical optimization. We implement the algorithm by combining a highly reconfigurable photonic quantum processor with a conventional computer. We experimentally demonstrate the feasibility of this approach with an example from quantum chemistry—calculating the ground-state molecular energy for He–H+. The proposed approach drastically reduces the coherence time requirements, enhancing the potential of quantum resources available today and in the near future.},
  author = {Peruzzo, Alberto and McClean, Jarrod and Shadbolt, Peter and Yung, Man-Hong and Zhou, Xiao-Qi and Love, Peter J and Aspuru-Guzik, Alán and O'Brien, Jeremy L},
  doi = {10.1038/ncomms5213},
  file = {:/home/b/documents/articles/peruzzo2014variational.pdf:pdf},
  journal = {Nature Communications},
  openalex = {W2161685427},
  pages = {4213},
  pdf = {https://arxiv.org/pdf/1304.3061},
  publisher = {Nature Publishing Group},
  title = {A variational eigenvalue solver on a photonic quantum processor},
  volume = {5},
  year = {2014}
}

@article{farhi2014quantum,
  abstract = {We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. The algorithm is applied to MaxCut on regular graphs and analyzed for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut.},
  archiveprefix = {arXiv},
  author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
  doi = {10.48550/arXiv.1411.4028},
  eprint = {1411.4028},
  file = {:/home/b/documents/articles/farhi2014quantum.pdf:pdf},
  journal = {arXiv preprint arXiv:1411.4028},
  month = {11},
  note = {MIT-CTP/4610},
  pdf = {https://arxiv.org/pdf/1411.4028.pdf},
  primaryclass = {quant-ph},
  title = {A quantum approximate optimization algorithm},
  year = {2014}
}

@article{lloyd2013quantum,
  abstract = {Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.},
  archiveprefix = {arXiv},
  author = {Lloyd, Seth and Mohseni, Masoud and Rebentrost, Patrick},
  eprint = {1307.0411},
  file = {:/home/b/documents/articles/lloyd2013quantum.pdf:pdf},
  journal = {arXiv preprint arXiv:1307.0411},
  month = {7},
  openalex = {W199424061},
  pdf = {https://arxiv.org/pdf/1307.0411.pdf},
  primaryclass = {quant-ph},
  title = {Quantum algorithms for supervised and unsupervised machine learning},
  url = {https://arxiv.org/abs/1307.0411},
  year = {2013}
}

@article{wiebe2012quantum,
  abstract = {We provide a new quantum algorithm that efficiently determines the quality of a least-squares fit over an exponentially large data set by building upon an algorithm for solving systems of linear equations efficiently (Harrow et al., Phys. Rev. Lett. \bf 103, 150502 (2009)). In many cases, our algorithm can also efficiently find a concise function that approximates the data to be fitted and bound the approximation error. In cases where the input data is a pure quantum state, the algorithm can be used to provide an efficient parametric estimation of the quantum state and therefore can be applied as an alternative to full quantum state tomography given a fault tolerant quantum computer.},
  author = {Wiebe, Nathan and Braun, Daniel and Lloyd, Seth},
  doi = {10.1103/PhysRevLett.109.050505},
  file = {:/home/b/documents/articles/wiebe2012quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {8},
  number = {5},
  openalex = {W1981783889},
  pages = {050505},
  pdf = {https://arxiv.org/pdf/1204.5242.pdf},
  publisher = {American Physical Society},
  title = {Quantum algorithm for data fitting},
  url = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.109.050505},
  volume = {109},
  year = {2012}
}

@article{harrow2009quantum,
  abstract = {Solving linear systems of equations is a common problem that arises both on its own and as a subroutine in more complex problems: given a matrix A and a vector b, find a vector x such that Ax=b. We consider the case where one doesn't need to know the solution x itself, but rather an approximation of the expectation value of some operator associated with x, e.g., x'Mx for some matrix M. In this case, when A is sparse, N by N and has condition number $ąppa$, classical algorithms can find x and estimate x'Mx in O(N $\sqrt\p̨pa$) time. Here, we exhibit a quantum algorithm for this task that runs in poly(log N, $p̨̨a$) time, an exponential improvement over the best classical algorithm.},
  author = {Harrow, Aram W. and Hassidim, Avinatan and Lloyd, Seth},
  doi = {10.1103/PhysRevLett.103.150502},
  file = {:/home/b/documents/articles/harrow2009quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {10},
  number = {15},
  openalex = {W1492999010},
  pages = {150502},
  pdf = {https://arxiv.org/pdf/0811.3171.pdf},
  publisher = {American Physical Society},
  title = {Quantum algorithm for linear systems of equations},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.103.150502},
  volume = {103},
  year = {2009}
}

@article{giovannetti2008quantum,
  abstract = {A random access memory (RAM) uses n bits to randomly address N=2^n distinct cells. We present an architecture that exponentially reduces the requirements for a call: O(log N) switches need be thrown instead of used in conventional RAM designs.},
  author = {Giovannetti, Vittorio and Lloyd, Seth and Maccone, Lorenzo},
  doi = {10.1103/PhysRevLett.100.160501},
  file = {:/home/b/documents/articles/giovannetti2008quantum.pdf:pdf},
  journal = {Physical Review Letters},
  month = {4},
  number = {16},
  openalex = {W2051446825},
  pages = {160501},
  pdf = {https://arxiv.org/pdf/0708.1879},
  publisher = {American Physical Society},
  title = {Quantum random access memory},
  volume = {100},
  year = {2008}
}
