@inproceedings{pham2025memorization,
  abstract = {Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In this paper, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. We conceptualize the training phase of diffusion models as memory encoding (training data is stored in the memory), while the generation phase is viewed as an attempt of memory retrieval. This recall can be successful, resulting in the retrieval of training samples (memorization), or unsuccessful, resulting in the generation of new previously unseen samples (generalization). Thus, according to this theory, generalization (creation of genuinely novel samples) is a failure of memory recall. In the small data regime, the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, and empirical validation of this prediction in commonly-used diffusion models.},
  address = {Singapore},
  arxiv = {2505.21777},
  author = {Bao Pham and Gabriel Raya and Matteo Negri and Mohammed J. Zaki and Luca Ambrogioni and Dmitry Krotov},
  booktitle = {New Frontiers in Associative Memories},
  month = {4},
  note = {Oral presentation at NFAM 2025 workshop},
  organization = {ICLR},
  pdf = {https://arxiv.org/pdf/2505.21777.pdf},
  title = {Memorization to Generalization: Emergence of Diffusion Models from Associative Memory Networks},
  url = {https://openreview.net/forum?id=IWZnhP3YgK},
  venue = {ICLR 2025 Workshop},
  year = {2025}
}

@inproceedings{hoover2025dense,
  abstract = {We propose a novel energy function for Dense Associative Memory (DenseAM) networks, the log-sum-ReLU (LSR), inspired by optimal kernel density estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based on the Epanechnikov kernel and enables exact memory retrieval with exponential capacity without requiring exponential separation functions. Uniquely, it introduces abundant additional emergent local minima while preserving perfect pattern recovery--a characteristic previously unseen in DenseAM literature. Empirical results show LSR generates significantly more local minima and produces samples with higher log-likelihood than LSE-based models, making it promising for both memory storage and generative tasks.},
  author = {Benjamin Hoover and Krishna Balasubramanian and Dmitry Krotov and Parikshit Ram},
  booktitle = {Proceedings of the ICLR 2025 Workshop on New Frontiers in Associative Memories},
  keywords = {Associative Memory, Dense Associative Memory, Kernel Density Estimation, Epanechnikov kernel, energy-based models},
  month = {3},
  pages = {1--5},
  pdf = {https://openreview.net/pdf?id=LOAkHpRSlZ},
  publisher = {OpenReview.net},
  title = {Dense Associative Memory with Epanechnikov Energy},
  url = {https://openreview.net/forum?id=LOAkHpRSlZ},
  year = {2025}
}

@misc{fanaskov2025binding,
  abstract = {Artificial Kuramoto oscillatory neurons were recently introduced as an alternative to threshold units. Empirical evidence suggests that oscillatory units outperform threshold units in several tasks including unsupervised object discovery and certain reasoning problems. The proposed coupling mechanism for these oscillatory neurons is heterogeneous, combining a generalized Kuramoto equation with standard coupling methods used for threshold units. In this research note, we present a theoretical framework that clearly distinguishes oscillatory neurons from threshold units and establishes a coupling mechanism between them. We argue that, from a biological standpoint, oscillatory and threshold units realise distinct aspects of neural coding: roughly, threshold units model intensity of neuron firing, while oscillatory units facilitate information exchange by frequency modulation. To derive interaction between these two types of units, we constrain their dynamics by focusing on dynamical systems that admit Lyapunov functions. For threshold units, this leads to Hopfield associative memory model, and for oscillatory units it yields a specific form of generalized Kuramoto model. The resulting dynamical systems can be naturally coupled to form a Hopfield-Kuramoto associative memory model, which also admits a Lyapunov function. Various forms of coupling are possible. Notably, oscillatory neurons can be employed to implement a low-rank correction to the weight matrix of a Hopfield network. This correction can be viewed either as a form of Hebbian learning or as a popular LoRA method used for fine-tuning of large language models. We demonstrate the practical realization of this particular coupling through illustrative toy experiments.},
  archiveprefix = {arXiv},
  author = {Fanaskov, Vladimir and Oseledets, Ivan},
  eprint = {2505.03648},
  keywords = {artificial neural networks, oscillatory neurons, Hopfield networks, Kuramoto model, associative memory, threshold units, neural coding},
  month = {5},
  note = {Artificial Intelligence Research Institute (AIRI) and Skolkovo Institute of Science and Technology (Skoltech)},
  pdf = {https://arxiv.org/pdf/2505.03648.pdf},
  primaryclass = {q-bio.NC},
  title = {Binding threshold units with artificial oscillatory neurons},
  url = {https://arxiv.org/abs/2505.03648},
  year = {2025}
}

@article{betteti2025inputdriven,
  abstract = {The Hopfield model provides a mathematical framework for understanding the mechanisms of memory storage and retrieval in the human brain. This model has inspired decades of research on learning and retrieval dynamics, capacity estimates, and sequential transitions among memories. Notably, the role of external inputs has been largely underexplored, from their effects on neural dynamics to how they facilitate effective memory retrieval. To bridge this gap, we propose a dynamical system framework in which the external input directly influences the neural synapses and shapes the energy landscape of the Hopfield model. This plasticity-based mechanism provides a clear energetic interpretation of the memory retrieval process and proves effective at correctly classifying mixed inputs. Furthermore, we integrate this model within the framework of modern Hopfield architectures to elucidate how current and past information are combined during the retrieval process. Last, we embed both the classic and the proposed model in an environment disrupted by noise and compare their robustness during memory retrieval. External inputs reshape the Hopfield energy landscape and drive memory retrieval, even under noise and ambiguity.},
  author = {Simone Betteti and Giacomo Baggio and Francesco Bullo and Sandro Zampieri},
  doi = {10.1126/sciadv.adu6991},
  journal = {Science Advances},
  month = {4},
  number = {17},
  openalex = {W4409723675},
  pages = {eadu6991},
  pdf = {https://www.science.org/doi/pdf/10.1126/sciadv.adu6991},
  pmcid = {PMC12017325},
  pmid = {40267196},
  title = {Input-driven dynamics for robust memory retrieval in Hopfield networks},
  url = {https://www.science.org/doi/abs/10.1126/sciadv.adu6991},
  volume = {11},
  year = {2025}
}

@inproceedings{achilli2025capacity,
  abstract = {We generalize the computation of the capacity of exponential Hopfield model from Lucibello and Mézard (2024) to more generic pattern ensembles, including binary patterns and patterns generated from a hidden manifold model. Even if the typical distance between patterns does not depend on the hidden dimension, we find that it affects the retrieval threshold, which decreases with a smaller latent dimension. We stress the importance of studying Modern Hopfield Networks because of their formal equivalence with empirical-score-driven Diffusion Models, which are state-of-the-art generative models.},
  address = {Singapore},
  archiveprefix = {arXiv},
  author = {Achilli, Beatrice and Ambrogioni, Luca and Lucibello, Carlo and Mézard, Marc and Ventura, Enrico},
  booktitle = {Workshop on New Frontiers in Associative Memories at the International Conference on Learning Representations},
  doi = {10.48550/arXiv.2503.09518},
  eprint = {2503.09518},
  keywords = {Modern Hopfield Networks, manifold, generalization, associative memory, diffusion models},
  month = {4},
  note = {Workshop paper accepted at NFAM 2025, a workshop at ICLR 2025},
  pdf = {https://arxiv.org/pdf/2503.09518.pdf},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  series = {ICLR 2025 Workshop Proceedings},
  title = {The Capacity of Modern Hopfield Networks under the Data Manifold Hypothesis},
  url = {https://openreview.net/forum?id=OBQwZaO4pt},
  year = {2025}
}

@article{aguilera2025explosive,
  abstract = {Higher-order interactions underlie complex phenomena in systems such as biological and artificial neural networks, but their study is challenging due to the scarcity of tractable models. By leveraging a generalisation of the maximum entropy principle, here we introduce curved neural networks as a class of prototypical models with a limited number of parameters that are particularly well-suited for studying higher-order phenomena. Through exact mean-field descriptions, we show that these curved neural networks implement a self-regulating annealing process that can accelerate memory retrieval, leading to explosive order-disorder phase transitions with multi-stability and hysteresis effects. Moreover, by analytically exploring their memory-retrieval capacity using the replica trick near ferromagnetic and spin-glass phase boundaries, we demonstrate that these networks can enhance memory capacity and robustness of retrieval over classical associative-memory networks. Overall, the proposed framework provides parsimonious models amenable to analytical study, revealing novel higher-order phenomena in complex networks.},
  archiveprefix = {arXiv},
  author = {Aguilera, Miguel and Morales, Pablo A. and Rosas, Fernando E. and Shimazaki, Hideaki},
  doi = {10.1038/s41467-025-61475-w},
  eprint = {2408.02326},
  journal = {Nature Communications},
  keywords = {Neural Networks, Higher-order Interactions, Statistical Physics, Computational Neuroscience},
  month = {7},
  openalex = {W4412640184},
  pages = {6511},
  pdf = {https://arxiv.org/pdf/2408.02326},
  primaryclass = {cond-mat.dis-nn},
  publisher = {Springer Nature},
  title = {Explosive Neural Networks via Higher-Order Interactions in Curved Statistical Manifolds},
  url = {https://doi.org/10.1038/s41467-025-61475-w},
  volume = {16},
  year = {2025}
}

@article{mcalister2025sequential,
  abstract = {Sequential learning involves learning tasks in a sequence and proves challenging for most neural networks. Biological neural networks regularly succeed at the sequential learning challenge and are even capable of transferring knowledge both forward and backward between tasks. Artificial neural networks often totally fail to transfer performance between tasks and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is the most studied model. The dense associative memory (DAM), or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors while still retaining the associative memory structure. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories. We present the first published benchmarks of sequential learning in the DAM using various sequential learning techniques and analyze the results of the sequential learning to demonstrate previously unseen transitions in the behavior of the DAM. This letter also discusses the departure from biological plausibility that may affect the utility of the DAM as a tool for studying biological neural networks. We present our findings, including the effectiveness of a range of state-of-the-art sequential learning methods when applied to the DAM, and use these methods to further the understanding of DAM properties and behaviors.},
  address = {Cambridge, MA, USA},
  author = {Hayden McAlister and Anthony Robins and Lech Szymanski},
  doi = {10.1162/neco.a.20},
  issn = {0899-7667},
  journal = {Neural Computation},
  month = {7},
  number = {7},
  openalex = {W4412630092},
  pages = {1--48},
  pdf = {https://direct.mit.edu/neco/article-pdf/doi/10.1162/neco.a.20/2538527/neco.a.20.pdf},
  publisher = {MIT Press},
  title = {Sequential Learning in the Dense Associative Memory},
  url = {https://doi.org/10.1162/neco.a.20},
  volume = {37},
  year = {2025}
}

@inproceedings{sato2025rectified,
  abstract = {Modern Hopfield networks (MHNs) have recently gained significant attention in the field of artificial intelligence because they can store and retrieve a large set of patterns with an exponentially large memory capacity. A MHN is generally a dynamical system defined with Lagrangians of memory and feature neurons, where memories associated with in-distribution (ID) samples are represented by attractors in the feature space. However, one major problem in existing MHNs lies in managing out-of-distribution (OOD) samples because it was originally assumed that all samples are ID samples. This paper proposes the rectified Lagrangian (RecLag), a new Lagrangian for memory neurons that explicitly incorporates an attractor for OOD samples in the dynamical system of MHNs. RecLag creates a trivial point attractor for any interaction matrix, enabling OOD detection by identifying samples that fall into this attractor as OOD. The interaction matrix is optimized so that the probability densities can be estimated to identify ID/OOD. We prove that samples with low probability fall into the special attractor created by RecLag. We demonstrate the effectiveness of RecLag-based MHNs compared to energy-based OOD detection methods, including those using state-of-the-art Hopfield energies, across nine image datasets.},
  archiveprefix = {arXiv},
  author = {Moriai, Ryo and Inoue, Nakamasa and Tanaka, Masayuki and Kawakami, Rei and Ikehata, Satoshi and Sato, Ikuro},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v39i18.34153},
  eprint = {2502.14003},
  keywords = {Machine Learning, Out-of-Distribution Detection, Modern Hopfield Networks},
  month = {2},
  note = {Proposes a novel MHN architecture with a dedicated attractor for out-of-distribution samples, providing a native mechanism for robust novelty detection.},
  number = {18},
  openalex = {W4409363813},
  pages = {19554--19562},
  pdf = {https://arxiv.org/pdf/2502.14003},
  primaryclass = {cs.LG},
  publisher = {AAAI Press},
  title = {Rectified Lagrangian for Out-of-Distribution Detection in Modern Hopfield Networks},
  url = {https://doi.org/10.1609/aaai.v39i18.34153},
  volume = {39},
  year = {2025}
}

@misc{santos2025modern,
  abstract = {Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.},
  archiveprefix = {arXiv},
  author = {Santos, Saul and Farinhas, António and McNamee, Daniel C. and Martins, André F. T.},
  eprint = {2502.10122},
  journal = {arXiv preprint arXiv:2502.10122},
  month = {February},
  note = {Addresses storage efficiency by proposing a method to compress large discrete memories into a smaller, continuous-time memory representation inspired by cognitive science},
  openalex = {W4407632579},
  pdf = {https://arxiv.org/pdf/2502.10122},
  primaryclass = {cs.LG},
  title = {Modern Hopfield Networks with Continuous-Time Memories},
  url = {https://arxiv.org/abs/2502.10122},
  year = {2025}
}

@inproceedings{smart2025incontext,
  abstract = {This paper introduces in-context denoising, demonstrating how a single-layer transformer can solve denoising problems by performing a single gradient descent update on a context-aware dense associative memory (DAM) energy landscape. Using a Bayesian framework, the authors show that context tokens serve as associative memories while the query token acts as an initial state, revealing deep connections between attention mechanisms and memory retrieval. The work provides theoretical and empirical evidence that certain restricted denoising problems can be solved optimally even by a single-layer transformer, with the trained attention layer processing each denoising prompt by performing one-step gradient descent updates that yield better solutions than exact retrieval.},
  address = {Vancouver, BC, Canada},
  archiveprefix = {arXiv},
  author = {Smart, Matthew and Bietti, Alberto and Sengupta, Anirvan M.},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  code = {https://github.com/mattsmart/in-context-denoising},
  eprint = {2502.05164},
  keywords = {machine learning, transformers, in-context learning, associative memory, denoising},
  month = {7},
  note = {Spotlight paper with oral presentation (Oral 6B Deep Learning Architectures) at ICML 2025. Shows that trained attention layers perform single gradient descent updates on context-aware dense associative memory energy landscapes for in-context denoising tasks.},
  openalex = {W4407310656},
  pages = {TBD},
  pdf = {https://arxiv.org/pdf/2502.05164.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval},
  url = {https://icml.cc/virtual/2025/poster/45913},
  volume = {TBD},
  year = {2025}
}

@misc{tamamori2025kernel,
  abstract = {This paper addresses the limitation of Hopfield networks using Hebbian learning, which suffer from limited storage capacity. While supervised methods like Linear Logistic Regression (LLR) offer some improvement, kernel methods like Kernel Logistic Regression (KLR) significantly enhance capacity and noise robustness. The authors propose Kernel Ridge Regression (KRR) as an efficient kernel-based alternative for learning high-capacity Hopfield networks. KRR utilizes the kernel trick and predicts bipolar states via regression, crucially offering a non-iterative, closed-form solution for learning dual variables. Results demonstrate that KRR achieves state-of-the-art storage capacity (reaching $β=1.5$) and noise robustness, comparable to KLR, while drastically reducing training time by orders of magnitude compared to iterative methods.},
  archiveprefix = {arXiv},
  author = {Akira Tamamori},
  eprint = {2504.12561},
  keywords = {Hopfield Networks, Kernel Ridge Regression, Machine Learning, Neural Networks, Associative Memory},
  month = {4},
  note = {Submitted to APSIPA ASC 2025. Proposes Kernel Ridge Regression (KRR) as a highly efficient, non-iterative method for learning the weights of high-capacity Hopfield networks, achieving state-of-the-art performance with significantly reduced training time},
  pdf = {https://arxiv.org/pdf/2504.12561.pdf},
  primaryclass = {cs.LG},
  title = {Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks},
  url = {https://arxiv.org/abs/2504.12561},
  year = {2025}
}

@article{betteti2025inputdriven,
  abstract = {The Hopfield model provides a mathematical framework for understanding the mechanisms of memory storage and retrieval in the human brain. This model has inspired decades of research on learning and retrieval dynamics, capacity estimates, and sequential transitions among memories. Notably, the role of external inputs has been largely underexplored, from their effects on neural dynamics to how they facilitate effective memory retrieval. To bridge this gap, we propose a dynamical system framework in which the external input directly influences the neural synapses and shapes the energy landscape of the Hopfield model. This plasticity-based mechanism provides a clear energetic interpretation of the memory retrieval process and proves effective at correctly classifying mixed inputs. Furthermore, we integrate this model within the framework of modern Hopfield architectures to elucidate how current and past information are combined during the retrieval process. Last, we embed both the classic and the proposed model in an environment disrupted by noise and compare their robustness during memory retrieval.},
  archiveprefix = {arXiv},
  author = {Betteti, Simone and Baggio, Giacomo and Bullo, Francesco and Zampieri, Sandro},
  doi = {10.1126/sciadv.adu6991},
  eprint = {2411.05849},
  journal = {Science Advances},
  keywords = {Hopfield Networks, Neural Dynamics, Memory Retrieval, Computational Neuroscience},
  month = {4},
  number = {17},
  openalex = {W4409723675},
  pages = {eadu6991},
  pdf = {https://www.science.org/doi/epdf/10.1126/sciadv.adu6991},
  pmcid = {PMC12017325},
  pmid = {40267196},
  primaryclass = {cs.NE},
  title = {Input-Driven Dynamics for Robust Memory Retrieval in Hopfield Networks},
  url = {https://www.science.org/doi/10.1126/sciadv.adu6991},
  volume = {11},
  year = {2025}
}

@misc{farooq2025framework,
  abstract = {In this work we propose an energy functional along the lines of Modern Hopfield Networks (MHN), the stationary points of which correspond to the attention due to Vaswani et al. [12], thus unifying both frameworks. The minima of this landscape form 'context wells' - stable configurations that encapsulate the contextual relationships among tokens. A compelling picture emerges: across $n$ token embeddings an energy landscape is defined whose gradient corresponds to the attention computation. Non-linear attention mechanisms offer a means to enhance the capabilities of transformer models for various sequence modeling tasks by improving the model's understanding of complex relationships, learning of representations, and overall efficiency and performance. A rough analogy can be seen via cubic splines which offer a richer representation of non-linear data where a simpler linear model may be inadequate. This approach can be used for the introduction of non-linear heads in transformer based models such as BERT, [6], etc.},
  archiveprefix = {arXiv},
  author = {Ahmed Farooq},
  eprint = {2506.11043},
  month = {5},
  note = {15 pages. Cross-listed in cs.LG and cs.NE. Proposes a framework for moving beyond the linear retrieval of standard attention, introducing a path toward richer, non-linear attention mechanisms based on the MHN energy landscape.},
  pdf = {https://arxiv.org/pdf/2506.11043.pdf},
  primaryclass = {stat.ML},
  title = {A Framework for Non-Linear Attention via Modern Hopfield Networks},
  url = {https://arxiv.org/abs/2506.11043},
  year = {2025}
}

@misc{yampolskaya2024models,
  abstract = {Hopfield models, originally developed to study memory retrieval in neural networks, have become versatile tools for modeling diverse biological systems in which function emerges from collective dynamics. This paper provides a pedagogical introduction to both classical and modern Hopfield networks from a biophysical perspective, offering three complementary interpretations of Hopfield dynamics: as noise discrimination, as a geometric construction defining a natural coordinate system in pattern space, and as gradient-like descent on an energy landscape. The review demonstrates how Hopfield models serve as a compelling example of how a well-posed mathematical model can connect the properties of microscopic interactions to macroscopic behaviors, making them an essential conceptual tool for understanding emergent function in biophysical systems.},
  archiveprefix = {arXiv},
  author = {Maria Yampolskaya and Pankaj Mehta},
  doi = {10.48550/arXiv.2506.13076},
  eprint = {2506.13076},
  institution = {Boston University},
  keywords = {Hopfield networks, computational biology, biophysics, neural networks, emergent function, theoretical physics, statistical mechanics},
  month = {6},
  note = {Provides a comprehensive review of Hopfield networks from a biophysical perspective, framing their dynamics as models for disparate biological phenomena like cell fate transitions and molecular self-assembly},
  pdf = {https://arxiv.org/pdf/2506.13076},
  primaryclass = {physics.bio-ph},
  title = {Hopfield Networks as Models of Emergent Function in Biology},
  url = {https://arxiv.org/abs/2506.13076},
  year = {2025}
}

@article{kozachkov2024neuronastrocyte,
  abstract = {Astrocytes, the most abundant type of glial cell, play a fundamental role in memory. Despite most hippocampal synapses being contacted by an astrocyte, there are no current theories that explain how neurons, synapses, and astrocytes might collectively contribute to memory function. We demonstrate that fundamental aspects of astrocyte morphology and physiology naturally lead to a dynamic, high-capacity associative memory system. The neuron--astrocyte networks generated by our framework are closely related to popular machine learning architectures known as Dense Associative Memories. Adjusting the connectivity pattern, the model developed here leads to a family of associative memory networks that includes a Dense Associative Memory and a Transformer as two limiting cases. In the known biological implementations of Dense Associative Memories, the ratio of stored memories to the number of neurons remains constant, despite the growth of the network size. Our work demonstrates that neuron--astrocyte networks follow a superior memory scaling law, outperforming known biological implementations of Dense Associative Memory. Our model suggests an exciting and previously unnoticed possibility that memories could be stored, at least in part, within the network of astrocyte processes rather than solely in the synaptic weights between neurons.},
  author = {Kozachkov, Leo and Krotov, Dmitry and Slotine, Jean-Jacques},
  doi = {10.1073/pnas.2417788122},
  journal = {Proceedings of the National Academy of Sciences},
  month = {5},
  note = {Proposes a theory of neuron-astrocyte networks for memory processing within the Dense Associative Memory framework, suggesting astrocytes can serve as computational units that enhance memory capacity.},
  number = {21},
  openalex = {W4410638676},
  pages = {e2417788122},
  pdf = {https://www.pnas.org/doi/epdf/10.1073/pnas.2417788122},
  title = {Neuron--astrocyte associative memory},
  url = {https://www.pnas.org/doi/10.1073/pnas.2417788122},
  volume = {122},
  year = {2025}
}

@inproceedings{ma2024associative,
  abstract = {This study addresses the critical challenge of predicting the Q-distribution in long-term stable nuclear fusion task, a key component for advancing clean energy solutions. We introduce an innovative deep learning framework that employs Modern Hopfield Networks to incorporate associative memory from historical shots. Utilizing a newly compiled dataset, we demonstrate the effectiveness of our approach in enhancing Q-distribution prediction. The proposed method represents a significant advancement by leveraging historical memory information for the first time in this context, showcasing improved prediction accuracy and contributing to the optimization of nuclear fusion research.},
  archiveprefix = {arXiv},
  author = {Ma, Qingchuan and Wang, Shiao and Zheng, Tong and Dai, Xiaodong and Wang, Yifeng and Yang, Qingquan and Wang, Xiao},
  booktitle = {Advances in Brain Inspired Cognitive Systems},
  doi = {10.1007/978-981-96-2882-7_11},
  eprint = {2410.08889},
  isbn = {978-981-96-2882-7},
  keywords = {Modern Hopfield Networks, Nuclear Fusion, Q-distribution Prediction, Associative Memory},
  openalex = {W4408251459},
  pages = {104--114},
  pdf = {https://arxiv.org/pdf/2410.08889.pdf},
  primaryclass = {cs.CV},
  publisher = {Springer, Singapore},
  series = {Lecture Notes in Computer Science},
  title = {Exploiting Memory-aware Q-distribution Prediction for Nuclear Fusion via Modern Hopfield Network},
  url = {https://link.springer.com/chapter/10.1007/978-981-96-2882-7_11},
  volume = {15497},
  year = {2025}
}

@misc{zhong2025understanding,
  abstract = {This paper explores Transformer architectures through the lens of associative memory, focusing on two key dimensions: Memory Capacity, examining how much a Transformer can remember, and Memory Update, understanding how Transformer memories learn and evolve. The authors aim to demystify Transformer architecture, offering a clearer understanding of existing designs and sparking innovation in the field. The work is presented as a blog-style sharing of current reflections rather than a formal research paper.},
  archiveprefix = {arXiv},
  author = {Shu Zhong and Mingyu Xu and Tenglong Ao and Guang Shi},
  eprint = {2505.19488},
  keywords = {transformers, associative memory, attention mechanism, feed-forward networks, memory capacity, cognitive psychology},
  month = {5},
  note = {Provides a unifying framework that interprets both the self-attention mechanism and the feed-forward networks within Transformers as distinct forms of associative memory, explaining their architectural differences and properties},
  pdf = {https://arxiv.org/pdf/2505.19488.pdf},
  primaryclass = {cs.LG},
  title = {Understanding Transformer from the Perspective of Associative Memory},
  url = {https://arxiv.org/abs/2505.19488},
  year = {2025}
}

@article{schimunek2023mhnfs,
  abstract = {Today's drug discovery increasingly relies on computational and machine learning approaches to identify novel candidates, yet data scarcity remains a significant challenge. To address this limitation, we present MHNfs, an application specifically designed to predict molecular activity in low-data scenarios. At its core, MHNfs leverages a state-of-the-art few-shot activity prediction model, named MHNfs, which has demonstrated strong performance across a large set of prediction tasks in the benchmark data set FS-Mol. The application features an intuitive interface that enables users to prompt the model for precise activity predictions based on a small number of known active and inactive molecules, akin to interactive interfaces for large language models. To evaluate its efficacy, we simulate real-world scenarios by recasting PubChem bioassays as few-shot prediction tasks. MHNfs offers a streamlined and accessible solution for deploying advanced few-shot learning models, providing a valuable tool for accelerating drug discovery.},
  author = {Johannes Schimunek and Sohvi Luukkonen and Günter Klambauer},
  doi = {10.1021/acs.jcim.4c02373},
  journal = {Journal of Chemical Information and Modeling},
  month = {4},
  number = {9},
  openalex = {W4409968355},
  pages = {4243--4250},
  pdf = {https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.4c02373},
  publisher = {American Chemical Society},
  title = {MHNfs: Prompting In-Context Bioactivity Predictions for Low-Data Drug Discovery},
  url = {https://doi.org/10.1021/acs.jcim.4c02373},
  volume = {65},
  year = {2025}
}

@misc{burns2024associative,
  abstract = {Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.},
  archiveprefix = {arXiv},
  author = {Thomas F. Burns and Tomoki Fukai and Christopher J. Earls},
  doi = {10.48550/arXiv.2412.15113},
  eprint = {2412.15113},
  keywords = {Machine Learning, Artificial Intelligence, Transformers, In-Context Learning, Associative Memory},
  month = {12},
  note = {arXiv preprint},
  openalex = {W4405655186},
  pdf = {https://arxiv.org/pdf/2412.15113.pdf},
  primaryclass = {cs.NE},
  title = {Associative Memory Inspires Improvements for In-Context Learning Using a Novel Attention Residual Stream Architecture},
  url = {https://arxiv.org/abs/2412.15113},
  year = {2024}
}

@article{chaudhry2024long,
  abstract = {Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity by introducing a nonlinear interaction term, enhancing separation between patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly correlated patterns. Finally, we extend our model to store sequences with variable timing between states' transitions and describe a biologically-plausible implementation, with connections to motor neuroscience.},
  archiveprefix = {arXiv},
  author = {Chaudhry, Hamza Tahir and Zavatone-Veth, Jacob A. and Krotov, Dmitry and Pehlevan, Cengiz},
  doi = {10.1088/1742-5468/ad6427},
  eprint = {2306.04532},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  month = {10},
  note = {Develops a recurrent Hopfield-like network with a nonlinear interaction term that significantly expands sequence storage capacity beyond traditional models by enhancing pattern separation.},
  number = {10},
  openalex = {W4379958458},
  pages = {014001},
  pdf = {https://iopscience.iop.org/article/10.1088/1742-5468/ad6427/pdf},
  primaryclass = {cs.LG},
  publisher = {IOP Publishing},
  title = {Long Sequence Hopfield Memory},
  url = {https://arxiv.org/abs/2306.04532},
  volume = {2024},
  year = {2024}
}

@inproceedings{hofmann2024energybased,
  abstract = {Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. We evaluate Hopfield Boosting on established OOD detection benchmarks and achieve state-of-the-art performance in OOD detection with outlier exposure. On CIFAR-10, we improve the FPR95 metric from 2.28 to 0.92, on CIFAR-100 from 11.76 to 7.94, and on ImageNet-1K from 50.74 to 36.60.},
  archiveprefix = {arXiv},
  author = {Claus Hofmann and Simon Schmid and Bernhard Lehner and Daniel Klotz and Sepp Hochreiter},
  booktitle = {Advances in Neural Information Processing Systems 37},
  eprint = {2405.08766},
  github = {https://github.com/ml-jku/hopfield-boosting},
  keywords = {out-of-distribution detection, boosting, modern Hopfield networks, machine learning safety, outlier exposure},
  month = {12},
  openalex = {W4396945124},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ee20461718669c6c9c5da478d46d60d9-Paper-Conference.pdf},
  primaryclass = {cs.LG},
  publisher = {Curran Associates, Inc.},
  series = {Advances in Neural Information Processing Systems},
  title = {Energy-based Hopfield Boosting for Out-of-Distribution Detection},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee20461718669c6c9c5da478d46d60d9-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{hoover2024dense,
  abstract = {Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties.},
  arxiv = {2410.24153},
  author = {Benjamin Hoover and Duen Horng Chau and Hendrik Strobelt and Parikshit Ram and Dmitry Krotov},
  booktitle = {Advances in Neural Information Processing Systems 37},
  note = {Paper ID: 23653},
  openalex = {W4404348738},
  pages = {},
  pdf = {https://neurips.cc/paper_files/paper/2024/file/29ff36c8fbed10819b2e50267862a52a-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Dense Associative Memory Through the Lens of Random Features},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/29ff36c8fbed10819b2e50267862a52a-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{hu2024outlier,
  abstract = {We introduce an Outlier-Efficient Modern Hopfield Model (termed $\mathrmOutEffHop$) and use it to address the outlier inefficiency problem of training gigantic transformer-based models. Our main contribution is a novel associative memory model facilitating  extitoutlier-efficient associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism ($rm Softmax_1$): it is an approximation of the memory retrieval process of $\mathrmOutEffHop$. Methodologically, this allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives to traditional attention mechanisms, with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the efficacy of the proposed model across large-scale transformer-based and Hopfield-based models (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods like $\mathttClipped_Softmax$ and $\mathttGated_Attention$. Notably, $\mathrmOutEffHop$ achieves an average reduction of 22+% in average kurtosis and 26+% in the maximum infinity norm of model outputs across four models.},
  archiveprefix = {arXiv},
  author = {Hu, Jerry Yao-Chieh and Chang, Pei-Hsuan and Luo, Robin H. and Chen, Hongyu and Li, Weijian and Wang, Wei-Po and Liu, Han},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  doi = {10.48550/arXiv.2404.03828},
  editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  eprint = {2404.03828},
  month = {7},
  openalex = {W4394591418},
  pages = {19123--19152},
  pdf = {https://proceedings.mlr.press/v235/hu24a/hu24a.pdf},
  primaryclass = {cs.LG},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Outlier-Efficient Hopfield Layers for Large Transformer-Based Models},
  url = {https://proceedings.mlr.press/v235/hu24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{kashyap2024modern,
  abstract = {Content-addressable memories such as Modern Hopfield Networks (MHN) have been studied as mathematical models of auto-association and storage/retrieval in the human declarative memory, yet their practical use for large-scale content storage faces challenges. Chief among them is the occurrence of meta-stable states, particularly when handling large amounts of high dimensional content. This paper introduces Hopfield Encoding Networks (HEN), a framework that integrates encoded neural representations into MHNs to improve pattern separability and reduce meta-stable states. We show that HEN can also be used for retrieval in the context of hetero association of images with natural language queries, thus removing the limitation of requiring access to partial content in the same domain. Experimental results demonstrate substantial reduction in meta-stable states and increased storage capacity while still enabling perfect recall of a significantly larger number of inputs advancing the practical utility of associative memory networks for real-world tasks.},
  archiveprefix = {arXiv},
  author = {Satyananda Kashyap and Niharika S. D'Souza and Luyao Shi and Ken C. L. Wong and Hongzhi Wang and Tanveer Syeda-Mahmood},
  booktitle = {Workshop on Unifying Representations in Neural Models (UniReps) at NeurIPS 2024},
  eprint = {2409.16408},
  institution = {IBM Research - Almaden},
  keywords = {Machine Learning, Hopfield Networks, Neural Representations, Content-Addressable Memory},
  month = {10},
  note = {Workshop paper accepted at UniReps @ NeurIPS 2024},
  openalex = {W4403789117},
  pdf = {https://arxiv.org/pdf/2409.16408},
  primaryclass = {cs.LG},
  title = {Modern Hopfield Networks meet Encoded Neural Representations -- Addressing Practical Considerations},
  url = {https://arxiv.org/abs/2409.16408},
  year = {2024}
}

@misc{li2024expressive,
  abstract = {Modern Hopfield networks (MHNs) have emerged as powerful tools in deep learning, capable of replacing components such as pooling layers, LSTMs, and attention mechanisms. Recent advancements have enhanced their storage capacity, retrieval speed, and error rates. However, the fundamental limits of their computational expressiveness remain unexplored. Understanding the expressive power of MHNs is crucial for optimizing their integration into deep learning architectures. In this work, we establish rigorous theoretical bounds on the computational capabilities of MHNs using circuit complexity theory. Our key contribution is that we show that MHNs are $\mathsfDLOGTIME$-uniform $\mathsfTC^0$. Hence, unless $\mathsfTC^0 = \mathsfNC^1$, a $\mathrmpoly(n)$-precision modern Hopfield networks with a constant number of layers and $O(n)$ hidden dimension cannot solve $\mathsfNC^1$-hard problems such as the undirected graph connectivity problem and the tree isomorphism problem. We also extended our results to Kernelized Hopfield Networks. These results demonstrate the limitation in the expressive power of the modern Hopfield networks. Moreover, Our theoretical analysis provides insights to guide the development of new Hopfield-based architectures.},
  archiveprefix = {arXiv},
  author = {Li, Xiaoyu and Li, Yuanpeng and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  doi = {10.48550/arXiv.2412.05562},
  eprint = {2412.05562},
  month = {12},
  note = {Establishes rigorous theoretical bounds on the computational capabilities of Modern Hopfield Networks (MHNs) using circuit complexity theory, showing they belong to the class TC0 and thus have limitations on solving certain hard problems.},
  openalex = {W4405253287},
  pdf = {https://arxiv.org/pdf/2412.05562.pdf},
  primaryclass = {cs.LG},
  title = {On the Expressive Power of Modern Hopfield Networks},
  url = {https://arxiv.org/abs/2412.05562},
  year = {2024}
}

@article{lucibello2024exponential,
  abstract = {Recent generalizations of the Hopfield model of associative memories are able to store a number $P$ of random patterns that grows exponentially with the number $N$ of neurons, $P=\exp(α N)$. Besides the huge storage capacity, another interesting feature of these networks is their connection to the attention mechanism which is part of the Transformer architectures widely applied in deep learning. In this work, we study a generic family of pattern ensembles using a statistical mechanics analysis which gives exact asymptotic thresholds for the retrieval of a typical pattern, $α_1$, and lower bounds for the maximum of the load $α$ for which all patterns can be retrieved, $α_c$, as well as sizes of attraction basins. We discuss in detail the cases of Gaussian and spherical patterns, and show that they display rich and qualitatively different phase diagrams.},
  archiveprefix = {arXiv},
  author = {Lucibello, Carlo and Mézard, Marc},
  doi = {10.1103/PhysRevLett.132.077301},
  eprint = {2304.14964},
  journal = {Physical Review Letters},
  month = {2},
  note = {Provides a statistical mechanics analysis that gives exact asymptotic thresholds for pattern retrieval in exponential-capacity DAMs},
  number = {7},
  openalex = {W4391776157},
  pages = {077301},
  pdf = {https://arxiv.org/pdf/2304.14964.pdf},
  primaryclass = {cond-mat.dis-nn},
  publisher = {American Physical Society},
  title = {Exponential Capacity of Dense Associative Memories},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.132.077301},
  volume = {132},
  year = {2024}
}

@article{mcalister2024prototype,
  abstract = {We discuss prototype formation in the Hopfield network. Typically, Hebbian learning with highly correlated states leads to degraded memory performance. We show that this type of learning can lead to prototype formation, where unlearned states emerge as representatives of large correlated subsets of states, alleviating capacity woes. This process has similarities to prototype learning in human cognition. We provide a substantial literature review of prototype learning in associative memories, covering contributions from psychology, statistical physics, and computer science. We analyze prototype formation from a theoretical perspective and derive a stability condition for these states based on the number of examples of the prototype presented for learning, the noise in those examples, and the number of nonexample states presented. The stability condition is used to construct a probability of stability for a prototype state as the factors of stability change. We also note similarities to traditional network analysis, allowing us to find a prototype capacity. We corroborate these expectations of prototype formation with experiments using a simple Hopfield network with standard Hebbian learning. We extend our experiments to a Hopfield network trained on data with multiple prototypes and find the network is capable of stabilizing multiple prototypes concurrently. We measure the basins of attraction of the multiple prototype states, finding attractor strength grows with the number of examples and the agreement of examples. We link the stability and dominance of prototype states to the energy profile of these states, particularly when comparing the profile shape to target states or other spurious states.},
  author = {McAlister, Hayden and Robins, Anthony and Szymanski, Lech},
  doi = {10.1162/neco_a_01704},
  issn = {0899-7667},
  journal = {Neural Computation},
  month = {10},
  number = {11},
  openalex = {W4402045923},
  pages = {2322--2364},
  pdf = {https://direct.mit.edu/neco/article-pdf/36/11/2322/2474365/neco_a_01704.pdf},
  publisher = {MIT Press},
  title = {Prototype Analysis in Hopfield Networks With Hebbian Learning},
  url = {https://doi.org/10.1162/neco_a_01704},
  volume = {36},
  year = {2024}
}

@inproceedings{mcalister2024improved,
  abstract = {The modern Hopfield network generalizes the classical network by allowing for sharper interaction functions. This increases the capacity of the network as an autoassociative memory as nearby learned attractors will not interfere with one another. However, the implementation of the network relies on applying large exponents to the dot product of memory vectors and probe vectors. If the dimension of the data is large the calculation can be very large and result in imprecisions and overflow when using floating point numbers in a practical implementation. We describe the computational issues in detail, modify the original network description to mitigate the problem, and show the modification will not alter the networks' dynamics during update or training. We also show our modification greatly improves hyperparameter selection for the modern Hopfield network, removing dependence on the interaction vertex and resulting in an optimal region of hyperparameters that does not significantly change with the interaction vertex as it does in the original network.},
  address = {Cham},
  archiveprefix = {arXiv},
  author = {McAlister, Hayden A. and Robins, Anthony and Szymanski, Lech},
  booktitle = {Neural Information Processing: 31st International Conference, ICONIP 2024, Auckland, New Zealand, December 2--6, 2024, Proceedings},
  doi = {10.48550/arXiv.2407.08742},
  eprint = {2407.08742},
  month = {12},
  note = {Proposes a modification to the MHN relaxation function that stabilizes hyperparameter selection and demonstrates that higher-order interaction terms improve robustness against adversarial inputs.},
  openalex = {W4400667283},
  pdf = {https://arxiv.org/pdf/2407.08742.pdf},
  primaryclass = {cs.LG},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Improved Robustness and Hyperparameter Selection in Modern Hopfield Networks},
  url = {https://arxiv.org/abs/2407.08742},
  year = {2024}
}

@article{teixeira2024liquid,
  abstract = {The cellular cytoplasm is a liquid mixture of thousands of different components that interact in diverse ways. This mixture self-organizes into ordered structures, such as liquid phases with controlled composition. It remains unclear what types of interactions among components ensure the reliable assembly of these phases. To answer this question, we establish an analogy between neural networks and multicomponent liquid mixtures. This allows us to demonstrate that reliable assembly of target liquid phases is at odds with the tendency of mixtures to segregate into regions enriched in few components. This work unravels a trade-off between these types of phase behavior and constitutes a step toward linking the fields of neural networks to liquid mixtures. Biological mixtures, such as the cellular cytoplasm, are composed of a large number of different components. From this heterogeneity, ordered mesoscopic structures emerge, such as liquid phases with controlled composition. The competition of these structures for the same components raises several questions: what types of interactions allow the retrieval of multiple ordered mesoscopic structures, and what are the physical limitations for the retrieval of said structures. In this work, we develop an analytically tractable model for multicomponent liquids capable of retrieving states with target compositions. We name this model the liquid Hopfield model in reference to corresponding work in the theory of associative neural networks. In this model, we show that nonlinear repulsive interactions are a general requirement for retrieval of target structures. We demonstrate that this is because liquid mixtures at low temperatures tend to transition to phases with few components, a phenomenon that we term localization. Taken together, our results reveal a trade-off between retrieval and localization phenomena in liquid mixtures, and pave the way for other connections between the phenomenologies of neural computation and liquid mixtures.},
  author = {Rodrigo Braz Teixeira and Giorgio Carugno and Izaak Neri and Pablo Sartori},
  doi = {10.1073/pnas.2320504121},
  eissn = {1091-6490},
  issn = {0027-8424},
  journal = {Proceedings of the National Academy of Sciences},
  month = {11},
  number = {48},
  openalex = {W4404568941},
  pages = {e2320504121},
  pdf = {https://www.pnas.org/doi/pdf/10.1073/pnas.2320504121},
  publisher = {National Academy of Sciences},
  title = {Liquid Hopfield model: Retrieval and localization in multicomponent liquid mixtures},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2320504121},
  volume = {121},
  year = {2024}
}

@inproceedings{santos2024sparse,
  abstract = {Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.},
  archiveprefix = {arXiv},
  author = {Santos, Saul and Niculae, Vlad and McNamee, Daniel and Martins, André F. T.},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  eprint = {2402.13725},
  location = {Vienna, Austria},
  month = {7},
  note = {Introduces a unified framework for sparse and structured Hopfield networks by linking them to Fenchel-Young losses, enabling exact memory retrieval with end-to-end differentiable sparse transformations.},
  openalex = {W4392118265},
  pages = {43368--43388},
  pdf = {https://proceedings.mlr.press/v235/santos24a.pdf},
  primaryclass = {cs.LG},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sparse and Structured Hopfield Networks},
  url = {https://proceedings.mlr.press/v235/santos24a.html},
  volume = {235},
  year = {2024}
}

@article{niu2024selflearning,
  abstract = {Physical neural networks (PNNs) using physical materials and devices to mimic synapses and neurons offer an energy-efficient way to implement artificial neural networks. An emerging concept called physical self-learning uses intrinsic physical parameters as trainable weights, where training is achieved by the natural evolution of physical parameters that intrinsically adapt modern learning rules via an autonomous physical process, eliminating the requirements on external computation resources. However, existing PNN approaches still require external computational resources for training or rely on simple learning rules that limit their computational capability. Here, we demonstrate a physical self-learning Hopfield neural network using a magnetic thin film. The intrinsic magnetization evolution under external inputs naturally performs unsupervised learning via gradient descent adaptation using magnetic texture-defined conductance matrix as trainable weights. Under external voltage inputs, the conductance matrix naturally evolves and adapts Oja's learning algorithm in a gradient descent manner. Our approach enables autonomous learning without external computational resources while maintaining the computational power of modern neural networks. The self-learning HNN is scalable and can achieve associative memories on patterns with high similarities. The fast spin dynamics and reconfigurability of magnetic textures offer an advantageous platform towards efficient autonomous training directly in materials. Experimental results show successful pattern storage and retrieval, demonstrating the potential for energy-efficient neuromorphic computing applications.},
  author = {Chang Niu and Huanyu Zhang and Chuanlong Xu and Wenjie Hu and Yunzhuo Wu and Yu Wu and Yadi Wang and Tong Wu and Yi Zhu and Yinyan Zhu and Wenbin Wang and Yizheng Wu and Lifeng Yin and Jiang Xiao and Weichao Yu and Hangwen Guo and Jian Shen},
  doi = {10.1073/pnas.2416294121},
  eissn = {1091-6490},
  issn = {0027-8424},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {Hopfield neural network, physical neural networks, neuromorphic computing, self-learning, magnetic thin film, gradient descent, autonomous learning, spintronic systems, Oja's learning algorithm},
  month = {12},
  note = {Demonstrates a physical, self-learning Hopfield network using a magnetic thin film where the material's physical evolution under external inputs intrinsically performs unsupervised learning via gradient descent. Published online December 13, 2024.},
  number = {51},
  openalex = {W4405366043},
  pages = {e2416294121},
  pdf = {https://www.pnas.org/doi/epdf/10.1073/pnas.2416294121},
  publisher = {National Academy of Sciences},
  title = {A Self-Learning Magnetic Hopfield Neural Network with Intrinsic Gradient Descent Adaption},
  url = {https://www.pnas.org/doi/10.1073/pnas.2416294121},
  volume = {121},
  year = {2024}
}

@inproceedings{gutierrez2024hipporag,
  abstract = {In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering (QA) and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10--20 times cheaper and 6--13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.},
  address = {Vancouver, Canada},
  arxiv = {2405.14831},
  author = {Gutiérrez, Bernal Jiménez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  booktitle = {Advances in Neural Information Processing Systems 37},
  doi = {10.48550/arXiv.2405.14831},
  note = {Introduces a retrieval-augmented generation (RAG) system inspired by the hippocampus that uses associative memory principles to improve continual learning and relational reasoning in LLMs},
  openalex = {W4398886972},
  pdf = {https://papers.nips.cc/paper_files/paper/2024/file/6ddc001d07ca4f319af96a3024f6dbd1-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS'24},
  title = {HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  url = {https://openreview.net/forum?id=hkujvAPVsg},
  year = {2024}
}

@inproceedings{hu2024provably,
  abstract = {This paper studies the optimal memorization capacity of modern Hopfield models and Kernelized Hopfield Models (KHMs), a transformer-compatible class of Dense Associative Memories. The key innovation is establishing a connection between the memory configuration of KHMs and spherical codes from information theory by treating the stored memory set as a specialized spherical code. This enables casting the memorization problem in KHMs into a point arrangement problem on a hypersphere. The authors provide the first tight and optimal asymptotic memory capacity bound for modern Hopfield models by establishing an upper capacity bound that matches the well-known exponential lower bound in the literature. They develop a sub-linear time algorithm called U-Hop+ to reach KHMs' optimal capacity and analyze the scaling behavior of the required feature dimension relative to the number of stored memories. The research aims to improve retrieval capability of KHMs and the representation learning of corresponding transformers, supported by comprehensive numerical experimental results.},
  author = {Jerry Yao-Chieh Hu and Dennis Wu and Han Liu},
  booktitle = {Advances in Neural Information Processing Systems 37 (NeurIPS 2024)},
  note = {Introduced Kernelized MHNs and used a connection to spherical codes to derive the first tight and provably optimal asymptotic memory capacity bound for this class of models.},
  openalex = {W4404344201},
  pages = {1--12},
  pdf = {https://papers.nips.cc/paper_files/paper/2024/file/82846e19e6d42ebfd4ace4361def29ae-Paper-Conference.pdf},
  publisher = {Neural Information Processing Systems Foundation},
  title = {Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/82846e19e6d42ebfd4ace4361def29ae-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{xu2024bishop,
  abstract = {We introduce the Bi-Directional Sparse Hopfield Network (BiSHop), a novel end-to-end framework for deep tabular learning. BiSHop handles two major challenges: non-rotationally invariant data structure and feature sparsity. It uses a dual-component approach, sequentially processing column-wise and row-wise through interconnected directional learning modules with generalized sparse modern Hopfield layers. Empirically, experiments demonstrate BiSHop surpasses current state-of-the-art methods with significantly fewer hyperparameter optimization runs.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Xu, Chenwei and Huang, Yu-Chao and Hu, Jerry Yao-Chieh and Li, Weijian and Gilani, Ammar and Goan, Hsi-Sheng and Liu, Han},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  doi = {10.5555/3692070.3694337},
  eprint = {2404.03830},
  month = {7},
  openalex = {W4394591405},
  pages = {55048--55075},
  pdf = {https://proceedings.mlr.press/v235/xu24l/xu24l.pdf},
  primaryclass = {cs.LG},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model},
  url = {https://proceedings.mlr.press/v235/xu24l.html},
  volume = {235},
  year = {2024}
}

@article{ambrogioni2024search,
  abstract = {Uncovering the mechanisms behind long-term memory is one of the most fascinating open problems in neuroscience and artificial intelligence. Artificial associative memory networks have been used to formalize important aspects of biological memory. Generative diffusion models are a type of generative machine learning techniques that have shown great performance in many tasks. Similar to associative memory systems, these networks define a dynamical system that converges to a set of target states. In this work, we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is (asymptotically) identical to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure of a deep neural network. Leveraging this connection, we formulate a generalized framework for understanding the formation of long-term memory, where creative generation and memory recall can be seen as parts of a unified continuum.},
  archiveprefix = {arXiv},
  author = {Ambrogioni, Luca},
  doi = {10.3390/e26050381},
  eprint = {2309.17290},
  issn = {1099-4300},
  journal = {Entropy},
  month = {4},
  number = {5},
  openalex = {W4396512244},
  pages = {381},
  pdf = {https://www.mdpi.com/1099-4300/26/5/381/pdf?version=1715241648},
  publisher = {MDPI},
  title = {In Search of Dispersed Memories: Generative Diffusion Models Are Associative Memory Networks},
  url = {https://www.mdpi.com/1099-4300/26/5/381},
  volume = {26},
  year = {2024}
}

@inproceedings{wu2023stanhop,
  abstract = {We present STanHop-Net (Sparse Tandem Hopfield Network) for multivariate time series prediction with memory-enhanced capabilities. At the heart of our approach is STanHop, a novel Hopfield-based neural network block, which sparsely learns and stores both temporal and cross-series representations in a data-dependent fashion. In essence, STanHop sequentially learns temporal representation and cross-series representation using two tandem sparse Hopfield layers. Additionally, STanHop incorporates two external memory modules: Plug-and-Play and Tune-and-Play for train-less and task-aware memory enhancements, respectively. They allow StanHop-Net to swiftly respond to sudden events. Methodologically, we construct the STanHop-Net by stacking STanHop blocks in a hierarchical fashion, enabling multi-resolution feature extraction with resolution-specific sparsity. Theoretically, we introduce a unified construction (Generalized Sparse Modern Hopfield Model) for both dense and sparse modern Hopfield models and show that it endows a tighter memory retrieval error compared to the dense counterpart without sacrificing memory capacity. Empirically, we validate the efficacy of STanHop-Net on many settings: time series prediction, fast test-time adaptation, and strongly correlated time series prediction.},
  archiveprefix = {arXiv},
  author = {Dennis Wu and Jerry Yao-Chieh Hu and Weijian Li and Bo-Yu Chen and Han Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  doi = {10.48550/arXiv.2312.17346},
  eprint = {2312.17346},
  openalex = {W4390489593},
  pdf = {https://openreview.net/pdf?id=6iwg437CZs},
  primaryclass = {stat.ML},
  publisher = {OpenReview.net},
  title = {STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction},
  url = {https://openreview.net/forum?id=6iwg437CZs},
  year = {2024}
}

@article{kang2023hopfieldlike,
  abstract = {We present a Hopfield-like autoassociative network for memories representing examples of concepts. Each memory is encoded by two activity patterns with complementary properties. The first is dense and correlated across examples within concepts, and the second is sparse and exhibits no correlation among examples. The network stores each memory as a linear combination of its encodings. During retrieval, the network recovers sparse or dense patterns with a high or low activity threshold, respectively. As more memories are stored, the dense representation at low threshold shifts from examples to concepts, which are learned from accumulating common example features. Meanwhile, the sparse representation at high threshold maintains distinctions between examples due to the high capacity of sparse, decorrelated patterns. Thus, a single network can retrieve memories at both example and concept scales and perform heteroassociation between them. We obtain our results by deriving macroscopic mean-field equations that yield capacity formulas for sparse examples, dense examples, and dense concepts. We also perform network simulations that verify our theoretical results and explicitly demonstrate the capabilities of the network.},
  author = {Kang, Louis and Toyoizumi, Taro},
  doi = {10.1103/PhysRevE.108.054410},
  journal = {Physical Review E},
  month = {11},
  note = {Analyzes the performance of heteroassociative memory models in the challenging scenario where stored patterns are correlated, a critical step towards more realistic memory modeling.},
  number = {5},
  numpages = {28},
  openalex = {W4389206820},
  pages = {054410},
  pdf = {https://arxiv.org/pdf/2302.04481.pdf},
  publisher = {American Physical Society},
  title = {Hopfield-like network with complementary encodings of memories},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.108.054410},
  volume = {108},
  year = {2023}
}

@inproceedings{auer2023conformal,
  abstract = {To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.},
  archiveprefix = {arXiv},
  author = {Auer, Andreas and Gauch, Martin and Klotz, Daniel and Hochreiter, Sepp},
  booktitle = {Advances in Neural Information Processing Systems 36},
  doi = {10.5555/3618408.3618756},
  eprint = {2303.12783},
  openalex = {W4360837636},
  pages = {53864--53879},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/aef75887979ae1287b5deb54a1e3cbda-Paper-Conference.pdf},
  primaryclass = {cs.LG},
  publisher = {Curran Associates, Inc.},
  title = {Conformal Prediction for Time Series with Modern Hopfield Networks},
  url = {https://arxiv.org/abs/2303.12783},
  volume = {36},
  year = {2023}
}

@article{negri2023storage,
  abstract = {The Hopfield model is a paradigmatic model of neural networks that has been analyzed for many decades in the statistical physics, neuroscience, and machine learning communities. Inspired by the manifold hypothesis in machine learning, we propose and investigate a generalization of the standard setting that we name Random-Features Hopfield Model. Here $P$ binary patterns of length $N$ are generated by applying to Gaussian vectors sampled in a latent space of dimension $D$ a random projection followed by a non-linearity. Using the replica method from statistical physics, we derive the phase diagram of the model in the limit $P,N,D oınfty$ with fixed ratios $α=P/N$ and $α_D=D/N$. Besides the usual retrieval phase, where the patterns can be dynamically recovered from some initial corruption, we uncover a new phase where the features characterizing the projection can be recovered instead. We call this phenomena the learning phase transition, as the features are not explicitly given to the model but rather are inferred from the patterns in an unsupervised fashion.},
  author = {Matteo Negri and Clarissa Lauditi and Gabriele Perugini and Carlo Lucibello and Enrico Malatesta},
  doi = {10.1103/PhysRevLett.131.257301},
  issue = {25},
  journal = {Physical Review Letters},
  month = {12},
  numpages = {6},
  openalex = {W4389936021},
  pages = {257301},
  pdf = {http://arxiv.org/pdf/2303.16880},
  publisher = {American Physical Society},
  title = {Storage and Learning Phase Transitions in the Random-Features Hopfield Model},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.131.257301},
  volume = {131},
  year = {2023}
}

@inproceedings{burns2023simplicial,
  abstract = {Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using $N$ neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include non-trivial simplicial topology. We also test analogous modern continuous Hopfield networks, offering a potentially promising avenue for improving the attention mechanism in Transformer models.},
  archiveprefix = {arXiv},
  author = {Burns, Thomas F. and Fukai, Tomoki},
  booktitle = {The Eleventh International Conference on Learning Representations},
  eprint = {2305.05179},
  keywords = {Hopfield networks, associative memory, attention, computational neuroscience, simplicial complex, topology, memory capacity},
  month = {5},
  openalex = {W4376122733},
  pdf = {https://openreview.net/pdf?id=_QLsH8gatwx},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Simplicial Hopfield Networks},
  url = {https://openreview.net/forum?id=_QLsH8gatwx},
  year = {2023}
}

@inproceedings{hoover2023energy,
  abstract = {Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.},
  address = {Red Hook, NY, USA},
  archiveprefix = {arXiv},
  articleno = {1197},
  author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.5555/3666122.3667319},
  editor = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  eprint = {2302.07253},
  location = {New Orleans, LA, USA},
  month = {12},
  numpages = {28},
  pages = {27532--27559},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/57a9b97477b67936298489e3c1417b0a-Paper-Conference.pdf},
  primaryclass = {cs.LG},
  publisher = {Curran Associates Inc.},
  series = {NIPS '23},
  title = {Energy Transformer},
  url = {https://dl.acm.org/doi/10.5555/3666122.3667319},
  volume = {36},
  year = {2023}
}

@inproceedings{hu2023sparse,
  abstract = {We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Our key theoretical contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. The sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point convergence and exponential memory capacity. Empirically, we use both synthetic and real-world datasets to demonstrate that the sparse Hopfield model outperforms its dense counterpart in many situations.},
  archiveprefix = {arXiv},
  author = {Hu, Jerry Yao-Chieh and Yang, Donglin and Wu, Dennis and Xu, Chenwei and Chen, Boyu and Liu, Han},
  booktitle = {Advances in Neural Information Processing Systems 36},
  eprint = {2309.12673},
  note = {Introduced a principled sparse Modern Hopfield Network (MHN) whose update rule corresponds to sparsemax attention, providing an energy-based foundation for sparse attention mechanisms.},
  openalex = {W4387031432},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/57bc0a850255e2041341bf74c7e2b9fa-Paper-Conference.pdf},
  primaryclass = {cs.LG},
  publisher = {Curran Associates, Inc.},
  title = {On Sparse Modern Hopfield Model},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/57bc0a850255e2041341bf74c7e2b9fa-Paper-Conference.pdf},
  volume = {36},
  year = {2023}
}

@inproceedings{ota2023learning,
  abstract = {It has been known by neuroscience studies that partial and transient forgetting of memory often plays an important role in the brain to improve performance for certain intellectual activities. In machine learning, associative memory models such as classical and modern Hopfield networks have been proposed to express memories as attractors in the feature space of a closed recurrent network. In this work, we propose learning with partial forgetting (LwPF), where a partial forgetting functionality is designed by element-wise non-bijective projections, for memory neurons in modern Hopfield networks to improve model performance. We incorporate LwPF into the attention mechanism also, whose process has been shown to be identical to the update rule of a certain modern Hopfield network, by modifying the corresponding Lagrangian. We evaluated the effectiveness of LwPF on three diverse tasks such as bit-pattern classification, immune repertoire classification for computational biology, and image classification for computer vision, and confirmed that LwPF consistently improves the performance of existing neural networks including DeepRC and vision transformers.},
  address = {Valencia, Spain},
  author = {Ota, Toshihiro and Sato, Ikuro and Kawakami, Rei and Tanaka, Masayuki and Inoue, Nakamasa},
  booktitle = {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  editor = {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  keywords = {Modern Hopfield Networks, Machine Learning, Artificial Intelligence, Memory Networks, Partial Forgetting, Attention Mechanism, Associative Memory},
  month = {4},
  note = {Introduced a biologically inspired "forgetting" mechanism into the MHN update rule, demonstrating improved performance by preventing overfitting to memory patterns. Authors affiliated with Institute of Science Tokyo (formerly Tokyo Institute of Technology)},
  pages = {6661--6673},
  pdf = {https://proceedings.mlr.press/v206/ota23a/ota23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  shortbooktitle = {AISTATS},
  title = {Learning with Partial Forgetting in Modern Hopfield Networks},
  url = {https://proceedings.mlr.press/v206/ota23a.html},
  volume = {206},
  year = {2023}
}

@inproceedings{rasul2023probabilistic,
  abstract = {Hopfield networks, originally introduced as associative memory models, have shown promise in pattern recognition, optimization problems, and tabular datasets. However, their application to time series data has been limited. We introduce a temporal version that leverages the associative memory properties of the Hopfield architecture while accounting for temporal dependencies present in time series data. Our results suggest that the proposed model demonstrates competitive performance compared to state-of-the-art probabilistic forecasting models.},
  address = {New Orleans, LA, USA},
  author = {Kashif Rasul and Pablo Vicente and Anderson Schneider and Alexander März},
  booktitle = {Associative Memory & Hopfield Networks in 2023},
  month = {12},
  publisher = {NeurIPS Workshop},
  title = {Probabilistic Forecasting via Modern Hopfield Networks},
  url = {https://openreview.net/forum?id=463RlISt9t},
  year = {2023}
}

@inproceedings{furst2022cloob,
  abstract = {CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel 'Contrastive Leave One Out Boost' (CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.},
  address = {New Orleans, LA, USA},
  author = {Fürst, Andreas and Rumetshofer, Elisabeth and Lehner, Johannes and Tran, Viet T. and Tang, Fei and Ramsauer, Hubert and Kreil, David and Kopp, Michael and Klambauer, Günter and Bitto, Angela and Hochreiter, Sepp},
  booktitle = {Advances in Neural Information Processing Systems},
  month = {11},
  openalex = {W4286897344},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8078e76f913e31b8467e85b4c0f0d22b-Paper-Conference.pdf},
  publisher = {Curran Associates},
  title = {CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8078e76f913e31b8467e85b4c0f0d22b-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@inproceedings{iatropoulos2022kernel,
  abstract = {We consider the problem of training a neural network to store a set of patterns with maximal noise robustness. A solution, in terms of optimal weights and state update rules, is derived by training each individual neuron to perform either kernel classification or interpolation with a minimum weight norm. By applying this method to feed-forward and recurrent networks, we derive optimal models, termed kernel memory networks, that include, as special cases, many of the hetero-and auto-associative memory models that have been proposed over the past years, such as modern Hopfield networks and Kanerva's sparse distributed memory. We modify Kanerva's model and demonstrate a simple way to design a kernel memory network that can store an exponential number of continuous-valued patterns with a finite basin of attraction. The framework of kernel memory networks offers a simple and intuitive way to understand the storage capacity of previous memory models, and allows for new biological interpretations in terms of dendritic non-linearities and synaptic cross-talk.},
  address = {Red Hook, NY, USA},
  author = {Iatropoulos, Georgios and Brea, Johanni and Gerstner, Wulfram},
  booktitle = {Advances in Neural Information Processing Systems 35},
  location = {New Orleans, LA, USA},
  pages = {33291--33303},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e55d081280e79e714debf2902e18eb69-Paper-Conference.pdf},
  publisher = {Curran Associates Inc.},
  series = {NeurIPS '22},
  title = {Kernel Memory Networks: A Unifying Framework for Memory Modeling},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e55d081280e79e714debf2902e18eb69-Abstract-Conference.html},
  year = {2022}
}

@article{iatropoulos2022three,
  abstract = {Three-factor synaptic plasticity rules generalize Hebbian rules by incorporating a third factor, such as dopamine or noradrenaline. This paper develops a three-factor learning rule specifically designed for associative memory retrieval in cortical networks, demonstrating how specific microcircuit architectures can implement efficient memory storage and retrieval through biologically plausible mechanisms.},
  author = {Iatropoulos, Georgios and Milekovic, Tomislav and Simonyan, Vahe and Gerstner, Wulfram},
  doi = {10.3389/fncom.2022.1044709},
  journal = {Frontiers in Computational Neuroscience},
  keywords = {associative memory, synaptic plasticity, three-factor learning, cortical microcircuits, computational neuroscience},
  month = {11},
  note = {Develops a biologically plausible three-factor learning rule for associative memory retrieval, proposing cortical microcircuit architectures.},
  number = {5},
  openalex = {W4387520314},
  pages = {1044709},
  publisher = {Frontiers Media SA},
  title = {Three-factor Learning of Associative Memory Retrieval in the Cortex},
  url = {https://www.frontiersin.org/articles/10.3389/fncom.2022.1044709/full},
  volume = {16},
  year = {2022}
}

@inproceedings{millidge2022universal,
  abstract = {A large number of neural network models of associative memory have been proposed in the literature. These include the classical Hopfield networks (HNs), sparse distributed memories (SDMs), and more recently the modern continuous Hopfield networks (MCHNs), which possess close links with self-attention in machine learning. In this paper, we propose a general framework for understanding the operation of such memory networks as a sequence of three operations: similarity, separation, and projection. We derive all these memory models as instances of our general framework with differing similarity and separation functions. We extend the mathematical framework of Krotov et al (2020) to express general associative memory models using neural network dynamics with local computation, and derive a general energy function that is a Lyapunov function of the dynamics. Finally, using our framework, we empirically investigate the capacity of using different similarity functions for these associative memory models, beyond the dot product similarity measure, and demonstrate empirically that Euclidean or Manhattan distance similarity metrics perform substantially better in practice on many tasks, enabling a more robust retrieval and higher memory capacity than existing models.},
  author = {Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  doi = {10.48550/arXiv.2202.04557},
  keywords = {Hopfield Networks, Associative Memory, Machine Learning},
  openalex = {W4319460387},
  pages = {15561--15583},
  pdf = {https://proceedings.mlr.press/v162/millidge22a/millidge22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models},
  url = {https://proceedings.mlr.press/v162/millidge22a.html},
  volume = {162},
  year = {2022}
}

@inproceedings{whittington2022relating,
  abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
  archiveprefix = {arXiv},
  author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  doi = {10.48550/arxiv.2112.04035},
  eprint = {2112.04035},
  openalex = {W4200634264},
  pdf = {https://openreview.net/pdf?id=B8DVo9B1YE0},
  primaryclass = {cs.LG},
  title = {Relating Transformers to Models and Neural Representations of the Hippocampal Formation},
  url = {https://openreview.net/forum?id=B8DVo9B1YE0},
  year = {2022}
}

@inproceedings{krotov2021large,
  abstract = {Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. In this paper, we show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature including the model presented in "Hopfield Networks is All You Need" paper.},
  archiveprefix = {arXiv},
  author = {Krotov, Dmitry and Hopfield, John J.},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  doi = {10.48550/arXiv.2008.06996},
  eprint = {2008.06996},
  month = {5},
  openalex = {W3122855748},
  pdf = {https://openreview.net/pdf?id=X4y_10OX-hX},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Large Associative Memory Problem in Neurobiology and Machine Learning},
  url = {https://openreview.net/forum?id=X4y_10OX-hX},
  year = {2021}
}

@misc{krotov2021hierarchical,
  abstract = {Dense Associative Memories or Modern Hopfield Networks have many appealing properties of associative memory. They can do pattern completion, store a large number of memories, and can be described using a recurrent neural network with a degree of biological plausibility and rich feedback between the neurons. At the same time, up until now all the models of this class have had only one hidden layer, and have only been formulated with densely connected network architectures, two aspects that hinder their machine learning applications. This paper tackles this gap and describes a fully recurrent model of associative memory with an arbitrary large number of layers, some of which can be locally connected (convolutional), and a corresponding energy function that decreases on the dynamical trajectory of the neurons' activations. The memories of the full network are dynamically "assembled" using primitives encoded in the synaptic weights of the lower layers, with the "assembling rules" encoded in the synaptic weights of the higher layers. In addition to the bottom-up propagation of information, typical of commonly used feedforward neural networks, the model described has rich top-down feedback from higher layers that help the lower-layer neurons to decide on their response to the input stimuli.},
  archiveprefix = {arXiv},
  author = {Dmitry Krotov},
  doi = {10.48550/arXiv.2107.06446},
  eprint = {2107.06446},
  note = {Extends Dense Associative Memory (DAM) framework to multi-layered structures, creating a hierarchical memory system capable of storing concepts at multiple levels of abstraction},
  openalex = {W3182042768},
  pdf = {https://arxiv.org/pdf/2107.06446.pdf},
  primaryclass = {cs.LG},
  title = {Hierarchical Associative Memory},
  url = {https://arxiv.org/abs/2107.06446},
  year = {2021}
}

@article{jiang2021working,
  abstract = {Conversion of temporal to spatial correlations in the cortex is one of the most intriguing functions in the brain. The learning at synapses triggering the correlation conversion can take place in a wide integration window, whose influence on the correlation conversion remains elusive. Here, we propose a generalized associative memory model with arbitrary Hebbian length. The model can be analytically solved, and predicts that a small Hebbian length can already significantly enhance the correlation conversion, i.e., the stimulus-induced attractor can be highly correlated with a significant number of patterns in the stored sequence, thereby facilitating state transitions in the neural representation space. Moreover, an anti-Hebbian component is able to reshape the energy landscape of memories, akin to the function of sleep. Our work thus establishes the fundamental connection between associative memory, Hebbian length, and correlation conversion in the brain.},
  archiveprefix = {arXiv},
  author = {Zijian Jiang and Jianwen Zhou and Tianqi Hou and K. Y. Michael Wong and Haiping Huang},
  doi = {10.1103/PhysRevE.104.064306},
  eprint = {2103.14317},
  issn = {2470-0053},
  journal = {Physical Review E},
  month = {12},
  number = {6},
  openalex = {W4226442802},
  pages = {064306},
  pdf = {https://arxiv.org/pdf/2103.14317.pdf},
  primaryclass = {cond-mat.dis-nn},
  publisher = {American Physical Society},
  title = {Associative memory model with arbitrary Hebbian length},
  url = {https://doi.org/10.1103/PhysRevE.104.064306},
  volume = {104},
  year = {2021}
}

@article{zhou2021eigenvalue,
  abstract = {Associative memory is a fundamental function in the brain. Here, we generalize the standard associative memory model to include long-range Hebbian interactions at the learning stage, corresponding to a large synaptic integration window. In our model, the Hebbian length can be arbitrarily large.},
  archiveprefix = {arXiv},
  author = {Zhou, Jianwen and Jiang, Zijian and Hou, Tianqi and Chen, Ziming and Wong, K. Y. Michael and Huang, Haiping},
  doi = {10.1103/PhysRevE.104.064307},
  eprint = {2103.14324},
  journal = {Physical Review E},
  keywords = {Neural Networks, Associative Memory, Hebbian Learning, Statistical Physics},
  month = {12},
  number = {6},
  openalex = {W4226345475},
  pages = {064307},
  pdf = {https://arxiv.org/pdf/2103.14324.pdf},
  primaryclass = {q-bio.NC},
  title = {Eigenvalue spectrum of neural networks with arbitrary Hebbian length},
  url = {https://doi.org/10.1103/PhysRevE.104.064307},
  volume = {104},
  year = {2021}
}

@inproceedings{ramsauer2020hopfield,
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieve the pattern with one update, and has exponentially small retrieval errors. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. We show that the transformer attention heads operate by three types of (fixed-point) update rules: (1) averaging over all inputs, (2) making a piece-wise constant approximation, and (3) single-pattern retrieval. We further show empirically that modern Hopfield networks outperform LSTM networks on immune repertoire classification and achieve a new state-of-the-art on multiple instance learning benchmark datasets.},
  archiveprefix = {arXiv},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  booktitle = {Proceedings of the 9th International Conference on Learning Representations},
  eprint = {2008.02217},
  note = {A seminal paper that formally established the mathematical equivalence between the update rule of continuous modern Hopfield networks and the attention mechanism in Transformers, bridging the two fields.},
  openalex = {W3127151792},
  pdf = {https://openreview.net/pdf?id=tL89RnzIiCd},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Hopfield Networks is All You Need},
  url = {https://openreview.net/forum?id=tL89RnzIiCd},
  year = {2021}
}

@article{evans2021inference,
  abstract = {This paper addresses the problem of making statistical inference about a population that can only be identified through classifier predictions. The problem is motivated by scientific studies in which human labels of a population are replaced by a classifier. For downstream analysis of the population based on classifier predictions to be sound, the predictions must generalize equally across experimental conditions. In this paper, we formalize the task of statistical inference using classifier predictions, and propose bootstrap procedures to allow inference with a generalizable classifier. We demonstrate our methods through simulations and a live cell imaging case study.},
  archiveprefix = {arXiv},
  author = {Ciaran Evans and Zara Y. Weinberg and Manojkumar A. Puthenveedu and Max G'Sell},
  eprint = {2106.07623},
  journal = {arXiv preprint arXiv:2106.07623},
  month = {6},
  note = {Submitted on 14 Jun 2021, 26 pages, 9 figures},
  pdf = {https://arxiv.org/pdf/2106.07623.pdf},
  primaryclass = {stat.AP},
  title = {Inference with generalizable classifier predictions},
  url = {https://arxiv.org/abs/2106.07623},
  year = {2021}
}

@article{whittington2022tolmaneichenbaum,
  abstract = {The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine (TEM). After learning, TEM entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. TEM hippocampal cells include place and landmark cells that remap between environments. Crucially, TEM also aligns with empirically recorded representations in complex non-spatial tasks. TEM also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We confirm this structural transfer over remapping in simultaneously recorded place and grid cells.},
  author = {Whittington, James C. R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E. J.},
  doi = {10.1016/j.cell.2020.10.024},
  journal = {Cell},
  keywords = {hippocampus, computational neuroscience, memory, computational model, cognitive map},
  month = {November},
  note = {Proposes a computational model of the hippocampus that uses vector-based representations and associative memory principles to unify the learning of spatial maps and relational knowledge.},
  number = {5},
  openalex = {W3103493959},
  pages = {1249--1263.e23},
  pdf = {https://www.cell.com/cell/fulltext/S0092-8674(20)31388-X},
  pmid = {33181068},
  publisher = {Elsevier},
  title = {The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation},
  url = {https://www.cell.com/cell/fulltext/S0092-8674(20)31388-X},
  volume = {183},
  year = {2020}
}

@inproceedings{widrich2020modern,
  abstract = {A central mechanism in machine learning is to identify, store, and recognize patterns. How to learn, access, and retrieve such patterns is crucial in Hopfield networks and the more recent transformer architectures. We show that the attention mechanism of transformer architectures is actually the update rule of modern Hopfield networks that can store exponentially many patterns. We exploit this high storage capacity of modern Hopfield networks to solve a challenging multiple instance learning (MIL) problem in computational biology: immune repertoire classification. In immune repertoire classification, a vast number of immune receptors are used to predict the immune status of an individual. This constitutes a MIL problem with an unprecedentedly massive number of instances, two orders of magnitude larger than currently considered problems, and with an extremely low witness rate. Accurate and interpretable machine learning methods solving this problem could pave the way towards new vaccines and therapies, which is currently a very relevant research topic intensified by the COVID-19 crisis. In this work, we present our novel method DeepRC that integrates transformer-like attention, or equivalently modern Hopfield networks, into deep learning architectures for massive MIL such as immune repertoire classification. We demonstrate that DeepRC outperforms all other methods with respect to predictive performance on large-scale experiments including simulated and real-world virus infection data and enables the extraction of sequence motifs that are connected to a given disease class.},
  archiveprefix = {arXiv},
  author = {Widrich, Michael and Schäfl, Bernhard and Pavlović, Milena and Ramsauer, Hubert and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and Klambauer, Günter},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  eprint = {2007.13505},
  month = {12},
  openalex = {W3103855458},
  pages = {20156--20166},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/da4902cb0bc38210839714ebdcf0efc3-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {Modern Hopfield Networks and Attention for Immune Repertoire Classification},
  url = {https://proceedings.neurips.cc/paper/2020/hash/da4902cb0bc38210839714ebdcf0efc3-Abstract.html},
  volume = {33},
  year = {2020}
}

@article{fachechi2019dreaming,
  abstract = {The standard Hopfield model for associative neural networks accounts for biological Hebbian learning and acts as the harmonic oscillator for pattern recognition, however its maximal storage capacity is $α ∼ 0.14$, far from the theoretical bound for symmetric networks, i.e. $α =1$. Inspired by sleeping and dreaming mechanisms in mammal brains, we propose an extension of this model displaying the standard on-line (awake) learning mechanism (that allows the storage of external information in terms of patterns) and an off-line (sleep) unlearning$&$consolidating mechanism (that allows spurious-pattern removal and pure-pattern reinforcement): this obtained daily prescription is able to saturate the theoretical bound $α=1$, remaining also extremely robust against thermal noise. Both neural and synaptic features are analyzed both analytically and numerically. In particular, beyond obtaining a phase diagram for neural dynamics, we focus on synaptic plasticity and we give explicit prescriptions on the temporal evolution of the synaptic matrix. We analytically prove that our algorithm makes the Hebbian kernel converge with high probability to the projection matrix built over the pure stored patterns. Furthermore, we obtain a sharp and explicit estimate for the "sleep rate" in order to ensure such a convergence. Finally, we run extensive numerical simulations (mainly Monte Carlo sampling) to check the approximations underlying the analytical investigations (e.g., we developed the whole theory at the so called replica-symmetric level, as standard in the Amit-Gutfreund-Sompolinsky reference framework) and possible finite-size effects, finding overall full agreement with the theory.},
  author = {Fachechi, Alberto and Agliari, Elena and Barra, Adriano},
  doi = {10.1016/j.neunet.2019.01.006},
  issn = {0893-6080},
  journal = {Neural Networks},
  month = {4},
  openalex = {W2963208784},
  pages = {24--40},
  pdf = {https://www.sciencedirect.com/science/article/pii/S0893608019300176},
  publisher = {Elsevier},
  title = {Dreaming Neural Networks: Forgetting Spurious Memories and Reinforcing Pure Ones},
  url = {https://doi.org/10.1016/j.neunet.2019.01.006},
  volume = {112},
  year = {2019}
}

@article{krotov2018dense,
  abstract = {Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. The paper examines these questions within the framework of Dense Associative Memory (DAM) models, which are defined by an energy function with higher order interactions between neurons. When the power of the interaction vertex is sufficiently large, these models have three key properties: (1) The minima of the objective function are free from rubbish images, with each minimum being a semantically meaningful pattern. (2) Artificial patterns at the decision boundary look ambiguous to human subjects and share aspects of both separated classes. (3) Adversarial images constructed with small interaction vertex power fail to transfer and fool models with higher order interactions. The results suggest that DAM with higher order energy functions are closer to human visual perception than deep neural networks with ReLUs.},
  archiveprefix = {arXiv},
  author = {Krotov, Dmitry and Hopfield, John J.},
  doi = {10.1162/neco_a_01143},
  eprint = {1701.00939},
  journal = {Neural Computation},
  month = {12},
  note = {Demonstrated a key practical benefit of the new models, showing that DAMs with high-power energy functions are inherently more robust to adversarial attacks than standard DNNs.},
  number = {12},
  openalex = {W3105463048},
  pages = {3151--3167},
  pdf = {https://arxiv.org/pdf/1701.00939.pdf},
  pmid = {30314425},
  publisher = {MIT Press},
  title = {Dense Associative Memory Is Robust to Adversarial Inputs},
  url = {https://doi.org/10.1162/neco_a_01143},
  volume = {30},
  year = {2018}
}

@article{gerstner2018eligibility,
  abstract = {Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules.},
  author = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
  doi = {10.3389/fncir.2018.00053},
  issn = {1662-5277},
  journal = {Frontiers in Neural Circuits},
  keywords = {synaptic plasticity, eligibility traces, learning rules, neuroscience, neural circuits},
  month = {7},
  openalex = {W3105516366},
  pages = {53},
  pdf = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00053/pdf},
  publisher = {Frontiers Media S.A.},
  title = {Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules},
  url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00053/full},
  volume = {12},
  year = {2018}
}

@article{demircigil2017model,
  abstract = {In [7] Krotov and Hopfield suggest a generalized version of the well-known Hopfield model of associative memory. In their version they consider a polynomial interaction function and claim that this increases the storage capacity of the model. We prove this claim and take the 'limit' as the degree of the polynomial becomes infinite, i.e. an exponential interaction function. With this interaction we prove that model has an exponential storage capacity in the number of neurons, yet the basins of attraction are almost as large as in the standard Hopfield model.},
  archiveprefix = {arXiv},
  author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
  doi = {10.1007/s10955-017-1806-y},
  eprint = {1702.01929},
  issn = {0022-4715},
  journal = {Journal of Statistical Physics},
  month = {7},
  note = {Rigorous mathematical proof of exponential energy function endowing Hopfield-style networks with exponential storage capacity while maintaining large basins of attraction},
  number = {2},
  openalex = {W3102889924},
  pages = {288--299},
  pdf = {https://arxiv.org/pdf/1702.01929.pdf},
  primaryclass = {math.PR},
  publisher = {Springer},
  title = {On a Model of Associative Memory with Huge Storage Capacity},
  url = {https://link.springer.com/article/10.1007/s10955-017-1806-y},
  volume = {168},
  year = {2017}
}

@inproceedings{vaswani2017attention,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show that these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  address = {Red Hook, NY, USA},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.5555/3295222.3295349},
  note = {The landmark paper that introduced the Transformer architecture and the attention mechanism, which would later be shown to be equivalent to a modern Hopfield network update rule.},
  openalex = {W2626778328},
  pages = {5998--6008},
  pdf = {https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf},
  publisher = {Curran Associates Inc.},
  title = {Attention Is All You Need},
  url = {https://arxiv.org/abs/1706.03762},
  volume = {30},
  year = {2017}
}

@article{bittner2017behavioral,
  abstract = {Learning is primarily mediated by activity-dependent modifications of synaptic strength within neuronal circuits. We discovered that place fields in hippocampal area CA1 are produced by a synaptic potentiation notably different from Hebbian plasticity. Place fields could be produced in vivo in a single trial by potentiation of input that arrived seconds before and after complex spiking. This rule, named behavioral time scale synaptic plasticity, can rapidly store entire sequences of events within the synaptic weights of area CA1.},
  author = {Bittner, Katie C. and Milstein, Aaron D. and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C.},
  doi = {10.1126/science.aan3846},
  institution = {Howard Hughes Medical Institute, Janelia Research Campus},
  journal = {Science},
  keywords = {neuroscience, synaptic plasticity, hippocampus, learning, memory},
  month = {9},
  number = {6355},
  openalex = {W2751166226},
  pages = {1033--1036},
  pdf = {https://www.science.org/doi/10.1126/science.aan3846},
  pmid = {28883072},
  publisher = {American Association for the Advancement of Science},
  title = {Behavioral Time Scale Synaptic Plasticity Underlies CA1 Place Fields},
  url = {https://www.science.org/doi/10.1126/science.aan3846},
  volume = {357},
  year = {2017}
}

@inproceedings{krotov2016dense,
  abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed - one limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The storage capacity of the proposed dense associative memory is analyzed using the methods of statistical mechanics, and it is shown that in the limit of large number of neurons, the energy landscape of the proposed model has barriers to protect pattern recognition from noise. The utility of the dense memories is demonstrated on the XOR logical gate and on the MNIST handwritten digit recognition task.},
  author = {Krotov, Dmitry and Hopfield, John J.},
  booktitle = {Advances in Neural Information Processing Systems 29},
  doi = {10.5555/3157096.3157228},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  note = {The foundational paper that introduced "Dense Associative Memories" (DAMs) by using higher-order polynomial energy functions to break the classical linear storage capacity limit.},
  openalex = {W2962940432},
  pages = {1172--1180},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/eaae339c4d89fc102edd9dbdb6a28915-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Dense Associative Memory for Pattern Recognition},
  url = {https://proceedings.neurips.cc/paper/2016/hash/eaae339c4d89fc102edd9dbdb6a28915-Abstract.html},
  volume = {29},
  year = {2016}
}

@inproceedings{martins2016from,
  abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
  address = {New York, New York, USA},
  archiveprefix = {arXiv},
  author = {Martins, André F. T. and Astudillo, Ramón Fernández},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
  doi = {10.5555/3045390.3045561},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  eprint = {1602.02068},
  month = {6},
  note = {Introduced the sparsemax function, which became the key component for developing sparse modern Hopfield networks seven years later.},
  openalex = {W2253795368},
  pages = {1614--1623},
  pdf = {http://proceedings.mlr.press/v48/martins16.pdf},
  primaryclass = {cs.LG},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-label Classification},
  url = {https://proceedings.mlr.press/v48/martins16.html},
  volume = {48},
  year = {2016}
}

@article{huang2010reconstructing,
  abstract = {We test four fast mean-field-type algorithms on Hopfield networks as an inverse Ising problem. The equilibrium behavior of Hopfield networks is simulated through Glauber dynamics. In the retrieval phase favored when the network wants to memory one of stored patterns, all the reconstruction algorithms fail to extract interactions within a desired accuracy, and the same failure occurs in the spin-glass phase where spurious minima show up, while in the paramagnetic phase, albeit unfavored during the retrieval dynamics, the algorithms work well to reconstruct the network itself. This implies that as an inverse problem, the paramagnetic phase is conversely useful for reconstructing the network while the retrieval phase loses all the information about interactions in the network except for the case where only one pattern is stored. The performances of algorithms are studied with respect to the system size, memory load, and temperature, with sample-to-sample fluctuations also considered.},
  author = {Haiping Huang},
  doi = {10.1103/PhysRevE.81.036104},
  issue = {3},
  journal = {Physical Review E},
  month = {3},
  numpages = {7},
  openalex = {W1973822833},
  pages = {036104},
  pdf = {https://arxiv.org/pdf/0909.1885.pdf},
  publisher = {American Physical Society},
  title = {Reconstructing the Hopfield network as an inverse Ising problem},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.81.036104},
  volume = {81},
  year = {2010}
}

@article{huang2010message,
  abstract = {The Hopfield network is reconstructed as an inverse Ising problem by passing messages. The applied susceptibility propagation algorithm is shown to improve significantly on other mean-field-type methods and extends well into the low-temperature region. However, this iterative algorithm is limited by the nature of the supplied data. Its performance deteriorates as the data become highly magnetized, and this method finally fails in the presence of the frozen type data where at least two of its magnetizations are equal to 1 in absolute value. On the other hand, a threshold behavior is observed for the susceptibility propagation algorithm and the transition from good reconstruction to poor one becomes sharper as the network size increases.},
  author = {Huang, Haiping},
  doi = {10.1103/PhysRevE.82.056111},
  journal = {Physical Review E},
  month = {11},
  number = {5},
  numpages = {7},
  openalex = {W1999472851},
  pages = {056111},
  pdf = {https://arxiv.org/pdf/1009.0069.pdf},
  publisher = {American Physical Society},
  title = {Message passing algorithms for the Hopfield network reconstruction: Threshold behavior and limitation},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.82.056111},
  volume = {82},
  year = {2010}
}

@article{nokura1998spin,
  abstract = {We study the Hopfield model with an opposite interactional sign by using a one-step replica symmetry breaking ansatz and the marginality condition. We show that this model belongs to spin glass models which have a dynamical phase transition, which is not associated with usual replica instability. Although this model is motivated by the observations about unlearning, it has various interesting aspects: correlations among interactions, a simple meaning as an optimization problem, and SK-like and non-SK-like spin glass phases depending on the number of 'memorized' patterns.},
  author = {Kazuo Nokura},
  doi = {10.1088/0305-4470/31/37/007},
  journal = {Journal of Physics A: Mathematical and General},
  number = {37},
  openalex = {W2040260270},
  pages = {7447--7461},
  pdf = {https://iopscience.iop.org/article/10.1088/0305-4470/31/37/007/pdf},
  publisher = {IOP Publishing},
  title = {Spin glass states of the anti-Hopfield model},
  url = {https://iopscience.iop.org/article/10.1088/0305-4470/31/37/007},
  volume = {31},
  year = {1998}
}

@article{cugliandolo1994capacity,
  abstract = {We analyse the extensive loading of a neural network model proposed to describe neurophysiological experiments in which correlated attractors associated with uncorrelated patterns are found. The phase diagram is obtained and discussed. Some generalizations of the original model are also considered. In all cases, we demonstrate the existence of a region with attractors. Results from numerical simulations confirm mean-field theory results presented.},
  author = {Cugliandolo, L. F. and Tsodyks, M. V.},
  doi = {10.1088/0305-4470/27/3/018},
  issn = {0305-4470},
  journal = {Journal of Physics A: Mathematical and General},
  month = {1},
  number = {3},
  openalex = {W2085358606},
  pages = {741--755},
  publisher = {Institute of Physics Publishing},
  title = {Capacity of networks with correlated attractors},
  url = {https://iopscience.iop.org/article/10.1088/0305-4470/27/3/018},
  volume = {27},
  year = {1994}
}

@article{griniasty1993conversion,
  abstract = {It is shown that a simple modification of synaptic structures (of the Hopfield type) constructed to produce autoassociative attractors, produces neural networks whose attractors are correlated with several (learned) patterns used in the construction of the matrix. The modification stores in the matrix a fixed sequence of uncorrelated patterns. The network then has correlated attractors, provoked by the uncorrelated stimuli. Thus, the network converts the temporal order (or temporal correlation) expressed by the sequence of patterns, into spatial correlations expressed in the distributions of neural activities in attractors.},
  author = {Griniasty, Mauro and Tsodyks, Misha V. and Amit, Daniel J.},
  doi = {10.1162/neco.1993.5.1.1},
  issn = {0899-7667},
  journal = {Neural Computation},
  month = {1},
  number = {1},
  openalex = {W2100686511},
  pages = {1--17},
  publisher = {MIT Press},
  title = {Conversion of Temporal Correlations Between Stimuli to Spatial Correlations Between Attractors},
  url = {https://direct.mit.edu/neco/article-abstract/5/1/1/5694/Conversion-of-Temporal-Correlations-Between},
  volume = {5},
  year = {1993}
}

@article{dotsenko1991statistical,
  abstract = {Statistical mechanics analysis of Hopfield-like neural networks with modified interactions using mean-field theory. The modifications are achieved through a special thermally noised iterative procedure, where the couplings have an intermediate form between Hebb-like learning and pseudo inverse rules. A replica-symmetric free energy analysis is performed and the phase diagram is studied across different parameters of the modified interaction scheme.},
  author = {Dotsenko, V. S. and Yarunin, N. D. and Dorotheyev, E. A.},
  doi = {10.1088/0305-4470/24/10/026},
  journal = {Journal of Physics A: Mathematical and General},
  month = {5},
  number = {10},
  openalex = {W2090110907},
  pages = {2419--2429},
  publisher = {IOP Publishing},
  title = {Statistical mechanics of Hopfield-like neural networks with modified interactions},
  volume = {24},
  year = {1991}
}

@article{miyashita1988neuronallong,
  abstract = {In human long-term memory, ideas and concepts become associated in the learning process. No neuronal correlate for this cognitive function has so far been described, except that memory traces are thought to be localized in the cerebral cortex; the temporal lobe has been assigned as the site for visual experience because electric stimulation of this area results in imagery recall and lesions produce deficits in visual recognition of objects. We previously reported that in the anterior ventral temporal cortex of monkeys, individual neurons have a sustained activity that is highly selective for a few of the 100 coloured fractal patterns used in a visual working-memory task. Here I report the development of this selectivity through repeated trials involving the working memory. The few patterns for which a neuron was conjointly selective were frequently related to each other through stimulus-stimulus association imposed during training. The results indicate that the selectivity acquired by these cells represents a neuronal correlate of the associative long-term memory of pictures.},
  author = {Miyashita, Yasushi},
  doi = {10.1038/335817a0},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {neuroscience, memory, primate, temporal cortex, visual memory},
  month = {10},
  number = {6193},
  openalex = {W2012074763},
  pages = {817--820},
  pmid = {3185711},
  publisher = {Nature Publishing Group},
  title = {Neuronal Correlate of Visual Associative Long-Term Memory in the Primate Temporal Cortex},
  url = {https://www.nature.com/articles/335817a0},
  volume = {335},
  year = {1988}
}

@article{miyashita1988neuronalshort,
  abstract = {It has been proposed that visual-memory traces are located in the temporal lobes of the cerebral cortex, as electric stimulation of this area in humans results in recall of imagery. Lesions in this area also affect recognition of an object after a delay in both humans and monkeys, indicating a role in short-term memory of images. Single-unit recordings from the temporal cortex have shown that some neurons continue to fire when one of two or four colours are to be remembered temporarily. But neuronal responses selective to specific complex objects, including hands and faces, cease soon after the offset of stimulus presentation. These results led to the question of whether any of these neurons could serve the memory of complex objects. We report here a group of shape-selective neurons in an anterior ventral part of the temporal cortex of monkeys that exhibited sustained activity during the delay period of a visual short-term memory task. The activity was highly selective for the pictorial information to be memorized and was independent of the physical attributes such as size, orientation, colour or position of the object. These observations show that the delay activity represents the short-term memory of the categorized percept of a picture.},
  author = {Miyashita, Yasushi and Chang, Han Soo},
  doi = {10.1038/331068a0},
  issn = {0028-0836},
  journal = {Nature},
  month = {1},
  number = {6151},
  openalex = {W2962321420},
  pages = {68--70},
  pmid = {3340148},
  publisher = {Nature Publishing Group},
  title = {Neuronal Correlate of Pictorial Short-Term Memory in the Primate Temporal Cortex},
  url = {https://doi.org/10.1038/331068a0},
  volume = {331},
  year = {1988}
}

@article{amit1987statistical,
  abstract = {The Hopfield model of a neural network is studied near its saturation, i.e., when the number p of stored patterns increases with the size of the network N, as p = $α$N. The mean-field theory for this system is described in detail. The system possesses, at low $α$, both a spin-glass phase and 2p dynamically stable degenerate ferromagnetic phases. The latter have essentially full macroscopic overlaps with the memorized patterns, and provide effective associative memory, despite the spin-glass features. One of the most important results is that the network can retrieve patterns, at T = 0, with an error of less than 1.5% for $α < α_c = 0.14$. At $α_c$ the ferromagnetic (FM) retrieval states disappear discontinuously.},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, Haim},
  doi = {10.1016/0003-4916(87)90092-3},
  issn = {0003-4916},
  journal = {Annals of Physics},
  keywords = {statistical mechanics, neural networks, Hopfield model, spin-glass, mean-field theory, associative memory},
  note = {Seminal work on statistical mechanics of neural networks with 846+ citations},
  number = {1},
  openalex = {W2043014754},
  pages = {30--67},
  publisher = {Elsevier},
  title = {Statistical Mechanics of Neural Networks Near Saturation},
  url = {https://www.sciencedirect.com/science/article/abs/pii/0003491687900923},
  volume = {173},
  year = {1987}
}

@article{amit1985spinglass,
  abstract = {The Hopfield model for a neural network is studied in the limit when the number p of stored patterns increases with the size N of the network, as p=αN. It is shown that, despite its spin-glass features, the model exhibits associative memory for α<αc, αc≈0.14. This is due to the existence at low temperature of 2p dynamically stable degenerate states, each of which is almost fully correlated with one of the patterns. These states become ground states at α<0.05. The phase diagram of this rich spin-glass system is described.},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, Haim},
  doi = {10.1103/PhysRevLett.55.1530},
  issn = {0031-9007},
  journal = {Physical Review Letters},
  month = {9},
  number = {14},
  openalex = {W2080792322},
  pages = {1530--1533},
  pdf = {https://www.semanticscholar.org/paper/Storing-infinite-numbers-of-patterns-in-a-model-of-Amit-Gutfreund/b0f93cf5b155e9deccec9ddd6978fb3b4775878b},
  publisher = {American Physical Society},
  title = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
  url = {https://doi.org/10.1103/PhysRevLett.55.1530},
  volume = {55},
  year = {1985}
}

@article{amit1985storing,
  abstract = {The Hopfield model for a neural network is studied in the limit when the number $p$ of stored patterns increases with the size $N$ of the network, as $p=α N$. It is shown that, despite its spin-glass features, the model exhibits associative memory for $α<α_c$, $α_c ≈ 0.14$. This is a result of the existence at low temperature of $2p$ dynamically stable degenerate states, each of which is almost fully correlated with one of the patterns. These states become ground states at $α<0.05$. The phase diagram of this rich spin-glass is described.},
  author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, Haim},
  doi = {10.1103/PhysRevLett.55.1530},
  journal = {Physical Review Letters},
  month = {9},
  number = {14},
  openalex = {W2080792322},
  pages = {1530--1533},
  pdf = {https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.55.1530},
  publisher = {American Physical Society},
  title = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.55.1530},
  volume = {55},
  year = {1985}
}

@article{hopfield1982neural,
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention.},
  author = {Hopfield, John J.},
  doi = {10.1073/pnas.79.8.2554},
  issn = {0027-8424},
  journal = {Proceedings of the National Academy of Sciences},
  month = {4},
  note = {Introduces a recurrent neural network model for associative memory and defines an energy function, formally linking network dynamics to the statistical mechanics of Ising models. Shows how memories can be stored as attractors of the dynamics.},
  number = {8},
  openalex = {W4312702277},
  pages = {2554--2558},
  pdf = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554},
  publisher = {National Academy of Sciences},
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  url = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554},
  volume = {79},
  year = {1982}
}

@article{amari1977dynamics,
  author = {Amari, Shun-ichi},
  doi = {10.1007/BF00337259},
  issn = {0340-1200},
  journal = {Biological Cybernetics},
  number = {2},
  openalex = {W2029374903},
  pages = {77--87},
  pdf = {https://link.springer.com/article/10.1007/BF00337259},
  publisher = {Springer-Verlag},
  title = {Dynamics of Pattern Formation in Lateral-Inhibition Type Neural Fields},
  volume = {27},
  year = {1977}
}
