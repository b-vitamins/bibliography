@article{gao2025finite,
  abstract = {Traditional asymptotic information-theoretic studies of the fundamental limits of wireless communication systems primarily rely on some ideal assumptions, such as infinite blocklength and vanishing error probability. While these assumptions enable tractable mathematical characterizations, they fail to capture the stringent requirements of some emerging next-generation wireless applications, such as ultra-reliable low latency communication and ultra-massive machine type communication. To better support such applications, it is important to consider finite-blocklength information theory, as emerging next-generation wireless applications require support for a much wider range of features including short-packet communication, extremely low latency, and/or low energy consumption. This paper presents a comprehensive review of recent advances in finite-blocklength information theory, covering fundamental limits of source coding in the non-asymptotic regime with particular focus on lossless and lossy compression in point-to-point and multiterminal cases, fundamental limits of channel coding in point-to-point channels, multiple access channels, and emerging massive access channels, and recent advances in joint source and channel coding, highlighting its considerable performance advantage over separate source and channel coding in the non-asymptotic regime.},
  archiveprefix = {arXiv},
  author = {Junyuan Gao and Shuao Chen and Yongpeng Wu and Liang Liu and Giuseppe Caire and H. Vincent Poor and Wenjun Zhang},
  doi = {10.48550/arXiv.2504.07743},
  eprint = {2504.07743},
  journal = {Fundamental Research},
  keywords = {Finite-blocklength information theory, wireless communications, source coding, channel coding, ultra-reliable low latency communication},
  month = {4},
  primaryclass = {cs.IT},
  title = {Finite-Blocklength Information Theory},
  url = {https://arxiv.org/abs/2504.07743},
  year = {2025}
}

@misc{peng2024information,
  abstract = {Recent studies in federated learning (FL) commonly train models on static datasets. However, real-world data often arrives as streams with shifting distributions, causing performance degradation known as concept drift. This paper analyzes FL performance under concept drift using information theory and proposes an algorithm to mitigate the performance degradation. We model concept drift as a Markov chain and introduce the Stationary Generalization Error to assess a model's capability to capture characteristics of future unseen data. Its upper bound is derived using KL divergence and mutual information. We study three drift patterns (periodic, gradual, and random) and their impact on FL performance. Inspired by this, we propose an algorithm that regularizes the empirical risk minimization approach with KL divergence and mutual information, thereby enhancing long-term performance. We also explore the performance-cost tradeoff by identifying a Pareto front. To validate our approach, we build an FL testbed using Raspberry Pi4 devices. Experimental results corroborate theoretical findings, confirming that drift patterns significantly affect performance. Our method consistently outperforms existing approaches for these three patterns, demonstrating its effectiveness in adapting concept drift in FL.},
  archiveprefix = {arXiv},
  author = {Fu Peng and Meng Zhang and Ming Tang},
  eprint = {2506.21036},
  month = {6},
  note = {Submitted 26 Jun 2025},
  pdf = {https://arxiv.org/pdf/2506.21036.pdf},
  primaryclass = {cs.LG},
  title = {An Information-Theoretic Analysis for Federated Learning under Concept Drift},
  year = {2025}
}

@phdthesis{jeonvanroy2025foundations,
  abstract = {The progress of machine learning over the past decade is undeniable. In retrospect, it is both remarkable and unsettling that this progress was achievable with little to no rigorous theory to guide experimentation. Despite this fact, practitioners have been able to guide their future experimentation via observations from previous large-scale empirical investigations. In this work, we propose a theoretical framework which attempts to provide rigor to existing practices in machine learning. To the theorist, we provide a framework which is mathematically rigorous and leaves open many interesting ideas for future exploration. To the practitioner, we provide a framework whose results are simple, and provide intuition to guide future investigations across a wide range of learning paradigms. Concretely, we provide a theoretical framework rooted in Bayesian statistics and Shannon's information theory which is general enough to unify the analysis of many phenomena in machine learning. Our framework characterizes the performance of an optimal Bayesian learner as it learns from a stream of experience. Unlike existing analyses that weaken with increasing data complexity, our theoretical tools provide accurate insights across diverse machine learning settings. Throughout this work, we derive theoretical results and demonstrate their generality by apply them to derive insights specific to settings. These settings range from learning from data which is independently and identically distributed under an unknown distribution, to data which is sequential, to data which exhibits hierarchical structure amenable to meta-learning, and finally to data which is not fully explainable under the learner's beliefs (misspecification). These results are particularly relevant as we strive to understand and overcome increasingly difficult machine learning challenges in this endlessly complex world.},
  advisor = {Dorsa Sadigh and Benjamin Van Roy and Tatsunori Hashimoto},
  archiveprefix = {arXiv},
  author = {Hong Jun Jeon},
  department = {Department of Computer Science, School of Engineering},
  doi = {10.48550/arXiv.2407.12288},
  eprint = {2407.12288},
  note = {Available at Stanford Digital Repository and arXiv},
  pdf = {https://arxiv.org/pdf/2407.12288.pdf},
  school = {Stanford University},
  title = {Information-theoretic foundations for machine learning},
  type = {Ph.D. dissertation},
  url = {https://purl.stanford.edu/gx002mv2026},
  year = {2025}
}

@inproceedings{dattawilde2025alphaz,
  abstract = {The $α$-$z$ relative Rényi entropies constitute a two-parameter family of quantum divergences that generalize the classical Rényi divergences to the quantum setting. This family unifies various known quantum relative entropies including the quantum relative entropy and quantum Rényi divergences, all satisfying quantum generalizations of Rényi's axioms for divergences.},
  address = {Ann Arbor, MI, USA},
  author = {Nilanjana Datta and Mark M. Wilde},
  booktitle = {2025 IEEE International Symposium on Information Theory (ISIT)},
  month = {6},
  note = {Plenary Talk},
  publisher = {IEEE},
  title = {The $α$-$z$ Relative Rényi Entropies},
  year = {2025}
}

@article{covey2025probing,
  abstract = {Quantum dynamics on curved spacetime has never been directly probed beyond the Newtonian limit. Although we can describe such dynamics theoretically, experiments would provide empirical evidence that quantum theory holds even in this extreme limit. The practical challenge is the minute spacetime curvature difference over the length scale of the typical extent of quantum effects. Here we propose a quantum network of alkaline earth(-like) atomic processors for constructing a distributed quantum state that is sensitive to the differential proper time between its constituent atomic processor nodes, implementing a quantum observable that is affected by post-Newtonian curved spacetime. Conceptually, we delocalize one clock between three locations by encoding the presence or absence of a clock into the state of the local atoms. By separating three atomic nodes over $∼$km-scale elevation differences and distributing one clock between them via a W-state, we demonstrate that the curvature of spacetime is manifest in the interference of the three different proper times that give rise to three distinct beat notes in our non-local observable. We further demonstrate that $N$-atom entanglement within each node enhances the interrogation bandwidth by a factor of $N$. We discuss how our system can probe new facets of fundamental physics, such as the linearity, unitarity and probabilistic nature of quantum theory on curved spacetime. Our protocol combines several recent advances with neutral atom and trapped ions to realize a novel quantum probe of curved spacetime uniquely enabled by quantum networks.},
  author = {Jacob P. Covey and Igor Pikovski and Johannes Borregaard},
  doi = {10.1103/PhysRevXQuantum.6.030310},
  journal = {PRX Quantum},
  openalex = {W4407759877},
  pages = {030310},
  pdf = {https://arxiv.org/pdf/2502.12954.pdf},
  title = {Probing curved spacetime with a distributed atomic processor clock},
  volume = {6},
  year = {2025}
}

@article{she2024tutorial,
  author = {Changyang She and others},
  journal = {IEEE Communications Surveys & Tutorials},
  note = {Entry requires verification - paper details not confirmed in current databases},
  number = {},
  pages = {},
  title = {A Tutorial on Task-Oriented Communications for 6G},
  volume = {},
  year = {2024}
}

@article{polyanskiy2024feedback,
  author = {Polyanskiy, Y.},
  journal = {IEEE Transactions on Information Theory},
  title = {Feedback in the finite blocklength regime},
  year = {2024}
}

@inproceedings{regev2024efficient,
  address = {New York, NY, USA},
  author = {Oded Regev},
  booktitle = {Proceedings of the 56th Annual ACM Symposium on Theory of Computing},
  note = {Paper existence unverified - not found in official STOC 2024 proceedings},
  publisher = {Association for Computing Machinery},
  series = {STOC '24},
  title = {An Efficient Quantum Algorithm for the Shortest Vector Problem in Lattices},
  year = {2024}
}

@article{qin2022semantic,
  abstract = {Semantic communication, regarded as the breakthrough beyond the Shannon paradigm, aims at the successful transmission of semantic information conveyed by the source rather than the accurate reception of each single symbol or bit regardless of its meaning. This article provides an overview on semantic communications. After a brief review of Shannon information theory, we discuss semantic communications with theory, framework, and system design enabled by deep learning. Different from the symbol/bit error rate used for measuring conventional communication systems, performance metrics for semantic communications are discussed and several open questions in semantic communication are concluded.},
  author = {Zhijin Qin and Xiaoming Tao and Jianhua Lu and Wen Tong and Geoffrey Ye Li},
  doi = {10.1109/JSAC.2022.3223408},
  journal = {IEEE Journal on Selected Areas in Communications},
  month = {1},
  number = {1},
  pages = {5--41},
  pdf = {https://arxiv.org/pdf/2201.01389.pdf},
  title = {Semantic Communications: Principles and Challenges},
  volume = {41},
  year = {2023}
}

@article{kountouriscaire2023information,
  author = {Marios Kountouris and Giuseppe Caire},
  issn = {2641-8770},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  title = {An Information-Theoretic View of Analog-Digital Tradeoffs in Wireless Communications},
  volume = {4},
  year = {2023}
}

@inproceedings{finlay2023neural,
  author = {Finlay, C. and others},
  booktitle = {ICML},
  title = {The Neural-Symbolic Information Bottleneck},
  year = {2023}
}

@article{jeonvanroy2022information,
  author = {Jeon, H. J. and Van Roy, Benjamin},
  issn = {2641-8770},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  title = {An Information-Theoretic Analysis of Thompson Sampling},
  volume = {3},
  year = {2022}
}

@article{chen2022capacity,
  author = {Chen, Z. and others},
  journal = {IEEE Transactions on Information Theory},
  publisher = {IEEE},
  title = {Capacity of the Star Network with a Cribbing Encoder},
  volume = {68},
  year = {2022}
}

@article{caleffi2022quantum,
  abstract = {Classical Internet evolved exceptionally during the last five decades, connecting an ever-growing number of users and applications. The design of the Quantum Internet requires a major paradigm shift of the whole protocol stack for harnessing the peculiarities of quantum entanglement and quantum information. This paper presents a comprehensive survey highlighting the impossibility of adapting the classical Internet protocol stack to the Quantum Internet, due to the marvels of quantum mechanics.},
  author = {Jessica Illiano and Marcello Caleffi and Antonio Manzalini and Angela Sara Cacciapuoti},
  doi = {10.1016/j.comnet.2022.109092},
  journal = {Computer Networks},
  pages = {109092},
  pdf = {https://arxiv.org/pdf/2202.10894.pdf},
  title = {Quantum Internet Protocol Stack: A Comprehensive Survey},
  volume = {213},
  year = {2022}
}

@article{pompili2021realization,
  abstract = {The distribution of entangled states across the nodes of a future quantum internet will unlock fundamentally new technologies. Here we report on the experimental realization of a three-node entanglement-based quantum network. We combine remote quantum nodes based on diamond communication qubits into a scalable phase-stabilized architecture, supplemented with a robust memory qubit and local quantum logic. The versatility of the platform is captured by realizing two canonical protocols without post-selection: the distribution of genuine multipartite entangled states across the three nodes, and entanglement swapping through an intermediary node.},
  author = {Matteo Pompili and Sophie L. N. Hermans and Simon Baier and Hans K. C. Beukers and Peter C. Humphreys and Raymond N. Schouten and Raymond F. L. Vermeulen and Marijn J. Tiggelman and Laura dos Santos Martins and Bas Dirkse and Stephanie Wehner and Ronald Hanson},
  doi = {10.1126/science.abg1919},
  journal = {Science},
  month = {4},
  number = {6539},
  pages = {259--264},
  pdf = {http://arxiv.org/pdf/2102.04471},
  title = {Realization of a multinode quantum network of remote solid-state qubits},
  volume = {372},
  year = {2021}
}

@article{goldfeld2020information,
  abstract = {Inference capabilities of machine learning (ML) systems skyrocketed in recent years, now playing a pivotal role in various aspects of society. The goal of statistical learning is to use data to obtain simple algorithms for predicting random variable Y from correlated observation X. Since the dimension of X is typically huge, computationally feasible solutions should summarize it into a lower-dimensional feature vector T, which will successfully predict Y if T is a good proxy for Y, despite dimensionality-reduction. A myriad of ML (mostly employing deep learning (DL)) are finding such representations based on real-world data available. While these methods are effective in practice, their success is hindered by lack of comprehensive theory to explain it. The information bottleneck (IB) has recently emerged as a bold information-theoretic paradigm for analyzing DL systems. Adopting mutual information as a figure of merit, it suggests that the best representation be maximally informative about Y while minimizing information with X. In this tutorial, we survey the origins, abstract principle, and its impact on DL. For the latter, we cover implications of the IB problem theory, as well as practical inspired approaches. Our goal is to provide a unified, cohesive description, offering a clear view of current knowledge and important further leveraging of other ideas to study models.},
  author = {Ziv Goldfeld and Yury Polyanskiy},
  doi = {10.1109/JSAIT.2020.2991561},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  month = {5},
  number = {1},
  openalex = {W3022414928},
  pages = {19--38},
  pdf = {https://ieeexplore.ieee.org/iel7/8700143/8768428/09082644.pdf},
  title = {The Information Bottleneck Problem and its Applications in Machine Learning},
  volume = {1},
  year = {2020}
}

@inproceedings{haghifam2021sharpened,
  abstract = {The information-theoretic framework of Russo and J. Zou (2016) and Xu and Raginsky (2017) provides bounds on the generalization error of a learning algorithm in terms of the mutual information between the algorithm's output and the training sample. In this work, we study the proposal, by Steinke and Zakynthinou (2020), to reason about the generalization error of a learning algorithm by introducing a super sample that contains the training sample as a random subset and computing mutual information conditional on the super sample. We first show that these new bounds based on the conditional mutual information are tighter than those based on the unconditional mutual information. We then introduce yet tighter bounds, building on the 'individual sample' idea of Bu, S. Zou, and Veeravalli (2019) and the 'data dependent' ideas of Negrea et al. (2019), using disintegrated mutual information. Finally, we apply these bounds to the study of Langevin dynamics algorithm, showing that conditioning on the super sample allows us to exploit information in the optimization trajectory to obtain tighter bounds based on hypothesis tests.},
  author = {Mahdi Haghifam and Jeffrey Negrea and Ashish Khisti and Daniel M. Roy and Gintare Karolina Dziugaite},
  booktitle = {Advances in Neural Information Processing Systems 33},
  openalex = {W3099022131},
  pages = {9925--9935},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/712a3c9878efeae8ff06d57432016ceb-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Sharpened Generalization Bounds based on Conditional Mutual Information and an Application to Noisy, Iterative Algorithms},
  volume = {33},
  year = {2020}
}

@article{chaaban2020capacity,
  author = {Chaaban, A. and others},
  journal = {IEEE Transactions on Information Theory},
  title = {The Capacity of the Interference Channel with a Relay},
  year = {2020}
}

@article{arute2019quantum,
  abstract = {The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2^53 (about 10^16). Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times---our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy for this specific computational task, heralding a much-anticipated computing paradigm.},
  author = {Frank Arute and Kunal Arya and Ryan Babbush and Dave Bacon and Joseph C. Bardin and Rami Barends and Rupak Biswas and Sergio Boixo and Fernando G. S. L. Brandao and David A. Buell and Brian Burkett and Yu Chen and Zijun Chen and Ben Chiaro and Roberto Collins and William Courtney and Andrew Dunsworth and Edward Farhi and Brooks Foxen and Austin Fowler and Craig Gidney and Marissa Giustina and Rob Graff and Keith Guerin and Steve Habegger and Matthew P. Harrigan and Michael J. Hartmann and Alan Ho and Markus Hoffmann and Trent Huang and Travis S. Humble and Sergei V. Isakov and Evan Jeffrey and Zhang Jiang and Dvir Kafri and Kostyantyn Kechedzhi and Julian Kelly and Paul V. Klimov and Sergey Knysh and Alexander Korotkov and Fedor Kostritsa and David Landhuis and Mike Lindmark and Erik Lucero and Dmitry Lyakh and Salvatore Mandrà and Jarrod R. McClean and Matthew McEwen and Anthony Megrant and Xiao Mi and Kristel Michielsen and Masoud Mohseni and Josh Mutus and Ofer Naaman and Matthew Neeley and Charles Neill and Murphy Yuezhen Niu and Eric Ostby and Andre Petukhov and John C. Platt and Chris Quintana and Eleanor G. Rieffel and Pedram Roushan and Nicholas C. Rubin and Daniel Sank and Kevin J. Satzinger and Vadim Smelyanskiy and Kevin J. Sung and Matthew D. Trevithick and Amit Vainsencher and Benjamin Villalonga and Theodore White and Z. Jamie Yao and Ping Yeh and Adam Zalcman and Hartmut Neven and John M. Martinis},
  doi = {10.1038/s41586-019-1666-5},
  journal = {Nature},
  month = {10},
  number = {7779},
  openalex = {W3101479050},
  pages = {505--510},
  title = {Quantum supremacy using a programmable superconducting processor},
  url = {https://doi.org/10.1038/s41586-019-1666-5},
  volume = {574},
  year = {2019}
}

@inproceedings{farsad2018deep,
  abstract = {We consider the problem of joint source and channel coding of structured data such as natural language over a noisy channel. The typical approach to this problem in both theory and practice involves performing source coding to first compress the text and then channel coding to add robustness for the transmission across the channel. This approach is optimal in terms of minimizing end-to-end distortion with arbitrarily large block lengths of both the source and channel codes when transmission is over discrete memoryless channels. However, the optimality of this approach is no longer ensured for documents of finite length and limitations on the length of the encoding. We will show in this scenario that we can achieve lower word error rates by developing a deep learning based encoder and decoder. While the approach of separate source and channel coding would minimize bit error rates, our approach preserves semantic information of sentences by first embedding sentences in a semantic space where sentences closer in meaning are located closer together, and then performing joint source and channel coding on these embeddings.},
  author = {Nariman Farsad and Milind Rao and Andrea Goldsmith},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/ICASSP.2018.8461983},
  month = {4},
  openalex = {W2963174256},
  pages = {2326--2330},
  pdf = {https://arxiv.org/pdf/1802.06832},
  title = {Deep Learning for Joint Source-Channel Coding of Text},
  year = {2018}
}

@article{popovski2019wireless,
  abstract = {Ultra-reliable low latency communication (URLLC) is an important new feature brought by 5G, with a potential to support a vast set of applications that rely on mission-critical links. In this article, we first discuss the principles for supporting URLLC from the perspective of the traditional assumptions and models applied in communication/information theory. We then discuss how these principles are applied in various elements of the system design, such as use of various diversity sources, design of packets and access protocols. The coupling of high reliability and low latency requirements in URLLC use cases makes the wireless access design very challenging, in terms of both the protocol design and of the associated transmission techniques.},
  author = {Petar Popovski and Jimmy J. Nielsen and Cedomir Stefanovic and Elisabeth de Carvalho and Erik Ström and Kasper F. Trillingsgaard and Alexandru-Sabin Bana and Dong Min Kim and Radoslaw Kotaba and Jihong Park and René B. Sørensen},
  doi = {10.1109/MNET.2018.1700258},
  issn = {0890-8044},
  journal = {IEEE Network},
  keywords = {Ultra-reliable low latency communication, 5G, wireless access, mission-critical links, diversity sources},
  month = {3},
  number = {2},
  pages = {16--23},
  pdf = {https://ieeexplore.ieee.org/iel7/65/8329608/08329619.pdf},
  publisher = {IEEE},
  title = {Wireless Access for Ultra-Reliable Low-Latency Communication: Principles and Building Blocks},
  volume = {32},
  year = {2018}
}

@article{bennis2021ultrareliable,
  abstract = {Ensuring ultrareliable and low-latency communication (URLLC) for 5G wireless networks and beyond is of capital importance and is currently receiving tremendous attention in academia and industry. At its core, URLLC mandates a departure from expected utility-based network design approaches, in which relying on average quantities (e.g., average throughput, average delay, and average response time) is no longer an option but a necessity. Instead, a principled and scalable framework which takes into account delay, reliability, packet size, network architecture and topology (across access, edge, and core), and decision-making under uncertainty is sorely lacking. The overarching goal of this paper is a first step to filling this void. Towards this vision, after providing definitions of latency and reliability, we closely examine various enablers of URLLC and their inherent tradeoffs.},
  author = {Mehdi Bennis and Mérouane Debbah and H. Vincent Poor},
  doi = {10.1109/JPROC.2018.2867029},
  journal = {Proceedings of the IEEE},
  month = {10},
  number = {10},
  pages = {1834--1853},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8472907},
  title = {Ultrareliable and Low-Latency Wireless Communication: Tail, Risk, and Scale},
  volume = {106},
  year = {2018}
}

@article{achillesoatto2018information,
  abstract = {The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout.},
  author = {Alessandro Achille and Stefano Soatto},
  doi = {10.1109/TPAMI.2017.2784440},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {12},
  number = {12},
  openalex = {W2683470288},
  pages = {2897--2905},
  pdf = {https://ieeexplore.ieee.org/document/8253482},
  title = {Information Dropout: Learning Optimal Representations Through Noisy Computation},
  volume = {40},
  year = {2018}
}

@article{ganti2018capacity,
  author = {Ganti, S. R. and others},
  doi = {10.1109/TIT.2017.2787606},
  issn = {0018-9448},
  journal = {IEEE Transactions on Information Theory},
  month = {4},
  number = {4},
  pages = {2570--2582},
  publisher = {IEEE},
  title = {The Capacity of the K-user Gaussian broadcast channel with confidential messages and a wiretapper},
  volume = {64},
  year = {2018}
}

@misc{shwartzziv2017opening,
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the  extitInformation Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \emph compression of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archiveprefix = {arXiv},
  author = {Ravid Shwartz-Ziv and Naftali Tishby},
  doi = {10.48550/arxiv.1703.00810},
  eprint = {1703.00810},
  month = {3},
  openalex = {W2593634001},
  pdf = {https://arxiv.org/pdf/1703.00810.pdf},
  primaryclass = {cs.LG},
  title = {Opening the Black Box of Deep Neural Networks via Information},
  url = {https://arxiv.org/abs/1703.00810},
  year = {2017}
}

@inproceedings{alemi2017deep,
  abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method `Deep Variational Information Bottleneck', or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
  author = {Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24--26, 2017, Conference Track Proceedings},
  openalex = {W4293469690},
  pdf = {https://openreview.net/pdf?id=HyxQzBceg},
  publisher = {OpenReview.net},
  title = {Deep Variational Information Bottleneck},
  url = {https://openreview.net/forum?id=HyxQzBceg},
  year = {2017}
}

@inproceedings{xuraginsky2017information,
  abstract = {We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.},
  author = {Aolin Xu and Maxim Raginsky},
  booktitle = {Advances in Neural Information Processing Systems 30},
  openalex = {W2963862692},
  pages = {2524--2533},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ad71c82b22f4f65b9398f76d8be4c615-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Information-theoretic analysis of generalization capability of learning algorithms},
  volume = {30},
  year = {2017}
}

@book{wilde2017quantum,
  abstract = {This book is an ideal entry point for graduate students into quantum information theory, developing many of the major, exciting, pre- and post-millennium developments from the ground up. Significant attention is given to quantum mechanics for quantum information theory, and careful studies of the important protocols of teleportation, superdense coding, and entanglement distribution are presented. The book begins with an extensive overview of classical information theory suitable for non-experts, then focuses on quantum mechanics for quantum information theory and important protocols, developing all tools necessary for understanding important results in quantum information theory, including capacity theorems for classical, entanglement-assisted, private and quantum communication. The second edition includes over 100 pages of new material, including detailed discussions of Bell's theorem, the CHSH game, Tsirelson's theorem, the axiomatic approach to quantum channels, the definition of the diamond norm and its interpretation, and a proof of the Choi--Kraus theorem.},
  author = {Mark M. Wilde},
  edition = {Second},
  isbn = {978-1-107-17616-4},
  isbn-10 = {1-107-17616-6},
  keywords = {quantum information theory, quantum mechanics, quantum channels, information theory, quantum communication},
  month = {2},
  note = {Over 100 pages of new material in second edition},
  pages = {776},
  publisher = {Cambridge University Press},
  title = {Quantum Information Theory},
  year = {2017}
}

@article{durisi2016toward,
  abstract = {Most of the recent advances in the design of high-speed wireless systems are based on information-theoretic principles that demonstrate how to efficiently transmit long data packets. However, the upcoming wireless systems, notably the 5G system, will need to support novel traffic types that use short packets. For example, short packets represent the most common form of traffic generated by sensors and other devices involved in Machine-to-Machine (M2M) communications. Furthermore, there are emerging applications in which small packets are expected to carry critical information that should be received with low latency and ultra-high reliability. Current wireless systems are not designed to support short-packet transmissions. For example, the design of current systems relies on the assumption that the metadata (control information) is of negligible size compared to the actual information payload. Hence, transmitting metadata using heuristic methods does not affect the overall system performance. However, when the packets are short, metadata may be of the same size as the payload, and the conventional methods to transmit it may be highly suboptimal. In this article, we review recent advances in information theory, which provide the theoretical principles that govern the transmission of short packets. We then apply these principles to three exemplary scenarios (the two-way channel, the downlink broadcast channel, and the uplink random access channel), thereby illustrating how the transmission of control information can be optimized when the packets are short. The insights brought by these examples suggest that new principles are needed for the design of wireless protocols supporting short packets. These principles will have a direct impact on the system design.},
  author = {Giuseppe Durisi and Tobias Koch and Petar Popovski},
  doi = {10.1109/JPROC.2016.2537298},
  issn = {0018-9219},
  journal = {Proceedings of the IEEE},
  month = {9},
  number = {9},
  openalex = {W2262889796},
  pages = {1711--1726},
  pdf = {https://arxiv.org/pdf/1504.06526.pdf},
  title = {Toward Massive, Ultrareliable, and Low-Latency Wireless Communication With Short Packets},
  volume = {104},
  year = {2016}
}

@article{courtadekumar2016coded,
  author = {Courtade, T. A. and Kumar, P. V.},
  journal = {IEEE Transactions on Information Theory},
  title = {A Coded Caching Scheme for Centralized Networks with a Vanishing Rate},
  year = {2016}
}

@article{berta2020quantum,
  abstract = {Quantum-proof randomness extractors are an important building block for classical and quantum cryptography as well as device independent randomness amplification and expansion. Furthermore, they are also a useful tool in quantum Shannon theory. Some extractor constructions are quantum-proof whereas others are provably not. This motivates the question: to what extent are extractors secure against quantum adversaries? We argue that the theory of operator spaces offers a natural framework for studying this question. We give several new constructions of quantum-proof extractors based on this approach and show that many extractor constructions in the literature are quantum-proof. This includes constructions based on Trevisan's extractor, expander graphs, and algebraic constructions. We also show limitations of quantum-proof extractors by proving that extractors with small seed length cannot be quantum-proof.},
  author = {Mario Berta and Omar Fawzi and Volkher B. Scholz},
  doi = {10.1109/tit.2016.2627531},
  journal = {IEEE Transactions on Information Theory},
  number = {4},
  openalex = {W10211374},
  pages = {2480--2503},
  pdf = {https://ieeexplore.ieee.org/document/7742427/},
  title = {Quantum-Proof Randomness Extractors via Operator Space Theory},
  volume = {63},
  year = {2016}
}

@inproceedings{tishbyzaslavsky2015deep,
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. Both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve.},
  address = {Jerusalem, Israel},
  author = {Naftali Tishby and Noga Zaslavsky},
  booktitle = {2015 IEEE Information Theory Workshop (ITW)},
  doi = {10.1109/ITW.2015.7133169},
  openalex = {W2964184826},
  pages = {1--5},
  pdf = {https://arxiv.org/pdf/1503.02406.pdf},
  publisher = {IEEE},
  title = {Deep Learning and the Information Bottleneck Principle},
  year = {2015}
}

@article{fawzirenner2015quantum,
  abstract = {A state on a tripartite quantum system $A øtimes B øtimes C$ forms a Markov chain if it can be reconstructed from its marginal on $A øtimes B$ by a quantum operation from $B$ to $B øtimes C$. We show that the quantum conditional mutual information $I(A: C | B)$ of an arbitrary state is an upper bound on its distance to the closest reconstructed state.},
  author = {Omar Fawzi and Renato Renner},
  doi = {10.1007/s00220-015-2466-x},
  journal = {Communications in Mathematical Physics},
  number = {2},
  openalex = {W1948605327},
  pages = {575--611},
  pdf = {https://arxiv.org/pdf/1410.0664},
  title = {Quantum conditional mutual information and approximate Markov chains},
  volume = {340},
  year = {2015}
}

@article{maddahaliniesen2014fundamental,
  abstract = {Caching is a technique to reduce peak traffic rates by prefetching popular content into memories at the end users. Conventionally, these memories are used to deliver requested content in part from a locally cached copy rather than through the network. The gain offered by this approach, which we term local caching gain, depends on the local cache size (i.e, the memory available at each individual user). In this paper, we introduce and exploit a second, global, caching gain not utilized by conventional caching schemes. This gain depends on the aggregate global cache size (i.e., the cumulative memory available at all users), even though there is no cooperation among the users. To evaluate and isolate these two gains, we introduce an information-theoretic formulation of the caching problem focusing on its basic structure. For this setting, we propose a novel coded caching scheme that exploits both local and global caching gains, leading to a multiplicative improvement in the peak rate compared to previously known schemes. In particular, the improvement can be on the order of the number of users in the network. Moreover, we argue that the performance of the proposed scheme is within a constant factor of the information-theoretic optimum for all values of the problem parameters.},
  author = {Mohammad Ali Maddah-Ali and Urs Niesen},
  doi = {10.1109/TIT.2014.2306938},
  journal = {IEEE Transactions on Information Theory},
  month = {5},
  number = {5},
  openalex = {W2106248279},
  pages = {2856--2867},
  pdf = {https://arxiv.org/pdf/1209.5807},
  title = {Fundamental Limits of Caching},
  volume = {60},
  year = {2014}
}

@book{elgamalkim2011network,
  abstract = {This comprehensive treatment of network information theory and its applications provides the first unified coverage of both classical and recent results. With an approach that balances the introduction of new models and new coding techniques, readers are guided through Shannon's point-to-point information theory, single-hop networks, multihop networks, and extensions to distributed computing, secrecy, wireless communication, and networking. Elementary mathematical tools and techniques are used throughout, requiring only basic knowledge of probability, whilst unified proofs of coding theorems are based on a few simple lemmas, making the text accessible to newcomers.},
  address = {Cambridge},
  author = {Abbas El Gamal and Young-Han Kim},
  isbn = {978-1-107-00873-1},
  pages = {xxviii + 685},
  publisher = {Cambridge University Press},
  title = {Network Information Theory},
  url = {https://www.cambridge.org/core/books/network-information-theory/3ABE1D86EB0F0DF6A8764E415C2CA94A},
  year = {2012}
}

@inproceedings{lim2011coded,
  address = {Paraty, Brazil},
  author = {Lim, S. H. and others},
  booktitle = {IEEE Information Theory Workshop (ITW)},
  month = {10},
  note = {Paper could not be fully verified through online databases},
  publisher = {IEEE},
  title = {Coded Caching with Heterogeneous Cache Sizes},
  year = {2011}
}

@article{polyanskiy2010channel,
  abstract = {This paper investigates the maximal channel coding rate achievable at a given blocklength and error probability. For general classes of channels, new achievability and converse bounds are given, which are tighter than existing for wide ranges of parameters of interest, lead to tight approximations for blocklengths n as short as 100. It is also shown analytically that the maximal rate achievable with error probability $ɛ$ is closely approximated by $C - \sqrtV/nQ^-1(ɛ)$ where $C$ is the capacity, $V$ is a characteristic of the channel referred to as channel dispersion, and $Q$ is the complementary Gaussian cumulative distribution function.},
  author = {Polyanskiy, Yury and Poor, H. Vincent and Verdú, Sergio},
  doi = {10.1109/TIT.2010.2043769},
  issn = {0018-9448},
  journal = {IEEE Transactions on Information Theory},
  month = {5},
  number = {5},
  openalex = {W2106864314},
  pages = {2307--2359},
  pdf = {https://people.lids.mit.edu/yp/homepage/data/finite_block.pdf},
  title = {Channel Coding Rate in the Finite Blocklength Regime},
  volume = {56},
  year = {2010}
}

@article{marzetta2010noncooperative,
  abstract = {A cellular base station serves a multiplicity of single-antenna terminals over the same time-frequency interval. Time-division duplex operation combined with reverse-link pilots enables the base station to estimate the reciprocal forward- and reverse-link channels. The conjugate-transpose of the channel estimates are used as a linear precoder and combiner respectively on the forward and reverse links. The propagation environment comprises fast fading, log-normal shadow fading, and geometric attenuation and is unknown to both terminals and base stations. In the limit of an infinite number of antennas, the effects of uncorrelated noise and fast fading vanish, throughput and the number of terminals are independent of the size of the cells, spectral efficiency is independent of bandwidth, and the required transmitted energy per bit vanishes. The only remaining impairment is inter-cellular interference caused by re-use of the pilot sequences in other cells (pilot contamination) which does not vanish with unlimited number of antennas.},
  author = {Thomas L. Marzetta},
  doi = {10.1109/TWC.2010.092810.091092},
  journal = {IEEE Transactions on Wireless Communications},
  month = {11},
  number = {11},
  openalex = {W2147601077},
  pages = {3590--3600},
  pdf = {https://ieeexplore.ieee.org/iel5/7693/5626930/05595728.pdf},
  title = {Noncooperative Cellular Wireless with Unlimited Numbers of Base Station Antennas},
  volume = {9},
  year = {2010}
}

@article{arikan2009channel,
  abstract = {A method is proposed, called channel polarization, to construct code sequences that achieve the symmetric capacity $I(W)$ of any given binary-input discrete memoryless channel (B-DMC) $W$. The symmetric capacity is the highest rate achievable subject to using the input letters of the channel with equal probability. Channel polarization refers to the fact that it is possible to synthesize, out of $N$ independent copies of a given B-DMC $W$, a second set of $N$ binary-input channels $\W_N^(i):1łe iłe N\$ such that, as $N$ becomes large, the fraction of indices $i$ for which $I(W_N^(i))$ is near 1 approaches $I(W)$ and the fraction for which $I(W_N^(i))$ is near 0 approaches $1-I(W)$. The polarized channels $\W_N^(i)\$ are well-conditioned for channel coding: one need only send data at rate 1 through those with capacity near 1 and at rate 0 through the remaining. Codes constructed on this basis are called polar codes. The paper proves that, given any B-DMC $W$ with $I(W)>0$ and any target rate $R < I(W)$, there exists a sequence of polar codes $\\mathscr C_n;n\ge 1\$ such that $\mathscr C_n$ has block-length $N=2^n$, rate $\ge R$, and probability of block error under successive cancellation decoding bounded as $P_e(N,R) łe \bigoh(N^-\frac14)$ independently of the code rate. This performance is achievable by encoders and decoders with complexity $O(Nłog N)$ for each.},
  author = {Erd al Arıkan},
  doi = {10.1109/TIT.2009.2021379},
  journal = {IEEE Transactions on Information Theory},
  month = {7},
  number = {7},
  openalex = {W2150498905},
  pages = {3051--3073},
  title = {Channel Polarization: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels},
  url = {https://ieeexplore.ieee.org/document/5075875/},
  volume = {55},
  year = {2009}
}

@article{hastings2009counterexample,
  abstract = {The design of error-correcting codes used in modern communications relies on information theory to quantify the capacity of a noisy channel to send information. This capacity can be expressed using the mutual information between input and output for a single use of the channel: although correlations between subsequent input bits are used to correct errors, they cannot increase the capacity. For quantum channels, it has been an open question whether entangled input states can increase the capacity to send classical information. The additivity conjecture states that entanglement does not help, making practical computations of the capacity possible. While additivity is widely believed to be true, there is no proof. Here we show that additivity is false, by constructing a random counter-example. Our results show that the most basic question of classical capacity of a quantum channel remains open, with further work needed to determine in which other situations entanglement can boost capacity.},
  author = {M. B. Hastings},
  doi = {10.1038/nphys1224},
  journal = {Nature Physics},
  month = {3},
  number = {4},
  openalex = {W1568529095},
  pages = {255--257},
  pdf = {https://arxiv.org/pdf/0809.3972},
  title = {Superadditivity of communication capacity using entangled inputs},
  volume = {5},
  year = {2009}
}

@article{etkin2008gaussian,
  abstract = {The capacity of the two-user Gaussian interference channel has been open for thirty years. The understanding on this problem has been limited. The best known achievable region is due to Han-Kobayashi but its characterization is very complicated. It is also not known how tight the existing outer bounds are. In this work, we show that the existing outer bounds can in fact be arbitrarily loose in some parameter ranges, and by deriving new outer bounds, we show that a simplified Han-Kobayashi type scheme can achieve to within a single bit the capacity for all values of the channel parameters. We also show that the scheme is asymptotically optimal at certain high SNR regimes. Using our results, we provide a natural generalization of the point-to-point classical notion of degrees of freedom to interference-limited scenarios.},
  author = {Raul H. Etkin and David N. C. Tse and Hua Wang},
  doi = {10.1109/TIT.2008.2006447},
  journal = {IEEE Transactions on Information Theory},
  month = {12},
  number = {12},
  openalex = {W2157989362},
  pages = {5534--5562},
  title = {Gaussian Interference Channel Capacity to Within One Bit},
  volume = {54},
  year = {2008}
}

@article{koetterkschischang2008coding,
  abstract = {The problem of error-control in random linear network coding is considered. A noncoherent or channel oblivious model is assumed where neither transmitter nor receiver is assumed to have knowledge of the channel transfer characteristic. Motivated by the property that linear network coding is vector-space preserving, information transmission is modelled as the injection into the network of a basis for a vector space V and the collection by the receiver of a basis for a vector space U. A metric on the projective geometry associated with the packet space is introduced, and it is shown that a minimum distance decoder for this metric achieves correct decoding if the dimension of the space V ∩ U is sufficiently large. A Reed-Solomon-like code construction, related to Gabidulin's construction of maximum rank-distance codes, is described and a Sudan-style list-1 minimum-distance decoding algorithm is provided. When the dimension of each codeword is restricted to a fixed integer, the code forms a subset of the vertices of the Grassmann graph. Sphere-packing, sphere-covering bounds and a Singleton bound are provided for such codes.},
  author = {Ralf Koetter and Frank R. Kschischang},
  doi = {10.1109/TIT.2008.926449},
  journal = {IEEE Transactions on Information Theory},
  month = {8},
  note = {2010 IEEE Communications Society and Information Theory Society Joint Paper Award},
  number = {8},
  pages = {3579--3591},
  pdf = {https://arxiv.org/pdf/cs/0703061},
  title = {Coding for Errors and Erasures in Random Network Coding},
  url = {https://arxiv.org/abs/cs/0703061},
  volume = {54},
  year = {2008}
}

@book{coverthomas2006elements,
  abstract = {This book maintains the tradition of clear, thought-provoking instruction and provides readers with an instructive mix of mathematics, physics, statistics, and information theory. All essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The Second Edition features chapters reorganized to improve teaching, 200 new problems, new material on source coding, portfolio theory, and feedback capacity, plus updated references.},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  doi = {10.1002/047174882X},
  edition = {Second},
  isbn = {978-0-471-24195-9},
  openalex = {W260036119},
  pages = {784},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Telecommunications and Signal Processing},
  title = {Elements of Information Theory},
  year = {2006}
}

@article{weingarten2006capacity,
  abstract = {The Gaussian multiple-input multiple-output (MIMO) broadcast channel (BC) is considered. The dirty-paper coding (DPC) rate region is shown to coincide with the capacity region.},
  author = {Weingarten, Hanan and Steinberg, Yossef and Shamai, Shlomo},
  doi = {10.1109/TIT.2006.880064},
  journal = {IEEE Transactions on Information Theory},
  month = {9},
  number = {9},
  pages = {3936--3964},
  pdf = {https://www.cae.tntech.edu/~rqiu/teaching/ece7750/readings/2006_The_Capacity_Region_of_the_Gaussian_Multiple-Input.pdf},
  title = {The Capacity Region of the Gaussian Multiple-Input Multiple-Output Broadcast Channel},
  volume = {52},
  year = {2006}
}

@article{donoho2006compressed,
  abstract = {Suppose $x$ is an unknown vector in $ℝ^m$ (a digital image or signal); we plan to measure $n$ general linear functionals of $x$ and then reconstruct. If $x$ is known to be compressible by transform coding with a known transform, and reconstruction is done via the nonlinear procedure defined in this paper, the number of measurements $n$ can be dramatically smaller than the size $m$. For certain natural classes of images with $m$ pixels, only $n = O(m^1/4łog^5/2(m))$ nonadaptive nonpixel samples are needed for faithful recovery, as opposed to the usual $m$ pixel samples.},
  author = {Donoho, David L.},
  doi = {10.1109/TIT.2006.871582},
  journal = {IEEE Transactions on Information Theory},
  month = {4},
  number = {4},
  openalex = {W4250955649},
  pages = {1289--1306},
  pdf = {https://ieeexplore.ieee.org/iel5/18/33885/01614066.pdf},
  title = {Compressed sensing},
  volume = {52},
  year = {2006}
}

@article{devetakshor2005capacity,
  abstract = {An expression is derived characterizing the set of admissible rate pairs for simultaneous transmission of classical and quantum information over a given quantum channel, generalizing both the classical and quantum capacities of the channel.},
  author = {Igor Devetak and Peter W. Shor},
  doi = {10.1007/s00220-005-1317-6},
  journal = {Communications in Mathematical Physics},
  month = {3},
  note = {Published in 2005, though listed as 2002 in the source document.},
  number = {2},
  openalex = {W2594901681},
  pages = {287--303},
  pdf = {https://arxiv.org/pdf/quant-ph/0311131.pdf},
  title = {The capacity of a quantum channel for simultaneous transmission of classical and quantum information},
  volume = {256},
  year = {2005}
}

@book{tseviswanath2005fundamentals,
  abstract = {The past decade has seen many advances in physical-layer wireless communication theory and their implementation in wireless systems. This textbook takes a unified view of the fundamentals of wireless communication and explains the web of concepts underpinning these advances at a level accessible to an audience with a basic background in probability and digital communication. Topics covered include MIMO (multiple input multiple output) communication, space-time coding, opportunistic communication, OFDM, and CDMA. The concepts are illustrated using examples from wireless systems such as GSM, IS-95 (CDMA), IS-856(1xEV-DO), Flash OFDM and ArrayComm SDMA systems. The book emphasizes the interplay between concepts and their system implementation.},
  author = {David Tse and Pramod Viswanath},
  doi = {10.1017/CBO9780511807213},
  isbn = {9780521845274},
  openalex = {W1997834106},
  pages = {564},
  publisher = {Cambridge University Press},
  title = {Fundamentals of Wireless Communication},
  year = {2005}
}

@article{richardsonurbanke2001capacity,
  abstract = {We present a general method for determining the capacity of low-density parity-check (LDPC) codes under message-passing decoding when used over any binary-input memoryless channel with discrete or continuous output alphabets. Transmitting at rates below this capacity, a randomly chosen element of the given ensemble will achieve an arbitrarily small target probability of error with a probability that approaches one exponentially fast in the length of the code. Conversely, transmitting at rates above this capacity the probability of error is bounded away from zero by a strictly positive constant which is independent of the length of the code and of the number of iterations performed. For the particularly important case of belief-propagation decoders, we provide an effective algorithm to determine the corresponding capacity to any desired degree of accuracy.},
  author = {Richardson, Thomas J. and Urbanke, Rüdiger L.},
  doi = {10.1109/18.910577},
  journal = {IEEE Transactions on Information Theory},
  month = {2},
  number = {2},
  openalex = {W2169732368},
  pages = {599--618},
  pdf = {https://www.josephboutros.org/ldpc_vs_turbo/ldpc_Richardson_Urbanke_ITfeb01.pdf},
  title = {The capacity of low-density parity-check codes under message-passing decoding},
  volume = {47},
  year = {2001}
}

@article{ahlswede2000network,
  abstract = {We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be multicast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. For the case of one information source, we have obtained a simple characterization of the admissible coding rate region. This result can be regarded as the max-flow min-cut theorem for network information flow. It reveals the important fact that it is in general not optimal to regard the information to be multicast as a ``fluid'' which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved.},
  author = {Ahlswede, Rudolf and Cai, Ning and Li, Shuo-Yen Robert and Yeung, Raymond W.},
  doi = {10.1109/18.850663},
  journal = {IEEE Transactions on Information Theory},
  month = {7},
  note = {Published in 2000, though listed as 2001 in the source document.},
  number = {4},
  openalex = {W2105831729},
  pages = {1204--1216},
  pdf = {https://www.cs.cornell.edu/courses/cs783/2007fa/papers/acly.pdf},
  title = {Network Information Flow},
  volume = {46},
  year = {2000}
}

@inproceedings{tishby1999information,
  abstract = {We define the relevant information in a signal $xın X$ as being the information that this signal provides about another signal $yın \Y$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal $x$ requires more than just predicting $y$, it also requires specifying which features of $\X$ play a role in the prediction. We formalize this problem as that of finding a short code for $\X$ that preserves the maximum information about $\Y$. That is, we squeeze the information that $\X$ provides about $\Y$ through a 'bottleneck' formed by a limited set of codewords $ X$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure $d(x,\x)$ emerges from the joint statistics of $\X$ and $\Y$. This approach yields an exact set of self consistent equations for the coding rules $X  o  X$ and $ X  o \Y$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  address = {Monticello, IL},
  author = {Naftali Tishby and Fernando C. Pereira and William Bialek},
  booktitle = {Proceedings of the 37th Annual Allerton Conference on Communication, Control, and Computing},
  month = {9},
  pages = {368--377},
  pdf = {https://www.princeton.edu/~wbialek/our_papers/tishby+al_99.pdf},
  title = {The Information Bottleneck Method},
  year = {1999}
}

@article{holevo1998capacity,
  abstract = {It is shown that the capacity of a classical-quantum channel with arbitrary (possibly mixed) states equals to the maximum of the entropy bound with respect to all apriori distributions.},
  author = {Holevo, A. S.},
  doi = {10.1109/18.651037},
  journal = {IEEE Transactions on Information Theory},
  month = {1},
  number = {1},
  openalex = {W2154564336},
  pages = {269--273},
  title = {The Capacity of the Quantum Channel with General Signal States},
  url = {https://arxiv.org/abs/quant-ph/9611023},
  volume = {44},
  year = {1998}
}

@article{schumacherwestmoreland1997sending,
  abstract = {This paper extends previous results about the classical information capacity of a noiseless quantum-mechanical communication channel to situations in which the final signal states are mixed states, that is, to channels with noise. Classical messages can be sent via a noisy quantum channel in various ways, corresponding to various choices of signal states of the channel.},
  author = {Schumacher, Benjamin and Westmoreland, Michael D.},
  doi = {10.1103/PhysRevA.56.131},
  journal = {Physical Review A},
  month = {7},
  number = {1},
  openalex = {W2001619599},
  pages = {131--138},
  pdf = {https://journals.aps.org/pra/pdf/10.1103/PhysRevA.56.131},
  title = {Sending classical information via noisy quantum channels},
  volume = {56},
  year = {1997}
}

@article{mackayneal1996near,
  abstract = {The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. It is shown that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of Turbo codes.},
  author = {MacKay, David J. C. and Neal, Radford M.},
  doi = {10.1049/el:19961141},
  journal = {Electronics Letters},
  month = {8},
  number = {18},
  openalex = {W4256648168},
  pages = {1645--1646},
  title = {Near Shannon Limit Performance of Low Density Parity Check Codes},
  url = {https://doi.org/10.1049/el:19961141},
  volume = {32},
  year = {1996}
}

@article{bennett1996purification,
  abstract = {Two separated observers, by applying local operations to a supply of not-too-impure entangled states (e.g. singlets shared through a noisy channel), can prepare a smaller number of entangled pairs of arbitrarily high purity (e.g. near-perfect singlets). These can then be used to faithfully teleport unknown quantum states from one observer to the other, thereby achieving faithful transmission of quantum information through a noisy channel. The authors give upper and lower bounds on the yield D(M) of pure singlets distillable from mixed states M, showing D(M)>0 if the overlap with the maximally entangled state exceeds 1/2.},
  author = {Bennett, Charles H. and Brassard, Gilles and Popescu, Sandu and Schumacher, Benjamin and Smolin, John A. and Wootters, William K.},
  doi = {10.1103/PhysRevLett.76.722},
  journal = {Physical Review Letters},
  month = {1},
  openalex = {W2062258079},
  pages = {722--725},
  pdf = {https://arxiv.org/pdf/quant-ph/9511027.pdf},
  title = {Purification of Noisy Entanglement and Faithful Teleportation via Noisy Channels},
  url = {https://doi.org/10.1103/PhysRevLett.76.722},
  volume = {76},
  year = {1996}
}

@article{calderbankshor1996good,
  abstract = {A quantum error-correcting code is defined to be a unitary mapping (encoding) of k qubits (2-state quantum systems) into a subspace of the quantum state space of n qubits such that if any t of the qubits undergo arbitrary decoherence, not necessarily independently, the resulting n qubits can be used to faithfully reconstruct the original quantum state of the k encoded qubits. Quantum error-correcting codes are shown to exist with asymptotic rate k/n = 1 - 2H(2t/n) where H(p) is the binary entropy function -p log p - (1-p) log (1-p).},
  archiveprefix = {arXiv},
  author = {Calderbank, A. R. and Shor, Peter W.},
  doi = {10.1103/PhysRevA.54.1098},
  eprint = {quant-ph/9512032},
  journal = {Physical Review A},
  month = {8},
  number = {2},
  pages = {1098--1105},
  pdf = {https://math.mit.edu/~shor/papers/good-codes.pdf},
  publisher = {American Physical Society},
  title = {Good quantum error-correcting codes exist},
  url = {https://arxiv.org/abs/quant-ph/9512032},
  volume = {54},
  year = {1996}
}

@article{schumacher1995quantum,
  abstract = {A theorem is proven for quantum information theory that is analogous to the noiseless coding theorem of classical information theory. In the quantum result, the von Neumann entropy S of the density operator describing an ensemble of pure quantum signal states is equal to the number of spin-1/2 systems (quantum bits or qubits) necessary to represent the signal faithfully. The theorem holds whether or not the signal states are orthogonal. Related results are presented concerning the fidelity of quantum coding and the representation of entangled quantum states.},
  author = {Schumacher, Benjamin},
  doi = {10.1103/PhysRevA.51.2738},
  journal = {Physical Review A},
  month = {4},
  number = {4},
  pages = {2738--2747},
  pdf = {https://link.aps.org/pdf/10.1103/PhysRevA.51.2738},
  title = {Quantum coding},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.51.2738},
  volume = {51},
  year = {1995}
}

@inproceedings{shor1994algorithms,
  abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.},
  address = {Santa Fe, New Mexico, USA},
  author = {Shor, Peter W.},
  booktitle = {Proceedings 35th Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/SFCS.1994.365700},
  isbn = {0-8186-6580-7},
  month = {11},
  pages = {124--134},
  pdf = {https://www.semanticscholar.org/paper/Algorithms-for-quantum-computation:-discrete-and-Shor/2273d9829cdf7fc9d3be3cbecb961c7a6e4a34ea},
  publisher = {IEEE Computer Society},
  title = {Algorithms for Quantum Computation: Discrete Logarithms and Factoring},
  url = {https://ieeexplore.ieee.org/document/365700/},
  year = {1994}
}

@inproceedings{berrou1993near,
  abstract = {A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is presented. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as identical elementary decoders. The decoding complexity is thus increased only linearly with the number of states of the component codes. The performance of such codes with a constraint length as low as 4 is within 0.7 dB of the Shannon limit.},
  address = {Geneva, Switzerland},
  author = {Berrou, Claude and Glavieux, Alain and Thitimajshima, Punya},
  booktitle = {Proceedings of ICC '93 - IEEE International Conference on Communications},
  doi = {10.1109/ICC.1993.397441},
  month = {5},
  openalex = {W2987657883},
  pages = {1064--1070},
  publisher = {IEEE},
  title = {Near Shannon Limit Error-correcting Coding and Decoding: Turbo-codes},
  url = {https://ieeexplore.ieee.org/document/397441/},
  volume = {2},
  year = {1993}
}

@article{bennett1993teleporting,
  abstract = {An unknown quantum state can be disassembled into, then later reconstructed from, purely classical information and purely nonclassical Einstein-Podolsky-Rosen (EPR) correlations. To do so the sender, ``Alice,'' and the receiver, ``Bob,'' must prearrange the sharing of an EPR-correlated pair of particles. Alice makes a joint measurement on her EPR particle and the unknown quantum system, and sends Bob the classical result of this measurement. Knowing this, Bob can convert the state of his EPR particle into an exact replica of the unknown state which Alice destroyed.},
  author = {Bennett, Charles H. and Brassard, Gilles and Crépeau, Claude and Jozsa, Richard and Peres, Asher and Wootters, William K.},
  doi = {10.1103/PhysRevLett.70.1895},
  journal = {Physical Review Letters},
  month = {3},
  number = {13},
  openalex = {W1525305151},
  pages = {1895--1899},
  pdf = {https://link.aps.org/pdf/10.1103/PhysRevLett.70.1895},
  publisher = {American Physical Society},
  title = {Teleporting an Unknown Quantum State via Dual Classical and Einstein-Podolsky-Rosen Channels},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.70.1895},
  volume = {70},
  year = {1993}
}

@article{welch1984technique,
  author = {Terry A. Welch},
  doi = {10.1109/MC.1984.1659158},
  journal = {IEEE Computer},
  month = {6},
  number = {6},
  openalex = {W1990653637},
  pages = {8--19},
  pdf = {https://courses.cs.duke.edu/spring03/cps296.5/papers/welch_1984_technique_for.pdf},
  title = {A Technique for High-Performance Data Compression},
  volume = {17},
  year = {1984}
}

@article{hankobayashi1981new,
  abstract = {A new achievable rate region for the general interference channel which extends previous results is presented and evaluated. The technique used is a generalization of superposition coding to the multivariable case.},
  author = {Te Sun Han and Kingo Kobayashi},
  doi = {10.1109/TIT.1981.1056307},
  journal = {IEEE Transactions on Information Theory},
  month = {1},
  number = {1},
  pages = {49--60},
  title = {A New Achievable Rate Region for the Interference Channel},
  url = {https://ieeexplore.ieee.org/document/1056307/},
  volume = {27},
  year = {1981}
}

@article{coverelgamal1979capacity,
  abstract = {This paper establishes capacity theorems for relay channels. A relay channel consists of an input $x_1$, a relay output $y_1$, a channel output $y$, and a relay sender $x_2$ whose transmission is allowed to depend on the past symbols $y_1$. Four theorems are proved: capacity of the degraded relay channel, capacity of the reversely degraded relay channel, capacity of an arbitrary relay channel with feedback, and an achievable lower bound to the capacity of the general relay channel. The first two main relaying schemes (Decode-and-Forward and Compress-and-Forward) are introduced. Superposition block Markov encoding is used to show achievability, and the first upper bound on relay channel capacity (Cut-set upper bound) is derived.},
  author = {Cover, Thomas M. and El Gamal, Abbas A.},
  doi = {10.1109/TIT.1979.1056084},
  journal = {IEEE Transactions on Information Theory},
  month = {9},
  number = {5},
  openalex = {W2167447263},
  pages = {572--584},
  publisher = {IEEE},
  title = {Capacity Theorems for the Relay Channel},
  volume = {25},
  year = {1979}
}

@article{zivlempel1978compression,
  abstract = {Compressibility of individual sequences by the class of generalized finite-state information-lossless encoders is investigated. These can operate in a variable-rate mode as well as fixed-rate one, and they allow for any variable-length-to-variable-length coding. For every infinite sequence a quantity $h̊o(x)$ is defined, called compressibility, which is shown to be the asymptotically attainable lower bound on the compression ratio that can be achieved by any finite-state encoder. This is demonstrated by means of a constructive coding theorem and its converse that, apart from their asymptotic significance, also provide useful performance criteria for finite practical data-compression tasks. The proposed concept of compressibility is shown to play a role analogous to that of entropy in classical information theory where one deals with probabilistic ensembles of sequences rather than with individual sequences. While the definition of compressibility allows a different machine for each sequence to be compressed, it also leads to a universal algorithm which is optimal for all sequences.},
  author = {Ziv, Jacob and Lempel, Abraham},
  doi = {10.1109/TIT.1978.1055934},
  journal = {IEEE Transactions on Information Theory},
  month = {9},
  number = {5},
  openalex = {W2122962290},
  pages = {530--536},
  title = {Compression of Individual Sequences via Variable-Rate Coding},
  volume = {24},
  year = {1978}
}

@article{rissanen1978modeling,
  abstract = {The number of digits it takes to write down an observed sequence $x_1, łdots, x_N$ of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.},
  author = {Jorma Rissanen},
  doi = {10.1016/0005-1098(78)90005-5},
  journal = {Automatica},
  keywords = {Minimum description length, Model selection, Information theory, Parameter estimation, Time series analysis},
  month = {9},
  note = {Introduces the foundational Minimum Description Length (MDL) principle},
  number = {5},
  openalex = {W2054658115},
  pages = {465--471},
  publisher = {Elsevier},
  title = {Modeling by Shortest Data Description},
  volume = {14},
  year = {1978}
}

@article{zivlempel1977universal,
  abstract = {A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.},
  author = {Jacob Ziv and Abraham Lempel},
  doi = {10.1109/TIT.1977.1055714},
  journal = {IEEE Transactions on Information Theory},
  month = {5},
  number = {3},
  openalex = {W2107745473},
  pages = {337--343},
  title = {A Universal Algorithm for Sequential Data Compression},
  volume = {23},
  year = {1977}
}

@article{wynerziv1976ratedistortion,
  abstract = {A sequence of independent drawings of a pair (X_k, Y_k) of dependent random variables is observed. It is desired to encode the sequence X_k in blocks of length n into a binary stream of rate R (in bits per sample) which can be decoded as a sequence X̂_k such that the average per-letter distortion between X_k and X̂_k is within a prescribed bound. The problem differs from the classical rate-distortion problem in that the decoder (but not the encoder) is assumed to observe the Y_k sequence. We determine R*(d), the minimum rate required for encoding X_k at distortion level d. The main result is the characterization of R*(d) by an information theoretic minimization. We show that when (X, Y) is jointly Gaussian, R*(d) equals the minimum rate required in the classical problem where the encoder also observes the Y_k sequence.},
  author = {Wyner, Aaron D. and Ziv, Jacob},
  doi = {10.1109/TIT.1976.1055508},
  issn = {0018-9448},
  journal = {IEEE Transactions on Information Theory},
  month = {1},
  number = {1},
  openalex = {W2150412388},
  pages = {1--10},
  title = {The Rate-Distortion Function for Source Coding with Side Information at the Decoder},
  url = {https://doi.org/10.1109/TIT.1976.1055508},
  volume = {22},
  year = {1976}
}

@article{rissanen1976generalized,
  abstract = {Algorithms for encoding and decoding finite strings over a finite alphabet are described. The coding operations are arithmetic involving rational numbers $l_i$ as parameters such that $∑_i 2^-l_i łeq 2^-ɛ$. The coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within $ɛ$. The coding speed is comparable to that of conventional coding methods.},
  author = {Jorma Rissanen},
  doi = {10.1147/RD.203.0198},
  journal = {IBM Journal of Research and Development},
  month = {5},
  number = {3},
  pages = {198--203},
  title = {Generalized Kraft Inequality and Arithmetic Coding},
  volume = {20},
  year = {1976}
}

@article{wyner1975wiretap,
  abstract = {We consider the situation in which digital data is to be reliably transmitted over a discrete, memoryless channel (dmc) that is subjected to a wire-tap at the receiver. The wire-tapper views the channel output via a second dmc. The code books used for encoding and decoding are assumed to be known by the wire-tapper. The trade-off curve between transmission rate R and equivocation d is found, assuming essentially perfect (``error-free'') transmission, and implies that there exists a Cs > 0, such that reliable transmission at rates up to Cs is possible in approximately perfect secrecy.},
  author = {Wyner, Aaron D.},
  doi = {10.1002/j.1538-7305.1975.tb02040.x},
  journal = {Bell System Technical Journal},
  month = {10},
  note = {Seminal paper establishing the theoretical framework for information-theoretic security in the presence of eavesdroppers.},
  number = {8},
  pages = {1355--1387},
  pdf = {https://archive.org/details/bstj54-8-1355},
  title = {The Wire-Tap Channel},
  volume = {54},
  year = {1975}
}

@article{bahl1974optimal,
  abstract = {The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived.},
  author = {Bahl, L. R. and Cocke, J. and Jelinek, F. and Raviv, J.},
  doi = {10.1109/TIT.1974.1055186},
  journal = {IEEE Transactions on Information Theory},
  month = {3},
  number = {2},
  openalex = {W2045407304},
  pages = {284--287},
  title = {Optimal decoding of linear codes for minimizing symbol error rate (Corresp.)},
  volume = {20},
  year = {1974}
}

@inproceedings{ahlswede1971multiway,
  abstract = {This foundational paper establishes the theoretical framework for multi-way communication channels, presenting fundamental results in multi-user information theory that have become cornerstone contributions to the field of network information theory.},
  address = {Budapest},
  author = {Rudolf Ahlswede},
  booktitle = {Second International Symposium on Information Theory: Tsahkadsor, Armenia, USSR, Sept. 2--8, 1971},
  editor = {B. N. Petrov},
  pages = {23--51},
  publisher = {Akadémiai Kiadó},
  title = {Multi-way Communication Channels},
  year = {1973}
}

@article{slepianwolf1973noiseless,
  abstract = {This paper deals with correlated information sequences generated by repeated independent drawings of a pair of discrete random variables X, Y from a given bivariate distribution. The authors determine the minimum number of bits per character R_X and R_Y needed to encode these sequences so that they can be faithfully reproduced under a variety of assumptions regarding the encoders and decoders. The results are presented as an admissible rate region in the R_X - R_Y plane, generalizing the well-known result for a single information sequence, namely R_X ≥ H(X) for faithful reproduction.},
  author = {Slepian, D. and Wolf, J. K.},
  doi = {10.1109/TIT.1973.1055037},
  journal = {IEEE Transactions on Information Theory},
  month = {7},
  number = {4},
  openalex = {W2099213070},
  pages = {471--480},
  pdf = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1055037},
  title = {Noiseless Coding of Correlated Information Sources},
  volume = {19},
  year = {1973}
}

@article{cover1972broadcast,
  abstract = {We introduce the problem of a single source attempting to communicate information simultaneously to several receivers. The intent is to model the situation of a broadcaster with multiple receivers or a lecturer with many listeners.},
  author = {Cover, Thomas M.},
  doi = {10.1109/TIT.1972.1054727},
  issn = {0018-9448},
  journal = {IEEE Transactions on Information Theory},
  keywords = {Broadcast Channel, Transmission Rate, Achievable Rate, Multiple Receivers, Receivers, Codes, Decoding, Transmitters, Probability distribution, Encoding, Channel capacity},
  month = {1},
  number = {1},
  pages = {2--14},
  pdf = {https://isl.stanford.edu/~cover/papers/transIT/0002cove.pdf},
  title = {Broadcast Channels},
  url = {https://ieeexplore.ieee.org/document/1054727/},
  volume = {18},
  year = {1972}
}

@article{blahut1972computation,
  abstract = {By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. The paper proposes algorithms for computing channel capacity and rate-distortion functions that apply to both discrete and continuous alphabet channels or sources.},
  author = {Blahut, Richard E.},
  doi = {10.1109/TIT.1972.1054855},
  journal = {IEEE Transactions on Information Theory},
  keywords = {information theory, channel capacity, rate-distortion, mutual information, Blahut-Arimoto algorithm},
  month = {7},
  note = {Seminal paper introducing the Blahut-Arimoto algorithm for computing channel capacities and rate-distortion functions},
  number = {4},
  pages = {460--473},
  title = {Computation of Channel Capacity and Rate-Distortion Functions},
  volume = {18},
  year = {1972}
}

@book{berlekamp1968algebraic,
  abstract = {A landmark book that introduced several groundbreaking algorithms including the Berlekamp-Massey algorithm for decoding Reed-Solomon and BCH codes, the Berlekamp algorithm for factoring polynomials over finite fields, novel arithmetic operations in finite fields of characteristic two, and a new class of Lee metric codes. The book provides precise asymptotic results on the number of information symbols in long binary BCH codes and has become a fundamental reference in algebraic coding theory.},
  address = {New York},
  author = {Elwyn R. Berlekamp},
  isbn = {0070049033},
  lccn = {68017175},
  pages = {xiv + 466},
  publisher = {McGraw-Hill},
  series = {McGraw-Hill series in systems science},
  title = {Algebraic Coding Theory},
  year = {1968}
}

@article{viterbi1967error,
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all rates above R$_0$ (the computational cutoff rate of sequential decoding), these bounds are asymptotically (exponentially) tight for the best convolutional codes. The bounds are much looser for rates below R$_0$. The upper bound is attained by a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R$_0$. It is also shown that the performance achieved by this algorithm is quite close to that achieved by the Fano sequential decoding algorithm for rates above R$_0$.},
  author = {Andrew J. Viterbi},
  doi = {10.1109/tit.1967.1054010},
  journal = {IEEE Transactions on Information Theory},
  month = {4},
  number = {2},
  openalex = {W1991133427},
  pages = {260--269},
  pdf = {https://ieeexplore.ieee.org/iel5/18/22634/01054010.pdf},
  title = {Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
  volume = {13},
  year = {1967}
}

@book{forney1966concatenated,
  abstract = {Introduces concatenated codes, a new class of error-correcting codes formed by combining an outer code and an inner code. This work demonstrates that concatenated codes can achieve exponentially decreasing error probabilities at all data rates less than capacity, with decoding complexity that increases only polynomially with the code block length. The technique has become fundamental to modern error correction in communication systems and storage devices.},
  address = {Cambridge, MA},
  author = {Forney, Jr., G. David},
  isbn = {9780262060158},
  note = {Originally published as Technical Report 440, Research Laboratory of Electronics, MIT},
  number = {37},
  pages = {147},
  pdf = {https://dspace.mit.edu/bitstream/handle/1721.1/4303/RLE-TR-440-04743368.pdf},
  publisher = {MIT Press},
  series = {MIT Research Monograph},
  title = {Concatenated Codes},
  year = {1966}
}

@book{gallager1963lowdensity,
  abstract = {This is a complete presentation of all important theoretical and experimental work done on low-density codes. Low-density coding is one of the three techniques developed for efficient communication over noisy channels with an arbitrarily low probability of error. The book analyzes coding schemes where computational costs grow approximately linearly with code length and demonstrates that error probability approaches zero exponentially.},
  address = {Cambridge, MA},
  author = {Robert G. Gallager},
  isbn = {9780262070072},
  month = {9},
  pages = {ix + 102},
  pdf = {https://web.stanford.edu/class/ee388/papers/ldpc.pdf},
  publisher = {MIT Press},
  series = {MIT Press Classics},
  title = {Low-Density Parity-Check Codes},
  year = {1963}
}

@article{landauer1961irreversibility,
  abstract = {It is argued that computing machines inevitably involve devices which perform logical functions that do not have a single-valued inverse. The irreversibility associated with physical processes requires minimal heat generation, typically of the order kT for each irreversible function. Dissipation serves the purpose of standardizing signals, making them independent of their exact history. Two simple models of bistable devices are analyzed in terms of switching kinetics, yielding a relationship between speed, energy dissipation, and thermal fluctuation-induced errors.},
  author = {Landauer, Rolf},
  doi = {10.1147/rd.53.0183},
  journal = {IBM Journal of Research and Development},
  month = {7},
  number = {3},
  openalex = {W4233798822},
  pages = {183--191},
  pdf = {https://dl.acm.org/doi/10.1147/rd.53.0183},
  publisher = {IBM},
  title = {Irreversibility and Heat Generation in the Computing Process},
  url = {https://ieeexplore.ieee.org/document/5392446},
  volume = {5},
  year = {1961}
}

@article{reedsolomon1960polynomial,
  abstract = {A mapping of m symbols into 2 symbols will be shown to be (2 m)/2 or ( 2 m 1)/2 symbol correcting, depending on whether m is even or odd. A natural correspondence is established between the field elements of K and certain binary sequences of length n. Under this correspondence, code E may be regarded as a mapping of binary sequences of mn bits into binary sequences of n2 bits. Thus code E can be interpreted to be a systematic multiple-error-correcting code of binary sequences.},
  author = {Reed, Irving S. and Solomon, Gustave},
  doi = {10.1137/0108018},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  month = {6},
  number = {2},
  openalex = {W2148575324},
  pages = {300--304},
  title = {Polynomial Codes over Certain Finite Fields},
  url = {https://epubs.siam.org/doi/10.1137/0108018},
  volume = {8},
  year = {1960}
}

@inproceedings{shannon1959coding,
  abstract = {This paper considers a discrete source producing a sequence of message letters from a finite alphabet. A single-letter distortion measure is given by a non-negative matrix (d_ij), where the entry d_ij measures the cost or distortion if letter i is reproduced at the receiver as letter j. For a wide class of distortion measures and discrete sources of information there exists a function R(d) which measures the equivalent rate R of the source (in bits per letter produced) when d is the allowed distortion level. For coding purposes where a level d of distortion can be tolerated, the source acts like one with information rate R(d). The paper also develops generalizations to ergodic sources, to continuous sources, and to distortion measures involving blocks of letters.},
  address = {New York, NY, USA},
  author = {Claude E. Shannon},
  booktitle = {IRE International Convention Record},
  note = {Foundational paper in rate-distortion theory},
  number = {4},
  pages = {142--163},
  publisher = {Institute of Radio Engineers},
  title = {Coding Theorems for a Discrete Source with a Fidelity Criterion},
  volume = {7},
  year = {1959}
}

@inproceedings{wozencraft1957sequential,
  abstract = {This paper presents the first practical algorithm for error-correcting codes that enables arbitrarily accurate fixed-data-rate communication over noisy transmission channels with reasonable computational complexity. The sequential decoding technique developed here became fundamental to the design of reliable digital communication systems and was later adopted as the standard coding system for NASA deep space missions for nearly a decade.},
  address = {New York},
  author = {Wozencraft, John McReynolds},
  booktitle = {IRE WESCON Convention Record, Part 2: Circuit Theory; Information Theory},
  note = {Also published as MIT Technical Report 325, Research Laboratory of Electronics},
  publisher = {Institute of Radio Engineers},
  title = {Sequential Decoding for Reliable Communications},
  volume = {5},
  year = {1957}
}

@inproceedings{elias1955coding,
  abstract = {This seminal paper introduces the concept of convolutional codes as an alternative to block codes for error correction in noisy communication channels. The work establishes fundamental trade-offs between block length, code rate, and error probability on the binary symmetric channel. Elias demonstrates that the average performance of convolutional codes equals that of block codes while offering greater practical simplicity for implementation and decoding. This foundational contribution shifted the focus of coding theory from seeking optimal specific codes to developing classes of codes with practical decoding algorithms, establishing convolutional codes as the workhorse of modern communication systems.},
  address = {New York, NY, USA},
  author = {Peter Elias},
  booktitle = {IRE Convention Record},
  note = {Reprinted in Key Papers in the Development of Coding Theory, E. Berlekamp (ed.), pp. 48--55, IEEE Press, 1974},
  number = {4},
  organization = {Institute of Radio Engineers},
  pages = {37--46},
  title = {Coding for Noisy Channels},
  volume = {3},
  year = {1955}
}

@article{reedmuller1954signaling,
  abstract = {This paper introduces a class of error-correcting codes that can reliably transmit information over noisy communication channels. The codes presented form the foundation of what became known as Reed-Muller codes, which are constructed using Boolean functions and provide multiple error correction capabilities. These linear codes have found extensive applications in digital communications, particularly in scenarios requiring high reliability such as deep space communications.},
  author = {Reed, Irving S. and Muller, David E.},
  journal = {IRE Transactions on Information Theory},
  month = {9},
  note = {Foundational work on Reed-Muller error-correcting codes},
  number = {4},
  pages = {38--49},
  publisher = {Institute of Radio Engineers},
  title = {A class of codes for signaling on a noisy channel},
  volume = {4},
  year = {1954}
}

@article{reedmuller1954efficient,
  abstract = {A class of codes is developed which will correct any pattern of $t$ or fewer errors. The codes are readily adaptable to automatic error-correcting equipment. A simple decoding scheme is described.},
  author = {Reed, Irving S.},
  doi = {10.1109/TIT.1954.1057465},
  journal = {Transactions of the IRE Professional Group on Information Theory},
  month = {9},
  number = {4},
  openalex = {W2121417875},
  pages = {38--49},
  title = {A Class of Multiple-Error-Correcting Codes and the Decoding Scheme},
  volume = {4},
  year = {1954}
}

@inproceedings{huffman1952method,
  abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
  author = {David A. Huffman},
  booktitle = {Proceedings of the IRE},
  doi = {10.1109/JRPROC.1952.273898},
  month = {9},
  number = {9},
  pages = {1098--1101},
  pdf = {https://ieeexplore.ieee.org/iel5/10933/4051100/04051119.pdf},
  publisher = {IEEE},
  title = {A Method for the Construction of Minimum-Redundancy Codes},
  volume = {40},
  year = {1952}
}

@article{hamming1950error,
  abstract = {The author was led to the study given in this paper from a consideration of large scale computing machines in which a large number of operations must be performed without a single error in the end result. This problem of doing things right on a large scale is not essentially new; in a telephone central office, for example, a very large number of operations are performed while the errors leading to wrong numbers are kept well under control, though they have not been completely eliminated. This has been achieved, in part, through the use of self-checking circuits.},
  author = {Hamming, Richard Wesley},
  doi = {10.1002/j.1538-7305.1950.tb00463.x},
  issn = {1538-7305},
  journal = {Bell System Technical Journal},
  month = {4},
  note = {Seminal paper introducing Hamming codes for error detection and correction},
  number = {2},
  pages = {147--160},
  publisher = {American Telephone and Telegraph Company},
  title = {Error Detecting and Error Correcting Codes},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1950.tb00463.x},
  volume = {29},
  year = {1950}
}

@article{shannon1949communication,
  abstract = {The problems of cryptography and secrecy systems furnish an interesting application of communication theory. In this paper a theory of secrecy systems is developed. The approach is on a theoretical level and is intended to complement the treatment found in standard works on cryptography. The treatment is restricted to the case of discrete information where the message to be enciphered consists of a sequence of discrete symbols, each chosen from a finite set. The paper defines the concept of perfect secrecy, proves that all theoretically unbreakable ciphers must have the same requirements as the one-time pad, and introduces fundamental cryptographic concepts including unicity distance and the principles of confusion and diffusion.},
  author = {Shannon, Claude Elwood},
  doi = {10.1002/j.1538-7305.1949.tb00928.x},
  journal = {Bell System Technical Journal},
  month = {10},
  note = {Originally issued as classified document ``A Mathematical Theory of Cryptography'', Bell Telephone Labs, September 1, 1945. Foundational paper that established modern cryptography as a mathematical science.},
  number = {4},
  pages = {656--715},
  pdf = {https://archive.org/details/bstj28-4-656},
  title = {Communication Theory of Secrecy Systems},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1949.tb00928.x},
  volume = {28},
  year = {1949}
}

@book{wiener1948cybernetics,
  abstract = {With the influential book Cybernetics, first published in 1948, Norbert Wiener laid the theoretical foundations for the multidisciplinary field of cybernetics, the study of controlling the flow of information in systems with feedback loops, be they biological, mechanical, cognitive, or social. This book is the first public usage of the term cybernetics to refer to self-regulating mechanisms and established the theoretical foundation for servomechanisms, automatic navigation, analog computing, artificial intelligence, neuroscience, and reliable communications.},
  address = {Cambridge, Massachusetts},
  author = {Norbert Wiener},
  edition = {Second},
  isbn = {9780262537841},
  keywords = {cybernetics, control theory, information theory, communication, artificial intelligence, feedback systems},
  note = {Reissued by MIT Press in 2019 with forewords by Doug Hill and Sanjoy Mitter},
  pages = {194},
  publisher = {MIT Press},
  title = {Cybernetics: Or Control and Communication in the Animal and the Machine},
  url = {https://mitpress.mit.edu/9780262537841/cybernetics-or-control-and-communication-in-the-animal-and-the-machine/},
  year = {1948}
}

@article{shannon1948mathematical,
  abstract = {The fundamental problem of communication is that of reproducing at one point, either exactly or approximately, a message selected at another point. The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  author = {Claude Elwood Shannon},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  journal = {Bell System Technical Journal},
  month = {7, 10},
  note = {Published in two parts: Part I in July 1948 (pages 379--423) and Part II in October 1948 (pages 623--656). This seminal paper established the mathematical foundations of information theory},
  number = {3, 4},
  pages = {379--423, 623--656},
  pdf = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x},
  title = {A Mathematical Theory of Communication},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x},
  volume = {27},
  year = {1948}
}

@article{szilard1929uber,
  abstract = {This seminal paper investigates the circumstances under which one can apparently construct a perpetual motion machine of the second kind when an intelligent being intervenes in a thermodynamic system. Szilárd described a theoretical model that served both as a heat engine and an information engine, establishing the fundamental relationship between thermodynamics and information theory. The work addresses Maxwell's demon paradox and demonstrates that measurements themselves necessarily increase entropy. This paper is considered the founding document of information theory and first linked the concepts of intelligence, memory, entropy and information.},
  author = {Szilárd, Leó},
  doi = {10.1007/BF01341281},
  journal = {Zeitschrift für Physik},
  note = {Original German title. English translation published in Behavioral Science 9(4):301-310, 1964},
  pages = {840--856},
  publisher = {Springer},
  title = {Über die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen},
  url = {https://doi.org/10.1007/BF01341281},
  volume = {53},
  year = {1929}
}

@article{hartley1928transmission,
  abstract = {A quantitative measure of ``information'' which is based on physical as contrasted with psychological considerations is developed. The measure is employed to compute how the rate of transmission of this information over a system is limited by the distortion resulting from storage of energy from the transient viewpoint. It is shown that when the storage of energy is used to restrict the steady state transmission to a limited range of frequencies the amount of information that can be transmitted is proportional to the product of the width of the frequency-range by the time it is available.},
  author = {Hartley, Ralph Vinton Lyon},
  doi = {10.1002/j.1538-7305.1928.tb01236.x},
  journal = {Bell System Technical Journal},
  month = {7},
  note = {Originally presented at the International Congress of Telegraphy and Telephony, Lake Como, Italy, September 1927. This paper established the first mathematical foundations for information theory and was acknowledged by Shannon as a key prerequisite for his 1948 work.},
  number = {3},
  pages = {535--563},
  pdf = {https://monoskop.org/images/a/a6/Hartley_Ralph_VL_1928_Transmission_of_Information.pdf},
  title = {Transmission of Information},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1928.tb01236.x},
  volume = {7},
  year = {1928}
}

@article{nyquist1924certain,
  abstract = {This paper considers two factors which enter into the maximum speed of transmission of intelligence by telegraph: (1) the best wave shape to be impressed upon the transmitting medium in order that it may be transmitted at maximum speed without interference and (2) the choice of codes which will permit of the maximum transmission of intelligence with a given number of current elements.},
  author = {Nyquist, H.},
  doi = {10.1002/j.1538-7305.1924.tb01361.x},
  journal = {Bell System Technical Journal},
  month = {4},
  number = {2},
  openalex = {W1981409066},
  pages = {324--346},
  pdf = {https://ieeexplore.ieee.org/iel7/6528086/6534456/06534511.pdf},
  title = {Certain Factors Affecting Telegraph Speed},
  volume = {3},
  year = {1924}
}
