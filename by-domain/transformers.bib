@inproceedings{rusch2025oscillatory,
  abstract = {We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with sequences of length 50k.},
  arxiv = {2410.03943},
  author = {T. Konstantin Rusch and Daniela Rus},
  booktitle = {International Conference on Learning Representations},
  note = {Oral},
  pdf = {https://openreview.net/pdf?id=GRMfXcAAFh},
  title = {Oscillatory State-Space Models},
  url = {https://openreview.net/forum?id=GRMfXcAAFh},
  year = {2025}
}

@inproceedings{pradeep2025state,
  abstract = {Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly O(1) time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking -- a task requiring fine-grained query-document interaction and long-context understanding -- remains underexplored. This study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. The main findings are: (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency.},
  author = {Xu, Zhichao and Yan, Jinghua and Gupta, Ashim and Srikumar, Vivek},
  booktitle = {arXiv preprint},
  doi = {10.48550/arXiv.2412.14354},
  month = {12},
  note = {arXiv:2412.14354 [cs.CL]},
  openalex = {W4411119048},
  pages = {1--15},
  pdf = {https://arxiv.org/pdf/2412.14354.pdf},
  publisher = {arXiv},
  title = {State Space Models are Strong Text Rerankers},
  url = {https://arxiv.org/abs/2412.14354},
  year = {2025}
}

@inproceedings{dogra2025chaos,
  abstract = {Generating long-term trajectories of dissipative chaotic systems autoregressively represents a highly challenging task due to the inherent positive Lyapunov exponents that amplify prediction errors over time. Many chaotic systems, however, possess a crucial property -- ergodicity on their attractors, which makes long-term prediction possible. State-of-the-art methods address this issue by preserving statistical properties using optimal transport techniques. Nevertheless, these methods face scalability challenges due to the curse of dimensionality when matching distributions. We propose a scalable transformer-based framework capable of stably generating long-term high-dimensional and high-resolution chaotic dynamics while preserving ergodicity. Our method is grounded in a physical perspective, revisiting the Von Neumann mean ergodic theorem to ensure the preservation of long-term statistical properties in L² space. We introduce novel modifications to the attention mechanism, making our transformer architecture well-suited for learning large-scale chaotic systems. We benchmark our approach against several operator-based and transformer-based methods and demonstrate superior performance across five metrics. Additionally, we introduce a new chaotic system benchmark: a machine learning dataset of 140k snapshots of turbulent channel flow along with various evaluation metrics for both short- and long-term performance assessment.},
  address = {Vancouver, Canada},
  author = {He, Yi and Yang, Yiming and Cheng, Xiaoyuan and Wang, Hai and Xue, Xiao and Chen, Boli and Hu, Yukun},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  note = {To appear},
  pages = {},
  pdf = {https://arxiv.org/pdf/2504.20858.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Chaos Meets Attention: Transformers for Large-Scale Dynamical Prediction},
  volume = {267},
  year = {2025}
}

@inproceedings{fan2025looped,
  abstract = {Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation—a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.},
  author = {Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  openalex = {W4403788968},
  pdf = {https://openreview.net/pdf?id=2edigk8yoU},
  title = {Looped Transformers for Length Generalization},
  url = {https://openreview.net/forum?id=2edigk8yoU},
  year = {2025}
}

@inproceedings{zhu2025transformers,
  abstract = {Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(αx), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.},
  author = {Jiachen Zhu and Xinlei Chen and Kaiming He and Yann LeCun and Zhuang Liu},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  month = {6},
  pages = {14901--14911},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Transformers_without_Normalization_CVPR_2025_paper.pdf},
  title = {Transformers without Normalization},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Transformers_without_Normalization_CVPR_2025_paper.html},
  year = {2025}
}

@inproceedings{sun2025transformer,
  abstract = {Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific 'expert' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. The method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks.},
  archiveprefix = {arXiv},
  author = {Qi Sun and Edoardo Cetin and Yujin Tang},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  eprint = {2501.06252},
  note = {To appear at ICLR 2025},
  pdf = {https://openreview.net/pdf?id=dh4t9qmcvK},
  primaryclass = {cs.LG},
  title = {Transformer-Squared: Self-adaptive LLMs},
  url = {https://openreview.net/forum?id=dh4t9qmcvK},
  year = {2025}
}

@inproceedings{xia2025mmie,
  abstract = {Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.},
  author = {Peng Xia and Siwei Han and Shi Qiu and Yiyang Zhou and Zhaoyang Wang and Wenhao Zheng and Zhaorun Chen and Chenhang Cui and Mingyu Ding and Linjie Li and Lijuan Wang and Huaxiu Yao},
  booktitle = {International Conference on Learning Representations},
  note = {Oral},
  openalex = {W4403580683},
  pdf = {https://openreview.net/pdf?id=HnhNRrLPwm},
  title = {MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models},
  url = {https://openreview.net/forum?id=HnhNRrLPwm},
  year = {2025}
}

@inproceedings{anonymous2025mambavision,
  abstract = {We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture long-range spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance.},
  address = {Nashville, TN, USA},
  author = {Ali Hatamizadeh and Jan Kautz},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  month = {6},
  openalex = {W4400611405},
  pages = {TBD},
  pdf = {https://openaccess.thecvf.com/content/CVPR2025/papers/Hatamizadeh_MambaVision_A_Hybrid_Mamba-Transformer_Vision_Backbone_CVPR_2025_paper.pdf},
  publisher = {IEEE},
  title = {MambaVision: A Hybrid Mamba-Transformer Vision Backbone},
  year = {2025}
}

@inproceedings{anonymous2025chatqa2,
  abstract = {In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K context window, designed to bridge the gap between open-source LLMs and leading proprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context understanding and retrieval-augmented generation (RAG) capabilities. These two capabilities are complementary to each other and essential for LLMs to process large volumes of information that cannot fit into a single prompt. We present a detailed continued training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, along with a three-stage instruction tuning process to enhance the model's instruction-following, RAG performance, and long-context understanding capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model outperforms most existing state-of-the-art models, including GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only a 4K context window, showing the strong long context capability across varying sequence lengths. We further provide extensive comparisons between direct long-context and RAG solutions using the same state-of-the-art long-context LLMs. Interestingly, we find that the performance of strong long-context LLMs using RAG improves when retrieving a larger number of chunks. With a large set of top-k chunks, RAG consistently outperforms direct long-context solution using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both 32K and 128K benchmarks. We open-source the model weights, training data, and the evaluation setup for the community: https://chatqa2-project.github.io/},
  author = {Peng Xu and Wei Ping and Xianchao Wu and Chejian Xu and Zihan Liu and Mohammad Shoeybi and Bryan Catanzaro},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4402856762},
  pdf = {https://arxiv.org/pdf/2407.14482.pdf},
  title = {ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities},
  url = {https://openreview.net/forum?id=cPD2hU35x3},
  year = {2025}
}

@inproceedings{anonymous2025differential,
  abstract = {Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.},
  author = {Tianzhu Ye and Li Dong and Yuqing Xia and Yutao Sun and Yi Zhu and Gao Huang and Furu Wei},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  openalex = {W4403324167},
  pdf = {https://openreview.net/pdf?id=OvoCm1gGhN},
  title = {Differential Transformer},
  url = {https://openreview.net/forum?id=OvoCm1gGhN},
  year = {2025}
}

@inproceedings{anonymous2025olmoe,
  abstract = {We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. The models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs.},
  author = {Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4402955413},
  pdf = {https://openreview.net/pdf?id=xXTkbTBmqq},
  title = {OLMoE: Open Mixture-of-Experts Language Models},
  url = {https://openreview.net/forum?id=xXTkbTBmqq},
  year = {2025}
}

@inproceedings{gao2025scaling,
  abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. The authors propose using k-sparse autoencoders to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, they find modifications that result in few dead latents, even at the largest scales they tried. Using these techniques, they find clean scaling laws with respect to autoencoder size and sparsity. They also introduce several new metrics for evaluating feature quality based on: 1. Recovery of hypothesized features 2. Explainability of activation patterns 3. Sparsity of downstream effects These metrics generally improve with autoencoder size. To demonstrate the scalability of their approach, they train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. The authors release code and autoencoders for open-source models, as well as a visualizer.},
  address = {Singapore},
  arxiv = {2406.04093},
  author = {Leo Gao and Tom Dupré la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  month = {5},
  openalex = {W4399455238},
  pdf = {https://cdn.openai.com/papers/sparse-autoencoders.pdf},
  title = {Scaling and evaluating sparse autoencoders},
  url = {https://openreview.net/forum?id=tcsZt9ZNKD},
  year = {2025}
}

@inproceedings{anonymous2025judge,
  abstract = {The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target. We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements'' of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9 imes$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively.},
  author = {Gregor Bachmann and Sotiris Anagnostidis and Albert Pumarola and Markos Georgopoulos and Artsiom Sanakoyeu and Yuming Du and Edgar Schönfeld and Ali Thabet and Jonas Kohler},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4407091762},
  pdf = {https://arxiv.org/pdf/2501.19309.pdf},
  title = {Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment},
  url = {https://openreview.net/forum?id=mtSSFiqW6y},
  year = {2025}
}

@inproceedings{anonymous2025towards,
  abstract = {Causal Transformers are trained to predict the next token for a given context. While it is widely accepted that self-attention is crucial for encoding the causal structure of sequences, the precise underlying mechanism behind this in-context autoregressive learning ability remains unclear. In this paper, we take a step towards understanding this phenomenon by studying the approximation ability of Transformers for next-token prediction. Specifically, we explore the capacity of causal Transformers to predict the next token $x_t+1$ given an autoregressive sequence $(x_1, \dots, x_t)$ as a prompt, where $ x_t+1 = f(x_t) $, and $ f $ is a context-dependent function that varies with each sequence. On the theoretical side, we focus on specific instances, namely when $ f $ is linear or when $ (x_t)_t ≥ 1 $ is periodic. We explicitly construct a Transformer (with linear, exponential, or softmax attention) that learns the mapping $f$ in-context through a causal kernel descent method. The causal kernel descent method we propose provably estimates $x_t+1 $ based solely on past and current observations $ (x_1, \dots, x_t) $, with connections to the Kaczmarz algorithm in Hilbert spaces. We present experimental results that validate our theoretical findings and suggest their applicability to more general mappings $f$.},
  arxiv = {2410.03011},
  author = {Michael E. Sander and Gabriel Peyré},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  openalex = {W4403885568},
  pdf = {https://openreview.net/pdf?id=yWoV4Ca6ji},
  title = {Towards Understanding the Universality of Transformers for Next-Token Prediction},
  url = {https://openreview.net/forum?id=yWoV4Ca6ji},
  year = {2025}
}

@inproceedings{anonymous2025policy,
  abstract = {Policy gradient methods have become a staple of single-agent reinforcement learning, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating counterfactual values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.},
  archiveprefix = {arXiv},
  author = {Mingyang Liu and Gabriele Farina and Asuman E. Ozdaglar},
  booktitle = {International Conference on Learning Representations},
  eprint = {2408.00751},
  note = {Under review},
  pdf = {https://openreview.net/pdf?id=ZW4MRZrmSA},
  primaryclass = {cs.GT},
  title = {A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence},
  url = {https://openreview.net/forum?id=ZW4MRZrmSA},
  year = {2025}
}

@inproceedings{anonymous2025attention,
  abstract = {Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena.},
  author = {Ashok Vardhan Makkuva and Marco Bondaschi and Adway Girish and Alliot Nagle and Martin Jaggi and Hyeji Kim and Michael Gastpar},
  booktitle = {International Conference on Learning Representations},
  pdf = {https://openreview.net/pdf?id=SqZ0KY4qBD},
  title = {Attention with Markov: A Curious Case of Single-layer Transformers},
  url = {https://openreview.net/forum?id=SqZ0KY4qBD},
  year = {2025}
}

@inproceedings{anonymous2025lora,
  abstract = {Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).},
  author = {Jui-Nan Yen and Si Si and Zhao Meng and Felix Yu and Sai Surya Duvvuri and Inderjit S. Dhillon and Cho-Jui Hsieh and Sanjiv Kumar},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4404314198},
  pdf = {https://openreview.net/pdf?id=VpWki1v2P8},
  title = {LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization},
  url = {https://openreview.net/forum?id=VpWki1v2P8},
  year = {2025}
}

@inproceedings{anonymous2025fusing,
  address = {Seoul, Korea},
  author = {Anonymous},
  booktitle = {Conference on Robot Learning},
  month = {9},
  note = {Paper under review},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fusing World Models and Foundation Models for Embodied AI},
  year = {2025}
}

@misc{gladstone2025energybased,
  abstract = {Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question 'Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?' Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models},
  archiveprefix = {arXiv},
  author = {Alexi Gladstone and Ganesh Nanduru and Md Mofijul Islam and Peixuan Han and Hyeonjeong Ha and Aman Chadha and Yilun Du and Heng Ji and Jundong Li and Tariq Iqbal},
  doi = {10.48550/arXiv.2507.02092},
  eprint = {2507.02092},
  keywords = {Machine Learning, Transformers, Energy-Based Models, System 2 Thinking},
  month = {7},
  note = {Introduces Energy-Based Transformers (EBTs) that frame prediction as energy minimization, demonstrating improved scaling laws during training and enhanced performance through iterative System 2 Thinking at inference},
  pdf = {https://arxiv.org/pdf/2507.02092.pdf},
  primaryclass = {cs.LG},
  publisher = {arXiv},
  title = {Energy-Based Transformers are Scalable Learners and Thinkers},
  url = {https://arxiv.org/abs/2507.02092},
  year = {2025}
}

@article{dadgar2025forecite,
  abstract = {Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present ForeCite, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $h̊o = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.},
  archiveprefix = {arXiv},
  author = {Dadgar, Saeed and Amos, Ido and Berant, Jonathan and Levy, Omer},
  eprint = {2505.08941},
  journal = {arXiv preprint arXiv:2505.08941},
  month = {5},
  pdf = {https://arxiv.org/pdf/2505.08941.pdf},
  primaryclass = {cs.CL},
  title = {ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers},
  year = {2025}
}

@misc{anonymous2025mmllms,
  author = {Anonymous},
  howpublished = {Unpublished manuscript or conference submission under review},
  note = {Placeholder entry - paper not found with provided arXiv ID 2506.14585, which corresponds to a different paper about star clusters. The title suggests research in multimodal large language models for video question answering, but no matching publication was found in major databases or arXiv.},
  title = {MM-LLMs with Reasoning Chains for Video Question Answering},
  year = {2025}
}

@inproceedings{dao2023flashattention2,
  abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4× compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2× speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72% model FLOPs utilization).},
  address = {Vienna, Austria},
  author = {Dao, Tri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  doi = {10.48550/arXiv.2307.08691},
  month = {5},
  openalex = {W4384648639},
  pages = {},
  pdf = {https://openreview.net/pdf?id=mZn2Xyh9Ec},
  publisher = {OpenReview.net},
  title = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  url = {https://openreview.net/forum?id=mZn2Xyh9Ec},
  year = {2024}
}

@techreport{anthropic2024claude3,
  abstract = {We introduce Claude 3, a new family of large multimodal models. Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA, MMLU, MMMU and many more.},
  address = {San Francisco, CA},
  author = {Anthropic},
  institution = {Anthropic},
  month = {3},
  note = {Released March 4, 2024},
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  type = {Technical Report},
  url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
  year = {2024}
}

@article{jiang2024mixtral,
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only accesses a fraction of the total set of parameters, Mixtral 8x7B efficiently leverages the equivalent of 47B parameters. However, only 13B are active during inference, allowing for much faster inference at a fraction of the cost compared to a 47B dense model. Along with this, Mixtral demonstrates superior performance across multiple benchmarks compared to Llama 2 70B and matches or outperforms GPT-3.5 on most standard benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. On the popular chat benchmark MT-Bench, Mixtral reaches a score of 8.3. Trained with a context size of 32k tokens and published under the Apache 2.0 license, Mixtral represents a significant step forward in providing high-performance, cost-effective language models with open access.},
  archiveprefix = {arXiv},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  doi = {10.48550/arXiv.2401.04088},
  eprint = {2401.04088},
  journal = {arXiv preprint arXiv:2401.04088},
  month = {1},
  openalex = {W4390723197},
  pdf = {https://arxiv.org/pdf/2401.04088},
  primaryclass = {cs.LG},
  title = {Mixtral of Experts},
  url = {https://arxiv.org/abs/2401.04088},
  year = {2024}
}

@techreport{mosaicml2024dbrx,
  abstract = {DBRX is a transformer-based decoder-only large language model that uses a fine-grained mixture-of-experts (MoE) architecture. With 132B total parameters and 36B active parameters, DBRX uses 16 experts with 4 active during training or inference. The model was pre-trained on 12T tokens of text and code data with a 32K token context length. DBRX sets new state-of-the-art performance among open LLMs and provides capabilities previously limited to closed model APIs, surpassing GPT-3.5 and being competitive with Gemini 1.0 Pro. Inference is up to 2x faster than LLaMA2-70B while being approximately 40% of the size of Grok-1 in terms of both total and active parameter counts.},
  author = {Databricks MosaicML Team},
  institution = {Databricks},
  month = {3},
  note = {Available on GitHub: r̆lhttps://github.com/databricks/dbrx},
  title = {DBRX: A New State-of-the-Art Open LLM},
  type = {Technical Report},
  url = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
  year = {2024}
}

@misc{cohere2024commandr,
  abstract = {Command R+ is a large language model optimized for conversational interaction and long-context tasks, designed to enable companies to move beyond proof of concept and into production. The model features advanced capabilities including multi-step tool use, retrieval augmented generation (RAG), and multilingual support for over 10 languages. With a 128,000 token context window and enterprise-grade performance, Command R+ is specifically designed for complex RAG functionality and multi-step agents in business environments.},
  author = {Cohere Team},
  howpublished = {Large Language Model},
  note = {Optimized for conversational interaction, long-context tasks, and multi-step tool use. Features 128k token context window and multilingual support for 10+ languages.},
  title = {Command R+: A Scalable Tool-Use Language Model},
  url = {https://docs.cohere.com/docs/command-r-plus},
  year = {2024}
}

@article{grattafiori2024llama3,
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  author = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and others},
  institution = {Meta AI},
  journal = {arXiv preprint arXiv:2407.21783},
  month = {7},
  note = {Submitted 31 Jul 2024, last revised 23 Nov 2024},
  pdf = {https://arxiv.org/pdf/2407.21783},
  title = {The Llama 3 Herd of Models},
  url = {https://arxiv.org/abs/2407.21783},
  year = {2024}
}

@article{gemma2024gemma,
  abstract = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.},
  author = {Gemma Team and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and Sessa, Pier Giuseppe and Chowdhery, Aakanksha and Roberts, Adam and others},
  journal = {arXiv preprint arXiv:2403.08295},
  month = {3},
  note = {Submitted 13 Mar 2024, last revised 16 Apr 2024},
  openalex = {W4392822465},
  pdf = {https://arxiv.org/pdf/2403.08295.pdf},
  title = {Gemma: Open Models Based on Gemini Research and Technology},
  url = {https://arxiv.org/abs/2403.08295},
  year = {2024}
}

@misc{gu2024mamba,
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arXiv},
  author = {Gu, Albert and Dao, Tri},
  eprint = {2312.00752},
  note = {Outstanding Paper Award. arXiv preprint},
  openalex = {W4389326242},
  pdf = {https://arxiv.org/pdf/2312.00752.pdf},
  primaryclass = {cs.LG},
  title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  url = {https://arxiv.org/abs/2312.00752},
  year = {2024}
}

@article{lieber2024jamba,
  abstract = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration implemented, they end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. The authors study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. They also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. They make the weights of their implementation of Jamba publicly available under a permissive license.},
  author = {Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
  doi = {10.48550/arxiv.2403.19887},
  institution = {AI21 Labs},
  journal = {arXiv preprint arXiv:2403.19887},
  month = {3},
  openalex = {W4393399080},
  pdf = {https://arxiv.org/pdf/2403.19887.pdf},
  title = {Jamba: A Hybrid Transformer-Mamba Language Model},
  year = {2024}
}

@inproceedings{dao2024transformers,
  abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8× faster, while continuing to be competitive with Transformers on language modeling.},
  author = {Tri Dao and Albert Gu},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  editor = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  month = {7},
  openalex = {W4399317989},
  pages = {10041--10071},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/dao24a/dao24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  url = {https://proceedings.mlr.press/v235/dao24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{amos2024never,
  abstract = {Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $ extitonly the downstream task data$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.},
  author = {Ido Amos and Jonathan Berant and Ankit Gupta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Outstanding Paper Award, Oral Presentation},
  openalex = {W4387390330},
  pdf = {https://openreview.net/pdf?id=PdaPky8MUn},
  title = {Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors},
  url = {https://openreview.net/forum?id=PdaPky8MUn},
  year = {2024}
}

@inproceedings{darcet2024vision,
  abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle = {International Conference on Learning Representations},
  note = {Outstanding Paper Award},
  openalex = {W4387225729},
  pdf = {https://openreview.net/pdf?id=2dnO3LLiJ1},
  title = {Vision Transformers Need Registers},
  url = {https://openreview.net/forum?id=2dnO3LLiJ1},
  year = {2024}
}

@inproceedings{han2024lm,
  abstract = {Today's large language models (LLMs) typically train on short text segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues. Through theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis further reveals that commonly used techniques like truncating the attention window or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over the original model.},
  address = {Mexico City, Mexico},
  author = {Chi Han and Qifan Wang and Hao Peng and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  doi = {10.18653/v1/2024.naacl-long.222},
  month = {6},
  note = {Outstanding Paper Award},
  openalex = {W4401042914},
  pages = {3991--4008},
  pdf = {https://aclanthology.org/2024.naacl-long.222.pdf},
  publisher = {Association for Computational Linguistics},
  title = {LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models},
  url = {https://aclanthology.org/2024.naacl-long.222/},
  year = {2024}
}

@inproceedings{hahn2024why,
  abstract = {Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers' inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.},
  address = {Bangkok, Thailand},
  author = {Hahn, Michael and Rofin, Mark},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  doi = {10.18653/v1/2024.acl-long.800},
  month = {8},
  note = {Best Paper Award},
  openalex = {W4402671165},
  pages = {14973--15008},
  pdf = {https://aclanthology.org/2024.acl-long.800.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Why are Sensitive Functions Hard for Transformers?},
  url = {https://aclanthology.org/2024.acl-long.800},
  year = {2024}
}

@inproceedings{tian2024visual,
  abstract = {We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine 'next-scale prediction' or 'next-resolution prediction', diverging from the standard raster-scan 'next-token prediction'. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.},
  author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Best Paper Award},
  openalex = {W4393969561},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf},
  title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/9a24e284b187f662681440ba15c416fb-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{liu2025kan,
  abstract = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (``neurons''), KANs have learnable activation functions on edges (``weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. This seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, we show that KANs are useful collaborators helping scientists (re)discover mathematical and physical laws. We conclude that KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models.},
  author = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y and Tegmark, Max},
  booktitle = {International Conference on Learning Representations},
  pdf = {https://arxiv.org/pdf/2404.19756},
  title = {KAN: Kolmogorov-Arnold Networks},
  url = {https://openreview.net/forum?id=Ozo7qJ5vZi},
  year = {2024}
}

@article{grattafiori2024llama31,
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. The largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. It delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. The model publicly releases pre-trained and post-trained versions of the 405B parameter model. It includes Llama Guard 3 model for input and output safety. The paper presents experiments integrating image, video, and speech capabilities via a compositional approach and observes competitive performance with state-of-the-art on multimodal recognition tasks. The multimodal models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  author = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and others},
  eprint = {2407.21783},
  institution = {Meta AI},
  journal = {arXiv preprint arXiv:2407.21783},
  month = {7},
  pdf = {https://arxiv.org/pdf/2407.21783.pdf},
  primaryclass = {cs.CL},
  title = {The Llama 3 Herd of Models},
  url = {https://arxiv.org/abs/2407.21783},
  year = {2024}
}

@techreport{openai2024gpt4o,
  abstract = {GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. This system card provides detailed documentation of GPT-4o's capabilities, limitations, and safety evaluations across text, vision, and audio modalities.},
  author = {OpenAI},
  institution = {OpenAI},
  month = {10},
  note = {arXiv:2410.21276},
  pdf = {https://cdn.openai.com/gpt-4o-system-card.pdf},
  title = {GPT-4o System Card},
  url = {https://arxiv.org/abs/2410.21276},
  year = {2024}
}

@article{zelikman2024quiet,
  abstract = {When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%→10.9%) and CommonsenseQA (36.3%→47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.},
  author = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D.},
  doi = {10.48550/arXiv.2403.09629},
  journal = {arXiv preprint arXiv:2403.09629},
  month = {3},
  openalex = {W4392886466},
  pdf = {https://arxiv.org/pdf/2403.09629.pdf},
  title = {Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking},
  url = {https://arxiv.org/abs/2403.09629},
  year = {2024}
}

@inproceedings{anil2024small,
  abstract = {Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, the authors seek ways to reproduce and study training instability at smaller scales. First, they focus on two sources of training instability described in previous work: the growth of logits in attention layers and divergence of the output logits from the log probabilities. By measuring the relationship between learning rate and loss across scales, they show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts them to investigate the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate. They study methods such as warm-up, weight decay, and the MuParam, and combine techniques to train small models that achieve similar losses across orders of magnitude of learning rate variation. Finally, they conclude by studying two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model characteristics such as activation and gradient norms.},
  address = {Vienna, Austria},
  author = {Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie Everett and Alexander A. Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
  booktitle = {The Twelfth International Conference on Learning Representations},
  month = {5},
  note = {Oral},
  openalex = {W4387076505},
  pdf = {https://openreview.net/pdf?id=d8w0pmvXbZ},
  publisher = {OpenReview.net},
  title = {Small-scale proxies for large-scale Transformer training instabilities},
  url = {https://openreview.net/forum?id=d8w0pmvXbZ},
  year = {2024}
}

@inproceedings{puigcerver2024sparse,
  abstract = {Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.},
  address = {Vienna, Austria},
  author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Neumann, Cedric and Jenatton, Rodolphe and Susano Pinto, André and Keysers, Daniel and Houlsby, Neil},
  booktitle = {The Twelfth International Conference on Learning Representations},
  doi = {10.48550/arXiv.2308.00951},
  month = {5},
  openalex = {W4385968027},
  pdf = {https://openreview.net/pdf?id=jxpsAj7ltE},
  publisher = {OpenReview.net},
  title = {From Sparse to Soft Mixtures of Experts},
  url = {https://openreview.net/forum?id=jxpsAj7ltE},
  year = {2024}
}

@inproceedings{ahn2024transformers,
  abstract = {Transformers excel at in-context learning (ICL) -- learning from demonstrations without parameter updates. Recent work suggests that Transformers may internally run Gradient Descent (GD), a first-order optimization method, to perform ICL. This paper challenges this hypothesis and demonstrates that Transformers learn to approximate second-order optimization methods for ICL. For in-context linear regression, Transformers share a similar convergence rate as Iterative Newton's Method, both exponentially faster than GD. Empirically, predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations; thus, Transformers and Newton's method converge at roughly the same rate. The paper also shows that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. To corroborate the empirical findings, the authors prove that Transformers can implement k iterations of Newton's method with k + O(1) layers.},
  address = {Cambridge, MA},
  author = {Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria Florina and Lin, Hsuan-Tien},
  pages = {95617},
  pdf = {https://neurips.cc/paper_files/paper/2024/file/b2d4051f03a7038a2771dfbbe5c7b54e-Paper-Conference.pdf},
  publisher = {MIT Press},
  title = {Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/b2d4051f03a7038a2771dfbbe5c7b54e-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{wang2024loss,
  abstract = {The development of the internal structure of neural networks throughout training occurs in tandem with changes in the local geometry of the population loss. By quantifying the degeneracy of this geometry using the recently proposed Local Learning Coefficient, we show that the training process for a transformer language model can be decomposed into discrete developmental stages.},
  author = {Jesse Hoogland and George Wang and Matthew Farrugia-Roberts and Liam Carroll and Susan Wei and Daniel Murfet},
  booktitle = {HiLD at ICML 2024},
  month = {6},
  pages = {},
  pdf = {https://arxiv.org/pdf/2402.02364.pdf},
  title = {Loss landscape geometry reveals stagewise development of transformers},
  url = {https://openreview.net/forum?id=2JabyZjM5H},
  year = {2024}
}

@inproceedings{bick2024transformers,
  abstract = {Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key insight to our approach is that both Transformers and SSMs can be viewed as applying different forms of mixing matrices over the token sequences, which suggests a path for progressive distillation. We progressive distillation by first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Using only 3B tokens, we distill a Mamba-2 variant architecture from Phi-1.5, creating Phi-Mamba. On a variety of benchmarks, Phi-Mamba achieves performance that substantially exceeds typical subquadratic models trained from scratch, while maintaining fast inference characteristic of subquadratic architectures.},
  author = {Bick, Aviv and Li, Kevin Y. and Xing, Eric P. and Kolter, J. Zico and Gu, Albert},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4402502732},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3848fef259495bfd04d60cdc5c1b4db7-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/3848fef259495bfd04d60cdc5c1b4db7-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{wang2024mamba,
  abstract = {Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at MambaInLlama for distillation and SpeculativeMamba for speculative decoding.},
  address = {Red Hook, NY, USA},
  author = {Wang, Junxiong and Paliotta, Daniele and May, Avner and Rush, Alexander M. and Dao, Tri},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arxiv.2408.15237},
  openalex = {W4402705333},
  pages = {},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/723933067ad315269b620bc0d2c05cba-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {The Mamba in The Llama: Distilling and Accelerating Hybrid Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/723933067ad315269b620bc0d2c05cba-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{hwang2024hydra,
  abstract = {A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating mixer and channel layers. This paper studies a unifying matrix view of mixers that can be conceptualized as a linear map of the input sequence. Our framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs). Through this lens, we identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers. Our main contribution is Hydra, a natural bidirectional extension of the Mamba model, parameterized as a quasiseparable matrix mixer. As a drop-in replacement for attention layers, Hydra outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.},
  author = {Sukjun Hwang and Aakash Lahoti and Ratish Puduppully and Tri Dao and Albert Gu},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4400703021},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c7f795dc3b4eb6ae630695d90001a2f8-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/c7f795dc3b4eb6ae630695d90001a2f8-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{lin2024not,
  abstract = {Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that 'Not all tokens in a corpus are equally important for language model training'. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring training tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.},
  author = {Zhenghao Lin and Zhibin Gou and Yeyun Gong and Xiao Liu and Yelong Shen and Ruochen Xu and Chen Lin and Yujiu Yang and Jian Jiao and Nan Duan and Weizhu Chen},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Best Paper Runner-up Award},
  pages = {},
  pdf = {https://openreview.net/pdf?id=0NMzBwqaAJ},
  publisher = {Curran Associates, Inc.},
  title = {Not All Tokens Are What You Need for Pretraining},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/3322a9a72a1707de14badd5e552ff466-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{zisman2024road,
  abstract = {Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while maintaining the performance of $T$-dependent schedules. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum (such as AdamW). Our method is a direct consequence of a new theory which unifies scheduling and iterate averaging. An open source implementation of our method is available. The core algorithmic insight behind our method is that optimization can be viewed as a dynamical system, and iterate averaging is equivalent to applying a filter to this system. We leverage this insight to develop a new theory which unifies scheduling and iterate averaging. We show that our approach gives robust results on a wide range of problems from convex to large-scale deep learning.},
  author = {Aaron Defazio and Xingyu Alice Yang and Harsh Mehta and Konstantin Mishchenko and Ahmed Khaled and Ashok Cutkosky},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Main Conference Track},
  openalex = {W4399061508},
  pdf = {https://openreview.net/pdf?id=0XeNkkENuI},
  title = {The Road Less Scheduled},
  url = {https://openreview.net/forum?id=0XeNkkENuI},
  volume = {37},
  year = {2024}
}

@inproceedings{yun2024flexmoe,
  abstract = {Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router. The specialized router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer's Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE, highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios.},
  author = {Sukwon Yun and Inyoung Choi and Jie Peng and Yangfan Wu and Jingxuan Bao and Qiyiwen Zhang and Jiayi Xin and Qi Long and Tianlong Chen},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4403443979},
  pages = {},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b2f2af5403042b1344f4e93b35fb67d9-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts},
  volume = {37},
  year = {2024}
}

@inproceedings{thomm2024limits,
  abstract = {We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. The results demonstrate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Additionally, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models.},
  author = {Jonathan Thomm and Giacomo Camposampiero and Aleksandar Terzic and Michael Hersche and Bernhard Schölkopf and Abbas Rahimi},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {},
  pdf = {https://neurips.cc/paper_files/paper/2024/file/0e797d5139ad94fc2dc2080c09119f29-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Limits of Transformer Language Models on Learning to Compose Algorithms},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/0e797d5139ad94fc2dc2080c09119f29-Abstract-Conference.html},
  volume = {37},
  year = {2024}
}

@inproceedings{shi2024why,
  abstract = {Large language models (LLM) have emerged as a powerful tool for AI, with the key ability of in-context learning (ICL), where they can perform well on unseen tasks based on a brief series of task examples without necessitating any adjustments to the model parameters. One recent interesting mysterious observation is that models of different scales may have different ICL behaviors: larger models tend to be more sensitive to noise in the test context. This work studies this observation theoretically aiming to improve the understanding of LLM and ICL. We analyze two stylized settings: (1) linear regression with one-layer single-head linear transformers and (2) parity classification with two-layer multiple attention heads transformers (non-linear data and non-linear model). In both settings, we give closed-form optimal solutions and find that smaller models emphasize important hidden features while larger ones cover more hidden features; thus, smaller models are more robust to noise while larger ones are more easily distracted, leading to different ICL behaviors. This sheds light on where transformers pay attention to and how that affects ICL. Preliminary experimental results on large base and chat models provide positive support for our analysis.},
  author = {Zhenmei Shi and Junyi Wei and Zhuoyan Xu and Yingyu Liang},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  editor = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  month = {7},
  openalex = {W4399251925},
  pages = {44991--45013},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/shi24f/shi24f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why Larger Language Models Do In-context Learning Differently?},
  url = {https://proceedings.mlr.press/v235/shi24f.html},
  volume = {235},
  year = {2024}
}

@inproceedings{anonymous2025visionmamba,
  abstract = {Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$ imes$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$ imes$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models.},
  author = {Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  openalex = {W4391013663},
  pages = {62429--62442},
  pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24f/zhu24f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  volume = {235},
  year = {2024}
}

@inproceedings{jelassi2024transformers,
  abstract = {Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as ``generalized state space models'' (GSSMs). Previous work has shown that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. In this work, we study this trade-off in detail. We prove that a two-layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. When evaluating pretrained large language models, we find that transformer models dramatically outperform state space models at copying and retrieving information from context, even when the GSSM has 10$ imes$ more parameters. Our results suggest that the efficiency gains of GSSMs may come at the cost of their ability to extract information from context.},
  arxiv = {2402.01032},
  author = {Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  openalex = {W4391555700},
  pages = {21502--21521},
  pdf = {https://proceedings.mlr.press/v235/jelassi24a/jelassi24a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Repeat After Me: Transformers are Better than State Space Models at Copying},
  url = {https://proceedings.mlr.press/v235/jelassi24a.html},
  volume = {235},
  year = {2024}
}

@inproceedings{hashemi2024unveiling,
  address = {New York, NY, USA},
  author = {Hashemi, Soheil and Wang, Yizhi and Chen, Wenhao and Srinivasan, Krishna and Wu, Cheng-Hao and Singh, Avirup and Sil, Avi},
  booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  keywords = {dimension importance estimation, dense retrieval, information retrieval, reproducibility, generalizability},
  location = {Washington DC, USA},
  publisher = {Association for Computing Machinery},
  series = {SIGIR '24},
  title = {Unveiling DIME: Reproducibility, Generalizability, and Formal Analysis of Dimension Importance Estimation for Dense Retrieval},
  year = {2024}
}

@inproceedings{yang2024gated,
  abstract = {Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention has been found to underperform standard quadratic attention on a wide range of tasks. To close this performance gap, we study the mathematical properties of linear attention and identify two key limitations: (1) linear attention cannot express certain sparse attention patterns observed in standard attention; and (2) prior linear attention algorithms do not leverage the full representation capacity of the attention mechanism. Based on these insights, we develop a hardware-efficient algorithm for linear attention that is amenable to chunked computation and allows for varying chunk sizes at no additional cost during training. We also generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer performs competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments, while being more efficient in terms of both memory and wall-clock time. Notably, GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.},
  author = {Songlin Yang and Bailin Wang and Yikang Shen and Rameswar Panda and Yoon Kim},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  openalex = {W4389713725},
  pages = {56481--56520},
  pdf = {https://proceedings.mlr.press/v235/yang24ay/yang24ay.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  url = {https://proceedings.mlr.press/v235/yang24ay.html},
  volume = {235},
  year = {2024}
}

@article{chowdhery2023palm,
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Parker, Parth and Rauf, Toju and Wang, Yifeng and Karakic, Mahdis and Warkentin, Tara and Liu, Nan and Fiedel, Noah and Chen, Mark and Pellat, Thorsten and Kitching, James and Powell, Antonia and Dee, Michael and Lanham, Tris and Sanghai, Srivatsan and Lamm, Abe and Krikun, Maksym and Sohn, Kathy and Sprechmann, Pablo and Kwiatkowski, Tom and Wang, Ruoxin and Petrov, Slav and Dean, Jeff and others},
  doi = {10.48550/arXiv.2204.02311},
  journal = {Journal of Machine Learning Research},
  number = {240},
  openalex = {W4224308101},
  pages = {1--113},
  pdf = {https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf},
  title = {PaLM: Scaling Language Modeling with Pathways},
  url = {https://jmlr.org/papers/v24/22-1144.html},
  volume = {24},
  year = {2023}
}

@techreport{anil2023palm2,
  abstract = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM.},
  author = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chen, Zongwei and Chi, Ed H. and Cui, Jiaxi and Kenealy, Kathleen and Lauritzen, Koray and Lee, Kyu-Young and Mercier, Kevin and Ni, Jianmo and Norouzi, Mohammad and others},
  doi = {10.48550/arXiv.2305.10403},
  institution = {Google},
  month = {5},
  note = {arXiv:2305.10403},
  openalex = {W4377121468},
  pdf = {https://arxiv.org/pdf/2305.10403.pdf},
  title = {PaLM 2 Technical Report},
  url = {https://arxiv.org/abs/2305.10403},
  year = {2023}
}

@techreport{gemini2023gemini,
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. We report on a comprehensive evaluation on text, image, audio, and video benchmarks. Our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks -- notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward developing them responsibly.},
  archiveprefix = {arXiv},
  author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and others},
  eprint = {2312.11805},
  institution = {Google},
  month = {12},
  openalex = {W4390041933},
  pdf = {https://arxiv.org/pdf/2312.11805},
  primaryclass = {cs.CL},
  title = {Gemini: A Family of Highly Capable Multimodal Models},
  url = {https://arxiv.org/abs/2312.11805},
  year = {2023}
}

@article{touvron2023llama2,
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  doi = {10.48550/arxiv.2307.09288},
  eprint = {2307.09288},
  institution = {Meta AI},
  journal = {arXiv preprint arXiv:2307.09288},
  month = {7},
  openalex = {W4384918448},
  pdf = {https://arxiv.org/pdf/2307.09288.pdf},
  primaryclass = {cs.CL},
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  url = {https://arxiv.org/abs/2307.09288},
  year = {2023}
}

@article{jiang2023mistral,
  abstract = {We introduce Mistral 7B, a 7--billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses Llama 2 13B -- chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  doi = {10.48550/arXiv.2310.06825},
  institution = {Mistral AI},
  journal = {arXiv preprint arXiv:2310.06825},
  month = {10},
  openalex = {W4387561528},
  pdf = {https://arxiv.org/pdf/2310.06825.pdf},
  title = {Mistral 7B},
  url = {https://arxiv.org/abs/2310.06825},
  year = {2023}
}

@inproceedings{dettmers2023qlora,
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU.},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4378509449},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
  title = {QLoRA: Efficient Finetuning of Quantized LLMs},
  url = {https://openreview.net/forum?id=OUIFPHEgJU},
  volume = {36},
  year = {2023}
}

@inproceedings{poli2023hyena,
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. We propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
  address = {Honolulu, Hawaii, USA},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  doi = {10.48550/arxiv.2302.10866},
  month = {7},
  openalex = {W4321593177},
  pages = {28043--28078},
  pdf = {https://proceedings.mlr.press/v202/poli23a/poli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  volume = {202},
  year = {2023}
}

@article{sun2023retentive,
  abstract = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  doi = {10.48550/arXiv.2307.08621},
  eprint = {2307.08621},
  journal = {arXiv preprint arXiv:2307.08621},
  month = {7},
  openalex = {W4384648484},
  pdf = {https://arxiv.org/pdf/2307.08621.pdf},
  primaryclass = {cs.CL},
  title = {Retentive Network: A Successor to Transformer for Large Language Models},
  url = {https://arxiv.org/abs/2307.08621},
  year = {2023}
}

@techreport{poli2023stripedhyena,
  abstract = {StripedHyena is the first alternative model competitive with the best open-source Transformers in short and long-context evaluations. The model is a deep signal processing, hybrid architecture composed of rotary (grouped) attention and gated convolutions arranged in Hyena blocks, with improved scaling over decoder-only Transformers. StripedHyena is designed to leverage the specialization of each of its layer classes, with Hyena layers implementing the bulk of the computation required for sequence processing and attention layers supplementing the ability to perform targeted pattern recall. The model achieves comparable performance with Llama-2, Yi and Mistral 7B on OpenLLM leaderboard tasks while outperforming on long-context summarization tasks. For training and fine-tuning workloads on long sequences, StripedHyena 7B is consistently faster than optimized Transformers (>10%, >20% and >50% end-to-end faster than FlashAttention v2 processing sequences of lengths 32k, 64k and 128k, at batch size 1) with the speedup growing larger on longer sequences and with larger batches. Additionally, StripedHyena 7B provides a >50% reduced memory footprint during autoregressive generation compared to a Transformer (both with grouped-query attention).},
  author = {Poli, Michael and Wang, Jue and Massaroli, Stefano and Quesnelle, Jeffrey and Carlow, Ryan and Nguyen, Eric and Fu, Daniel Y},
  doi = {10.57967/hf/1595},
  institution = {Together AI},
  month = {12},
  note = {Technical report and model release. Available models: StripedHyena-Hessian-7B (base model) and StripedHyena-Nous-7B (chat model)},
  title = {StripedHyena-7B: A 7B Language Model that is 10% Attention and 90% Gated Convolution},
  type = {Technical Report},
  url = {https://github.com/togethercomputer/stripedhyena},
  year = {2023}
}

@inproceedings{ma2023mega,
  abstract = {The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.},
  author = {Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  booktitle = {The Eleventh International Conference on Learning Representations},
  month = {2},
  openalex = {W4296932804},
  pdf = {https://openreview.net/pdf?id=qNLe3iq2El},
  title = {Mega: Moving Average Equipped Gated Attention},
  url = {https://openreview.net/forum?id=qNLe3iq2El},
  year = {2023}
}

@article{li2023transformers,
  abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. The paper seeks to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
  author = {Li, Wenzhe and Luo, Hao and Lin, Zichuan and Zhang, Chongjie and Lu, Zongqing and Ye, Deheng},
  doi = {10.48550/arxiv.2301.03044},
  journal = {Transactions on Machine Learning Research},
  month = {9},
  openalex = {W4315588599},
  pdf = {https://openreview.net/pdf?id=r30yuDPvf2},
  title = {A Survey on Transformers in Reinforcement Learning},
  url = {https://arxiv.org/abs/2301.03044},
  volume = {2023},
  year = {2023}
}

@inproceedings{wen2023transformers,
  abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance.},
  author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  booktitle = {Proceedings of the 32nd International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2023/759},
  openalex = {W4385763767},
  pages = {6778--6786},
  pdf = {https://www.ijcai.org/proceedings/2023/0759.pdf},
  title = {Transformers in Time Series: A Survey},
  url = {https://www.ijcai.org/proceedings/2023/759},
  year = {2023}
}

@inproceedings{zeng2023transformers,
  abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research. Specifically, while Transformers are arguably the most successful solution for extracting semantic correlations among elements in a long sequence, the goal in time series modeling is to extract temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce an embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. We hope this surprising finding opens up new research directions for the LTSF task.},
  archiveprefix = {arXiv},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v37i9.26317},
  eprint = {2205.13504},
  number = {9},
  openalex = {W4382203079},
  pages = {11121--11128},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/26317/26089},
  primaryclass = {cs.AI},
  title = {Are Transformers Effective for Time Series Forecasting?},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/26317},
  volume = {37},
  year = {2023}
}

@inproceedings{wang2023decodingtrust,
  abstract = {Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing interest of practitioners and public. Yet, while literature on trustworthiness remains limited, these models are proposed for sensitive applications like healthcare and finance -- where mistakes can be costly. This work proposes a comprehensive evaluation of large language models, focusing on GPT-4 and GPT-3.5, considering perspectives including toxicity, stereotype bias, adversarial robustness, privacy, machine ethics, and fairness. Key findings reveal previously unpublished vulnerabilities: models can be easily misled to generate toxic, biased outputs and leak private information from training data and conversation history. While GPT-4 is usually more trustworthy than GPT-3.5, it remains vulnerable to jailbreaking through system or user prompts. The benchmark is publicly available at https://decodingtrust.github.io/.},
  address = {New Orleans, LA, USA},
  author = {Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and Truong, Sang T. and Arora, Simran and Mazeika, Mantas and Hendrycks, Dan and Lin, Zinan and Cheng, Yu and Koyejo, Sanmi and Song, Dawn and Li, Bo},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  month = {12},
  note = {Outstanding Paper Award, Datasets and Benchmarks Track},
  openalex = {W4381586841},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/63cb9921eecf51bfad27a99b2c53dd6d-Paper-Datasets_and_Benchmarks.pdf},
  publisher = {Curran Associates},
  series = {NIPS'23},
  title = {DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models},
  url = {https://openreview.net/forum?id=kaHpo8OZw2},
  volume = {36},
  year = {2023}
}

@techreport{openai2023gpt4,
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  author = {OpenAI},
  institution = {OpenAI},
  month = {3},
  number = {arXiv:2303.08774},
  openalex = {W4327810158},
  pdf = {https://arxiv.org/pdf/2303.08774.pdf},
  title = {GPT-4 Technical Report},
  type = {Technical Report},
  url = {https://arxiv.org/abs/2303.08774},
  year = {2023}
}

@article{you2023applications,
  abstract = {Transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. In this review, we provide a comprehensive viewpoint of facilitating research in the field of NLP and the applications of transformers in bioinformatics. We revisit the basics of transformer-based language models, summarize the latest developments in the transformer-based language models and then review the applications of transformers in bioinformatics and biomedical downstream tasks such as sequence analysis, gene expression, proteomics, spatial transcriptomics, etc. We hope that transformer-based language models not only benefit the computer science community but also the broader community of bioinformaticians and biologists, and further provide insights for future bioinformatics research across multiple disciplines that are unattainable by traditional methods.},
  author = {Zhang, Shuang and Fan, Rui and Liu, Yuti and Chen, Shuang and Liu, Qiao and Zeng, Wanwen},
  doi = {10.1093/bioadv/vbad001},
  journal = {Bioinformatics Advances},
  month = {1},
  number = {1},
  openalex = {W4315641887},
  pages = {vbad001},
  publisher = {Oxford University Press},
  title = {Applications of transformer-based language models in bioinformatics: a survey},
  url = {https://doi.org/10.1093/bioadv/vbad001},
  volume = {3},
  year = {2023}
}

@inproceedings{hoffmann2022training,
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. Following this approach, we predict that Chinchilla should be trained on 1.4 trillion tokens. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$ imes$ more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.48550/arXiv.2203.15556},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  openalex = {W4225591000},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Training Compute-Optimal Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  volume = {35},
  year = {2022}
}

@article{thoppilan2022lamda,
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Cohen, Aaron Daniel and Roberts, Adam and Molina, Alejandra and Butryna, Alena and Hutchinson, Ben and Zevenbergen, Ben and Aguera-Arcas, Blaise Hilary and Chang, Chung-ching and Cui, Claire and Chen, Dehao and Lepikhin, Dmitry and Chi, Ed H. and Hoffman-John, Erin and Lee, Hongrae and Krivokon, Igor and Qin, James and Fenton, Joe and Soraker, Johnny and Meier-Hellstern, Kathy and Olson, Kristen and Aroyo, Lora Mois and Bosma, Maarten Paul and Pickett, Marc Joseph and Menegali, Marcelo Amorim and Croak, Marian and Dı́az, Mark and Lamm, Matthew and Krikun, Maxim and Morris, Meredith Ringel and Le, Quoc V. and Bernstein, Rachel and Rajakumar, Ravi and Kurzweil, Ray and Zheng, Steven and Duke, Toju and Doshi, Tulsee and Zhao, Vincent Y. and Prabhakaran, Vinodkumar and Rusch, Will and Li, YaGuang and Huang, Yanping and Zhou, Yanqi and Xu, Yuanzhong and Chen, Zhifeng},
  doi = {10.48550/arXiv.2201.08239},
  journal = {arXiv preprint arXiv:2201.08239},
  month = {1},
  openalex = {W4226399820},
  title = {LaMDA: Language Models for Dialog Applications},
  url = {https://arxiv.org/abs/2201.08239},
  year = {2022}
}

@article{zhang2022opt,
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  eprint = {2205.01068},
  institution = {Meta AI},
  journal = {arXiv preprint arXiv:2205.01068},
  month = {5},
  pdf = {https://arxiv.org/pdf/2205.01068.pdf},
  primaryclass = {cs.CL},
  title = {OPT: Open Pre-trained Transformer Language Models},
  url = {https://arxiv.org/abs/2205.01068},
  year = {2022}
}

@article{bigscience2022bloom,
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  author = {Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and others},
  doi = {10.48550/arXiv.2211.05100},
  journal = {arXiv preprint arXiv:2211.05100},
  month = {11},
  openalex = {W4311642023},
  pages = {1--106},
  pdf = {https://arxiv.org/pdf/2211.05100.pdf},
  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  url = {https://arxiv.org/abs/2211.05100},
  year = {2022}
}

@article{fedus2022switch,
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources.},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  issn = {1533-7928},
  journal = {Journal of Machine Learning Research},
  number = {120},
  openalex = {W4287391717},
  pages = {1--39},
  pdf = {https://jmlr.org/papers/volume23/21-0998/21-0998.pdf},
  title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  url = {https://jmlr.org/papers/v23/21-0998.html},
  volume = {23},
  year = {2022}
}

@inproceedings{dao2022flashattention,
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware---accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention, 3x speedup on GPT-2 (seq. length 1K), and 2.4x speedup on long-range arena (seq. length 1K-4K). FlashAttention, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  openalex = {W4281758439},
  pages = {16344--16359},
  pdf = {https://neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  volume = {35},
  year = {2022}
}

@inproceedings{gu2022efficiently,
  abstract = {A central goal of sequence modeling is designing a single principled model that can address data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed by simulating the fundamental state space (SSM) showed that appropriate matrix choices could handle dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general solution. The authors propose Structured State Space (S4) based on a new parameterization of SSM, showing it can be computed much more efficiently than prior approaches while preserving theoretical strengths. Their technique involves conditioning with low-rank correction, allowing diagonalized stable reduction of SSM to a well-studied Cauchy kernel. S4 achieves strong empirical results across diverse established benchmarks, including: (i) 91% accuracy on sequential CIFAR-10 without augmentation or auxiliary losses, (ii) substantially closing the gap in image and language generation while being 60x faster, and (iii) state-of-the-art performance on every task in the Long Range Arena benchmark, solving challenging Path-X lengths up to 16k where all prior work fails.},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  booktitle = {International Conference on Learning Representations},
  note = {Outstanding Paper Honorable Mention},
  openalex = {W3209374680},
  pdf = {https://openreview.net/pdf?id=uYLFoz1vlAC},
  title = {Efficiently Modeling Long Sequences with Structured State Spaces},
  url = {https://openreview.net/forum?id=uYLFoz1vlAC},
  year = {2022}
}

@techreport{ramesh2022dalle2,
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that this approach improves image diversity with minimal loss in photorealism and caption similarity. We also discover that the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. Our decoders conditioned on image representations can produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the explicit image representations allow us to apply common image processing techniques such as interpolation, arithmetic, and nearest neighbor retrieval. Hierarchical generation also offers us insight into how the model organizes visual and semantic concepts.},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  doi = {10.48550/arxiv.2204.06125},
  institution = {OpenAI},
  month = {4},
  openalex = {W4224035735},
  pdf = {https://arxiv.org/pdf/2204.06125.pdf},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  url = {https://arxiv.org/abs/2204.06125},
  year = {2022}
}

@inproceedings{alayrac2022flamingo,
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L. and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\ŉkowski, Mikołaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4225323055},
  pages = {23716--23736},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  url = {https://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@misc{rae2021scaling,
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. We hope this analysis enables more informed decisions around whether and how to deploy large language models.},
  archiveprefix = {arXiv},
  author = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Chris Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
  eprint = {2112.11446},
  pdf = {https://arxiv.org/pdf/2112.11446.pdf},
  primaryclass = {cs.CL},
  title = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
  url = {https://arxiv.org/abs/2112.11446},
  year = {2021}
}

@inproceedings{choromanski2021rethinking,
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy J. and Weller, Adrian},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  month = {5},
  openalex = {W3125056032},
  pdf = {https://openreview.net/pdf?id=Ua6zuk0WRH},
  publisher = {OpenReview.net},
  title = {Rethinking Attention with Performers},
  year = {2021}
}

@inproceedings{tay2021long,
  abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. However, there is no well-established consensus on how to evaluate this class of models. Most efficient Transformers are often evaluated on a plethora of tasks and datasets, making it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.},
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  booktitle = {International Conference on Learning Representations},
  openalex = {W3103682594},
  pdf = {https://openreview.net/pdf?id=qVyeW-grC2k},
  title = {Long Range Arena: A Benchmark for Efficient Transformers},
  url = {https://openreview.net/forum?id=qVyeW-grC2k},
  year = {2021}
}

@inproceedings{dosovitskiy2021image,
  abstract = {While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer directly applied to sequences of image patches can perform very well on classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized and small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art, requiring substantially fewer computational resources to train.},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {International Conference on Learning Representations},
  month = {5},
  openalex = {W3119786062},
  pdf = {https://openreview.net/pdf?id=YicbFdNTTy},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  year = {2021}
}

@inproceedings{liu2021swin,
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. Swin Transformer achieves strong performance on the task of image classification (87.3 top-1 accuracy on ImageNet-1K), object detection (58.7 box AP and 51.1 mask AP on COCO test-dev), and semantic segmentation (53.5 mIoU on ADE20K val), significantly surpassing previous state-of-the-art by a large margin.},
  address = {Montreal, QC, Canada},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  doi = {10.1109/iccv48922.2021.00986},
  note = {Best Paper Award (Marr Prize)},
  openalex = {W3138516171},
  organization = {IEEE},
  pages = {10012--10022},
  pdf = {https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf},
  title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper},
  year = {2021}
}

@inproceedings{chen2021decision,
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, Decision Transformer can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  openalex = {W3169291081},
  pages = {15084--15097},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  volume = {34},
  year = {2021}
}

@inproceedings{he2021deberta,
  abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  openalex = {W3122890974},
  pdf = {https://openreview.net/pdf?id=XPZIaotutsD},
  publisher = {OpenReview.net},
  title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  url = {https://openreview.net/forum?id=XPZIaotutsD},
  year = {2021}
}

@inproceedings{radford2021learning,
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  openalex = {W3166396011},
  pages = {8748--8763},
  pdf = {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  volume = {139},
  year = {2021}
}

@inproceedings{jaegle2021perceiver,
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Meila, Marina and Zhang, Tong},
  openalex = {W3169064633},
  pages = {4651--4664},
  pdf = {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Perceiver: General Perception with Iterative Attention},
  volume = {139},
  year = {2021}
}

@inproceedings{lan2020albert,
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  address = {Addis Ababa, Ethiopia},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle = {International Conference on Learning Representations},
  doi = {10.48550/arxiv.1909.11942},
  month = {4},
  openalex = {W2996428491},
  pages = {1--17},
  pdf = {https://openreview.net/pdf?id=H1eA7AEtvS},
  publisher = {OpenReview.net},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  url = {https://openreview.net/forum?id=H1eA7AEtvS},
  year = {2020}
}

@inproceedings{brown2020language,
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  address = {Red Hook, NY, USA},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  openalex = {W4292779060},
  pages = {1877--1901},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume = {33},
  year = {2020}
}

@article{raffel2020exploring,
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new 'Colossal Clean Crawled Corpus', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam P. and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  issn = {1532-4435},
  journal = {Journal of Machine Learning Research},
  number = {140},
  openalex = {W3082274269},
  pages = {1--67},
  pdf = {https://jmlr.org/papers/volume21/20-074/20-074.pdf},
  publisher = {MIT Press},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url = {https://jmlr.org/papers/v21/20-074.html},
  volume = {21},
  year = {2020}
}

@article{beltagy2020longformer,
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  eprint = {2004.05150},
  journal = {arXiv preprint arXiv:2004.05150},
  month = {4},
  openalex = {W3024785695},
  pdf = {https://arxiv.org/pdf/2004.05150.pdf},
  primaryclass = {cs.CL},
  title = {Longformer: The Long-Document Transformer},
  url = {https://arxiv.org/abs/2004.05150},
  year = {2020}
}

@inproceedings{kitaev2020reformer,
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($Lłog L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  booktitle = {International Conference on Learning Representations},
  openalex = {W4295838474},
  pdf = {https://openreview.net/pdf?id=rkgNKkHtvB},
  title = {Reformer: The Efficient Transformer},
  url = {https://openreview.net/forum?id=rkgNKkHtvB},
  year = {2020}
}

@article{wang2020linformer,
  abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the  extitLinformer, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  eprint = {2006.04768},
  journal = {arXiv preprint arXiv:2006.04768},
  month = {6},
  openalex = {W3033529678},
  pages = {2006.04768},
  pdf = {http://arxiv.org/pdf/2006.04768},
  primaryclass = {cs.LG},
  title = {Linformer: Self-Attention with Linear Complexity},
  year = {2020}
}

@article{shazeer2020glu,
  abstract = {Gated Linear Units (GLU) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations of GLU are explored, using different nonlinear (or even linear) functions in place of sigmoid. GLU variants are tested in the feed-forward sublayers of the Transformer sequence-to-sequence model, finding that some of them yield quality improvements over the typically-used ReLU or GELU activations.},
  author = {Shazeer, Noam},
  doi = {10.48550/arxiv.2002.05202},
  journal = {arXiv preprint arXiv:2002.05202},
  month = {2},
  openalex = {W4301581299},
  pdf = {https://arxiv.org/pdf/2002.05202.pdf},
  title = {GLU Variants Improve Transformer},
  url = {https://arxiv.org/abs/2002.05202},
  year = {2020}
}

@inproceedings{clark2020electra,
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  booktitle = {8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  openalex = {W4287824654},
  pdf = {https://openreview.net/pdf?id=r1xMH1BtvB},
  publisher = {OpenReview.net},
  title = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  url = {https://openreview.net/forum?id=r1xMH1BtvB},
  year = {2020}
}

@article{kaplan2020scaling,
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  doi = {10.48550/arxiv.2001.08361},
  institution = {OpenAI},
  journal = {arXiv preprint arXiv:2001.08361},
  month = {1},
  openalex = {W3001279689},
  pdf = {https://arxiv.org/pdf/2001.08361.pdf},
  title = {Scaling Laws for Neural Language Models},
  url = {https://arxiv.org/abs/2001.08361},
  year = {2020}
}

@inproceedings{devlin2019bert,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address = {Minneapolis, Minnesota},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi = {10.18653/v1/N19-1423},
  month = {6},
  openalex = {W2896457183},
  pages = {4171--4186},
  pdf = {https://aclanthology.org/N19-1423.pdf},
  publisher = {Association for Computational Linguistics},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {https://aclanthology.org/N19-1423},
  year = {2019}
}

@techreport{radford2019language,
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. The model's capacity is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  institution = {OpenAI},
  pdf = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year = {2019}
}

@inproceedings{dai2019transformer,
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  address = {Florence, Italy},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  doi = {10.18653/v1/P19-1285},
  month = {7},
  openalex = {W2964110616},
  pages = {2978--2988},
  pdf = {https://aclanthology.org/P19-1285.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  url = {https://aclanthology.org/P19-1285/},
  year = {2019}
}

@article{liu2019roberta,
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  doi = {10.48550/arXiv.1907.11692},
  eprint = {1907.11692},
  journal = {arXiv preprint arXiv:1907.11692},
  openalex = {W2965373594},
  pdf = {https://arxiv.org/pdf/1907.11692.pdf},
  primaryclass = {cs.CL},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  url = {https://arxiv.org/abs/1907.11692},
  year = {2019}
}

@inproceedings{sanh2019distilbert,
  abstract = {As transfer learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets is challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by the teacher model, we introduce a triple loss combining language modeling, distillation and cosine-distance losses.},
  archiveprefix = {arXiv},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle = {5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
  doi = {10.48550/arxiv.1910.01108},
  eprint = {1910.01108},
  month = {12},
  openalex = {W2978017171},
  pages = {33},
  pdf = {https://www.emc2-ai.org/assets/docs/neurips-19/emc2-neurips19-paper-33.pdf},
  primaryclass = {cs.CL},
  publisher = {EMC2},
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  url = {https://www.emc2-ai.org/assets/docs/neurips-19/emc2-neurips19-paper-33.pdf},
  year = {2019}
}

@inproceedings{yun2019graph,
  abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, identifying useful connections between unconnected nodes on the original graph, and learning effective node representation on new graphs in an end-to-end fashion. The Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections (meta-paths). Our experiments show that GTNs learn new graph structures based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
  author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d' Alché-Buc, F. and Fox, E. and Garnett, R.},
  openalex = {W2970066309},
  pages = {11960--11970},
  pdf = {https://proceedings.neurips.cc/paper/2019/file/9d63484abb477c97640154d40595a3bb-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Graph Transformer Networks},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9d63484abb477c97640154d40595a3bb-Abstract.html},
  volume = {32},
  year = {2019}
}

@inproceedings{yang2019xlnet,
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
  address = {Red Hook, NY, USA},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan R. and Le, Quoc V.},
  booktitle = {Advances in Neural Information Processing Systems 32},
  openalex = {W2970597249},
  pages = {5753--5763},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html},
  volume = {32},
  year = {2019}
}

@article{shoeybi2019megatron,
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  doi = {10.48550/arXiv.1909.08053},
  journal = {arXiv preprint arXiv:1909.08053},
  month = {9},
  openalex = {W2973727699},
  pdf = {https://arxiv.org/pdf/1909.08053.pdf},
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  url = {https://arxiv.org/abs/1909.08053},
  year = {2019}
}

@techreport{radford2018improving,
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, improving upon the state of the art in 9 out of the 12 tasks studied.},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  institution = {OpenAI},
  month = {6},
  note = {Introduces GPT-1, the first generative pre-trained transformer model},
  pdf = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  title = {Improving Language Understanding by Generative Pre-Training},
  url = {https://openai.com/research/language-unsupervised},
  year = {2018}
}

@inproceedings{vaswani2017attention,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  address = {Red Hook, NY, USA},
  arxiv = {1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  openalex = {W2626778328},
  pages = {5998--6008},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Attention is All You Need},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
  volume = {30},
  year = {2017}
}
