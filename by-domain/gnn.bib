@comment{< /dev/null}

@comment{< /dev/null}

@misc{xiong2025survey,
  abstract      = {Temporal networks have gained significant prominence in the past decade for modelling dynamic interactions within complex systems. A key challenge in this domain is Temporal Link Prediction (TLP), which aims to forecast future connections by analysing historical network structures across various applications including social network analysis. While existing surveys have addressed specific aspects of TLP, they typically lack a comprehensive framework that distinguishes between representation and inference methods. This survey bridges this gap by introducing a novel taxonomy that explicitly examines representation and inference from existing methods, providing a novel classification of approaches for TLP. We analyse how different representation techniques capture temporal and structural dynamics, examining their compatibility with various inference methods for both transductive and inductive prediction tasks. Our taxonomy not only clarifies the methodological landscape but also reveals promising unexplored combinations of existing techniques. This taxonomy provides a systematic foundation for emerging challenges in TLP, including model explainability and scalable architectures for complex temporal networks.},
  archiveprefix = {arXiv},
  author        = {Xiong, Jiafeng and Zareie, Ahmad and Sakellariou, Rizos},
  eprint        = {2502.21185},
  pdf           = {https://arxiv.org/pdf/2502.21185},
  primaryclass  = {cs.LG},
  title         = {A Survey of Link Prediction in Temporal Networks},
  year          = {2025}
}

@inproceedings{baek2021accurate,
  abstract      = {Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.},
  author        = {Jinheon Baek and Minki Kang and Sung Ju Hwang},
  booktitle     = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3--7, 2021},
  month         = {5},
  openalex      = {W3128785555},
  pdf           = {https://openreview.net/pdf?id=JHcqXGaqiGn},
  publisher     = {OpenReview.net},
  title         = {Accurate Learning of Graph Representations with Graph Multiset Pooling},
  url           = {https://openreview.net/forum?id=JHcqXGaqiGn},
  year          = {2021}
}

@inproceedings{balcilar2021analyzing,
  abstract      = {In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically demonstrate some equivalence of the graph convolution process regardless of whether it is designed in the spatial or spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph neural networks into one common framework. This general framework allows to lead a spectral analysis of the most popular GNNs, explaining their performance and showing their limits according to spectral point of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases. Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the majority of GNN is limited to only low-pass and inevitably it fails.},
  author        = {Muhammet Balcilar and Guillaume Renton and Pierre Héroux and Benoit Gau ̆zère and Sébastien Adam and Paul Honeine},
  booktitle     = {Proceedings of the 9th International Conference on Learning Representations},
  note          = {This paper provides a spectral analysis of GNN expressivity. It demonstrates that the expressive power of GNNs is tied to their ability to compute graph moments (e.g., counts of closed walks), which are related to the eigenvalues of the graph's adjacency matrix. The authors show that GNNs can be designed to match the power of the 1-WL test by including a readout layer that computes these spectral moments.},
  openalex      = {W3123408649},
  pdf           = {https://openreview.net/pdf?id=-qh0M9XWxnv},
  publisher     = {OpenReview.net},
  title         = {Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective},
  url           = {https://openreview.net/forum?id=-qh0M9XWxnv},
  year          = {2021}
}

@misc{dwivedi2025relational,
  abstract      = {Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.},
  archiveprefix = {arXiv},
  author        = {Dwivedi, Vijay Prakash and Kanatsoulis, Charilaos I. and Huang, Shenyang and Leskovec, Jure},
  doi           = {10.48550/arXiv.2506.16654},
  eprint        = {2506.16654},
  month         = {June},
  note          = {KDD 2025 Tutorial. arXiv:2506.16654},
  pdf           = {https://arxiv.org/pdf/2506.16654},
  primaryclass  = {cs.LG},
  title         = {Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures},
  url           = {https://arxiv.org/abs/2506.16654},
  year          = {2025}
}

@inproceedings{bechler2024graph,
  abstract      = {Predictions over graphs play a crucial role in various domains, including social networks and medicine. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Although a graph-structure is provided as input to the GNN, in some cases the best solution can be obtained by ignoring it. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the given graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We analyze the implicit bias of gradient-descent learning of GNNs and prove that when the ground truth function does not use the graphs, GNNs are not guaranteed to learn a solution that ignores the graph, even with infinite data.},
  author        = {Bechler-Speicher, Maya and Amos, Ido and Gilad-Bachrach, Ran and Globerson, Amir},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4386620370},
  pages         = {3284--3304},
  pdf           = {https://proceedings.mlr.press/v235/bechler-speicher24a/bechler-speicher24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Graph Neural Networks Use Graphs When They Shouldn't},
  volume        = {235},
  year          = {2024}
}

@inproceedings{schutt2021equivariant,
  abstract      = {Message passing neural networks have become a method of choice for learning on graphs, in particular the prediction of chemical properties and the acceleration of molecular dynamics studies. While they readily scale to large training data sets, previous approaches have proven to be less data efficient than kernel methods. We identify limitations of invariant representations as a major reason and extend the message passing formulation to rotationally equivariant representations. On this basis, we propose the polarizable atom interaction neural network (PaiNN) and improve on common molecule benchmarks over previous networks, while reducing model size and inference time. We leverage the equivariant atomwise representations obtained by PaiNN for the prediction of tensorial properties. We apply this to the simulation of molecular spectra, achieving speedups of 4-5 orders of magnitude compared to the electronic structure reference.},
  author        = {Schütt, Kristof T. and Unke, Oliver T. and Gastegger, Michael},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  openalex      = {W3126679164},
  pages         = {9377--9388},
  pdf           = {http://proceedings.mlr.press/v139/schutt21a/schutt21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Equivariant Message Passing for the Prediction of Tensorial Properties and Molecular Spectra},
  volume        = {139},
  year          = {2021}
}

@inproceedings{bruna2014spectral,
  abstract      = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  archiveprefix = {arXiv},
  author        = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
  booktitle     = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint        = {1312.6203},
  note          = {This work was the first to rigorously generalize CNNs to arbitrary graphs by leveraging spectral graph theory. It defined the convolution operation in the graph Fourier domain, where the graph Laplacian's eigenvectors serve as the basis. While groundbreaking, the method's reliance on explicit eigendecomposition resulted in a computational complexity of O(N²), making it impractical for large graphs and dependent on a fixed graph structure.},
  pdf           = {https://openreview.net/pdf?id=DQNsQf-UsoDBa},
  title         = {Spectral Networks and Locally Connected Networks on Graphs},
  url           = {https://openreview.net/forum?id=DQNsQf-UsoDBa},
  year          = {2014}
}

@inproceedings{chamberlain2021grand,
  abstract      = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
  author        = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria I. and Bronstein, Michael M. and Webb, Stefan and Rossi, Emanuele},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  editor        = {Meila, Marina and Zhang, Tong},
  note          = {This paper connects GNNs to the physics of diffusion by framing graph convolutions as a discretization of a diffusion partial differential equation (PDE). This perspective allows for a principled way to design GNNs that are robust to oversmoothing. The authors propose Graph Neural Diffusion (GRAND), which learns the diffusion process itself, and show that it can handle both homophilic and heterophilic graphs effectively.},
  openalex      = {W4287115436},
  pages         = {1407--1418},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {GRAND: Graph Neural Diffusion},
  url           = {https://proceedings.mlr.press/v139/chamberlain21a/chamberlain21a.pdf},
  volume        = {139},
  year          = {2021}
}

@inproceedings{chen2020simple,
  abstract      = {Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks.},
  author        = {Chen, Ming and Wei, Zhewei and Huang, Zengfeng and Ding, Bolin and Li, Yaliang},
  booktitle     = {Proceedings of the 37th International Conference on Machine Learning},
  openalex      = {W3034492151},
  pages         = {1725--1735},
  pdf           = {https://proceedings.mlr.press/v119/chen20v/chen20v.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Simple and Deep Graph Convolutional Networks},
  volume        = {119},
  year          = {2020}
}

@article{chen2024survey,
  abstract      = {Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining, computer vision, and natural language processing. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this survey, we provide a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. We divide their applications in computer vision into five categories according to the modality of input data: 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of carefully designed taxonomies, providing a comprehensive summary and discussion on the recent works.},
  author        = {Chen, Chaoqi and Wu, Yushuang and Dai, Qiyuan and Zhou, Hong-Yu and Xu, Mutian and Yang, Sibei and Han, Xiaoguang and Yu, Yizhou},
  doi           = {10.1109/TPAMI.2024.3445463},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number        = {12},
  openalex      = {W4401692204},
  pages         = {10297--10318},
  pdf           = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10638815},
  publisher     = {IEEE},
  title         = {A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective},
  volume        = {46},
  year          = {2024}
}

@misc{chen2025residual,
  abstract      = {Graph neural networks (GNNs) have achieved remarkable empirical success in processing and representing graph-structured data across various domains. However, a significant challenge known as 'oversmoothing' persists, where vertex features become nearly indistinguishable in deep GNNs, severely restricting their expressive power and practical utility. In this work, we analyze the asymptotic oversmoothing rates of deep GNNs with and without residual connections by deriving explicit convergence rates for a normalized vertex similarity measure. Our analytical framework is grounded in the multiplicative ergodic theorem. Through this analysis, we demonstrate that adding residual connections effectively mitigates or prevents oversmoothing across several broad families of parameter distributions. Our theoretical findings are strongly supported by numerical experiments.},
  archiveprefix = {arXiv},
  author        = {Chen, Ziang and Lin, Zhengjiang and Chen, Shi and Polyanskiy, Yury and Rigollet, Philippe},
  doi           = {10.48550/arXiv.2501.00762},
  eprint        = {2501.00762},
  note          = {Submitted to arXiv on 1 Jan 2025, last revised 4 Jan 2025},
  pdf           = {https://arxiv.org/pdf/2501.00762},
  primaryclass  = {cs.LG},
  title         = {Residual connections provably mitigate oversmoothing in graph neural networks},
  url           = {https://arxiv.org/abs/2501.00762},
  year          = {2025}
}

@inproceedings{cotta2021reconstruction,
  abstract      = {Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are significantly harder to implement, may not scale well, and have not been shown to outperform well-tuned GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge. In this work, we show the extent to which graph reconstruction -- reconstructing a graph from its subgraphs -- can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals.},
  author        = {Cotta, Leonardo and Morris, Christopher and Ribeiro, Bruno},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann N. and Liang, Percy and Vaughan, Jennifer Wortman},
  openalex      = {W4286913454},
  pages         = {1713--1726},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {Reconstruction for Powerful Graph Representations},
  url           = {https://proceedings.neurips.cc/paper/2021/file/0d8080853a54f8985276b0130266a657-Paper.pdf},
  volume        = {34},
  year          = {2021}
}

@inproceedings{dai2024multi,
  abstract      = {Multi-view clustering has been widely explored for its effectiveness in harmonizing heterogeneity along with consistency in different views of data. Despite the significant progress made by recent works, the performance of most existing methods is heavily reliant on strong priori information regarding the true cluster number K, which is rarely feasible in real-world scenarios. In this paper, we propose a novel graph-based multi-view clustering algorithm to infer unknown K through a graph consistency reward mechanism. To be specific, we evaluate the cluster indicator matrix during each iteration with respect to diverse K. We formulate the inference process of unknown K as a parsimonious reinforcement learning paradigm, where the reward is measured by inter-cluster connectivity. As a result, our approach is capable of independently producing the final clustering result, free from the input of a predefined cluster number. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach in comparison to existing state-of-the-art methods.},
  author        = {Hao Dai and Yang Liu and Peng Su and Hecheng Cai and Shudong Huang and Jiancheng Lv},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {9846--9855},
  pdf           = {https://proceedings.mlr.press/v235/dai24b/dai24b.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Multi-View Clustering by Inter-cluster Connectivity Guided Reward},
  url           = {https://proceedings.mlr.press/v235/dai24b.html},
  volume        = {235},
  year          = {2024}
}

@misc{dang2025equivariant,
  abstract      = {Molecular interactions often involve high-order relationships that cannot be fully captured by traditional graph-based models limited to pairwise connections. Hypergraphs naturally extend graphs by enabling multi-way interactions, making them well-suited for modeling complex molecular systems. In this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network framework that integrates symmetry-aware representations to improve molecular modeling. By enforcing the equivariance under relevant transformation groups, our approach preserves geometric and topological properties, leading to more robust and physically meaningful representations. We examine a range of equivariant architectures and demonstrate that integrating symmetry constraints leads to notable performance gains on large-scale molecular datasets. Experiments on both small and large molecules show that high-order interactions offer limited benefits for small molecules but consistently outperform 2D graphs on larger ones. Adding geometric features to these high-order structures further improves the performance, emphasizing the value of spatial information in molecular learning.},
  archiveprefix = {arXiv},
  author        = {Tien Dang and Truong-Son Hy},
  eprint        = {2505.05650},
  month         = {5},
  pdf           = {https://arxiv.org/pdf/2505.05650},
  primaryclass  = {cs.LG},
  title         = {EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks},
  url           = {https://arxiv.org/abs/2505.05650},
  year          = {2025}
}

@inproceedings{defferrard2016convolutional,
  abstract      = {In many domains, data are represented by graphs. Examples include social networks, brain connectomes, and words' embedding. The challenge is then to design machine learning algorithms for this type of data. In this work, we present a generalization of convolutional neural networks (CNNs) to work on irregularly structured data. We build upon spectral graph theory to define localized convolutional filters on graphs. The filters are defined as Chebyshev polynomials of the scaled graph Laplacian to overcome the prohibitive computational cost of eigendecomposition. We show that the resulting networks are fast to train and have the same computational complexity as classical CNNs. Experiments demonstrate the ability of the system to learn local, stationary, and compositional features on graphs.},
  author        = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Known as ChebNet, this paper provided a crucial breakthrough for spectral GNNs by making them computationally feasible. It cleverly avoids the costly eigendecomposition by approximating spectral filters with Chebyshev polynomials, which are defined recursively on the graph Laplacian. This approach not only reduced complexity but also produced filters that are localized in the vertex domain, a key property of traditional CNNs.},
  openalex      = {W2964321699},
  pages         = {3844--3852},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
  series        = {NIPS'16},
  title         = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  volume        = {29},
  year          = {2016}
}

@inproceedings{deng2021vector,
  abstract      = {Invariance and equivariance to rotations have been widely discussed in the 3D deep learning community for point clouds. Most proposed methods either use complex mathematical tools that may limit their applicability, or are tied to specific network architectures. We introduce a general framework built on top of Vector Neuron representations for creating SO(3)-equivariant neural networks for 3D point cloud processing. Our vector neurons extend neurons from 1D scalars to 3D vectors and enable a simple mapping of SO(3) actions to latent spaces, thereby providing a framework for building equivariance in common neural operations including linear layers, non-linearities, pooling, and normalizations. Despite its simplicity, networks built with vector neurons perform comparably to specialized equivariant methods on 3D classification and segmentation tasks while being more general and applicable to diverse network backbones.},
  author        = {Deng, Congyue and Litany, Or and Duan, Yueqi and Poulenard, Adrien and Tagliasacchi, Andrea and Guibas, Leonidas J.},
  booktitle     = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi           = {10.1109/iccv48922.2021.01198},
  month         = {10},
  note          = {This paper introduces "vector neurons," a simple and general framework for building SO(3)-equivariant neural networks. Each feature is represented as a 3D vector that rotates with the input. Linear layers are replaced by equivariant matrix-vector multiplications, and nonlinearities are applied to vector norms. This framework provides a conceptually simple yet powerful way to construct equivariant models for 3D data.},
  openalex      = {W3158545781},
  pages         = {12200--12209},
  pdf           = {https://openaccess.thecvf.com/content/ICCV2021/papers/Deng_Vector_Neurons_A_General_Framework_for_SO3-Equivariant_Networks_ICCV_2021_paper.pdf},
  publisher     = {IEEE},
  title         = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},
  url           = {https://github.com/FlyingGiraffe/vnn},
  year          = {2021}
}

@inproceedings{dwivedi2021generalization,
  abstract      = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  author        = {Vijay Prakash Dwivedi and Xavier Bresson},
  booktitle     = {Proceedings of the Fifth International Workshop on Deep Learning on Graphs: Methods and Applications (DLG-AAAI)},
  note          = {This work was one of the first to propose a full generalization of the Transformer architecture for graphs. It incorporates graph structure by injecting Laplacian eigenvectors as positional encodings and using the adjacency matrix to bias the attention scores. This paper helped establish the core components---structural encoding and attention biasing---that define most modern Graph Transformer architectures.},
  openalex      = {W3113177135},
  pdf           = {https://arxiv.org/pdf/2012.09699},
  title         = {A Generalization of Transformer Networks to Graphs},
  url           = {https://arxiv.org/abs/2012.09699},
  year          = {2021}
}

@inproceedings{joly2024graph,
  abstract      = {Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph. In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.},
  author        = {Antonin Joly and Nicolas Keriven},
  booktitle     = {Advances in Neural Information Processing Systems 37},
  openalex      = {W4399151906},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d041d6bb47c01a4ce327a1773703e9a0-Paper-Conference.pdf},
  title         = {Graph Coarsening with Message-Passing Guarantees},
  volume        = {37},
  year          = {2024}
}

@inproceedings{errica2023adaptive,
  abstract      = {Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven models for predicting properties of complex systems represented as graphs. These models rely on a message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectures with the ability to adapt their depth and filter messages along the way. With theoretical and empirical arguments, we show that this strategy better captures long-range interactions, by competing with the state of the art on five node and graph prediction datasets.},
  archiveprefix = {arXiv},
  author        = {Errica, Federico and Christiansen, Henrik and Zaverkin, Viktor and Maruyama, Takashi and Niepert, Mathias and Alesiani, Francesco},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  eprint        = {2312.16560},
  openalex      = {W4390437281},
  pdf           = {https://arxiv.org/pdf/2312.16560},
  primaryclass  = {cs.LG},
  series        = {Proceedings of Machine Learning Research},
  title         = {Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching},
  url           = {https://icml.cc/virtual/2025/poster/44906},
  year          = {2025}
}

@article{gao2024efficient,
  abstract      = {Training Temporal Graph Neural Networks (T-GNNs) on large-scale, long-horizon dynamic graphs can be computationally prohibitive due to the need to process a massive history of interactions. Dynamic graphs play a crucial role in various real-world applications, such as link prediction and node classification on social media and e-commerce platforms. T-GNNs have emerged as a leading approach for handling dynamic graphs, using temporal message passing to compute temporal node embeddings. However, training existing T-GNNs on large-scale dynamic graphs is prohibitively expensive due to the ill-suited batching scheme and significant data access overhead. This paper proposes ETC, a generic framework to accelerate T-GNN training. ETC incorporates novel data batching strategies that enable larger training batches and reduce redundant data access volume, thereby improving model computation efficiency and achieving significant training speedup.},
  author        = {Gao, Shihong and Li, Yiming and Zhang, Xin and Shen, Yanyan and Shao, Yingxia and Chen, Lei},
  doi           = {10.14778/3641204.3641215},
  journal       = {Proceedings of the VLDB Endowment},
  number        = {5},
  pages         = {1060--1072},
  pdf           = {https://www.vldb.org/pvldb/vol17/p1060-gao.pdf},
  title         = {ETC: Efficient Training of Temporal Graph Neural Networks over Large-scale Dynamic Graphs},
  volume        = {17},
  year          = {2024}
}

@inproceedings{geerts2022expressiveness,
  abstract      = {Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNN architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Through a simple analysis of the obtained expressions, in terms of the number of indexes and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.},
  author        = {Floris Geerts and Juan L. Reutter},
  booktitle     = {The Tenth International Conference on Learning Representations},
  month         = {4},
  note          = {This paper provides an elegant framework to analyze the expressive power of GNNs using a procedural tensor language. By viewing GNNs as expressions in this language, bounds on their separation power can be derived in terms of the Weisfeiler-Leman tests. This approach offers a general methodology for assessing any GNN architecture without needing deep knowledge of WL-tests.},
  openalex      = {W4223655873},
  pdf           = {https://openreview.net/pdf?id=wIzUeM3TAU},
  publisher     = {OpenReview.net},
  title         = {Expressiveness and Approximation Properties of Graph Neural Networks},
  url           = {https://openreview.net/forum?id=wIzUeM3TAU},
  year          = {2022}
}

@inproceedings{gilmer2017neural,
  abstract      = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  author        = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  booktitle     = {Proceedings of the 34th International Conference on Machine Learning},
  note          = {This paper introduced the Message Passing Neural Network (MPNN) framework, a highly influential abstraction that unified many prior GNN variants (including GCN) into a single, generalized algorithm. The framework consists of a message function, an update function, and a readout function. This work provided a common vocabulary and a powerful conceptual tool for the GNN community, demonstrating its effectiveness on quantum chemistry tasks and setting a new state-of-the-art on the QM9 benchmark.},
  openalex      = {W2606780347},
  pages         = {1263--1272},
  pdf           = {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Neural Message Passing for Quantum Chemistry},
  volume        = {70},
  year          = {2017}
}

@misc{guo2023gpt4graph,
  abstract      = {Large language models (LLM) like ChatGPT have become indispensable to artificial general intelligence (AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this paper, we conduct an empirical study to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks that evaluate the LLMs' capabilities in graph understanding. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities.},
  archiveprefix = {arXiv},
  author        = {Jiayan Guo and Lun Du and Hengyu Liu and Mengyu Zhou and Xinyi He and Shi Han},
  eprint        = {2305.15066},
  month         = {5},
  note          = {This paper provides one of the first comprehensive empirical evaluations of the ability of large language models like GPT-4 to understand and reason about graph-structured data. The authors design a benchmark with various graph tasks (e.g., node classification, graph classification, link prediction) and evaluate LLMs in a zero-shot setting. The results show that while LLMs have impressive reasoning capabilities, they still struggle with tasks that require complex structural understanding, highlighting key areas for future research.},
  openalex      = {W4378465454},
  pdf           = {https://arxiv.org/pdf/2305.15066.pdf},
  primaryclass  = {cs.LG},
  title         = {GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking},
  url           = {https://arxiv.org/abs/2305.15066},
  year          = {2023}
}

@inproceedings{hamilton2017inductive,
  abstract      = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  author        = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {GraphSAGE was a landmark paper that addressed the transductive limitation of GCNs, enabling GNNs to be applied to large, evolving graphs in an inductive setting. Instead of learning a unique embedding for each node, GraphSAGE learns a set of aggregator functions (e.g., mean, max, or LSTM) that sample and aggregate features from a node's local neighborhood. This allows it to efficiently generate embeddings for previously unseen nodes.},
  openalex      = {W4294558607},
  pages         = {1024--1034},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
  title         = {Inductive Representation Learning on Large Graphs},
  volume        = {30},
  year          = {2017}
}

@misc{han2022geometrically,
  abstract      = {Many scientific problems require to process data in the form of geometric graphs. Unlike generic graph data, geometric graphs exhibit symmetries of translations, rotations, and/or reflections. Researchers have leveraged such inductive bias and developed geometrically equivariant Graph Neural Networks (GNNs) to better characterize the geometry and topology of geometric graphs. Despite fruitful achievements, it still lacks a survey to depict how equivariant GNNs are progressed, which in turn hinders the further development of equivariant GNNs. To this end, based on the necessary but concise mathematical preliminaries, we analyze and classify existing methods into three groups regarding how the message passing and aggregation in GNNs are represented. We also summarize the benchmarks as well as the related datasets to facilitate later researches for methodology development and experimental evaluation.},
  archiveprefix = {arXiv},
  author        = {Han, Jiaqi and Rong, Yu and Xu, Tingyang and Huang, Wenbing},
  eprint        = {2202.07230},
  month         = {2},
  note          = {This survey provides a comprehensive overview of the rapidly growing field of geometrically equivariant GNNs. It introduces the necessary mathematical preliminaries of group theory and equivariance and proposes a taxonomy that classifies existing methods into three groups based on how they represent and process geometric information during message passing. The survey serves as an excellent entry point into the theoretical foundations and architectural designs of this subfield.},
  openalex      = {W4221149947},
  pdf           = {https://arxiv.org/pdf/2202.07230},
  primaryclass  = {cs.LG},
  title         = {Geometrically Equivariant Graph Neural Networks: A Survey},
  url           = {https://arxiv.org/abs/2202.07230},
  year          = {2022}
}

@inproceedings{han2022gmixup,
  abstract      = {This work develops \emphmixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose $\mathcalG$-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that $\mathcalG$-Mixup substantially improves the generalization and robustness of GNNs.},
  author        = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  doi           = {10.48550/arXiv.2202.07179},
  openalex      = {W4221150126},
  pages         = {8230--8248},
  pdf           = {https://proceedings.mlr.press/v162/han22c/han22c.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {G-Mixup: Graph Data Augmentation for Graph Classification},
  volume        = {162},
  year          = {2022}
}

@inproceedings{han2022vision,
  abstract      = {Network architecture plays a key role in the deep learning-based computer vision system. Like convolution operations in the Convolutional Neural Networks (CNNs), graph convolution is being investigated to replace or complement the standard convolution for visual tasks. In this paper, we study the vision graph neural network (ViG), a new backbone network that represents images as a graph structure. We split the image into a number of patches which are viewed as nodes and construct a graph by connecting the nearest neighbors. Based on this graph representation of images, we build our Vision Graph Neural Network (ViG) with two basic modules: a Grapher module with graph convolution for aggregating and updating graph information, and an FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our architecture. We hope our pioneer study of GNN on general visual tasks will provide useful inspiration and experience for future research.},
  author        = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {While not a traditional Graph Transformer, this paper showcases the power of applying graph-based reasoning to vision tasks. It proposes ViG, an architecture that represents an image as a graph of patches and uses GNN operations to exchange information. This approach is more flexible than the rigid grid structure of CNNs or the sequence structure of Vision Transformers, allowing it to better capture irregular objects and complex scenes.},
  openalex      = {W4320167334},
  pages         = {8291--8303},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/3743e69c8e47eb2e6d3afaea80e439fb-Paper-Conference.pdf},
  title         = {Vision GNN: An Image is Worth Graph of Nodes},
  volume        = {35},
  year          = {2022}
}

@misc{hoang2024survey,
  abstract      = {The transformer architecture has shown remarkable success in various domains, such as natural language processing and computer vision. When it comes to graph learning, transformers are required not only to capture the interactions between pairs of nodes but also to preserve graph structures connoting the underlying relations and proximity between them, showing the expressive power to capture different graph structures. Accordingly, various structure-preserving graph transformers have been proposed and widely used for various tasks, such as graph-level tasks in bioinformatics and chemoinformatics. However, strategies related to graph structure preservation have not been well organized and systematized in the literature. In this paper, we provide a comprehensive overview of structure-preserving graph transformers and generalize these methods from the perspective of their design objective. First, we divide strategies into four main groups: node feature modulation, context node sampling, graph rewriting, and transformer architecture improvements. We then further divide the strategies according to the coverage and goals of graph structure preservation. Furthermore, we also discuss challenges and future directions for graph transformer models to preserve the graph structure and understand the nature of graphs.},
  archiveprefix = {arXiv},
  author        = {Van Thuy Hoang and O-Joun Lee},
  eprint        = {2401.16176},
  month         = {1},
  openalex      = {W4391376854},
  pdf           = {https://arxiv.org/pdf/2401.16176},
  primaryclass  = {cs.LG},
  title         = {A Survey on Structure-Preserving Graph Transformers},
  url           = {https://arxiv.org/abs/2401.16176},
  year          = {2024}
}

@inproceedings{hou2022graphmae,
  abstract      = {Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning---which heavily relies on structural data augmentation and complicated training strategies---has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE---a simple graph autoencoder with careful designs---can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs.},
  address       = {Washington, DC, USA},
  author        = {Hou, Zhenyu and Liu, Xiao and Cen, Yukuo and Dong, Yuxiao and Yang, Hongxia and Wang, Chunjie and Tang, Jie},
  booktitle     = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi           = {10.1145/3534678.3539321},
  isbn          = {978-1-4503-9385-0},
  month         = {8},
  openalex      = {W4290876361},
  pages         = {594--604},
  pdf           = {https://dl.acm.org/doi/pdf/10.1145/3534678.3539321},
  publisher     = {ACM},
  series        = {KDD '22},
  title         = {GraphMAE: Self-Supervised Masked Graph Autoencoders},
  url           = {https://doi.org/10.1145/3534678.3539321},
  year          = {2022}
}

@inproceedings{huang2021combining,
  abstract      = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs, and there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. For many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods.},
  author        = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
  booktitle     = {International Conference on Learning Representations},
  note          = {This work challenges the necessity of complex GNNs for all node classification tasks. The authors show that a simple model combining two stages—label propagation (to spread label information across the graph) followed by a simple MLP on the resulting features—can outperform many state-of-the-art GNNs, especially on heterophilic graphs. This highlights the importance of separating feature transformation from label propagation.},
  openalex      = {W3120587221},
  pdf           = {https://arxiv.org/pdf/2010.13993.pdf},
  title         = {Combining Label Propagation and Simple Models Out-performs Graph Neural Networks},
  url           = {https://openreview.net/forum?id=8E1-f3VhX1o},
  year          = {2021}
}

@misc{huang2025graph,
  abstract      = {This tutorial explores the emerging paradigm of graph foundation models—pre-trained models that can be adapted for various graph tasks. Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. The tutorial covers current challenges including structural alignment, heterogeneity, scalability, and evaluation, along with methodological approaches encompassing backbone architectures, pretraining strategies, and adaptation mechanisms. We examine open research questions in developing generalizable graph models that can handle diverse graph structures and tasks across different domains.},
  address       = {Toronto, Canada},
  author        = {Huang, Qitian and He, Yiwen and Zhou, Xuanhao and Xie, Yuan and Wang, Liang and Zhang, Weinan and Qu, Meng and Yu, Yiping},
  howpublished  = {KDD 2025 Tutorial},
  note          = {Tutorial presentation at the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  title         = {Graph Foundation Models: Challenges, Methods, and Open Questions},
  url           = {https://kdd2025.kdd.org/tutorials/},
  year          = {2025}
}

@inproceedings{jin2022condensation,
  abstract      = {Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). The goal of graph condensation is to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach this goal by imitating the GNN training trajectory on the original graph via optimizing a gradient matching loss and design a strategy to condense node features and structural information simultaneously. Experiments on various datasets demonstrate the effectiveness of the proposed method. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer while reducing the graph size by more than 99.9%.},
  author        = {Wei Jin and Lingxiao Zhao and Shichang Zhang and Yozen Liu and Jiliang Tang and Neil Shah},
  booktitle     = {The Tenth International Conference on Learning Representations},
  note          = {To address the challenge of training GNNs on massive graphs, this paper introduces the concept of graph condensation. The goal is to synthesize a small, synthetic graph such that a GNN trained on this small graph achieves comparable performance to one trained on the original large graph. This work pioneers a new direction for scalable GNN training by focusing on data-centric compression rather than model-centric sampling.},
  openalex      = {W3206450082},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=WLEx3Jo4QaB},
  publisher     = {OpenReview.net},
  title         = {Graph Condensation for Graph Neural Networks},
  url           = {https://openreview.net/forum?id=WLEx3Jo4QaB},
  year          = {2022}
}

@article{jin2022forecasting,
  abstract      = {Multivariate time series forecasting has long received significant attention in real-world applications, such as energy consumption and traffic prediction. While recent methods demonstrate good forecasting abilities, they suffer from three fundamental limitations. (i) Discrete neural architectures: Interlacing individually parameterized spatial and temporal blocks to encode rich underlying patterns leads to discontinuous latent state trajectories and higher forecasting numerical errors. (ii) High complexity: Discrete approaches complicate models with dedicated designs and redundant parameters, leading to higher computational and memory overheads. (iii) Reliance on graph priors: Relying on predefined static graph structures limits their effectiveness and practicability in real-world applications. To address these limitations, we propose a continuous model to forecast multivariate time series with dynamic graph neural ODEs (MTGODE). Specifically, we first design a continuous graph neural network to capture spatial patterns and then use a neural ODE to model temporal dynamics. To reduce the reliance on graph priors, we implement a graph learning module to infer the dynamic graph structure at each time step. Experiments on five time series benchmarks consistently demonstrate the superiority of MTGODE, with an average improvement of 3.68 percent in mean absolute error (MAE) compared with state-of-the-art discrete methods.},
  author        = {Jin, Ming and Koh, Hong Yoon and Wen, Qingsong and Zambon, Daniele and Alippi, Cesare and Webb, Geoffrey I. and King, Irwin and Pan, Shirui},
  journal       = {arXiv preprint arXiv:2202.08408},
  month         = {2},
  note          = {This work focuses on multivariate time series forecasting, where dynamic graph neural ODEs are used to capture both spatial and temporal dependencies. A key contribution is a graph learning module that can infer the dynamic graph structure at each time step, rather than relying on predefined static graph structures.},
  title         = {Multivariate Time Series Forecasting with Dynamic Graph Neural ODEs},
  url           = {https://arxiv.org/abs/2202.08408},
  year          = {2022}
}

@inproceedings{kipf2017semi,
  abstract      = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  author        = {Thomas N. Kipf and Max Welling},
  booktitle     = {5th International Conference on Learning Representations, ICLR 2017},
  note          = {This seminal paper introduced the Graph Convolutional Network (GCN), which became the most widely adopted GNN architecture. The authors simplified ChebNet by restricting the spectral filters to a first-order approximation, resulting in a remarkably simple, effective, and efficient layer-wise propagation rule that has a direct spatial interpretation as aggregating and transforming messages from neighboring nodes. This work democratized GNNs, but its formulation is transductive, requiring the full graph to be known during training.},
  openalex      = {W2964015378},
  pdf           = {https://openreview.net/pdf?id=SJU4ayYgl},
  title         = {Semi-Supervised Classification with Graph Convolutional Networks},
  url           = {https://openreview.net/forum?id=SJU4ayYgl},
  year          = {2017}
}

@inproceedings{kofinas2024graph,
  abstract      = {Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods.},
  address       = {Vienna, Austria},
  arxiv         = {2403.12143},
  author        = {Kofinas, Miltiadis and Knyazev, Boris and Zhang, Yan and Chen, Yunlu and Burghouts, Gertjan J. and Gavves, Efstratios and Snoek, Cees G. M. and Zhang, David W.},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  code          = {https://github.com/mkofinas/neural-graphs},
  editor        = {Kim, B. and Yue, Y. and Chaudhuri, S. and Fragkiadaki, K. and Khan, M. and Sun, Y.},
  keywords      = {Deep weight space, Graph neural networks, Transformers, Permutation equivariance},
  month         = {5},
  note          = {ICLR 2024 Oral},
  openalex      = {W4393100134},
  pdf           = {https://openreview.net/pdf?id=oO6FsMyDBt},
  publisher     = {OpenReview.net},
  title         = {Graph Neural Networks for Learning Equivariant Representations of Neural Networks},
  url           = {https://openreview.net/forum?id=oO6FsMyDBt},
  year          = {2024}
}

@inproceedings{kolouri2021wasserstein,
  abstract      = {We present Wasserstein Embedding for Graph Learning (WEGL), a novel and fast framework for embedding entire graphs in a vector space, in which various machine learning models are applicable for graph-level prediction tasks. We leverage new insights on defining similarity between graphs as a function of the similarity between their node embedding distributions. Specifically, we use the Wasserstein distance to measure the dissimilarity between node embeddings of different graphs. Unlike prior work, we avoid pairwise calculation of distances between graphs and reduce the computational complexity from quadratic to linear in the number of graphs. WEGL calculates Monge maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph.},
  author        = {Kolouri, Soheil and Naderializadeh, Navid and Rohde, Gustavo K. and Hoffmann, Heiko},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W3119577510},
  pdf           = {https://openreview.net/pdf?id=9T5O27-UKR},
  title         = {Wasserstein Embedding for Graph Learning},
  url           = {https://openreview.net/forum?id=9T5O27-UKR},
  year          = {2021}
}

@inproceedings{kong2023conditional,
  abstract      = {Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. The method resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. We also adopt a multi-round progressive full-shot approach instead of an autoregressive approach to predict the CDR sequences. Empirically, the proposed model significantly surpasses state-of-the-art models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. The relative improvement to baselines is about 23% in antigen-binding CDR design and 34% for affinity optimization.},
  author        = {Kong, Xiangzhe and Huang, Wenbing and Liu, Yang},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  note          = {Outstanding Paper Award},
  openalex      = {W4291962725},
  pdf           = {https://openreview.net/pdf?id=LFHFQbjxIiP},
  title         = {Conditional Antibody Design as 3D Equivariant Graph Translation},
  url           = {https://openreview.net/forum?id=LFHFQbjxIiP},
  year          = {2023}
}

@inproceedings{airale2025simple,
  abstract      = {Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph learning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive power by encoding both structural and positional information into the edge representation. However, RWSE cannot always distinguish between edges that belong to different local graph patterns, which reduces its ability to capture the full structural complexity of graphs. This work introduces Simple Path Structural Encoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically and experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of graph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable, we propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant performance improvements over RWSE on various benchmarks, including molecular and long-range graph datasets.},
  archiveprefix = {arXiv},
  author        = {Airale, Louis and Longa, Antonio and Rigon, Mattia and Passerini, Andrea and Passerone, Roberto},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  doi           = {10.48550/arXiv.2502.09365},
  eprint        = {2502.09365},
  month         = {7},
  note          = {ICML 2025 Poster},
  openalex      = {W4407576002},
  pdf           = {https://arxiv.org/pdf/2502.09365},
  primaryclass  = {cs.LG},
  series        = {Proceedings of Machine Learning Research},
  title         = {Simple Path Structural Encoding for Graph Transformers},
  url           = {https://openreview.net/forum?id=t3zwUqibMq},
  volume        = {235},
  year          = {2025}
}

@misc{li2020deepergcn,
  abstract      = {Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations. We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction.},
  archiveprefix = {arXiv},
  author        = {Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
  eprint        = {2006.07739},
  month         = {6},
  openalex      = {W3035649237},
  pdf           = {https://arxiv.org/pdf/2006.07739.pdf},
  primaryclass  = {cs.LG},
  title         = {DeeperGCN: All You Need to Train Deeper GCNs},
  year          = {2020}
}

@inproceedings{li2023graph,
  author        = {Li, Haoyang and Zhang, Ziwei and Zhao, Xingqian and Shen, Zhitao and Guan, Dongxiao and Xu, Ming and Zhu, Junzhou},
  booktitle     = {Proceedings of the Eleventh International Conference on Learning Representations},
  note          = {This paper tackles the problem of out-of-distribution (OOD) generalization for GNNs. It proposes a new learning paradigm called Graph In-Distribution Invariance (GDI) that learns representations invariant to certain in-distribution variations (e.g., adding non-causal edges) that are assumed to be irrelevant to the task. This encourages the GNN to focus on the underlying causal structures, leading to better generalization when the graph distribution shifts at test time.},
  openalex      = {W4376114877},
  title         = {Graph Neural Networks with In-Distribution Invariance for Out-of-Distribution Generalization},
  url           = {https://openreview.net/pdf?id=8wqm2gIxxS},
  year          = {2023}
}

@inproceedings{li2024graph,
  abstract      = {Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data.},
  author        = {Li, Xiner and Gui, Shurui and Luo, Youzhi and Ji, Shuiwang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4380991315},
  pages         = {27846--27874},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24y/li24y.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization},
  volume        = {235},
  year          = {2024}
}

@inproceedings{li2024spatio,
  abstract      = {Traffic flow forecasting is a classical spatio-temporal data mining problem with many real-world applications. Recently, various methods based on Graph Neural Networks (GNN) have been proposed for the problem and achieved impressive prediction performance. However, we argue that the majority of existing methods disregard the importance of certain nodes (referred to as pivotal nodes) that naturally exhibit extensive connections with multiple other nodes. Predicting on pivotal nodes poses a challenge due to their complex spatio-temporal dependencies compared to other nodes. In this paper, we propose a novel GNN-based method called Spatio-Temporal Pivotal Graph Neural Networks (STPGNN) to address the above limitation. STPGNN includes three main components: a pivotal node identification module for identifying pivotal nodes, a novel pivotal graph convolution module enabling precise capture of spatio-temporal dependencies centered around pivotal nodes, and a parallel framework capable of extracting spatio-temporal traffic features on both pivotal and non-pivotal nodes. Experiments on seven real-world traffic datasets verify the proposed method's effectiveness and efficiency compared to state-of-the-art baselines.},
  author        = {Kong, Weiyang and Guo, Ziyu and Liu, Yubao},
  booktitle     = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi           = {10.1609/aaai.v38i8.28707},
  number        = {8},
  openalex      = {W4393159807},
  pages         = {8627--8635},
  pdf           = {https://ojs.aaai.org/index.php/AAAI/article/download/28707/29368},
  publisher     = {AAAI Press},
  title         = {Spatio-Temporal Pivotal Graph Neural Networks for Traffic Flow Forecasting},
  url           = {https://ojs.aaai.org/index.php/AAAI/article/view/28707},
  volume        = {38},
  year          = {2024}
}

@inproceedings{aykent2025gotennet,
  abstract      = {Understanding complex three-dimensional (3D) structures of graphs is essential for accurately modeling various properties, yet many existing approaches struggle with fully capturing the intricate spatial relationships and symmetries inherent in such systems, especially in large-scale, dynamic molecular datasets. These methods often must balance trade-offs between expressiveness and computational efficiency, limiting their scalability. To address this gap, we propose a novel Geometric Tensor Network (GotenNet) that effectively models the geometric intricacies of 3D graphs while ensuring strict equivariance under the Euclidean group E(3). We introduce a unified structural embedding, incorporating geometry-aware tensor attention and hierarchical tensor refinement that iteratively updates edge representations through inner product operations on high-degree steerable features, allowing for flexible and efficient representations for various tasks. We evaluated models on QM9, rMD17, MD22, and Molecule3D datasets, where the proposed model consistently outperforms state-of-the-art methods in both scalar and high-degree property predictions, demonstrating exceptional robustness across diverse datasets, and establishes GotenNet as a versatile and scalable framework for 3D equivariant Graph Neural Networks.},
  author        = {Aykent, Sarp and Xia, Tian},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  pdf           = {https://openreview.net/pdf?id=5wxCQDtbMo},
  publisher     = {ICLR},
  title         = {GotenNet: Rethinking Efficient 3D Equivariant Graph Neural Networks},
  url           = {https://openreview.net/pdf?id=5wxCQDtbMo},
  venue         = {ICLR},
  year          = {2025}
}

@inproceedings{liu2023graphprompt,
  abstract      = {Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner.},
  address       = {Austin, TX, USA},
  author        = {Liu, Zemin and Yu, Xingtong and Fang, Yuan and Zhang, Xinming},
  booktitle     = {Proceedings of the ACM Web Conference 2023},
  doi           = {10.1145/3543507.3583386},
  month         = {4},
  openalex      = {W4367046771},
  pages         = {417--428},
  publisher     = {ACM},
  series        = {WWW '23},
  title         = {GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks},
  url           = {https://dl.acm.org/doi/10.1145/3543507.3583386},
  year          = {2023}
}

@inproceedings{liu2024deep,
  abstract      = {Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. In many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve this problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. Our experimental results show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements, and our framework can effectively improve the performance of existing temporal graph learning methods.},
  author        = {Liu, Meng and Liu, Yue and Liang, Ke and Tu, Wenxuan and Wang, Siwei and Zhou, Sihang and Liu, Xinwang},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  note          = {While most T-GNN research focuses on supervised tasks like link prediction, this paper pioneers the area of deep unsupervised learning on temporal graphs. It proposes TGC, the first general framework for deep temporal graph clustering. The framework adapts deep clustering techniques, such as optimizing a clustering assignment distribution and reconstructing the graph, to the interaction sequence-based, batch-processing pattern common in temporal graphs. This work opens up new possibilities for unsupervised representation learning on dynamic data.},
  openalex      = {W4377130817},
  pdf           = {https://openreview.net/pdf?id=ViNe1fjGME},
  publisher     = {ICLR},
  title         = {Deep Temporal Graph Clustering},
  url           = {https://openreview.net/forum?id=ViNe1fjGME},
  year          = {2024}
}

@inproceedings{liu2024graph,
  abstract      = {This paper introduces a min-max optimization formulation for the Graph Signal Denoising (GSD) problem. In this formulation, we first maximize the second term of GSD by introducing perturbations to the graph structure based on Laplacian distance and then minimize the overall loss of the GSD. By solving the min-max optimization problem, we derive a new variant of the Graph Diffusion Convolution (GDC) architecture, called Graph Adversarial Diffusion Convolution (GADC). GADC differs from GDC by incorporating an additional term that enhances robustness against adversarial attacks on the graph structure and noise in node features. Moreover, GADC improves the performance of GDC on heterophilic graphs. Extensive experiments demonstrate the effectiveness of GADC across various datasets.},
  author        = {Songtao Liu and Jinghui Chen and Tianfan Fu and Lu Lin and Marinka Zitnik and Dinghao Wu},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4399414480},
  pages         = {30789--30806},
  pdf           = {https://proceedings.mlr.press/v235/liu24h/liu24h.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Graph Adversarial Diffusion Convolution},
  url           = {https://proceedings.mlr.press/v235/liu24h.html},
  volume        = {235},
  year          = {2024}
}

@misc{liu2025survey,
  abstract      = {With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy.},
  archiveprefix = {arXiv},
  author        = {Liu, Fei and Ma, Mingzhu and Lin, Yuxuan and Zhou, Guanghua and Wang, Sheng and Liu, Xiao and Li, Xuetao and Gao, Jie},
  eprint        = {2502.08353},
  month         = {2},
  note          = {This forward-looking survey focuses on a critical aspect of the GNN-LLM fusion: trustworthiness. It proposes a taxonomy that categorizes how the integration of LLMs can enhance the reliability, robustness, privacy, and reasoning capabilities of GNNs. By systematically organizing the rapidly developing approaches in this area, the paper highlights key challenges and proposes promising future research directions for building more dependable and secure graph-based AI systems.},
  pdf           = {https://arxiv.org/pdf/2502.08353},
  primaryclass  = {cs.LG},
  title         = {Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy},
  url           = {https://arxiv.org/abs/2502.08353},
  year          = {2025}
}

@inproceedings{han2022equivariant,
  abstract      = {Equivariant Graph neural Networks (EGNs) are powerful in characterizing the dynamics of multi-body physical systems. Existing EGNs conduct flat message passing, which, yet, is unable to capture the spatial/dynamical hierarchy for complex systems particularly, limiting substructure discovery and global information fusion. Here we propose Equivariant Hierarchy-based Graph Networks (EGHNs), which consist of the three key components: generalized Equivariant Matrix Message Passing (EMMP), E-Pool and E-UnPool. In particular, EMMP is able to improve the expressivity of conventional equivariant message passing, E-Pool assigns the quantities of the low-level nodes into high-level clusters, while E-UnPool leverages the high-level information to update the dynamics of the low-level nodes. Both E-Pool and E-UnPool are guaranteed to be equivariant to meet physic symmetry. Considerable experimental evaluations verify the effectiveness of our EGHN on several applications including multi-object dynamics simulation, motion capture, and protein dynamics modeling.},
  author        = {Jiaqi Han and Wenbing Huang and Tingyang Xu and Yu Rong},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  openalex      = {W4226457859},
  pages         = {29420--29434},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {Equivariant Graph Hierarchy-Based Neural Networks},
  url           = {https://papers.nips.cc/paper_files/paper/2022/file/3bdeb28a531f7af94b56bcdf8ee88f17-Paper-Conference.pdf},
  volume        = {35},
  year          = {2022}
}

@inproceedings{luo2024enhancing,
  abstract      = {Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current methods often fall short in capturing longer ranges, hierarchical structures, or community structures, which are common in various graphs such as molecules, social networks, and citation networks. This paper presents a Hierarchical Distance Structural Encoding (HDSE) method to model node distances in a graph, focusing on its multi-level, hierarchical nature. We introduce a novel framework to seamlessly integrate HDSE into the attention mechanism of existing graph transformers, allowing for simultaneous application with other positional encodings. To apply graph transformers with HDSE to large-scale graphs, we further propose a high-level HDSE that effectively biases the linear transformers towards graph hierarchies. We theoretically prove the superiority of HDSE over shortest path distances in terms of expressivity and generalization. Empirically, we demonstrate that graph transformers with HDSE excel in graph classification, regression on 7 graph-level datasets, and node classification on 11 large-scale graphs, including those with up to a billion nodes.},
  address       = {Vancouver, Canada},
  author        = {Luo, Yuankai and Li, Hongkang and Shi, Lei and Wu, Xiao-Ming},
  booktitle     = {Advances in Neural Information Processing Systems 37},
  month         = {12},
  note          = {This paper addresses the limitation of existing Graph Transformers in capturing hierarchical or community structures common in real-world graphs. It introduces HDSE, a method to model node distances based on the graph's multi-level hierarchy. HDSE is seamlessly integrated into the attention mechanism and can be scaled to large graphs via a high-level variant that biases linear transformers. The authors provide theoretical proofs of HDSE's superior expressivity and demonstrate state-of-the-art performance on numerous graph- and node-level tasks.},
  openalex      = {W4387078859},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/68a3919db3858f548dea769f2dbba611-Paper-Conference.pdf},
  publisher     = {Neural Information Processing Systems Foundation, Inc.},
  series        = {NeurIPS '24},
  title         = {Enhancing Graph Transformers with Hierarchical Distance Structural Encoding},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/68a3919db3858f548dea769f2dbba611-Paper-Conference.pdf},
  year          = {2024}
}

@inproceedings{pei2024multi,
  abstract      = {The advancement toward deeper graph neural networks is currently obscured by two inherent issues in message passing: oversmoothing and oversquashing. We identify the root cause of these issues as information loss due to heterophily mixing in aggregation, where messages of diverse category semantics are mixed. We propose a novel multi-track graph convolutional network to address oversmoothing and oversquashing effectively. Our basic idea is intuitive: if messages are separated and independently propagated according to their category semantics, heterophilic mixing can be prevented. Consequently, we present a novel multi-track message passing scheme capable of preventing heterophilic mixing, enhancing long-distance information flow, and improving separation condition. Empirical validations show that our model achieved state-of-the-art performance on several graph datasets and effectively tackled oversmoothing and oversquashing, setting a new benchmark of 86.4% accuracy on Cora.},
  author        = {Pei, Hongbin and Li, Yu and Deng, Huiqi and Hai, Jingxin and Wang, Pinghui and Ma, Jie and Tao, Jing and Xiong, Yuheng and Guan, Xiaohong},
  booktitle     = {International Conference on Machine Learning (ICML)},
  note          = {ICML 2024 Spotlight},
  openalex      = {W4399937616},
  pages         = {40078--40091},
  publisher     = {PMLR},
  title         = {Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing},
  url           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/pei24a/pei24a.pdf},
  volume        = {235},
  year          = {2024}
}

@inproceedings{molina2025graph,
  abstract      = {Graph learning models achieve state-of-the-art performance on many tasks, but this often requires increasingly large model sizes. Accordingly, the complexity of their representations increase. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-relational nature of GNNs makes it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embeddings. In this paper, we present a model-agnostic explainability pipeline employing diagnostic classifiers to probe and interpret the learned representations across different graph neural network architectures and datasets.},
  author        = {Tom Pelletreau-Duris and Ruud van Bakel and Michael Cochez},
  booktitle     = {International Conference on Learning Representations},
  month         = {9},
  note          = {ICLR 2025 Submission},
  openalex      = {W4404354921},
  pdf           = {https://openreview.net/pdf?id=RdTYx4jd7C},
  title         = {Do Graph Neural Network States Contain Graph Properties?},
  url           = {https://openreview.net/forum?id=RdTYx4jd7C},
  year          = {2025}
}

@inproceedings{puny2023equivariant,
  abstract      = {Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works (Xu et al., 2019; Morris et al., 2019b) introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a WL hierarchy that is too coarse to study current GNNs. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.},
  author        = {Puny, Omri and Lim, Derek and Kiani, Bobak T. and Maron, Haggai and Lipman, Yaron},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  doi           = {10.48550/arxiv.2302.11556},
  note          = {ICML 2023 Oral},
  openalex      = {W4321650178},
  pages         = {28191--28222},
  pdf           = {http://proceedings.mlr.press/v202/puny23a/puny23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Equivariant Polynomials for Graph Neural Networks},
  volume        = {202},
  year          = {2023}
}

@misc{rahman2024primer,
  abstract      = {This document aims to familiarize readers with temporal graph learning (TGL) through a concept-first approach. We have systematically presented vital concepts essential for understanding the workings of a TGL framework. In addition to qualitative explanations, we have incorporated mathematical formulations where applicable, enhancing the clarity of the text. Since TGL involves temporal and spatial learning, we introduce relevant learning architectures ranging from recurrent and convolutional neural networks to transformers and graph neural networks. We also discuss classical time series forecasting methods to inspire interpretable learning solutions for TGL.},
  archiveprefix = {arXiv},
  author        = {Rahman, Aniq Ur and Coon, Justin P.},
  eprint        = {2401.03988},
  month         = {1},
  note          = {This document provides an excellent, concept-first introduction to the field of temporal graph learning. It systematically presents the essential concepts, from definitions of temporal graphs to the various learning tasks (e.g., link prediction, node classification). It covers the key architectural components used to capture spatial and temporal dependencies, including GNNs, RNNs, and Transformers, making it a valuable resource for newcomers to the field.},
  openalex      = {W4390783846},
  pdf           = {https://arxiv.org/pdf/2401.03988.pdf},
  primaryclass  = {cs.LG},
  title         = {A Primer on Temporal Graph Learning},
  url           = {https://arxiv.org/abs/2401.03988},
  year          = {2024}
}

@inproceedings{rampasek2022recipe,
  abstract      = {We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. The GPS recipe consists of choosing three main ingredients: positional/structural encoding, local message-passing mechanism, and global attention mechanism. We achieve linear computational complexity O(N+E) by decoupling the local real-edge aggregation from the fully-connected Transformer.},
  author        = {Rampášek, Ladislav and Galkin, Mikhail and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4281706128},
  pages         = {14501--14515},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5d4834a159f1547b267a05a4e2b7cf5e-Paper-Conference.pdf},
  series        = {Advances in Neural Information Processing Systems},
  title         = {Recipe for a General, Powerful, Scalable Graph Transformer},
  volume        = {35},
  year          = {2022}
}

@article{jin2024survey,
  abstract      = {Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first provide a unified perspective to formulate the problems of LLMs on graphs, and then review existing methods to solve these problems. Our survey, to the best of our knowledge, is the first to provide a systematic categorization of LLM-graph methods, which we organize into four primary patterns: (1) LLM as Predictor, (2) LLM as Encoder, (3) LLM as Aligner, and (4) LLM as Reasoning Engine.},
  author        = {Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and Ji, Heng and Han, Jiawei},
  doi           = {10.1109/TKDE.2024.3469578},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  note          = {Also available as arXiv:2312.02783. This comprehensive survey provides the first systematic overview of the emerging field of LLMs on graphs, proposing a novel taxonomy that categorizes existing methods into four distinct patterns: (1) LLM as Predictor, (2) LLM as Encoder, (3) LLM as Aligner, and (4) LLM as Reasoning Engine.},
  publisher     = {IEEE},
  title         = {Large Language Models on Graphs: A Comprehensive Survey},
  url           = {https://arxiv.org/pdf/2312.02783},
  year          = {2024}
}

@inproceedings{reuss2023se3,
  author        = {Reuss, Philipp and Stegmaier, Dennis and Dutordoir, Vincent and Lengyel, Attila},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  note          = {This paper applies SE(3)-Transformers, an equivariant attention-based architecture, to the challenging task of learning fluid dynamics simulations. By incorporating physical symmetries into the model, it can learn the underlying dynamics more efficiently and generalize better to unseen simulation states. This work showcases the power of geometric deep learning for complex physical systems.},
  title         = {SE(3)-Transformers for Fluid Simulations},
  url           = {https://openreview.net/pdf?id=b-h2v6f2VSA},
  year          = {2023}
}

@inproceedings{rossi2020temporal,
  abstract      = {Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient.},
  archiveprefix = {arXiv},
  author        = {Rossi, Emanuele and Chamberlain, Ben and Frasca, Fabrizio and Eynard, Davide and Monti, Federico and Bronstein, Michael M.},
  booktitle     = {ICML Workshop on Graph Representation Learning},
  eprint        = {2006.10637},
  openalex      = {W4287755062},
  pdf           = {https://grlplus.github.io/papers/58.pdf},
  primaryclass  = {cs.LG},
  title         = {Temporal Graph Networks for Deep Learning on Dynamic Graphs},
  url           = {https://arxiv.org/abs/2006.10637},
  year          = {2020}
}

@inproceedings{rozemberczki2024learning,
  abstract      = {Message Passing Neural Networks (MPNNs) are a fundamental class of graph machine learning algorithms that iteratively update each node's representation by aggregating messages from its neighbors. This necessitates a memory complexity of the order of the number of graph edges, which can quickly become prohibitive for large graphs that are not very sparse. In this work, we propose to alleviate this memory complexity bottleneck by approximating the input graph as an intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph does not depend on the graph size. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the number of nodes (rather than edges). Our experimental results on tasks like node classification and spatio-temporal data processing on real-world datasets demonstrate on-par performance of the ICGs while using fewer computational resources.},
  author        = {Finkelshtein, Ben and Ceylan, İsmail \̇lkan and Bronstein, Michael and Levie, Ron},
  booktitle     = {Advances in Neural Information Processing Systems 37 (NeurIPS 2024)},
  openalex      = {W4399317664},
  pages         = {1--22},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6953cb03692e187e3acd5c3aada984e3-Paper-Conference.pdf},
  title         = {Learning on Large Graphs using Intersecting Communities},
  url           = {https://neurips.cc/virtual/2024/poster/93568},
  year          = {2024}
}

@inproceedings{satorras2021en,
  abstract      = {This paper introduces a generative model equivariant to Euclidean symmetries: E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing flow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the first likelihood-based deep generative model that generates molecules in 3D.},
  author        = {Satorras, Victor Garcia and Hoogeboom, Emiel and Fuchs, Fabian B. and Posner, Ingmar and Welling, Max},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  month         = {12},
  note          = {NeurIPS 2021 Oral},
  openalex      = {W4287179668},
  pages         = {4181--4192},
  pdf           = {https://proceedings.neurips.cc/paper/2021/file/21b5680d80f75a616096f2e791affac6-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {E(n) Equivariant Normalizing Flows},
  url           = {https://proceedings.neurips.cc/paper/2021/hash/21b5680d80f75a616096f2e791affac6-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{satorras2021n,
  abstract      = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
  author        = {Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  month         = {7},
  note          = {This paper introduced EGNN, a simple and efficient GNN architecture that is equivariant to E(n) transformations, including rotations, translations, reflections, and permutations. Unlike previous methods that relied on complex and computationally expensive representations like spherical harmonics, EGNN achieves equivariance by including coordinates as attributes and updating them in an equivariant manner within the message-passing scheme. This work made equivariant GNNs much more accessible and scalable.},
  openalex      = {W4287325738},
  pages         = {9323--9332},
  pdf           = {http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {E(n) Equivariant Graph Neural Networks},
  url           = {https://arxiv.org/abs/2102.09844},
  volume        = {139},
  year          = {2021}
}

@article{scarselli2009computational,
  abstract      = {In this paper, we will consider the approximation properties of a recently introduced neural network model called graph neural network (GNN), which can be used to process-structured data inputs, e.g., acyclic graphs, cyclic graphs, and directed or undirected graphs. This class of neural networks implements a function $τ(G, n) ın ℝ^m$ that maps a graph $G$ and one of its nodes $n$ onto an $m$-dimensional Euclidean space. We characterize the functions that can be approximated by GNNs up to any prescribed degree of precision. The set of functions that can be approximated includes most of the practically useful functions on graphs, with exceptions only when the input graph contains specific symmetry patterns.},
  author        = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  doi           = {10.1109/TNN.2008.2005141},
  journal       = {IEEE Transactions on Neural Networks},
  note          = {A companion to their foundational model paper, this work provided the first theoretical analysis of the capabilities of the GNN framework. The authors proved that the GNN model is a universal approximator for a class of functions on graphs, specifically those that are "unfolding-equivalent," meaning they can be computed by unfolding the graph structure into a tree. This established the theoretical power of the iterative diffusion approach.},
  number        = {1},
  openalex      = {W2150120952},
  pages         = {81--102},
  pdf           = {https://ro.uow.edu.au/cgi/viewcontent.cgi?article=2715&context=infopapers},
  publisher     = {IEEE},
  title         = {Computational Capabilities of Graph Neural Networks},
  volume        = {20},
  year          = {2009}
}

@article{scarselli2009graph,
  abstract      = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, which directly processes most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected. The GNN model extends existing neural network methods for processing data represented in graph domains. This model implements a function τ(G,n) ∈ ℝᵐ that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed model.},
  author        = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  doi           = {10.1109/TNN.2008.2005605},
  journal       = {IEEE Transactions on Neural Networks},
  note          = {This paper is widely recognized as the foundational work that formally introduced the Graph Neural Network (GNN) model. It proposed a framework where node representations are learned by iteratively propagating neighborhood information until the node states reach a stable fixed point. The state update function was defined as a contraction mapping to guarantee convergence, a concept borrowed from recurrent neural networks and Banach's fixed-point theorem. While computationally intensive, this work established the core message-passing paradigm.},
  number        = {1},
  openalex      = {W2116341502},
  pages         = {61--80},
  pdf           = {https://ieeexplore.ieee.org/iel5/72/4740185/04700287.pdf},
  publisher     = {IEEE},
  title         = {The Graph Neural Network Model},
  url           = {https://ieeexplore.ieee.org/document/4700287/},
  volume        = {20},
  year          = {2009}
}

@inproceedings{tang2024graphgpt,
  abstract      = {Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes. To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation. Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce. Our research tackles this by advancing graph model generalization in zero-shot learning environments. Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data. We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning. This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector. These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks. Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks.},
  address       = {Washington, DC, USA},
  author        = {Tang, Jiabin and Yang, Yuhao and Wei, Wei and Shi, Lei and Su, Lixin and Cheng, Suqi and Yin, Dawei and Huang, Chao},
  booktitle     = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi           = {10.1145/3626772.3657775},
  month         = {7},
  openalex      = {W4387891144},
  pages         = {1016--1025},
  pdf           = {https://arxiv.org/pdf/2310.13023},
  publisher     = {ACM},
  series        = {SIGIR '24},
  title         = {GraphGPT: Graph Instruction Tuning for Large Language Models},
  url           = {https://dl.acm.org/doi/10.1145/3626772.3657775},
  year          = {2024}
}

@inproceedings{geisler2024spatio,
  abstract      = {Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of ℓ-step MPGNNs are that their 'receptive field' is typically limited to the ℓ-hop neighborhood of a node and that information exchange between distant nodes is limited by over-squashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks (S²GNNs) – a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that S²GNNs vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, S²GNNs allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Leman (WL) test. Moreover, to obtain general-purpose S²GNNs, we propose spectrally parametrized filters for directed graphs. S²GNNs outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, S²GNNs scale to millions of nodes.},
  author        = {Simon Geisler and Arthur Kosmala and Daniel Herbst and Stephan Günnemann},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  openalex      = {W4399198545},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/580c4ec4738ff61d5862a122cdf139b6-Paper-Conference.pdf},
  title         = {Spatio-Spectral Graph Neural Networks},
  url           = {https://openreview.net/forum?id=Cb3kcwYBgw},
  volume        = {37},
  year          = {2024}
}

@inproceedings{topping2022understanding,
  abstract      = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
  author        = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  booktitle     = {The Tenth International Conference on Learning Representations},
  note          = {ICLR 2022 Outstanding Paper Honorable Mention},
  openalex      = {W4225751401},
  pdf           = {https://openreview.net/pdf?id=7UmjRGzp-A},
  title         = {Understanding over-squashing and bottlenecks on graphs via curvature},
  year          = {2022}
}

@inproceedings{trivedi2019dyrep,
  abstract      = {How can we effectively encode evolving information over dynamic graphs into low-dimensional representations? In this paper, we propose DyRep, an inductive deep representation learning framework that learns a set of functions to efficiently produce low-dimensional node embeddings that evolves over time. The learned embeddings drive the dynamics of two key processes namely, communication and association between nodes in dynamic graphs. These processes exhibit complex nonlinear dynamics that evolve at different time scales and subsequently contribute to the update of node embeddings. We employ a time-scale dependent multivariate point process model to capture these dynamics. We devise an efficient unsupervised learning procedure and demonstrate that our approach significantly outperforms representative baselines on two real-world datasets for the problem of dynamic link prediction and event time prediction.},
  author        = {Trivedi, Rakshit and Farajtabar, Mehrdad and Biswal, Prasenjeet and Zha, Hongyuan},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W2907192581},
  pdf           = {https://openreview.net/pdf?id=HyePrhR5KX},
  title         = {DyRep: Learning Representations over Dynamic Graphs},
  year          = {2019}
}

@inproceedings{velasco2024graph,
  abstract      = {Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN). In this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators.},
  author        = {Mauricio Velasco and Kaiying O'Hare and Bernardo Rychtenberg and Soledad Villar},
  booktitle     = {Advances in Neural Information Processing Systems},
  doi           = {10.48550/arXiv.2411.04265},
  openalex      = {W4404396557},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/ad9964b731bc7b5621a83c7869fc653b-Paper-Conference.pdf},
  title         = {Graph neural networks and non-commuting operators},
  url           = {https://openreview.net/forum?id=6aJrEC28hR},
  year          = {2024}
}

@inproceedings{velickovic2018graph,
  abstract      = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  author        = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  booktitle     = {International Conference on Learning Representations},
  note          = {GAT introduced the attention mechanism, popularized by the Transformer model, into the graph domain. By using masked self-attention, GAT layers allow nodes to assign different weights or ``importance'' to different nodes in their neighborhood, overcoming the limitations of GCN's fixed and structurally-dependent normalization. This mechanism is computationally efficient, applicable to both transductive and inductive problems, and has become a foundational GNN architecture.},
  openalex      = {W2963858333},
  pdf           = {https://openreview.net/pdf?id=rJXMpikCZ},
  title         = {Graph Attention Networks},
  url           = {https://openreview.net/forum?id=rJXMpikCZ},
  year          = {2018}
}

@inproceedings{velickovic2019deep,
  abstract      = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
  author        = {Veličković, Petar and Fedus, William and Hamilton, William L. and Liò, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
  booktitle     = {International Conference on Learning Representations},
  note          = {DGI was a pioneering work in self-supervised learning for GNNs. It adapts the principle of Deep InfoMax to graphs by learning node representations that maximize the mutual information between local patch representations and a global graph summary vector. It trains a GNN encoder by distinguishing whether a patch-summary pair is from the original graph or a corrupted version, providing a powerful unsupervised learning objective.},
  openalex      = {W2963782635},
  pdf           = {https://openreview.net/pdf?id=rklz9iAcKQ},
  title         = {Deep Graph Infomax},
  url           = {https://openreview.net/forum?id=rklz9iAcKQ},
  year          = {2019}
}

@inproceedings{wang2024opengraph,
  abstract      = {Graph learning has become essential in various domains, including recommendation systems and social network analysis. Graph Neural Networks (GNNs) have emerged as promising techniques for encoding structural information and improving performance in tasks like link prediction and node classification. However, a key challenge remains: the difficulty of generalizing to unseen graph data with different properties. In this work, we propose a novel graph foundation model, called OpenGraph, to address this challenge. Our approach tackles several technical obstacles. Firstly, we enhance data augmentation using a large language model (LLM) to overcome data scarcity in real-world scenarios. Secondly, we introduce a unified graph tokenizer that enables the model to generalize effectively to diverse graph data, even when encountering unseen properties during training. Thirdly, our developed scalable graph transformer captures node-wise dependencies within the global topological context. Extensive experiments validate the effectiveness of our framework. By adapting OpenGraph to new graph characteristics and comprehending diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings.},
  address       = {Miami, Florida, USA},
  author        = {Lianghao Xia and Ben Kao and Chao Huang},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2024},
  doi           = {10.18653/v1/2024.findings-emnlp.132},
  month         = {11},
  openalex      = {W4391845134},
  pages         = {2365--2379},
  publisher     = {Association for Computational Linguistics},
  title         = {OpenGraph: Towards Open Graph Foundation Models},
  url           = {https://aclanthology.org/2024.findings-emnlp.132},
  year          = {2024}
}

@inproceedings{wang2024understanding,
  abstract      = {Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt피[\mathrmdeg]$, where $피[\mathrmdeg]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $피[\mathrmdeg]$. Finally, when applying multiple GC operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. It indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory.},
  author        = {Wang, Junfu and Guo, Yuanfang and Yang, Liang and Wang, Yunhong},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391013400},
  pages         = {50489--50529},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/wang24u/wang24u.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Understanding Heterophily for Graph Neural Networks},
  volume        = {235},
  year          = {2024}
}

@inproceedings{wang2025graph,
  abstract      = {Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, while differential equations provide a principled framework for modeling continuous dynamics across time and space. The intersection of these fields has led to innovative approaches that leverage the strengths of both, enabling applications in physics-informed learning, spatiotemporal modeling, and scientific computing. This tutorial provides a comprehensive overview of the burgeoning research at the intersection of GNNs and differential equations, exploring methodologies, applications, and future directions in this rapidly evolving area.},
  address       = {New York, NY, USA},
  author        = {Zewen Liu and Xiaoda Wang and Bohan Wang and Zijie Huang and Carl Yang and Wei Jin},
  booktitle     = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi           = {10.1145/3711896.3736559},
  month         = {8},
  note          = {Tutorial paper. arXiv:2503.23167},
  openalex      = {W4412875475},
  pages         = {6538--6539},
  publisher     = {Association for Computing Machinery},
  series        = {KDD '25},
  title         = {Graph ODEs and Beyond: A Comprehensive Survey on Integrating Differential Equations with Graph Neural Networks},
  url           = {https://doi.org/10.1145/3711896.3736559},
  year          = {2025}
}

@inproceedings{wei2024gita,
  abstract      = {Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in the textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called Graph to vIsual and Textual IntegrAtion (GITA), which incorporates visual graphs into general graph reasoning. Besides, we construct the Graph-based Vision-Language Question Answering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs on general graph reasoning. Moreover, experimental results demonstrate the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.},
  author        = {Wei, Yanbin and Fu, Shuai and Jiang, Weisen and Zhang, Zejian and Zeng, Zhixiong and Wu, Qi and Kwok, James T. and Zhang, Yu},
  booktitle     = {Advances in Neural Information Processing Systems},
  doi           = {10.48550/arXiv.2402.02130},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/00295cede6e1600d344b5cd6d9fd4640-Paper-Conference.pdf},
  title         = {GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/00295cede6e1600d344b5cd6d9fd4640-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{wijesinghe2022new,
  author        = {Asiri Wijesinghe and Qing Wang},
  booktitle     = {International Conference on Learning Representations},
  month         = {4},
  note          = {Oral presentation},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=uxgg9o7bI_3},
  publisher     = {ICLR},
  series        = {ICLR 2022},
  title         = {A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"},
  url           = {https://openreview.net/forum?id=uxgg9o7bI_3},
  year          = {2022}
}

@article{wu2020graph,
  abstract      = {Recommender systems play a key role in alleviating information overload. The main challenge of recommender systems is learning effective user/item representations from their interactions and side information. Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Then, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works address these challenges. Finally, we outline several promising research directions in this rapidly growing area.},
  author        = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
  doi           = {10.1145/3535101},
  journal       = {ACM Computing Surveys},
  month         = {5},
  note          = {This survey provides a comprehensive overview of the application of GNNs in recommender systems. It categorizes GNN-based recommendation models based on the type of information they use (e.g., social relations, item-item graphs, user-item interaction graphs) and reviews the state-of-the-art methods. It serves as a foundational text for understanding how GNNs can model the complex relationships inherent in recommendation data.},
  number        = {5},
  openalex      = {W4296300780},
  pages         = {97:1--97:37},
  pdf           = {https://arxiv.org/pdf/2011.02260},
  publisher     = {ACM},
  title         = {Graph Neural Networks in Recommender Systems: A Survey},
  volume        = {55},
  year          = {2022}
}

@inproceedings{xia2023all,
  abstract      = {Recently, 'pre-training and fine-tuning' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ŉegative transfer' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.},
  address       = {New York, NY, USA},
  author        = {Sun, Xiangguo and Cheng, Hong and Li, Jia and Liu, Bo and Guan, Jihong},
  booktitle     = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi           = {10.1145/3580305.3599256},
  openalex      = {W4383468961},
  pages         = {2120--2131},
  pdf           = {https://arxiv.org/pdf/2307.01504},
  publisher     = {ACM},
  series        = {KDD '23},
  title         = {All in One: Multi-Task Prompting for Graph Neural Networks},
  year          = {2023}
}

@inproceedings{xia2024gnncert,
  abstract      = {Graph classification has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive attacks. Existing provable defenses have limitations: 1) they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. In this work, we propose GNNCert, a certified defense against both graph structure and node feature perturbations for graph classification. GNNCert provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Results on 8 benchmark datasets show that GNNCert outperforms three state-of-the-art methods.},
  author        = {Zaishuo Xia and Han Yang and Binghui Wang and Jinyuan Jia},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  note          = {Oral Presentation},
  pdf           = {https://openreview.net/pdf?id=IGzaH538fz},
  publisher     = {OpenReview.net},
  title         = {GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations},
  url           = {https://openreview.net/forum?id=IGzaH538fz},
  year          = {2024}
}

@inproceedings{xu2019how,
  abstract      = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  author        = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  booktitle     = {International Conference on Learning Representations},
  note          = {This landmark theoretical paper established a formal upper bound on the expressive power of message-passing GNNs, proving they are at most as powerful as the 1-dimensional Weisfeiler-Lehman (1-WL) graph isomorphism test. This insight explained why models like GCN and GraphSAGE fail to distinguish certain simple graph structures. The authors then proposed the Graph Isomorphism Network (GIN), a simple architecture with a sum aggregator and an MLP update function, and proved it is maximally powerful among the class of GNNs distinguishable by the 1-WL test.},
  openalex      = {W2962711740},
  pdf           = {https://openreview.net/pdf?id=ryGs6iA5Km},
  title         = {How Powerful are Graph Neural Networks?},
  url           = {https://openreview.net/forum?id=ryGs6iA5Km},
  year          = {2019}
}

@inproceedings{xu2020inductive,
  abstract      = {Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.},
  address       = {Addis Ababa, Ethiopia},
  author        = {Da Xu and Chuanwei Ruan and Evren Korpeoglu and Sushant Kumar and Kannan Achan},
  booktitle     = {8th International Conference on Learning Representations},
  month         = {4},
  note          = {This paper introduces TGAT (Temporal Graph Attention), a model for temporal graphs that uses a self-attention mechanism with novel functional time encoding based on Bochner's theorem. The approach enables inductive representation learning on evolving graphs by capturing both structural and temporal dynamics through attention mechanisms.},
  openalex      = {W3007404067},
  pdf           = {https://openreview.net/pdf?id=rJeW1yHYwH},
  publisher     = {OpenReview.net},
  series        = {ICLR 2020},
  title         = {Inductive Representation Learning on Temporal Graphs},
  url           = {https://openreview.net/forum?id=rJeW1yHYwH},
  year          = {2020}
}

@inproceedings{xu2021self,
  abstract      = {This paper studies unsupervised/self-supervised whole-graph representation learning, which is critical in many tasks such as molecule properties prediction in drug and material discovery. Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire data set. In this paper, we propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning. Specifically, besides preserving the local similarities, GraphLoG introduces the hierarchical prototypes to capture the global semantic clusters. An efficient online expectation-maximization (EM) algorithm is further developed for learning the model. We evaluate GraphLoG by pre-training it on massive unlabeled graphs followed by fine-tuning on downstream tasks. Extensive experiments on both chemical and biological benchmark data sets demonstrate the effectiveness of the proposed approach.},
  author        = {Xu, Minghao and Wang, Hang and Ni, Bingbing and Guo, Hongyu and Tang, Jian},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  editor        = {Meila, Marina and Zhang, Tong},
  month         = {7},
  openalex      = {W3170424177},
  pages         = {11548--11558},
  pdf           = {http://proceedings.mlr.press/v139/xu21g/xu21g.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Self-supervised Graph-level Representation Learning with Local and Global Structure},
  volume        = {139},
  year          = {2021}
}

@inproceedings{yang2023self,
  abstract      = {Self-supervised learning is introduced to train graph neural networks (GNNs) by employing propagation-based GNNs designed for semi-supervised learning tasks. Unfortunately, this common choice tends to cause two serious issues. Firstly, global parameters cause the model lack the ability to capture the local property. Secondly, it is difficult to handle networks beyond homophily without label information. To break through the common choice of employing propagation-based GNNs, which aggregate representations of nodes belonging to different classes and tend to lose discriminative information, we present the theoretical insight that if the propagation in each ego-network is just between the nodes from the same class, the obtained representation matrix should follow the low-rank characteristic. Consequently, Low-Rank Decomposition-based GNN (LRD-GNN-Matrix) is proposed. To incorporate long-distance information, Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) is proposed by constructing the node attribute tensor from selected similar ego-networks and performing Low-Rank Tensor Decomposition. The employed tensor nuclear norm facilitates the capture of the long-distance relationship between original and selected similar ego-networks. Extensive experiments demonstrate the superior performance and the robustness of LRD-GNNs.},
  author        = {Yang, Liang and Shi, Runjie and Zhang, Qiuliang and Niu, Bingxin and Wang, Zhen and Cao, Xiaochun and Wang, Chuan},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {This paper breaks from the common paradigm of using propagation-based GNNs for SSL, which can lose discriminative information by aggregating features from different classes. It proposes LRD-GNN, which is based on the theoretical insight that the representation matrix of an ego-network should be low-rank if propagation only occurs between same-class nodes. By employing low-rank matrix and tensor decomposition, the model better captures local properties and handles heterophilous graphs.},
  pages         = {42963--42978},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {Self-supervised Graph Neural Networks via Low-Rank Decomposition},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6c33e4ea4ddfb05a78541022ab5a1fb9-Paper-Conference.pdf},
  volume        = {36},
  year          = {2023}
}

@inproceedings{ye2024one,
  abstract      = {Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Third, an appropriate graph prompting paradigm for in-context learning is unclear. In this paper, we propose One for All (OFA), a general framework that can use a single graph model to solve graph problems across different areas and tasks. Specifically, OFA proposes a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. Meanwhile, OFA introduces a unified position encoding strategy to handle different graph structures, and a unified loss function to handle different tasks. We train the OFA model with over 100 graph datasets, and evaluate it on a wide range of tasks. Our experimental results suggest that OFA outperforms state-of-the-art methods on most tasks, demonstrating the effectiveness of our approach.},
  author        = {Hao Liu and Jiarui Feng and Lecheng Kong and Ningyue Liang and Dacheng Tao and Yixin Chen and Muhan Zhang},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  note          = {Spotlight presentation. This paper proposes OFA (One for All), a unified framework for training a single graph model that can handle diverse graph classification tasks across different domains without task-specific fine-tuning. The approach uses graph prompting with substructures and unified position encoding to enable zero-shot transfer to new datasets and tasks.},
  openalex      = {W4387323243},
  pdf           = {https://openreview.net/pdf?id=4IT2pgc9v6},
  title         = {One for All: Towards Training One Graph Model for All Classification Tasks},
  url           = {https://openreview.net/forum?id=4IT2pgc9v6},
  year          = {2024}
}

@inproceedings{ying2021transformers,
  abstract      = {The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. We solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to success is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and show that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as special cases of Graphormer.},
  author        = {Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  note          = {This paper challenged the early consensus that Transformers were ill-suited for graph tasks. The authors introduced Graphormer, a Transformer architecture enhanced with several novel structural encodings, including centrality encoding, spatial encoding (shortest path distance), and edge feature encoding in the attention mechanism. Graphormer achieved state-of-the-art results on the large-scale OGB-LSC quantum chemistry dataset (PCQM4M), demonstrating the immense potential of well-designed Graph Transformers.},
  openalex      = {W3169622372},
  pages         = {28877--28888},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {Advances in Neural Information Processing Systems},
  title         = {Do Transformers Really Perform Bad for Graph Representation?},
  url           = {https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{you2020graph,
  abstract      = {Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. This paper proposes GraphCL, a graph contrastive learning framework for learning unsupervised representations of graph data. The method systematically designs four types of graph augmentation strategies: node dropping, edge perturbation, attribute masking, and subgraph sampling to incorporate various priors into the learning process.},
  arxiv         = {2010.13902},
  author        = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {GraphCL established a general and effective framework for contrastive learning on graphs, inspired by SimCLR from computer vision. It systematically studies four types of graph data augmentations (node dropping, edge perturbation, attribute masking, and subgraph sampling) and shows that the choice of augmentation is critical for performance. The framework learns representations by maximizing the agreement between two augmented views of the same graph.},
  openalex      = {W3095602948},
  pages         = {5812--5823},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/3fe230348e9a12c13120749e3f9fa4cd-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Graph Contrastive Learning with Augmentations},
  url           = {https://proceedings.neurips.cc/paper/2020/hash/3fe230348e9a12c13120749e3f9fa4cd-Abstract.html},
  volume        = {33},
  year          = {2020}
}

@inproceedings{you2021graph,
  abstract      = {Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous "best practices" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.},
  author        = {You, Yuning and Chen, Tianlong and Shen, Yang and Wang, Zhangyang},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  note          = {ICML 2021 Oral},
  openalex      = {W3167553825},
  pages         = {12121--12132},
  pdf           = {http://proceedings.mlr.press/v139/you21a/you21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Graph Contrastive Learning Automated},
  url           = {https://proceedings.mlr.press/v139/you21a.html},
  volume        = {139},
  year          = {2021}
}

@inproceedings{yuan2023evaluating,
  abstract      = {This work studies the evaluation of explaining graph neural networks (GNNs), which is crucial to the credibility of post-hoc explainability in practical usage. Conventional evaluation metrics, and even explanation methods -- which mainly follow the paradigm of feeding the explanatory subgraph and measuring output difference -- always suffer from the notorious out-of-distribution (OOD) issue. In this work, we endeavor to confront the issue by introducing a novel evaluation metric, termed OOD-resistant Adversarial Robustness (OAR). Specifically, we draw inspiration from adversarial robustness and evaluate post-hoc explanation subgraphs by calculating their robustness under attack. On top of that, an elaborate OOD reweighting block is inserted into the pipeline to confine the evaluation process to the original data distribution.},
  author        = {Yuan, Haoyu and Li, Haiyang and Gu, Zhiming and Cheng, Shuiwang and Dong, Junchi and Wang, Shengjie and Zhu, Fei and Qian, Ziqi and Wang, Zhewei},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {NeurIPS 2023 Oral},
  pages         = {32966--32991},
  series        = {NeurIPS '23},
  title         = {Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/332894d329b32434316f44a8f3d1bdfa-Paper-Conference.pdf},
  volume        = {36},
  year          = {2023}
}

@inproceedings{zang2020neural,
  abstract      = {Learning continuous-time dynamics on complex networks is crucial for understanding, predicting, and controlling complex systems in science and engineering. However, this task is very challenging due to the combinatorial complexities in the structures of high dimensional systems, their elusive continuous-time nonlinear dynamics, and their structural-dynamic dependencies. To address these challenges, we propose to combine Ordinary Differential Equation Systems (ODEs) and Graph Neural Networks (GNNs) to learn continuous-time dynamics on complex networks in a data-driven manner.},
  author        = {Chengxi Zang and Fei Wang},
  booktitle     = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  doi           = {10.1145/3394486.3403132},
  pages         = {892--902},
  publisher     = {ACM},
  series        = {KDD '20},
  title         = {Neural Dynamics on Complex Networks},
  url           = {https://dl.acm.org/doi/10.1145/3394486.3403132},
  year          = {2020}
}

@inproceedings{zhang2021nested,
  abstract      = {Graph neural network (GNN)'s success in graph classification is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree representations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs). NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails. Moreover, unlike other more powerful GNNs, NGNN only introduces a constant-factor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets.},
  author        = {Muhan Zhang and Pan Li},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P. S. Liang and J. Wortman Vaughan},
  note          = {To create GNNs that are provably more powerful than the 1-WL test, this paper introduces Nested GNNs (NGNNs). The key idea is to shift the representational unit from a node's rooted subtree (as captured by 1-WL) to its rooted subgraph. NGNN achieves this by extracting a local subgraph around each node and applying a base GNN to learn a representation for that subgraph. The final graph representation is an aggregation of these subgraph representations. The authors prove NGNN is strictly more powerful than 1-WL and can distinguish almost all regular graphs, where 1-WL fails.},
  openalex      = {W3208638341},
  pages         = {15734--15747},
  publisher     = {Curran Associates, Inc.},
  title         = {Nested Graph Neural Networks},
  url           = {https://proceedings.neurips.cc/paper/2021/file/8462a7c229aea03dde69da754c3bbcc4-Paper.pdf},
  volume        = {34},
  year          = {2021}
}

@inproceedings{zhang2022graph,
  abstract      = {Graph neural networks (GNNs) have achieved great success in many graph-based applications. However, the enormous size and high sparsity level of graphs hinder their applications under industrial scenarios. Although some scalable GNNs are proposed for large-scale graphs, they adopt a fixed K-hop neighborhood for each node, thus facing the over-smoothing issue when adopting large propagation depths for nodes within sparse regions. To tackle these issues, we propose a new GNN architecture --- Graph Attention Multi-Layer Perceptron (GAMLP), which can capture the underlying correlations between different scales of graph knowledge. GAMLP circumvents limitations by introducing a scalable and flexible Graph Attention Multilayer Perceptron. With the separation of the non-linear transformation and feature propagation, GAMLP significantly improves the scalability and efficiency by performing the propagation procedure in a pre-compute manner. Extensive experiments on 14 graph datasets demonstrate that GAMLP achieves state-of-the-art performance while enjoying high scalability and efficiency.},
  address       = {Washington, DC, USA},
  author        = {Zhang, Wentao and Yin, Ziqi and Sheng, Zeang and Li, Yang and Ouyang, Wen and Li, Xiaosen and Tao, Yangyu and Yang, Zhi and Cui, Bin},
  booktitle     = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi           = {10.1145/3534678.3539121},
  isbn          = {978-1-4503-9385-0},
  month         = {8},
  note          = {To tackle both oversmoothing and scalability, this paper proposes GAMLP, a new GNN architecture. It decouples representation transformation and propagation by first using a deep MLP to learn node representations and then propagating these representations using a graph attention mechanism. This allows it to capture correlations between different scales of graph knowledge and avoid the issues associated with stacking many GCN-style layers.},
  openalex      = {W3196261868},
  pages         = {2439--2449},
  pdf           = {https://arxiv.org/pdf/2206.04355.pdf},
  publisher     = {ACM},
  title         = {Graph Attention Multi-Layer Perceptron},
  year          = {2022}
}

@inproceedings{zhang2023recurrent,
  abstract      = {Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.4% improvement on averaged precision in a real-world Ecommerce dataset over existing methods on 2-layer models.},
  author        = {Chen, Yizhou and Zeng, Anxiang and Yu, Qingtao and Zhang, Kerui and Yuanpeng, Cao and Wu, Kangle and Huzhang, Guangda and Yu, Han and Zhou, Zhiming},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4387031452},
  pages         = {5634--5647},
  pdf           = {https://openreview.net/pdf?id=B3UDx1rNOy},
  title         = {Recurrent Temporal Revision Graph Networks},
  year          = {2023}
}

@inproceedings{zhang2023rethinking,
  abstract      = {Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. After a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics, despite the fact that biconnectivity can be easily calculated using simple algorithms that have linear computational costs. The only exception is the ESAN framework, for which we provide theoretical justification of its power. To remedy this gap, we introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.},
  author        = {Zhang, Bohang and Luo, Shengjie and Wang, Liwei and He, Di},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  note          = {Notable top 5% paper},
  openalex      = {W4317951161},
  pdf           = {https://openreview.net/pdf?id=r9hNv76KoT3},
  title         = {Rethinking the Expressive Power of GNNs via Graph Biconnectivity},
  url           = {https://openreview.net/forum?id=r9hNv76KoT3},
  year          = {2023}
}

@inproceedings{zhang2024beyond,
  abstract      = {Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a unified framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.},
  author        = {Zhang, Bohang and Gai, Jingchu and Du, Yiheng and Ye, Qiwei and He, Di and Wang, Liwei},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2401.08514},
  note          = {Oral presentation, Outstanding Paper Award},
  openalex      = {W4390963127},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=HSKaGOi7Ar},
  publisher     = {OpenReview.net},
  title         = {Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness},
  url           = {https://openreview.net/forum?id=HSKaGOi7Ar},
  year          = {2024}
}

@inproceedings{zhang2024expressive,
  abstract      = {Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.},
  address       = {Vienna, Austria},
  author        = {Zhang, Bohang and Zhao, Lingxiao and Maron, Haggai},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  month         = {7},
  openalex      = {W4399455371},
  pages         = {58717--58759},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24ck/zhang24ck.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On the Expressive Power of Spectral Invariant Graph Neural Networks},
  volume        = {235},
  year          = {2024}
}

@inproceedings{zhang2024improving,
  abstract      = {Equivariant Graph Neural Networks (GNNs) have made remarkable success in a variety of scientific applications. However, existing equivariant GNNs encounter the efficiency issue for large geometric graphs and perform poorly if the input is reduced to sparse local graph for speed acceleration. In this paper, we propose FastEGNN, an enhanced model of equivariant GNNs on large geometric graphs. The central idea is leveraging a small ordered set of virtual nodes to approximate the large unordered graph of real nodes. We distinguish the message passing and aggregation for different virtual nodes to encourage mutual distinctiveness, and minimize Maximum Mean Discrepancy (MMD) between virtual and real coordinates to realize global distributedness.},
  author        = {Yuelin Zhang and Jiacheng Cen and Jiaqi Han and Zhiqiang Zhang and Jun Zhou and Wenbing Huang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {58662--58679},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24f/zhang24f.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning},
  url           = {https://proceedings.mlr.press/v235/zhang24f.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{zhang2024irregular,
  abstract      = {Accurate traffic forecasting is crucial for the development of Intelligent Transportation Systems (ITS), playing a pivotal role in modern urban traffic management. Traditional forecasting methods, however, struggle with the irregular traffic time series resulting from adaptive traffic signal controls, presenting challenges in asynchronous spatial dependency, irregular temporal dependency, and predicting variable-length sequences. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored for irregular traffic time series forecasting. Specifically, we first propose an Asynchronous Graph Diffusion Network to capture the spatial dependency between asynchronously measured traffic states regulated by adaptive traffic signals. After that, to capture the temporal dependency within irregular traffic state sequences, a personalized time encoding is devised to embed the continuous time signals. Then, we propose a Transformable Time-aware Convolution Network, which adapts meta-filters for time-aware convolution on the sequences with inconsistent temporal flow. Additionally, a Semi-Autoregressive Prediction Network, comprising a state evolution unit and a semi-autoregressive predictor, is designed to predict variable-length traffic sequences effectively and efficiently. Extensive experiments on a newly established benchmark demonstrate the superiority of ASeer compared with twelve competitive baselines across six metrics.},
  author        = {Zhang, Weijia and Zhang, Le and Han, Jindong and Liu, Hao and Fu, Yanjie and Zhou, Jingbo and Yu, Mei and Xiong, Hui},
  booktitle     = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi           = {10.1145/3637528.3671665},
  openalex      = {W4386384824},
  pages         = {3923--3934},
  publisher     = {ACM},
  series        = {KDD '24},
  title         = {Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Networks},
  url           = {https://dl.acm.org/doi/10.1145/3637528.3671665},
  year          = {2024}
}

@inproceedings{zhang2024towards,
  author        = {Zhang, Binwu and Wang, Pengkun and Wang, Xuanhao and Guan, Yue and Zhang, Zhengyang and Wang, Yang},
  booktitle     = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month         = {2},
  note          = {This work proposes a new perspective on learning from spatio-temporal graphs by decoupling the learning of spatial and temporal patterns. The model uses separate modules to capture spatial dependencies (via a GNN) and temporal dependencies (via a temporal convolution or recurrent network). A key innovation is an attention mechanism that dynamically fuses the spatial and temporal representations, allowing the model to adapt to changing spatio-temporal dynamics.},
  number        = {9},
  openalex      = {W4393153515},
  pages         = {16822--16830},
  publisher     = {AAAI Press},
  title         = {Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective},
  url           = {https://ojs.aaai.org/index.php/AAAI/article/view/29089/30311},
  volume        = {38},
  year          = {2024}
}

@inproceedings{zhao2024flexible,
  abstract      = {Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. The approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. The authors show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which they perform generalized message passing. Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. Additionally, they reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into their Subgraph GNN. They also introduce novel node marking strategies and provide a theoretical analysis of their expressive power. Extensive experiments on multiple graph learning benchmarks demonstrate that their method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches.},
  author        = {Guy Bar-Shalom and Yam Eitan and Fabrizio Frasca and Haggai Maron},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399695825},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b739d7ae14c0dd4c7619476f3f80ec98-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening},
  url           = {https://papers.nips.cc/paper_files/paper/2024/hash/b739d7ae14c0dd4c7619476f3f80ec98-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{zhao2024graphtranslator,
  abstract      = {Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By translating node representation into tokens, GraphTranslator empowers an LLM to make predictions based on language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results demonstrate the effectiveness of our proposed GraphTranslator on zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended tasks through language instructions.},
  address       = {New York, NY, USA},
  author        = {Zhang, Mengmei and Sun, Mingwei and Wang, Peng and Fan, Shen and Wu, Longtao and Wang, Jianbo and Jiang, He and Li, Zhenxi and Lai, Jiangtao and Tao, Chengming and Li, Jianfeng and Chen, Enhong},
  booktitle     = {Proceedings of the ACM Web Conference 2024},
  doi           = {10.1145/3589334.3645682},
  openalex      = {W4396722529},
  pages         = {1003--1014},
  pdf           = {https://arxiv.org/pdf/2402.07197},
  publisher     = {Association for Computing Machinery},
  series        = {WWW '24},
  title         = {GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks},
  url           = {https://doi.org/10.1145/3589334.3645682},
  year          = {2024}
}

@article{zhao2024survey,
  abstract      = {Graph self-supervised learning (SSL) is now a go-to method for pre-training graph foundation models (GFMs). There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial for learning generalized representations for GFMs. However, existing surveys of GFMs have several shortcomings: they lack comprehensiveness regarding the most recent progress, have unclear categorization of self-supervised methods, and take a limited architecture-based perspective that is restricted to only certain types of graph models. To address these issues, we provide a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective and propose a knowledge-based taxonomy, which categorizes self-supervised graph models by the specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and more than 25 pretext tasks for pre-training GFMs, as well as various downstream task generalization strategies.},
  author        = {Ziwen Zhao and Yixin Su and Yuhua Li and Yixiong Zou and Ruixuan Li and Rui Zhang},
  doi           = {10.1109/tkde.2025.3568147},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  month         = {1},
  note          = {This comprehensive survey organizes the vast literature on self-supervised graph learning from a novel knowledge-based perspective. It proposes a taxonomy that categorizes pre-training pretext tasks based on the type of knowledge they leverage: microscopic (nodes, links), mesoscopic (context, clusters), and macroscopic (global structure). This framework provides a systematic way to understand and compare over 300 references in the field, covering both GNNs and recent graph language models.},
  openalex      = {W4410204320},
  pages         = {1--20},
  publisher     = {IEEE},
  title         = {A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective},
  url           = {https://arxiv.org/pdf/2403.16137},
  year          = {2025}
}

@inproceedings{zhu2024manifold,
  abstract      = {This paper pioneers the development of spiking GNNs that operate on non-Euclidean Riemannian manifolds, specifically targeting energy efficiency and geometric structure preservation. The proposed Manifold Spiking GNN (MSG) designs a novel manifold spiking neuron that emits spikes to drive the evolution of manifold representations along geodesics. This brain-inspired, event-driven approach significantly reduces computational costs while effectively learning on graphs with inherent hierarchical or geometric structures.},
  author        = {Zhu, Bing-Xue and Cai, Chen-Rui and Ding, Jian-Liang and Zhang, Ming-Gui and Tian, Yonghong},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pages         = {28453--28467},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS '24},
  title         = {Manifold Spiking Graph Neural Network},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3ba7560b4c3e66d760fbdd472cf4a5a9-Paper-Conference.pdf},
  volume        = {37},
  year          = {2024}
}

@article{Scarselli2009GNN,
  abstract      = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) is an element of IR(m) that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  author        = {Franco Scarselli and Marco Gori and Ah Chung Tsoi and Markus Hagenbuchner and Gabriele Monfardini},
  doi           = {10.1109/TNN.2008.2005605},
  journal       = {IEEE Transactions on Neural Networks},
  month         = {1},
  number        = {1},
  openalex      = {W2130473099},
  pages         = {61--80},
  pdf           = {https://ieeexplore.ieee.org/iel5/72/4740185/04700287.pdf},
  pmid          = {19068426},
  title         = {The Graph Neural Network Model},
  volume        = {20},
  year          = {2009}
}

@inproceedings{Bruna2014Spectral,
  abstract      = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  address       = {Banff, Canada},
  author        = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
  booktitle     = {Proceedings of the 2nd International Conference on Learning Representations},
  doi           = {10.48550/arXiv.1312.6203},
  eprint        = {1312.6203},
  eprintclass   = {cs.LG},
  eprinttype    = {arXiv},
  month         = {4},
  openalex      = {W2963382218},
  pdf           = {https://arxiv.org/pdf/1312.6203.pdf},
  series        = {ICLR '14},
  title         = {Spectral Networks and Deep Locally Connected Networks on Graphs},
  url           = {https://openreview.net/forum?id=DQNsQf-UsoDBa},
  year          = {2014}
}

@inproceedings{Defferrard2016ChebNet,
  abstract      = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  author        = {Michaël Defferrard and Xavier Bresson and Pierre Vandergheynst},
  booktitle     = {Advances in Neural Information Processing Systems 29},
  editor        = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  openalex      = {W2964321699},
  pages         = {3844--3852},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  volume        = {29},
  year          = {2016}
}

@inproceedings{Kipf2017GCN,
  abstract      = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In experiments on citation networks and knowledge graph datasets, we demonstrate that our approach outperforms related methods by a significant margin.},
  author        = {Thomas N. Kipf and Max Welling},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W2964015378},
  pdf           = {https://openreview.net/pdf?id=SJU4ayYgl},
  title         = {Semi-Supervised Classification with Graph Convolutional Networks},
  url           = {https://openreview.net/forum?id=SJU4ayYgl},
  year          = {2017}
}

@inproceedings{Hamilton2017GraphSAGE,
  abstract      = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  author        = {William L. Hamilton and Rex Ying and Jure Leskovec},
  booktitle     = {Advances in Neural Information Processing Systems},
  pages         = {1024--1034},
  pdf           = {https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Inductive Representation Learning on Large Graphs},
  url           = {https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
  volume        = {30},
  year          = {2017}
}

@inproceedings{Gilmer2017MPNN,
  abstract      = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. In this work, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs, we demonstrate state of the art results on an important molecular property prediction benchmark; however, we also observe that test performance on this benchmark was not well correlated with test performance on new datasets we collected. Furthermore, we demonstrate that our model outperforms the previous state of the art methods on these new datasets.},
  author        = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle     = {Proceedings of the 34th International Conference on Machine Learning},
  openalex      = {W2606780347},
  pages         = {1263--1272},
  pdf           = {https://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Neural Message Passing for Quantum Chemistry},
  url           = {https://proceedings.mlr.press/v70/gilmer17a.html},
  volume        = {70},
  year          = {2017}
}

@inproceedings{Velickovic2018GAT,
  abstract      = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  author        = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
  booktitle     = {6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  openalex      = {W2963858333},
  pdf           = {https://openreview.net/pdf?id=rJXMpikCZ},
  publisher     = {OpenReview.net},
  title         = {Graph Attention Networks},
  url           = {https://openreview.net/forum?id=rJXMpikCZ},
  year          = {2018}
}

@inproceedings{Chen2018FastGCN,
  abstract      = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.},
  author        = {Jie Chen and Tengfei Ma and Cao Xiao},
  booktitle     = {International Conference on Learning Representations},
  month         = {5},
  openalex      = {W2963695795},
  pdf           = {https://openreview.net/pdf?id=rytstxWAW},
  title         = {FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},
  url           = {https://openreview.net/forum?id=rytstxWAW},
  year          = {2018}
}

@inproceedings{Wu2019SGC,
  abstract      = {Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.},
  author        = {Felix Wu and Tianyi Zhang and Amauri H. Souza and Christopher Fifty and Tao Yu and Kilian Q. Weinberger},
  booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
  editor        = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  month         = {6},
  openalex      = {W2916106175},
  pages         = {6861--6871},
  pdf           = {http://proceedings.mlr.press/v97/wu19e/wu19e.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Simplifying Graph Convolutional Networks},
  volume        = {97},
  year          = {2019}
}

@article{Wu2020Survey,
  abstract      = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  author        = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  doi           = {10.1109/TNNLS.2020.2978386},
  journal       = {IEEE Transactions on Neural Networks and Learning Systems},
  month         = {1},
  number        = {1},
  openalex      = {W4210257598},
  pages         = {4--24},
  pdf           = {https://arxiv.org/pdf/1901.00596.pdf},
  title         = {A Comprehensive Survey on Graph Neural Networks},
  volume        = {32},
  year          = {2021}
}

@inproceedings{Xu2019GIN,
  abstract      = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  author        = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019},
  openalex      = {W2962711740},
  pdf           = {https://openreview.net/pdf?id=ryGs6iA5Km},
  title         = {How Powerful are Graph Neural Networks?},
  url           = {https://openreview.net/forum?id=ryGs6iA5Km},
  year          = {2019}
}

@inproceedings{Morris2019kGNN,
  abstract      = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically -- showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called $k$-dimensional GNNs ($k$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
  author        = {Christopher Morris and Martin Ritzert and Matthias Fey and William L. Hamilton and Jan Eric Lenssen and Gaurav Rattan and Martin Grohe},
  booktitle     = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi           = {10.1609/aaai.v33i01.33014602},
  number        = {01},
  openalex      = {W2962810718},
  pages         = {4602--4609},
  pdf           = {https://aaai.org/ojs/index.php/AAAI/article/view/4384/4262},
  title         = {Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks},
  url           = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
  volume        = {33},
  year          = {2019}
}

@inproceedings{Sato2021RandomFeatures,
  abstract      = {Graph neural networks (GNNs) are powerful machine learning models for various graph learning tasks. Recently, the limitations of the expressive power of various GNN models have been revealed. For example, GNNs cannot distinguish some non-isomorphic graphs and they cannot learn efficient graph algorithms. In this paper, we demonstrate that GNNs become powerful just by adding a random feature to each node. We prove that the random features enable GNNs to learn almost optimal polynomial-time approximation algorithms for the minimum dominating set problem and maximum matching problem in terms of approximation ratios. The main advantage of our method is that it can be combined with off-the-shelf GNN models with slight modifications. Through experiments, we show that the addition of random features enables GNNs to solve various problems that normal GNNs, including the graph convolutional networks (GCNs) and graph isomorphism networks (GINs), cannot solve.},
  author        = {Ryoma Sato and Makoto Yamada and Hisashi Kashima},
  booktitle     = {Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)},
  doi           = {10.1137/1.9781611976700.38},
  openalex      = {W3005187920},
  pages         = {333--341},
  pdf           = {https://arxiv.org/pdf/2002.03155},
  title         = {Random Features Strengthen Graph Neural Networks},
  year          = {2021}
}

@inproceedings{Abboud2021RandomInit,
  abstract      = {Graph neural networks (GNNs) are effective models for representation learning on relational data. However, standard GNNs are limited in their expressive power, as they cannot distinguish graphs beyond the capability of the Weisfeiler-Leman graph isomorphism heuristic. In order to break this expressiveness barrier, GNNs have been enhanced with random node initialization (RNI), where the idea is to train and run the models with randomized initial node features. In this work, we analyze the expressive power of GNNs with RNI, and prove that these models are universal, a first such result for GNNs not relying on computationally demanding higher-order properties. This universality result holds even with partially randomized initial node features, and preserves the invariance properties of GNNs in expectation. We then empirically analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our empirical findings support the superior performance of GNNs with RNI over standard GNNs.},
  author        = {Ralph Abboud and İsmail \̇lkan Ceylan and Martin Grohe and Thomas Lukasiewicz},
  booktitle     = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
  doi           = {10.24963/ijcai.2021/291},
  month         = {8},
  openalex      = {W3187220393},
  pages         = {2112--2118},
  pdf           = {https://www.ijcai.org/proceedings/2021/0291.pdf},
  series        = {IJCAI-21},
  title         = {The Surprising Power of Graph Neural Networks with Random Node Initialization},
  year          = {2021}
}

@inproceedings{Balcilar2021SpectralExpressivity,
  abstract      = {In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs.},
  address       = {Vienna, Austria},
  author        = {Muhammet Balcilar and Guillaume Renton and Pierre Héroux and Benoit Gaüzère and Sébastien Adam and Paul Honeine},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  month         = {5},
  openalex      = {W3123408649},
  pdf           = {https://openreview.net/pdf?id=-qh0M9XWxnv},
  publisher     = {ICLR},
  title         = {Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective},
  url           = {https://normandie-univ.hal.science/hal-03135633},
  year          = {2021}
}

@inproceedings{Jogl2024IsExpressivityEssential,
  abstract      = {Motivated by the large amount of research on the expressivity of GNNs, we study the impact of expressivity on the predictive performance of GNNs. By performing knowledge distillation from highly expressive teacher GNNs to less expressive student GNNs, we demonstrate that knowledge distillation reduces the predictive performance gap between teachers and students significantly.},
  author        = {Fabian Jogl and Pascal Welke and Thomas Gärtner},
  booktitle     = {NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning},
  month         = {12},
  pdf           = {https://openreview.net/pdf?id=5v7hpSy3Ir},
  title         = {Is Expressivity Essential for the Predictive Performance of Graph Neural Networks?},
  url           = {https://openreview.net/forum?id=5v7hpSy3Ir},
  year          = {2024}
}

@inproceedings{Maron2019Powerful,
  abstract      = {Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the 1-WL test (Morris et al. 2018; Xu et al. 2019). Unfortunately, many simple instances of graphs are indistinguishable by the 1-WL test. In search for more expressive graph learning models we build upon the recent k-order invariant and equivariant graph neural networks (Maron et al. 2019a,b) and present two results: First, we show that such k-order networks can distinguish between non-isomorphic graphs as good as the k-WL tests, which are provably stronger than the 1-WL test for k>2. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors. Second, setting our goal at building a provably stronger, simple and scalable model we show that a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable 3-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed 3-WL expressiveness, strictly stronger than message passing models.},
  address       = {Red Hook, NY, USA},
  author        = {Haggai Maron and Heli Ben-Hamu and Hadar Serviansky and Yaron Lipman},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W2970493342},
  pages         = {2153--2164},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Provably Powerful Graph Networks},
  volume        = {32},
  year          = {2019}
}

@inproceedings{Morris2020PowerfulRelational,
  address       = {Santiago de Compostela, Spain},
  author        = {Christopher Morris and Gaurav Rattan and Petra Mutzel},
  booktitle     = {ECAI 2020 - 24th European Conference on Artificial Intelligence},
  month         = {9},
  note          = {Conference held online due to COVID-19},
  publisher     = {IOS Press},
  series        = {Frontiers in Artificial Intelligence and Applications},
  title         = {A More Powerful Graph-Based Relational Learner},
  volume        = {325},
  year          = {2020}
}

@inproceedings{Azizian2021InvariantEquivariant,
  abstract      = {Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. We prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. The theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures).},
  author        = {Waïss Azizian and Marc Lelarge},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W3123260146},
  pdf           = {https://openreview.net/pdf?id=lxHgXYN4bwl},
  title         = {Expressive Power of Invariant and Equivariant Graph Neural Networks},
  url           = {https://openreview.net/forum?id=lxHgXYN4bwl},
  year          = {2021}
}

@inproceedings{Geerts2022TensorHierarchy,
  abstract      = {Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNN architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. Loosely speaking, if tensor language expressions use k+1 indices, then their separation power is bounded by k-WL. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. We believe that the approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests.},
  author        = {Floris Geerts and Juan L. Reutter},
  booktitle     = {International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2204.04661},
  month         = {4},
  pdf           = {https://arxiv.org/pdf/2204.04661.pdf},
  title         = {Expressiveness and Approximation Properties of Graph Neural Networks},
  url           = {https://openreview.net/forum?id=wIzUeM3TAU},
  year          = {2022}
}

@inproceedings{Zhao2022ProgressivelyExpressive,
  abstract      = {Message passing neural networks (MPNNs) have become a dominant flavor of graph neural networks (GNNs) in recent years. Yet, MPNNs come with notable limitations; namely, they are at most as powerful as the 1-dimensional Weisfeiler-Leman (1-WL) test in distinguishing graphs in a graph isomorphism testing framework. To this end, researchers have drawn inspiration from the k-WL hierarchy to develop more expressive GNNs. However, current k-WL-equivalent GNNs are not practical for even small values of k, as k-WL becomes combinatorially more complex as k grows. At the same time, several works have found great empirical success in graph learning tasks without highly expressive models, implying that chasing expressiveness with a coarse-grained ruler of expressivity like k-WL is often unneeded in practical tasks. To truly understand the expressiveness-complexity tradeoff, one desires a more fine-grained ruler, which can more gradually increase expressiveness. Our work puts forth such a proposal: Namely, we first propose the (k, c)(<=)-SETWL hierarchy with greatly reduced complexity from k-WL, achieved by moving from k-tuples of nodes to sets with <=k nodes defined over <=c connected components in the induced original graph. We show favorable theoretical results for this model in relation to k-WL, and concretize it via (k, c)(<=)-SETGNN, which is as expressive as (k, c)(<=)-SETWL. Our model is practical and progressively-expressive, increasing in power with k and c. We demonstrate effectiveness on several benchmark datasets, achieving several state-of-the-art results with runtime and memory usage applicable to practical graphs. We open source our implementation at https://github.com/LingxiaoShawn/KCSetGNN.},
  author        = {Lingxiao Zhao and Neil Shah and Leman Akoglu},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4306887098},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/dc89a0709f213fd0ac4b1172719b2c38-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {A Practical, Progressively-Expressive GNN},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/dc89a0709f213fd0ac4b1172719b2c38-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Zhang2023Biconnectivity,
  abstract      = {Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.},
  author        = {Bohang Zhang and Shengjie Luo and Liwei Wang and Di He},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  pdf           = {https://openreview.net/pdf?id=r9hNv76KoT3},
  title         = {Rethinking the Expressive Power of GNNs via Graph Biconnectivity},
  url           = {https://openreview.net/forum?id=r9hNv76KoT3},
  year          = {2023}
}

@inproceedings{Puny2023EquivariantFeatures,
  abstract      = {Graph Neural Networks (GNN) are inherently limited in their expressive power. Recent seminal works introduced the Weisfeiler-Lehman (WL) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in GNN analysis and architecture developments, it suffers from several significant limitations. This paper introduces an alternative expressive power hierarchy based on the ability of GNNs to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Second, we propose algorithmic tools for evaluating the expressiveness of GNNs using tensor contraction sequences, and calculate the expressive power of popular GNNs. Finally, we enhance the expressivity of common GNN architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced GNNs demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.},
  author        = {Omri Puny and Derek Lim and Bobak T. Kiani and Haggai Maron and Yaron Lipman},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  month         = {7},
  pages         = {28191--28222},
  pdf           = {https://proceedings.mlr.press/v202/puny23a/puny23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Equivariant Polynomials for Graph Neural Networks},
  url           = {https://proceedings.mlr.press/v202/puny23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Piquenot2024G2N2,
  abstract      = {This paper introduces a framework for formally establishing a connection between a portion of an algebraic language and a Graph Neural Network (GNN). The framework leverages Context-Free Grammars (CFG) to organize algebraic operations into generative rules that can be translated into a GNN layer model. As CFGs derived directly from a language tend to contain redundancies in their rules and variables, we present a grammar reduction scheme. By applying this strategy, we define a CFG that conforms to the third-order Weisfeiler-Lehman (3-WL) test using the matricial language MATLANG. From this 3-WL CFG, we derive a GNN model, named G$^2$N$^2$, which is provably 3-WL compliant. Through various experiments, we demonstrate the superior efficiency of G$^2$N$^2$ compared to other 3-WL GNNs across numerous downstream tasks.},
  arxiv         = {2303.01590},
  author        = {Jason Piquenot and Aldo Moscatelli and Maxime Bérar and Pierre Héroux and Romain Raveaux and Jean-Yves Ramel and Sébastien Adam},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=eZneJ55mRO},
  publisher     = {OpenReview.net},
  title         = {G$^2$N$^2$: Weisfeiler and Lehman go grammatical},
  url           = {https://openreview.net/forum?id=eZneJ55mRO},
  year          = {2024}
}

@inproceedings{Graziani2024PathBased,
  abstract      = {We systematically investigate the expressive power of path-based graph neural networks. While it has been shown that path-based graph neural networks can achieve strong empirical results, an investigation into their expressive power is lacking. We propose PATH-WL, a general class of color refinement algorithms based on paths and shortest path distance information. PATH-WL is incomparable to a wide range of expressive graph neural networks, can count cycles, and achieves strong empirical results on the notoriously difficult family of strongly regular graphs. Our theoretical results indicate that PATH-WL forms a new hierarchy of highly expressive graph neural networks.},
  author        = {Caterina Graziani and Tamara Drucks and Fabian Jogl and Monica Bianchini and Franco Scarselli and Thomas Gärtner},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {16226--16249},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/graziani24a/graziani24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {The Expressive Power of Path-Based Graph Neural Networks},
  url           = {https://proceedings.mlr.press/v235/graziani24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Pellizzoni2024NodeIndividualized,
  abstract      = {Graph neural networks (GNNs) employing message passing for graph classification are inherently limited by the expressive power of the Weisfeiler-Leman (WL) test for graph isomorphism. Node individualization schemes, which assign unique identifiers to nodes (e.g., by adding random noise to features), are a common approach for achieving universal expressiveness. However, the ability of GNNs endowed with individualization schemes to generalize beyond the training data is still an open question. To address this question, this paper presents a theoretical analysis of the sample complexity of such GNNs from a statistical learning perspective, employing Vapnik--Chervonenkis (VC) dimension and covering methods.},
  address       = {Red Hook, NY, USA},
  author        = {Paolo Pellizzoni and Till Hendrik Schulz and Dexiong Chen and Karsten M. Borgwardt},
  booktitle     = {Advances in Neural Information Processing Systems},
  hdl           = {21.11116/0000-000F-EF28-8},
  keywords      = {Graph neural networks, graph learning, Weisfeiler-Leman, VC dimension},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS '24},
  title         = {On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks},
  url           = {https://neurips.cc/virtual/2024/poster/96388},
  volume        = {37},
  year          = {2024}
}

@inproceedings{Alsentzer2020SUBGNN,
  abstract      = {Deep learning methods for graphs achieve remarkable performance on many node-level and graph-level prediction tasks. However, despite the proliferation of the methods and their success, prevailing Graph Neural Networks (GNNs) neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in many impactful applications. Further, subgraph prediction tasks present several unique challenges: subgraphs can have non-trivial internal topology, but also carry a notion of position and external connectivity information relative to the underlying graph in which they exist. Here, we introduce SubGNN, a subgraph neural network to learn disentangled subgraph representations. We propose a novel subgraph routing mechanism that propagates neural messages between the subgraph's components and randomly sampled anchor patches from the underlying graph, yielding highly accurate subgraph representations. SubGNN specifies three channels, each designed to capture a distinct aspect of subgraph topology, and we provide empirical evidence that the channels encode their intended properties. We design a series of new synthetic and real-world subgraph datasets. Empirical results for subgraph classification on eight datasets show that SubGNN achieves considerable performance gains, outperforming strong baseline methods, including node-level and graph-level GNNs, by 19.8% over the strongest baseline. SubGNN performs exceptionally well on challenging biomedical datasets where subgraphs have complex topology and even comprise multiple disconnected components.},
  author        = {Emily Alsentzer and Samuel G. Finlayson and Michelle M. Li and Marinka Zitnik},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  openalex      = {W3036850272},
  pages         = {8017--8029},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/5bca8566db79f3788be9efd96c9ed70d-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Subgraph Neural Networks},
  volume        = {33},
  year          = {2020}
}

@article{Bouritsas2022SubgraphIsomorphism,
  abstract      = {While Graph Neural Networks (GNNs) have achieved remarkable results in various applications, recent studies exposed important shortcomings in their ability to capture the underlying graph structure. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman (WL) graph isomorphism test, which limits their ability to detect and count substructures. The authors propose 'Graph Substructure Networks' (GSN), a topologically-aware message passing scheme based on substructure encoding. They theoretically analyze the architecture, showing it is strictly more expressive than WL and provides conditions for universality. Importantly, the method retains attractive properties like locality and linear complexity, while being able to disambiguate even hard isomorphism instances. The researchers perform extensive experimental evaluations across classification and regression tasks, obtaining state-of-the-art results in diverse real-world settings including molecular graphs and social networks.},
  author        = {Giorgos Bouritsas and Fabrizio Frasca and Stefanos Zafeiriou and Michael M. Bronstein},
  doi           = {10.1109/TPAMI.2022.3154319},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month         = {1},
  number        = {1},
  openalex      = {W3035664258},
  pages         = {657--668},
  pdf           = {https://ieeexplore.ieee.org/iel7/34/9970415/09721082.pdf},
  title         = {Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting},
  volume        = {45},
  year          = {2023}
}

@inproceedings{Bevilacqua2022ESAN,
  abstract      = {Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures.},
  archiveprefix = {arXiv},
  author        = {Beatrice Bevilacqua and Fabrizio Frasca and Derek Lim and Balasubramaniam Srinivasan and Chen Cai and Gopinath Balamurugan and Michael M. Bronstein and Haggai Maron},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  eprint        = {2110.02910},
  month         = {4},
  pdf           = {https://arxiv.org/pdf/2110.02910.pdf},
  primaryclass  = {cs.LG},
  publisher     = {OpenReview.net},
  title         = {Equivariant Subgraph Aggregation Networks},
  url           = {https://openreview.net/forum?id=dFbKQaRk15w},
  year          = {2022}
}

@inproceedings{Frasca2022SubgraphSymmetries,
  abstract      = {Subgraph GNNs are a recent class of expressive Graph Neural Networks (GNNs) which model graphs as collections of subgraphs. So far, the design space of possible Subgraph GNN architectures as well as their basic theoretical properties are still largely unexplored. In this paper, we study the most prominent form of subgraph methods, which employs node-based subgraph selection policies such as ego-networks or node marking and deletion. We address two central questions: (1) What is the upper-bound of the expressive power of these methods? and (2) What is the family of equivariant message passing layers on these sets of subgraphs?. Our first step in answering these questions is a novel symmetry analysis which shows that modelling the symmetries of node-based subgraph collections requires a significantly smaller symmetry group than the one adopted in previous works. This analysis is then used to establish a link between Subgraph GNNs and Invariant Graph Networks (IGNs). We answer the questions above by first bounding the expressive power of subgraph methods by 3-WL, and then proposing a general family of message-passing layers for subgraph methods that generalises all previous node-based Subgraph GNNs. Finally, we design a novel Subgraph GNN dubbed SUN, which theoretically unifies previous architectures while providing better empirical performance on multiple benchmarks.},
  author        = {Fabrizio Frasca and Beatrice Bevilacqua and Michael M. Bronstein and Haggai Maron},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4283378708},
  pages         = {31376--31390},
  pdf           = {https://neurips.cc/paper_files/paper/2022/file/cb2a4cc70db72ea779abd01107782c7b-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb2a4cc70db72ea779abd01107782c7b-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Qian2022KHop,
  abstract      = {The most popular design paradigm for Graph Neural Networks (GNNs) is 1-hop message passing---aggregating information from 1-hop neighbors repeatedly. However, the expressive power of 1-hop message passing is bounded by the Weisfeiler-Lehman (1-WL) test. Recently, researchers extended 1-hop message passing to $K$-hop message passing by aggregating information from $K$-hop neighbors of nodes simultaneously. However, there is no work on analyzing the expressive power of $K$-hop message passing. In this work, we theoretically characterize the expressive power of $K$-hop message passing. Specifically, we first formally differentiate two different kernels of $K$-hop message passing which are often misused in previous works. We then characterize the expressive power of $K$-hop message passing by showing that it is more powerful than 1-WL and can distinguish almost all regular graphs. Despite the higher expressive power, we show that $K$-hop message passing still cannot distinguish some simple regular graphs and its expressive power is bounded by 3-WL. To further enhance its expressive power, we introduce a KP-GNN framework, which improves $K$-hop message passing by leveraging the peripheral subgraph information in each hop. We show that KP-GNN can distinguish many distance regular graphs which could not be distinguished by previous distance encoding or 3-WL methods. Experimental results verify the expressive power and effectiveness of KP-GNN. KP-GNN achieves competitive results across all benchmark datasets.},
  author        = {Jiarui Feng and Yixin Chen and Fuhai Li and Anindya Sarkar and Muhan Zhang},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4282046386},
  pages         = {36981--36992},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1ece70d2259b8e9510e2d4ca8754cecf-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {How Powerful are K-hop Message Passing Graph Neural Networks},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Zhang2023SWL,
  abstract      = {Recently, subgraph GNNs have emerged as an important direction for developing expressive graph neural networks (GNNs). While numerous architectures have been proposed, so far there is still a limited understanding of how various design paradigms differ in terms of expressive power, nor is it clear what design principle achieves maximal expressiveness with minimal architectural complexity. To address these fundamental questions, this paper conducts a systematic study of general node-based subgraph GNNs through the lens of Subgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build a complete hierarchy of SWL with strictly growing expressivity. Concretely, we prove that any node-based subgraph GNN falls into one of the six SWL equivalence classes, among which $\mathsfSSWL$ achieves the maximal expressive power. We also study how these equivalence classes differ in terms of their practical expressiveness such as encoding graph distance and biconnectivity. Furthermore, we give a tight expressivity upper bound of all SWL algorithms by establishing a close relation with localized versions of WL and Folklore WL (FWL) tests. Our results provide insights into the power of existing subgraph GNNs, guide the design of new architectures, and point out their limitations by revealing an inherent gap with the 2-FWL test. Finally, experiments demonstrate that $\mathsfSSWL$-inspired subgraph GNNs can significantly outperform prior architectures on multiple benchmarks despite great simplicity.},
  author        = {Bohang Zhang and Guhao Feng and Yiheng Du and Di He and Liwei Wang},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4321013606},
  pages         = {41019--41077},
  pdf           = {http://proceedings.mlr.press/v202/zhang23k/zhang23k.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Dimitrov2023PlanE,
  abstract      = {Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations. Yet, current graph neural networks are limited in their expressive power: it is well-known that standard graph neural networks cannot distinguish pairs of non-isomorphic graphs in certain cases. The key to the success of graph neural networks is the use of different node representations, which aggregate information from the local neighborhoods of nodes. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Motivated by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable.},
  author        = {Radoslav Dimitrov and Zeyang Zhao and Ralph Abboud and İsmail \̇lkan Ceylan},
  booktitle     = {Advances in Neural Information Processing Systems},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/33b47b3d2441a17b95344cd635f3dd01-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {PlanE: Representation Learning over Planar Graphs},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/33b47b3d2441a17b95344cd635f3dd01-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Jogl2023Simulation,
  abstract      = {We systematically investigate graph transformations that enable standard message passing to simulate state-of-the-art graph neural networks (GNNs) without loss of expressivity. Using these, many state-of-the-art GNNs can be implemented with message passing operations from standard libraries, eliminating many sources of implementation issues and allowing for better code optimization. We distinguish between weak and strong simulation: weak simulation achieves the same expressivity only after several message passing steps while strong simulation achieves this after every message passing step. Our contribution leads to a direct way to translate common operations of non-standard GNNs to graph transformations that allow for strong or weak simulation. Our empirical evaluation shows competitive predictive performance of message passing on transformed graphs for various molecular benchmark datasets, in several cases surpassing the original GNNs.},
  author        = {Fabian Jogl and Maximilian Thiessen and Thomas Gärtner},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  pdf           = {https://neurips.cc/paper_files/paper/2023/file/ebf95a6f3c575322da15d4fd0fc2b3c8-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Expressivity-Preserving GNN Simulation},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ebf95a6f3c575322da15d4fd0fc2b3c8-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Morris2023SpeqNets,
  abstract      = {While (message-passing) graph neural networks have clear limitations in approximating permutation-equivariant functions over graphs or general relational data, more expressive, higher-order graph neural networks do not scale to large graphs. They either operate on $k$-order tensors or consider all $k$-node subgraphs, implying an exponential dependence on $k$ in memory requirements, and do not adapt to the sparsity of the graph. By introducing new heuristics for the graph isomorphism problem, we devise a class of universal, permutation-equivariant graph networks, which, unlike previous architectures, offer a fine-grained control between expressivity and scalability and adapt to the sparsity of the graph. These architectures lead to vastly reduced computation times compared to standard higher-order graph networks in the supervised node- and graph-level classification and regression regime while significantly improving over standard graph neural network and graph kernel architectures in terms of predictive performance.},
  arxiv         = {2203.13913},
  author        = {Christopher Morris and Gaurav Rattan and Sandra Kiefer and Siamak Ravanbakhsh},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  editor        = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  pages         = {16017--16042},
  pdf           = {https://proceedings.mlr.press/v162/morris22a/morris22a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {SpeqNets: Sparsity-aware Permutation-equivariant Graph Networks},
  url           = {https://proceedings.mlr.press/v162/morris22a.html},
  volume        = {162},
  year          = {2022}
}

@inproceedings{Zhang2024HomomorphismExpressivity,
  abstract      = {Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a unified framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.},
  author        = {Bohang Zhang and Jingchu Gai and Yiheng Du and Qiwei Ye and Di He and Liwei Wang},
  booktitle     = {Proceedings of the Twelfth International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2401.08514},
  month         = {5},
  note          = {ICLR 2024 Outstanding Paper Award Honorable Mention},
  openalex      = {W4390963127},
  pdf           = {https://openreview.net/pdf?id=HSKaGOi7Ar},
  series        = {ICLR 2024},
  title         = {Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness},
  url           = {https://openreview.net/forum?id=HSKaGOi7Ar},
  year          = {2024}
}

@inproceedings{Wang2024EmpiricalStudy,
  abstract      = {Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the k-dimensional Weisfeiler-Lehman (k-WL) test hierarchy, leading to difficulties in quantitatively comparing their expressiveness. Previous research attempted to use datasets for measurement but faced problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (cannot compare models between 1-WL and 3-WL), and scale (only a few essentially different graphs). In this paper, we study the "realized expressive power"---the practical expressive power that a practical model instance can achieve. We conduct a thorough experimental study to measure the realized expressiveness using a novel expressiveness dataset called BREC. The BREC dataset poses greater difficulty (with graphs up to 4-WL-indistinguishable), finer granularity (enabling comparison of models between 1-WL and 3-WL), and larger scale (consisting of 800 1-WL-indistinguishable graphs that are non-isomorphic to each other). We synthetically test 23 models with higher-than-1-WL expressiveness on BREC. Our experiment gives the first thorough measurement of the realized expressiveness of those state-of-the-art beyond-1-WL GNN models and reveals the gap between theoretical and realized expressiveness.},
  author        = {Yanbo Wang and Muhan Zhang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  pages         = {52134--52155},
  pdf           = {https://proceedings.mlr.press/v235/wang24cl/wang24cl.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {An Empirical Study of Realized GNN Expressiveness},
  url           = {https://proceedings.mlr.press/v235/wang24cl.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Jin2024HomomorphismCounts,
  abstract      = {A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the ``basis'' of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical results on node-level and graph-level motif parameters and empirically validate them on standard benchmark datasets.},
  author        = {Emily Jin and Michael M. Bronstein and Ismail Ilkan Ceylan and Matthias Lanzinger},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  openalex      = {W4391832606},
  pages         = {22075--22098},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jin24a/jin24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Homomorphism Counts for Graph Neural Networks: All About That Basis},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Morris2024Margin,
  abstract      = {The Weisfeiler--Leman algorithm (1-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, the 1-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting 1-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we introduce variations of expressive 1-WL-based kernel and MPNN architectures with provable generalization properties. Our empirical study confirms the validity of our theoretical findings.},
  author        = {Billy Joe Franks and Christopher Morris and Ameya Velingker and Floris Geerts},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {13885--13926},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/franks24a/franks24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Weisfeiler-Leman at the margin: When more expressivity matters},
  url           = {https://proceedings.mlr.press/v235/franks24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Boker2024FineGrained,
  abstract      = {Numerous recent works have analyzed the expressive power of message-passing graph neural networks (MPNNs), primarily utilizing combinatorial techniques such as the $1$-dimensional Weisfeiler-Leman test ($1$-WL) for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. This work resolves this issue by considering continuous extensions of both $1$-WL and MPNNs to graphons. Concretely, we show that the continuous variant of $1$-WL delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the level of difficulty in separating them. We identify the finest topology where MPNNs separate points and prove a universal approximation theorem. Consequently, we provide a theoretical framework for graph and graphon similarity combining various topological variants of classical characterizations of the $1$-WL. In particular, we characterize the expressive power of MPNNs in terms of the tree distance, which is a graph distance based on the concept of fractional isomorphisms, and substructure counts via tree homomorphisms, showing that these concepts have the same expressive power as the $1$-WL and MPNNs on graphons. Empirically, we validate our theoretical findings by showing that randomly initialized MPNNs, without training, exhibit competitive performance compared to their trained counterparts. Moreover, we evaluate different MPNN architectures based on their ability to preserve graph distances, highlighting the significance of our continuous $1$-WL test in understanding MPNNs' expressivity.},
  author        = {Jan Böker and Ron Levie and Ningyuan Huang and Soledad Villar and Christopher Morris},
  booktitle     = {Advances in Neural Information Processing Systems},
  pages         = {1--12},
  pdf           = {https://neurips.cc/paper_files/paper/2023/file/9200d97ca2bf3a26db7b591844014f00-Paper-Conference.pdf},
  publisher     = {Curran Associates Inc.},
  series        = {NeurIPS '23},
  title         = {Fine-grained Expressivity of Graph Neural Networks},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9200d97ca2bf3a26db7b591844014f00-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Zhang2024SpectralInvariant,
  abstract      = {Incorporating spectral information to enhance Graph Neural Networks (GNNs) has shown promising results but raises a fundamental challenge due to the inherent ambiguity of eigenvectors. Various architectures have been proposed to address this ambiguity, referred to as spectral invariant architectures. Notable examples include GNNs and Graph Transformers that use spectral distances, spectral projection matrices, or other invariant spectral features. However, the potential expressive power of these spectral invariant architectures remains largely unclear. The goal of this work is to gain a deep theoretical understanding of the expressive power obtainable when using spectral features. We first introduce a unified message-passing framework for designing spectral invariant GNNs, called Eigenspace Projection GNN (EPNN). A comprehensive analysis shows that EPNN essentially unifies all prior spectral invariant architectures, in that they are either strictly less expressive or equivalent to EPNN. A fine-grained expressiveness hierarchy among different architectures is also established. On the other hand, we prove that EPNN itself is bounded by a recently proposed class of Subgraph GNNs, implying that all these spectral invariant architectures are strictly less expressive than 3-WL. Finally, we discuss whether using spectral features can gain additional expressiveness when combined with more expressive GNNs.},
  address       = {Vienna, Austria},
  author        = {Bohang Zhang and Lingxiao Zhao and Haggai Maron},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4399455371},
  pdf           = {https://arxiv.org/pdf/2406.04336.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On the Expressive Power of Spectral Invariant Graph Neural Networks},
  url           = {https://proceedings.mlr.press/v235/},
  volume        = {235},
  year          = {2024}
}

@inproceedings{BarShalom2024Subgraphormer,
  abstract      = {In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. Our experimental results demonstrate significant performance improvements over both Subgraph GNNs and Graph Transformers on a wide range of datasets.},
  address       = {Vienna, Austria},
  author        = {Guy Bar-Shalom and Beatrice Bevilacqua and Haggai Maron},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4391833874},
  pages         = {2959--2989},
  pdf           = {https://proceedings.mlr.press/v235/bar-shalom24a/bar-shalom24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products},
  url           = {https://proceedings.mlr.press/v235/bar-shalom24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Bevilacqua2024EfficientSubgraph,
  abstract      = {Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of WL-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called POLICY-LEARN, that learns how to select subgraphs in an iterative manner. We prove that, unlike popular random policies and prior work addressing the same problem, our architecture is able to learn the efficient policies mentioned above. Experimental results demonstrate that POLICY-LEARN outperforms existing baselines across a wide range of datasets.},
  archiveprefix = {arXiv},
  author        = {Beatrice Bevilacqua and Moshe Eliasof and Eli A. Meirom and Bruno Ribeiro and Haggai Maron},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  eprint        = {2310.20082},
  openalex      = {W4388184421},
  pdf           = {https://openreview.net/pdf?id=gppLqZLQeY},
  primaryclass  = {cs.LG},
  title         = {Efficient Subgraph GNNs by Learning Effective Selection Policies},
  url           = {https://openreview.net/forum?id=gppLqZLQeY},
  venue         = {ICLR},
  year          = {2024}
}

@inproceedings{BarShalom2024FlexibleFramework,
  abstract      = {Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing. Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches.},
  author        = {Guy Bar-Shalom and Yam Eitan and Fabrizio Frasca and Haggai Maron},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399695825},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b739d7ae14c0dd4c7619476f3f80ec98-Paper-Conference.pdf},
  title         = {A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening},
  url           = {https://papers.nips.cc/paper_files/paper/2024/hash/b739d7ae14c0dd4c7619476f3f80ec98-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@misc{Zeng2025SubgraphNodeClassification,
  abstract      = {Previous studies have demonstrated the strong performance of Graph Neural Networks (GNNs) in node classification. However, most existing GNNs adopt a node-centric perspective and rely on global message passing, leading to high computational and memory costs that hinder scalability. To mitigate these challenges, subgraph-based methods have been introduced, leveraging local subgraphs as approximations of full computational trees. While this approach improves efficiency, it often suffers from performance degradation due to the loss of global contextual information, limiting its effectiveness compared to global GNNs. To address this trade-off between scalability and classification accuracy, we reformulate the node classification task as a subgraph classification problem and propose SubGND (Subgraph GNN for NoDe). This framework introduces a differentiated zero-padding strategy and an Ego-Alter subgraph representation method to resolve label conflicts while incorporating an Adaptive Feature Scaling Mechanism to dynamically adjust feature contributions based on dataset-specific dependencies. Experimental results on six benchmark datasets demonstrate that SubGND achieves performance comparable to or surpassing global message-passing GNNs, particularly in heterophilic settings, highlighting its effectiveness and scalability as a promising solution for node classification.},
  archiveprefix = {arXiv},
  author        = {Qian Zeng and Xin Lin and Jingyi Gao and Yang Yu},
  eprint        = {2503.06614},
  keywords      = {Heterophily, Graph, Subgraph, GNN, Label Conflict},
  pdf           = {https://arxiv.org/pdf/2503.06614.pdf},
  primaryclass  = {cs.LG},
  title         = {Using Subgraph GNNs for Node Classification: an Overlooked Potential Approach},
  url           = {https://arxiv.org/abs/2503.06614},
  year          = {2025}
}

@misc{Chen2025BoundedCycles,
  abstract      = {Graph neural networks (GNNs) have been widely used in graph-related contexts. It is known that the separation power of GNNs is equivalent to Weisfeiler-Lehman (WL) test; hence, GNNs are imperfect at identifying all non-isomorphic graphs, which severely limits their expressive power. This work investigates $k$-hop subgraph aggregate information from neighbors with distances up to $k$ and incorporate structure. We prove that under appropriate assumptions, GNNs can approximate any permutation-invariant/equivariant continuous function over graphs without cycles length greater than $2k+1$ within error tolerance. We also provide an extension and numerical experiments on established benchmarks and novel architectures validate our theory relationship between aggregation distance and cycle size.},
  archiveprefix = {arXiv},
  author        = {Ziang Chen and Qiao Zhang and Runzhong Wang},
  eprint        = {2502.03703},
  month         = {2},
  openalex      = {W4407244638},
  pdf           = {https://arxiv.org/pdf/2502.03703.pdf},
  primaryclass  = {cs.LG},
  title         = {On the Expressive Power of Subgraph Graph Neural Networks for Graphs with Bounded Cycles},
  url           = {https://arxiv.org/abs/2502.03703},
  year          = {2025}
}

@misc{Thomas2018TensorField,
  abstract      = {We introduce tensor field neural networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. The networks use filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate the capabilities of tensor field networks with tasks in geometry, physics, and chemistry.},
  archiveprefix = {arXiv},
  author        = {Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley},
  doi           = {10.48550/arXiv.1802.08219},
  eprint        = {1802.08219},
  month         = {2},
  openalex      = {W2788775653},
  pdf           = {https://arxiv.org/pdf/1802.08219.pdf},
  primaryclass  = {cs.LG},
  title         = {Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds},
  url           = {https://arxiv.org/abs/1802.08219},
  year          = {2018}
}

@inproceedings{Kondor2018ClebschGordan,
  abstract      = {We build on the recent work of Cohen et al. that has achieved state-of-the-art results for learning spherical images in a rotation invariant way by using ideas from group representation theory and noncommutative harmonic analysis. We propose a generalization of this work that generally exhibits improved performance, but from an implementation point of view is actually simpler. An unusual feature of the proposed architecture is that it uses the Clebsch-Gordan transform as its only source of nonlinearity, thus avoiding repeated forward and backward Fourier transforms.},
  author        = {Risi Kondor and Zhen Lin and Shubhendu Trivedi},
  booktitle     = {Advances in Neural Information Processing Systems 31},
  pages         = {8215--8225},
  pdf           = {https://papers.nips.cc/paper_files/paper/2018/file/a3fc981af450752046be179185ebc8b5-Paper.pdf},
  series        = {NIPS'18},
  title         = {Clebsch--Gordan Nets: A Fully Fourier Space Spherical CNN},
  url           = {https://papers.nips.cc/paper/8215-clebschgordan-nets-a-fully-fourier-space-spherical-convolutional-neural-network},
  year          = {2018}
}

@inproceedings{Fuchs2020SE3Transformers,
  abstract      = {We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point-clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important for achieving stable and predictable performance for 3D tasks. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying numbers of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our method on N-body particle simulation, ScanObjectNN, and QM9 datasets.},
  author        = {Fabian B. Fuchs and Daniel E. Worrall and Volker Fischer and Max Welling},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  openalex      = {W3100269082},
  pages         = {1597--1608},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks},
  volume        = {33},
  year          = {2020}
}

@inproceedings{Satorras2021EGNN,
  abstract      = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces.},
  author        = {V\íctor Garcia Satorras and Emiel Hoogeboom and Max Welling},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  month         = {7},
  pages         = {9323--9332},
  pdf           = {http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {E(n) Equivariant Graph Neural Networks},
  volume        = {139},
  year          = {2021}
}

@article{Batzner2022E3Equivariant,
  abstract      = {Constructing fast and accurate force fields from ab-initio calculations is a long-standing challenge in computational chemistry and materials science. This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.},
  author        = {Simon Batzner and Albert Musaelian and Lixin Sun and Mario Geiger and Jonathan P. Mailoa and Mordechai Kornbluth and Nicola Molinari and Tess E. Smidt and Boris Kozinsky},
  doi           = {10.1038/s41467-022-29939-5},
  journal       = {Nature Communications},
  month         = {5},
  number        = {1},
  pages         = {2453},
  pdf           = {https://www.nature.com/articles/s41467-022-29939-5.pdf},
  pmid          = {35508450},
  title         = {E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials},
  url           = {https://www.nature.com/articles/s41467-022-29939-5},
  volume        = {13},
  year          = {2022}
}

@inproceedings{Liao2022GeneralFramework,
  abstract      = {Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or the unitary groups, play essential roles across scientific fields as diverse as high energy physics, quantum mechanics, quantum chromodynamics, molecular dynamics, computer vision, and imaging. In this paper, we present a general Equivariant Neural Network architecture capable of respecting the symmetries of the finite-dimensional representations of any reductive Lie Group G. Our approach generalizes the successful ACE and MACE architectures for atomistic point clouds to any data equivariant to a reductive Lie group action. We also introduce the lie-nn software library, which provides all the necessary tools to develop and implement such general G-equivariant neural networks. It implements routines for the reduction of generic tensor products of representations into irreducible representations, making it easy to apply our architecture to a wide range of problems and groups. The generality and performance of our approach are demonstrated by applying it to the tasks of top quark decay tagging (Lorentz group) and shape recognition (orthogonal group).},
  author        = {Ilyes Batatia and Mario Geiger and Jose Munoz and Tess Smidt and Lior Silberman and Christoph Ortner},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  openalex      = {W4379251772},
  pages         = {TBD},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ad1f2197941348b1c4373fd6c19ee0b4-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {A General Framework for Equivariant Neural Networks on Reductive Lie Groups},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Joshi2023GeometricExpressivePower,
  abstract      = {The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation. Synthetic experiments supplementing our results are available at r̆lhttps://github.com/chaitjo/geometric-gnn-dojo},
  author        = {Chaitanya K. Joshi and Cristian Bodnar and Simon V. Mathis and Taco Cohen and Pietro Liò},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4317941748},
  pages         = {15330--15355},
  pdf           = {https://proceedings.mlr.press/v202/joshi23a/joshi23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On the Expressive Power of Geometric Graph Neural Networks},
  url           = {https://proceedings.mlr.press/v202/joshi23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Puny2023FrameAveraging,
  abstract      = {Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.},
  author        = {Omri Puny and Matan Atzmon and Heli Ben-Hamu and Ishan Misra and Aditya Grover and Edward J. Smith and Yaron Lipman},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  openalex      = {W4286908904},
  pdf           = {https://arxiv.org/abs/2110.03336},
  title         = {Frame Averaging for Invariant and Equivariant Network Design},
  url           = {https://openreview.net/forum?id=zIUyj55nXR},
  year          = {2022}
}

@article{Chen2023ProteinModelQuality,
  abstract      = {Quality assessment (QA) of predicted protein tertiary structure models plays an important role in ranking and using them. With the recent development of deep learning end-to-end prediction techniques for generating highly confident tertiary structures, it is important to explore corresponding QA strategies since these models have better quality and different properties than traditional methods. We develop EnQA, a novel graph-based 3D-equivariant neural network method that is equivariant to rotation and translation of 3D objects to estimate the accuracy of protein structures. EnQA achieves state-of-the-art performance on protein structural models predicted by both traditional protein structure prediction methods and AlphaFold2, performing even better than the model QA scores provided by AlphaFold2 itself. The source code is available at https://github.com/BioinfoMachineLearning/EnQA.},
  author        = {Chen Chen and Xiao Chen and Alex Morehead and Tianqi Wu and Jianlin Cheng},
  doi           = {10.1093/bioinformatics/btad030},
  journal       = {Bioinformatics},
  month         = {1},
  number        = {1},
  openalex      = {W4315928370},
  pages         = {btad030},
  pdf           = {https://academic.oup.com/bioinformatics/article-pdf/39/1/btad030/48942769/btad030.pdf},
  publisher     = {Oxford University Press},
  title         = {3D-equivariant graph neural networks for protein model quality assessment},
  url           = {https://doi.org/10.1093/bioinformatics/btad030},
  volume        = {39},
  year          = {2023}
}

@inproceedings{Morris2024OrbitEquivariant,
  abstract      = {Equivariance is an important structural property that is captured by architectures such as graph neural networks (GNNs). However, equivariant graph functions cannot produce different outputs for similar nodes, which may be undesirable when the function is trying to optimize some global graph property. In this paper, we define orbit-equivariance, a relaxation of equivariance which allows for such functions whilst retaining important structural inductive biases.},
  address       = {Vienna, Austria},
  author        = {Matthew Morris and Bernardo Cuenca Grau and Ian Horrocks},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  pdf           = {https://openreview.net/pdf?id=GkJOCga62u},
  publisher     = {OpenReview.net},
  title         = {Orbit-Equivariant Graph Neural Networks},
  url           = {https://openreview.net/forum?id=GkJOCga62u},
  year          = {2024}
}

@inproceedings{Liu2024Clifford,
  abstract      = {We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to outperform both equivariant and simplicial graph neural networks on a variety of geometric tasks.},
  author        = {Cong Liu and David Ruhe and Floor Eijkelboom and Patrick Forré},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month         = {5},
  openalex      = {W4391900187},
  pdf           = {https://arxiv.org/pdf/2402.10011.pdf},
  publisher     = {OpenReview.net},
  title         = {Clifford Group Equivariant Simplicial Message Passing Networks},
  year          = {2024}
}

@inproceedings{Luo2024SteerableFeatures,
  author        = {Siyu Luo and Andrew Gordon Wilson},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  publisher     = {OpenReview.net},
  title         = {Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks},
  url           = {https://openreview.net/forum?id=mGHJAyR8w0},
  year          = {2024}
}

@inproceedings{Hordan2024WLEuclidean,
  abstract      = {The $k$-Weisfeiler-Leman ($k$-WL) graph isomorphism test hierarchy is commonly used to assess the expressive power of graph neural networks (GNNs). Recently, GNNs whose expressive power is equivalent to the 2-WL test were proven to be universal on weighted graphs which encode 3D point cloud data, yet this result is limited to invariant continuous functions on point clouds. We extend this result in three ways: 1) We show that PPGN can simulate 2-WL uniformly on all point clouds with low complexity. 2) We show that 2-WL tests can be extended to point clouds which include both positions and velocities, a scenario often encountered in applications. 3) We provide a general framework for proving equivariant universality and leverage it to prove that a simple modification of this invariant PPGN architecture can be used to obtain a universal equivariant architecture that can approximate all continuous equivariant functions uniformly. Building on these results, we develop the WeLNet architecture. Our theoretical results are complemented by experiments showing that WeLNet sets new state-of-the-art results on the N-Body dynamics task and the GEOM-QM9 molecular conformation generation task.},
  author        = {Snir Hordan and Tal Amir and Nadav Dym},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391591099},
  pages         = {18749--18784},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/hordan24a/hordan24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Weisfeiler Leman for Euclidean Equivariant Machine Learning},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Navon2024WeightSpaceAlignment,
  abstract      = {Permutation symmetries of deep networks make basic operations like model merging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first prove that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture that respects these symmetries. Notably, our framework does not require any labeled data. We provide a theoretical analysis of our approach and evaluate Deep-Align on several types of network architectures and learning setups. Our experimental results indicate that a feed-forward pass with Deep-Align produces better or equivalent alignments compared to those produced by current optimization algorithms. Additionally, our alignments can be used as an effective initialization for other methods, leading to improved solutions with a significant speedup in convergence.},
  address       = {Vienna, Austria},
  author        = {Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik and Nadav Dym and Haggai Maron},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4387891918},
  pdf           = {https://arxiv.org/pdf/2310.13397.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Equivariant Deep Weight Space Alignment},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Shen2025ZeroShot,
  abstract      = {There are no known graph machine learning methods that can zero-shot generalize across attributed graphs with very different node attribute domains and consistently outperform methods that ignore node attributes. For instance, no method can significantly outperform structure-only predictions in zero-shot link prediction by pretraining on online appliance store datasets (with node attributes such as brand, model, capacity, dimension, has ice maker, energy rating for refrigerators) and zero-shot at test on an electronics store dataset for smartphones (with attributes such as processor type, display type, storage, and battery capacity). In this work, we leverage concepts in statistical theory to design STAGE, a universally applicable approach for encoding node attributes in any GNN that facilitates such generalization. Empirically, we show that STAGE outperforms its natural baselines and can accurately make predictions when presented with completely new feature domains.},
  address       = {Vancouver, Canada},
  author        = {Yangyi Shen and Jincheng Zhou and Beatrice Bevilacqua and Joshua Robinson and Charilaos Kanatsoulis and Jure Leskovec and Bruno Ribeiro},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Zero-Shot Generalization of GNNs over Distinct Attribute Domains},
  url           = {https://icml.cc/virtual/2025/poster/45294},
  year          = {2025}
}

@inproceedings{Zhang2025TRIX,
  abstract      = {Fully inductive knowledge graph models can be trained on multiple domains and subsequently perform zero-shot knowledge graph completion (KGC) in new unseen domains. This is an important capability towards the goal of having foundation models for knowledge graphs. TRIX introduces a more expressive and capable fully inductive model, which not only yields strictly more expressive triplet embeddings (head entity, relation, tail entity) compared to state-of-the-art methods, but also introduces a new capability: directly handling both entity and relation prediction tasks in inductive settings.},
  author        = {Yucheng Zhang and Beatrice Bevilacqua and Mikhail Galkin and Bruno Ribeiro},
  booktitle     = {Proceedings of the Third Learning on Graphs Conference},
  month         = {11},
  note          = {Available at r̆lhttps://arxiv.org/abs/2502.19512},
  pdf           = {https://openreview.net/pdf?id=mRB0XkewKW},
  series        = {Proceedings of Machine Learning Research},
  title         = {TRIX: A More Expressive Model for Zero-shot Domain Transfer in Knowledge Graphs},
  url           = {https://openreview.net/forum?id=mRB0XkewKW},
  year          = {2024}
}

@inproceedings{Li2018DeeperInsights,
  abstract      = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although GCNs are able to achieve or exceed state-of-the-art performance in a number of benchmarks, their mechanisms are not clear and they still require a considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into GCNs. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks verify our theory and proposals.},
  address       = {New Orleans, Louisiana, USA},
  author        = {Qimai Li and Zhichao Han and Xiao-Ming Wu},
  booktitle     = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi           = {10.1609/aaai.v32i1.11604},
  month         = {2},
  number        = {1},
  openalex      = {W2964051675},
  pages         = {3538--3545},
  pdf           = {https://arxiv.org/pdf/1801.07606.pdf},
  publisher     = {AAAI Press},
  title         = {Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning},
  url           = {https://doi.org/10.1609/aaai.v32i1.11604},
  volume        = {32},
  year          = {2018}
}

@inproceedings{Oono2020ExponentialLoss,
  abstract      = {Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős – Rényi graph. We show that when the Erdős – Rényi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the "information loss" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data.},
  author        = {Kenta Oono and Taiji Suzuki},
  booktitle     = {Proceedings of the 8th International Conference on Learning Representations},
  month         = {4},
  openalex      = {W2995914187},
  pdf           = {https://openreview.net/pdf?id=S1ldO2EFPr},
  title         = {Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
  url           = {https://openreview.net/forum?id=S1ldO2EFPr},
  year          = {2020}
}

@misc{Cai2020NoteOnExpressivePower,
  abstract      = {Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results to further analyze the over-smoothing effect in the general graph neural network architecture. We show that when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power.},
  archiveprefix = {arXiv},
  author        = {Chen Cai and Yusu Wang},
  doi           = {10.48550/arXiv.2006.13318},
  eprint        = {2006.13318},
  note          = {Workshop paper at ICML 2020},
  pdf           = {https://arxiv.org/pdf/2006.13318.pdf},
  title         = {A Note on Over-Smoothing for Graph Neural Networks},
  url           = {https://arxiv.org/abs/2006.13318},
  venue         = {ICML 2020 Graph Representation Learning Workshop},
  year          = {2020}
}

@misc{Rusch2023SurveyOversmoothing,
  abstract      = {Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.},
  archiveprefix = {arXiv},
  author        = {T. Konstantin Rusch and Michael M. Bronstein and Siddhartha Mishra},
  eprint        = {2303.10993},
  keywords      = {graph neural networks, oversmoothing, survey},
  month         = {3},
  note          = {Survey paper on oversmoothing in graph neural networks},
  pdf           = {https://arxiv.org/pdf/2303.10993.pdf},
  primaryclass  = {cs.LG},
  title         = {A Survey on Oversmoothing in Graph Neural Networks},
  url           = {https://arxiv.org/abs/2303.10993},
  year          = {2023}
}

@inproceedings{DiGiovanni2023HowAttentive,
  abstract      = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs.},
  author        = {Shaked Brody and Uri Alon and Eran Yahav},
  booktitle     = {Proceedings of the 10th International Conference on Learning Representations (ICLR)},
  month         = {4},
  openalex      = {W4286795917},
  pdf           = {https://openreview.net/pdf?id=F72ximsx7C1},
  publisher     = {OpenReview.net},
  title         = {How Attentive are Graph Attention Networks?},
  url           = {https://openreview.net/forum?id=F72ximsx7C1},
  year          = {2022}
}

@inproceedings{Wang2023DoGATsLearn,
  author        = {Kaituo Wang and Xingyi Zhang and Zhaoliang Huang and Wenzheng Huang and Liwei Wang},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title         = {Do GATs really learn structural information?},
  year          = {2023}
}

@misc{Kim2024StationaryPerspective,
  author        = {Daehyun Kim and Jinyoung Kim},
  note          = {Manuscript},
  title         = {Oversmoothing in GNNs: A Stationary Perspective},
  year          = {2024}
}

@misc{Nguyen2025RankBased,
  abstract      = {Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing in the nonlinear setting without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.},
  archiveprefix = {arXiv},
  author        = {Kaicheng Zhang and Piero Deidda and Desmond J. Higham and Francesco Tudisco},
  eprint        = {2502.04591},
  openalex      = {W4407309021},
  pdf           = {https://arxiv.org/pdf/2502.04591.pdf},
  primaryclass  = {cs.LG},
  title         = {Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective},
  year          = {2025}
}

@inproceedings{Alon2021Bottleneck,
  abstract      = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights.},
  author        = {Uri Alon and Eran Yahav},
  booktitle     = {ICLR 2021},
  month         = {5},
  openalex      = {W4295728955},
  pdf           = {https://openreview.net/pdf?id=i80OPhOCVH2},
  title         = {On the Bottleneck of Graph Neural Networks and its Practical Implications},
  url           = {https://openreview.net/forum?id=i80OPhOCVH2},
  year          = {2021}
}

@inproceedings{Topping2022Curvature,
  abstract      = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
  author        = {Jake Topping and Francesco Di Giovanni and Benjamin Paul Chamberlain and Xiaowen Dong and Michael M. Bronstein},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W4225751401},
  pdf           = {https://openreview.net/pdf?id=7UmjRGzp-A},
  title         = {Understanding over-squashing and bottlenecks on graphs via curvature},
  url           = {https://openreview.net/forum?id=7UmjRGzp-A},
  year          = {2022}
}

@inproceedings{Black2023EffectiveResistance,
  abstract      = {Message passing graph neural networks (GNNs) are a popular learning architectures for graph-structured data. However, one problem GNNs experience is oversquashing, where a GNN has difficulty sending information between distant nodes. In this paper, we continue this line of work by analyzing oversquashing through the lens of the effective resistance between nodes in the input graph. Effective resistance intuitively captures the "strength" of connection between two nodes by paths in the graph, and has a rich literature spanning many areas of graph theory. We propose to use total effective resistance as a bound of the total amount of oversquashing in a graph and provide theoretical justification for its use. We further develop an algorithm to identify edges to be added to an input graph to minimize the total effective resistance, thereby alleviating oversquashing. We provide empirical evidence of the effectiveness of our total effective resistance based rewiring strategies for improving the performance of GNNs.},
  author        = {Mitchell Black and Zhengchao Wan and Amir Nayyeri and Yusu Wang},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  pages         = {2528--2547},
  pdf           = {https://proceedings.mlr.press/v202/black23a/black23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Understanding Oversquashing in GNNs through the Lens of Effective Resistance},
  url           = {https://proceedings.mlr.press/v202/black23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{DiGiovanni2023ImpactWidthDepth,
  abstract      = {Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.},
  address       = {Honolulu, Hawaii, USA},
  author        = {Francesco Di Giovanni and Lorenzo Giusti and Federico Barbero and Giulia Luise and Pietro Liò and Michael M. Bronstein},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  doi           = {10.48550/arxiv.2302.02941},
  month         = {7},
  openalex      = {W4319453689},
  pages         = {7865--7885},
  pdf           = {https://proceedings.mlr.press/v202/di-giovanni23a/di-giovanni23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology},
  url           = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Nguyen2023RevisitingOllivier,
  abstract      = {Graph Neural Networks (GNNs) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that over-smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.},
  author        = {Khang Nguyen and Nong Minh Hieu and Vinh Duc Nguyen and Nhat Ho and Stanley Osher and Tan Minh Nguyen},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4310509726},
  pages         = {25956--25979},
  pdf           = {https://proceedings.mlr.press/v202/nguyen23c/nguyen23c.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature},
  url           = {https://proceedings.mlr.press/v202/nguyen23c.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Attali2024Delaunay,
  abstract      = {GNNs rely on the exchange of messages to distribute information along the edges of the graph. This approach makes the efficiency of architectures highly dependent on the specific structure of the input graph. Certain graph topologies lead to inefficient information propagation, resulting in a phenomenon known as over-squashing. While the majority of existing methods address over-squashing by rewiring the input graph, our novel approach involves constructing a graph directly from features using Delaunay Triangulation. We posit that the topological properties of the resulting graph prove advantageous for mitigate oversmoothing and over-squashing. Our extensive experimentation demonstrates that our method consistently outperforms established graph rewiring methods.},
  address       = {Vienna, Austria},
  author        = {Hugo Attali and Davide Buscaldi and Nathalie Pernelle},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  pages         = {1992--2008},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/attali24a/attali24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Delaunay Graph: Addressing Over-Squashing and Over-Smoothing Using Delaunay Triangulation},
  url           = {https://proceedings.mlr.press/v235/attali24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Pei2024MultiTrack,
  abstract      = {The advancement toward deeper graph neural networks is currently obscured by two inherent issues in message passing, \emphoversmoothing and \emphoversquashing. We identify the root cause of these issues as information loss due to \emphheterophily mixing in aggregation, where messages of diverse category semantics are mixed. We propose a novel multi-track graph convolutional network to address oversmoothing and oversquashing effectively. Our basic idea is intuitive: if messages are separated and independently propagated according to their category semantics, heterophilic mixing can be prevented. Consequently, we present a novel multi-track message passing scheme capable of preventing heterophilic mixing, enhancing long-distance information flow, and improving separation condition. Empirical validations show that our model achieved state-of-the-art performance on several graph datasets and effectively tackled oversmoothing and oversquashing, setting a new benchmark of $86.4$% accuracy on Cora.},
  author        = {Hongbin Pei and Yu Li and Huiqi Deng and Jingxin Hai and Pinghui Wang and Jie Ma and Jing Tao and Yuheng Xiong and Xiaohong Guan},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  pages         = {40078--40091},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/pei24a/pei24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing},
  url           = {https://proceedings.mlr.press/v235/pei24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Jamadandi2024SpectralPruning,
  abstract      = {Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a more effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on large heterophilic datasets.},
  author        = {Adarsh Jamadandi and Celia Rubio-Madrigal and Rebekka Burkholz},
  booktitle     = {Advances in Neural Information Processing Systems 37},
  openalex      = {W4394654077},
  pages         = {TBD},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/140aac600566125915df7e74ff538f66-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Spectral Graph Pruning Against Over-Squashing and Over-Smoothing},
  url           = {https://arxiv.org/abs/2404.04612},
  year          = {2024}
}

@misc{Arroyo2025VanishingGradients,
  abstract      = {Graph Neural Networks (GNNs) are models that leverage the graph structure to transmit information between nodes, typically through the message-passing operation. While widely successful, this approach is well known to suffer from the over-smoothing and over-squashing phenomena, which result in representational collapse as the number of layers increases and insensitivity to the information contained at distant and poorly connected nodes, respectively. In this paper, we present a unified view of these problems through the lens of vanishing gradients, using ideas from linear control theory for our analysis. We propose an interpretation of GNNs as recurrent models and empirically demonstrate that a simple state-space formulation of a GNN effectively alleviates over-smoothing and over-squashing at no extra trainable parameter cost. Further, we show theoretically and empirically that (i) GNNs are by design prone to extreme gradient vanishing even after a few layers; (ii) Over-smoothing is directly related to the mechanism causing vanishing gradients; (iii) Over-squashing is most easily alleviated by a combination of graph rewiring and vanishing gradient mitigation. We believe our work will help bridge the gap between the recurrent and graph neural network literature and will unlock the design of new deep and performant GNNs.},
  archiveprefix = {arXiv},
  author        = {Álvaro Arroyo and Alessio Gravina and Benjamin Gutteridge and Federico Barbero and Claudio Gallicchio and Xiaowen Dong and Michael Bronstein and Pierre Vandergheynst},
  eprint        = {2502.10818},
  month         = {2},
  openalex      = {W4407684143},
  pdf           = {https://arxiv.org/pdf/2502.10818},
  primaryclass  = {cs.LG},
  title         = {On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning},
  url           = {https://arxiv.org/abs/2502.10818},
  year          = {2025}
}

@inproceedings{Eliasof2025DynamicalSystems,
  abstract      = {A common problem in Message-Passing Neural Networks is oversquashing -- the limited ability to facilitate effective information flow between distant nodes. Oversquashing is attributed to the exponential decay in information transmission as node distances increase. This paper introduces a novel perspective to address oversquashing, leveraging dynamical systems properties of global and local non-dissipativity, that enable the maintenance of a constant information flow rate. We present SWAN, a uniquely parameterized GNN model with antisymmetry both in space and weight domains, as a means to obtain non-dissipativity. Our theoretical analysis asserts that by implementing these properties, SWAN offers an enhanced ability to transmit information over extended distances. Empirical evaluations on synthetic and real-world benchmarks that emphasize long-range interactions validate the theoretical understanding of SWAN, and its ability to mitigate oversquashing.},
  author        = {Alessio Gravina and Moshe Eliasof and Claudio Gallicchio and Davide Bacciu and Carola-Bibiane Schönlieb},
  booktitle     = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi           = {10.1609/aaai.v39i16.33858},
  number        = {16},
  openalex      = {W4409364309},
  pages         = {16906--16914},
  pdf           = {https://arxiv.org/pdf/2405.01009.pdf},
  title         = {On Oversquashing in Graph Neural Networks Through the Lens of Dynamical Systems},
  url           = {https://ojs.aaai.org/index.php/AAAI/article/view/33858},
  volume        = {39},
  year          = {2025}
}

@inproceedings{Ebli2020SimplicialNN,
  abstract      = {We present simplicial neural networks (SNNs), a generalization of graph neural networks to data that live on a class of topological spaces called simplicial complexes. These are natural multi-dimensional extensions of graphs that encode not only pairwise relationships but also higher-order interactions between vertices - allowing us to consider richer data, including vector fields and $n$-fold collaboration networks.},
  archiveprefix = {arXiv},
  author        = {Stefania Ebli and Michaël Defferrard and Gard Spreemann},
  booktitle     = {Topological Data Analysis and Beyond: Workshop at the 34th Conference on Neural Information Processing Systems},
  eprint        = {2010.03633},
  month         = {12},
  pdf           = {https://openreview.net/pdf?id=nPCt39DVIfk},
  primaryclass  = {cs.LG},
  publisher     = {OpenReview.net},
  title         = {Simplicial Neural Networks},
  url           = {https://openreview.net/forum?id=nPCt39DVIfk},
  year          = {2020}
}

@inproceedings{Bodnar2021MPSN,
  abstract      = {The pairwise interaction paradigm of graph machine learning has predominantly governed the modelling of relational systems. However, graphs alone cannot capture the multi-level interactions present in many complex systems and the expressive power of such schemes was proven to be limited. To overcome these limitations, we propose Message Passing Simplicial Networks (MPSNs), a class of models that perform message passing on simplicial complexes (SCs). To theoretically analyse the expressivity of our model we introduce a Simplicial Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic SCs and show that SWL and MPSNs are strictly more powerful than the WL test and not less powerful than the 3-WL test. We empirically support our theoretical claims by showing that MPSNs can distinguish challenging strongly regular graphs for which GNNs fail and, when equipped with orientation equivariant layers, they can improve classification accuracy in oriented SCs compared to a GNN baseline.},
  author        = {Cristian Bodnar and Fabrizio Frasca and Yu Guang Wang and Nina Otter and Guido F. Montúfar and Pietro Liò and Michael M. Bronstein},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  editor        = {Marina Meila and Tong Zhang},
  month         = {7},
  pages         = {1026--1037},
  pdf           = {http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks},
  url           = {https://proceedings.mlr.press/v139/bodnar21a.html},
  volume        = {139},
  year          = {2021}
}

@inproceedings{Bodnar2021CWN,
  abstract      = {Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. Our model achieves state-of-the-art results on a variety of molecular datasets.},
  address       = {Red Hook, NY, USA},
  author        = {Cristian Bodnar and Fabrizio Frasca and Nina Otter and Yu Guang Wang and Pietro Liò and Guido F. Montúfar and Michael Bronstein},
  booktitle     = {Advances in Neural Information Processing Systems 34},
  editor        = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann Dauphin and Percy S. Liang and Jenn Wortman Vaughan},
  note          = {arXiv:2106.12575},
  pages         = {2625--2640},
  pdf           = {https://proceedings.neurips.cc/paper/2021/file/157792e4abb490f99dbd738483e0d2d4-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Weisfeiler and Lehman Go Cellular: CW Networks},
  url           = {https://proceedings.neurips.cc/paper/2021/hash/157792e4abb490f99dbd738483e0d2d4-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{Hajij2020CellComplex,
  abstract      = {Cell complexes are topological spaces constructed from simple blocks called cells. They generalize graphs, simplicial complexes, and polyhedral complexes that form important domains for practical applications. They also provide a combinatorial formalism that allows the inclusion of complicated relationships of restrictive structures such as graphs and meshes. In this paper, we propose Cell Complexes Neural Networks (CXNs), a general, combinatorial and unifying construction for performing neural network-type computations on cell complexes. We introduce an inter-cellular message passing scheme on cell complexes that takes the topology of the underlying space into account and generalizes message passing scheme to graphs. Not only do we detail the neural network construction, but we prove its approximation universality for a large class of problems. Finally, we demonstrate the effectiveness of the proposed method on a variety of tasks including node, link, and cell classification problems.},
  archiveprefix = {arXiv},
  author        = {Mustafa Hajij and Kyle Istvan and Ghada Zamzmi},
  booktitle     = {NeurIPS 2020 Workshop on Topological Data Analysis and Beyond},
  eprint        = {2010.00743},
  pdf           = {https://openreview.net/pdf?id=6Tq18ySFpGU},
  primaryclass  = {cs.LG},
  title         = {Cell Complex Neural Networks},
  url           = {https://openreview.net/forum?id=6Tq18ySFpGU},
  year          = {2020}
}

@article{Roost2022SimplicialGNN,
  author        = {Hannes Roost and Thomas M. Roddenberry and Santiago Segarra},
  journal       = {IEEE Transactions on Signal Processing},
  pages         = {4619--4631},
  title         = {Simplicial Graph Neural Networks},
  volume        = {70},
  year          = {2022}
}

@inproceedings{Bodnar2022SheafDiffusion,
  abstract      = {Cellular sheaves equip graphs with a 'geometrical' structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain competitive results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.},
  author        = {Cristian Bodnar and Francesco Di Giovanni and Benjamin Paul Chamberlain and Pietro Liò and Michael M. Bronstein},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  openalex      = {W4221155895},
  pdf           = {https://papers.nips.cc/paper_files/paper/2022/file/75c45fca2aa416ada062b26cc4fb7641-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Goh2022SimplicialAttention,
  abstract      = {Graph representation learning methods have mostly been limited to the modelling of node-wise interactions. Recently, there has been an increased interest in understanding how higher-order structures can be utilised to further enhance the learning abilities of graph neural networks (GNNs) in combinatorial spaces. Simplicial Neural Networks (SNNs) naturally model interactions by performing message passing on simplicial complexes, higher-order generalisations of graphs. Nonetheless, the literature has mostly focused on convolutional approaches for SNNs, building off local aggregation operations. In this work, we propose Simplicial Attention Networks (SAT), a new type of simplicial network that dynamically weighs the interactions between neighbouring simplicies and can readily adapt to novel structures. Additionally, we propose a signed attention mechanism that makes SAT orientation equivariant, a desirable property for models operating on (co)chain complexes. We demonstrate that SAT outperforms existent convolutional SNNs and GNNs in two image and trajectory classification tasks.},
  archiveprefix = {arXiv},
  author        = {Christopher Wei Jin Goh and Cristian Bodnar and Pietro Liò},
  booktitle     = {ICLR 2022 Workshop on Geometrical and Topological Representation Learning},
  doi           = {10.48550/arxiv.2204.09455},
  eprint        = {2204.09455},
  month         = {3},
  openalex      = {W4224903912},
  primaryclass  = {cs.LG},
  title         = {Simplicial Attention Networks},
  url           = {https://openreview.net/forum?id=ScfRNWkpec},
  year          = {2022}
}

@misc{Giusti2022SimplicialAttention,
  abstract      = {The aim of this work is to introduce simplicial attention networks (SANs), i.e., novel neural architectures that operate on data defined on simplicial complexes leveraging masked self-attentional layers. Hinging on formal arguments from topological signal processing, we introduce a proper self-attention mechanism able to process data components at different layers (e.g., nodes, edges, triangles, and so on), while learning how to weight both upper and lower neighborhoods of the given topological domain in a totally task-oriented fashion. The proposed SANs generalize most of the current architectures available for processing data defined on simplicial complexes.},
  archiveprefix = {arXiv},
  author        = {Lorenzo Giusti and Claudio Battiloro and Paolo Di Lorenzo and Stefania Sardellitti and Sergio Barbarossa},
  eprint        = {2203.07485},
  note          = {Later extended and published in IEEE Transactions on Signal and Information Processing over Networks, vol. 10, pp. 833-850, 2024},
  pdf           = {https://arxiv.org/pdf/2203.07485.pdf},
  primaryclass  = {cs.LG},
  title         = {Simplicial Attention Neural Networks},
  url           = {https://arxiv.org/abs/2203.07485},
  year          = {2022}
}

@misc{Hajij2023TopologicalDL,
  abstract      = {Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain that can be seen as generalizations of graphs that maintain certain desirable properties. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks.},
  archiveprefix = {arXiv},
  author        = {Mustafa Hajij and Ghada Zamzmi and Theodore Papamarkou and Nina Miolane and Aldo Guzmán-Sáenz and Karthikeyan Natesan Ramamurthy and Tolga Birdal and Tamal K. Dey and Soham Mukherjee and Shreyas N. Samaga and Neal Livesay and Robin Walters and Paul Rosen and Michael T. Schaub},
  doi           = {10.48550/arXiv.2206.00606},
  eprint        = {2206.00606},
  openalex      = {W4281710343},
  primaryclass  = {cs.LG},
  title         = {Topological Deep Learning: Going Beyond Graph Data},
  url           = {https://arxiv.org/abs/2206.00606},
  year          = {2022}
}

@misc{Papillon2023SurveyTNN,
  abstract      = {The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature for relational systems has also led to a lack of unification in notation and language across message-passing Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying message-passing TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL for relational systems, and compare the recently published message-passing TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.},
  archiveprefix = {arXiv},
  author        = {Mathilde Papillon and Sophia Sanborn and Mustafa Hajij and Nina Miolane},
  doi           = {10.48550/arXiv.2304.10031},
  eprint        = {2304.10031},
  note          = {Survey paper on topological neural network architectures},
  openalex      = {W4366733083},
  pdf           = {https://arxiv.org/pdf/2304.10031.pdf},
  primaryclass  = {cs.LG},
  title         = {Architectures of Topological Deep Learning: A Survey on Topological Neural Networks},
  url           = {https://arxiv.org/abs/2304.10031},
  year          = {2023}
}

@inproceedings{Eijkelboom2023EquivariantSimplicial,
  abstract      = {This paper presents E$(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, reflections. EMPSNs can learn high-dimensional simplex features, use the increase of information from higher-dimensional simplices, and simultaneously generalize Graph Neural Networks topologically.},
  author        = {Floor Eijkelboom and Rob Hesselink and Erik J. Bekkers},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  editor        = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  openalex      = {W4376632361},
  pages         = {9071--9081},
  pdf           = {https://proceedings.mlr.press/v202/eijkelboom23a/eijkelboom23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {E$(n)$ Equivariant Message Passing Simplicial Networks},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Bamberger2023Blindspots,
  author        = {Lewin Bamberger and James Tönshoff and Thomas Gärtner},
  booktitle     = {Proceedings of the International Conference on Machine Learning (ICML)},
  title         = {Topological Blindspots in GNNs: The Role of Symmetries and Covering Maps},
  year          = {2023}
}

@inproceedings{Papamarkou2024Position,
  abstract      = {Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL is the new frontier for relational learning. TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. The paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations, outlining potential solutions and future research opportunities for each problem. The paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.},
  author        = {Theodore Papamarkou and Tolga Birdal and Michael M. Bronstein and Gunnar E. Carlsson and Justin Curry and Yue Gao and Mustafa Hajij and Roland Kwitt and Pietro Lio and Paolo Di Lorenzo and Vasileios Maroulas and Nina Miolane and Farzana Nasrin and Karthikeyan Natesan Ramamurthy and Bastian Rieck and Simone Scardapane and Michael T. Schaub and Petar Veličković and Bei Wang and Yusu Wang and Guowei Wei and Ghada Zamzmi},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {39529--39555},
  pdf           = {https://openreview.net/pdf?id=Nl3RG5XWAt},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Position: Topological Deep Learning is the New Frontier for Relational Learning},
  url           = {https://proceedings.mlr.press/v235/papamarkou24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Verma2024TopNets,
  abstract      = {Topological Neural Networks (TNNs) incorporate higher-order relational information beyond pairwise interactions, enabling richer representations than Graph Neural Networks (GNNs). Concurrently, topological descriptors based on persistent homology (PH) are being increasingly employed to augment the GNNs. We investigate the benefits of integrating these two paradigms. Specifically, we introduce TopNets as a broad framework that subsumes and unifies various methods in the intersection of GNNs/TNNs and PH such as (generalizations of) RePEcHINE and TOGL. TopNets can also be readily adapted to handle (symmetries in) geometric complexes, extending the scope of TNNs and PH to spatial settings. Theoretically, we show that PH descriptors can provably enhance the expressivity of simplicial message-passing networks. Empirically, (continuous and $E(n)$-equivariant extensions of) TopNets achieve strong performance across diverse tasks, including antibody design, molecular dynamics simulation, and drug property prediction.},
  author        = {Yogeshwar Verma and Amauri H. Souza and Vikas Garg},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4399447898},
  pages         = {49388--49407},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/verma24a/verma24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Topological Neural Networks go Persistent, Equivariant, and Continuous},
  url           = {https://proceedings.mlr.press/v235/verma24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Alain2024GaussianProcesses,
  abstract      = {In recent years, there has been considerable interest in developing machine learning models on graphs to account for topological inductive biases. In particular, recent attention has been given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells.},
  author        = {Mathieu Alain and So Takao and Brooks Paige and Marc Peter Deisenroth},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  pages         = {879--905},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/alain24a/alain24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Gaussian Processes on Cellular Complexes},
  url           = {https://proceedings.mlr.press/v235/alain24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Huang2024UniversalPolynomial,
  abstract      = {Spectral Graph Neural Networks (GNNs), alternatively known as graph filters, have gained increasing prevalence for heterophily graphs. However, polynomials in the majority of these filters are predefined and remain fixed across different graphs, failing to accommodate the varying degrees of heterophily. We demystify the intrinsic correlation between the spectral property of desired polynomial bases and the heterophily degrees via thorough theoretical analyses. Subsequently, we develop a novel adaptive heterophily basis wherein the basis vectors mutually form angles reflecting the heterophily degree of the graph. By incorporating this heterophily basis with the homophily basis, we establish a universal polynomial basis UniBasis. We then integrate UniBasis into the polynomial filter-based graph neural network, dubbed UniFilter. Comprehensive experiments demonstrate the superior performance of our UniFilter over both the spatial-domain and spectral-domain baselines. Notably, UniFilter achieves substantial performance gains on heterophily graphs and effectively addresses the over-smoothing and over-squashing challenges that plague many existing methods.},
  author        = {Keke Huang and Yu Guang Wang and Ming Li and Pietro Liò},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  pages         = {20310--20330},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24z/huang24z.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing},
  url           = {https://proceedings.mlr.press/v235/huang24z.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Bamberger2024ExtendingTDL,
  author        = {Lewin Bamberger and Cristian Bodnar and Francesco Di Giovanni and Christopher Morris and James Tönshoff},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  note          = {Paper details could not be verified in available databases},
  publisher     = {OpenReview.net},
  series        = {Proceedings of the International Conference on Learning Representations},
  title         = {Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity},
  url           = {https://iclr.cc/},
  venue         = {ICLR},
  year          = {2024}
}

@inproceedings{Bernardez2024ICMLChallenge,
  abstract      = {This paper describes the 2nd edition of the ICML Topological Deep Learning Challenge hosted within the ICML 2024 ELLIS Workshop on Geometry-grounded Representation Learning and Generative Modeling (GRaM). The challenge focused on the problem of representing data in different discrete topological domains in order to bridge the gap between Topological Deep Learning (TDL) and other types of structured datasets. Specifically, participants were asked to design and implement topological liftings, i.e. mappings between different data structures and topological domains like hypergraphs, or simplicial/cell/combinatorial complexes. The challenge received 52 submissions satisfying all the requirements from 31 different teams with a total of 57 participants. The main purpose of this challenge is to foster new research and knowledge about effective mappings between different topological domains and data structures, helping to expand the current scope and impact of TDL to a much broader range of contexts.},
  author        = {Guillermo Bernárdez and Lev Telyatnikov and Marco Montagna and Federica Baccini and Mathilde Papillon and Miquel Ferriol-Galmés and Mustafa Hajij and Theodore Papamarkou and Maria Sofia Bucarelli and Olga Zaghen and Johan Mathe and Audun Myers and Scott Mahan and Hansen Lillemark and Sharvaree Vadgama and Erik Bekkers and Tim Doster and Tegan Emerson and Henry Kvinge and Katrina Agate and Nesreen K. Ahmed and Pengfei Bai and Michael Banf and Claudio Battiloro and Maxim Beketov and Paul Bogdan and Martin Carrasco and Andrea Cavallo and Yun Young Choi and George Dasoulas and Matouš Elphick and Giordan Escalona and Dominik Filipiak and Halley Fritze and Thomas Gebhart and Manel Gil-Sorribes and Salvish Goomanee and Victor Guallar and Liliya Imasheva and Andrei Irimia and Hongwei Jin and Graham Johnson and Nikos Kanakaris and Boshko Koloski and Veljko Kovač and Manuel Lecha and Minho Lee and Pierrick Leroy and Theodore Long and German Magai and Alvaro Martinez and Marissa Masden and Sebastian Mežnar and Bertran Miquel-Oliver and Alexis Molina and Alexander Nikitin and Marco Nurisso and Matt Piekenbrock and Yu Qin and Patryk Rygiel and Alessandro Salatiello and Max Schattauer and Pavel Snopov and Julian Suk and Valentina Sánchez and Mauricio Tec and Francesco Vaccarino and Jonas Verhellen and Frederic Wantiez and Alexander Weers and Patrik Zajec and Blaž Škrlj and Nina Miolane},
  booktitle     = {Proceedings of the Geometry-grounded Representation Learning and Generative Modeling Workshop (GRaM)},
  month         = {10},
  pages         = {420--428},
  pdf           = {https://proceedings.mlr.press/v251/bernardez24a/bernardez24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain},
  url           = {https://proceedings.mlr.press/v251/bernardez24a.html},
  volume        = {251},
  year          = {2024}
}

@inproceedings{Verma2025PointLevel,
  author        = {Yogeshwar Verma and Vineet Garg},
  booktitle     = {Proceedings of the International Conference on Machine Learning (ICML)},
  title         = {Point-Level Topological Representation Learning on Point Clouds},
  year          = {2025}
}

@misc{Wei2025ManifoldTDL,
  abstract      = {Recently, topological deep learning (TDL), which integrates algebraic topology with deep neural networks, has achieved tremendous success in processing point-cloud data, emerging as a promising paradigm in data science. However, TDL has not been developed for data on differentiable manifolds, including images, due to the challenges posed by differential topology. This work addresses this challenge by introducing manifold topological deep learning (MTDL) for the first time. In this novel framework, original images are represented as smooth manifolds with vector fields that are decomposed into three orthogonal components based on Hodge theory. The Hodge Laplacian theory is employed to decompose the vector field into three orthogonal components: curl-free, divergence-free, and harmonic parts. MTDL integrates the discrete Hodge theory from differential topology, the Transformer encoder architecture, and convolutional operations, providing a novel framework for extending TDL to differentiable manifold data. The performance of MTDL was evaluated using the MedMNIST v2 benchmark database, which comprises 717,287 biomedical images from eleven 2D and six 3D datasets. MTDL significantly outperforms other competing methods, establishing MTDL as an efficient framework for TDL on differentiable manifold data.},
  archiveprefix = {arXiv},
  author        = {Xiang Liu and Zhe Su and Yongyi Shi and Yiying Tong and Ge Wang and Guo-Wei Wei},
  doi           = {10.21203/rs.3.rs-6149503/v1},
  eprint        = {2503.00175},
  note          = {Preprint submitted to Research Square},
  pdf           = {https://arxiv.org/pdf/2503.00175.pdf},
  primaryclass  = {cs.LG},
  title         = {Manifold Topological Deep Learning for Biomedical Data},
  url           = {https://arxiv.org/abs/2503.00175},
  year          = {2025}
}

@misc{Battaglia2018RelationalInductive,
  abstract      = {Artificial intelligence has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arXiv},
  author        = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Álvaro Sánchez-González and Vinícius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çağlar Gülçehre and H. Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matthew Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  eprint        = {1806.01261},
  month         = {6},
  openalex      = {W2805516822},
  pdf           = {https://arxiv.org/pdf/1806.01261.pdf},
  primaryclass  = {cs.LG},
  title         = {Relational inductive biases, deep learning, and graph networks},
  year          = {2018}
}

@article{Bronstein2017GeometricDL,
  abstract      = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  author        = {Michael M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and Pierre Vandergheynst},
  doi           = {10.1109/MSP.2017.2693418},
  issn          = {1053-5888},
  journal       = {IEEE Signal Processing Magazine},
  month         = {7},
  number        = {4},
  openalex      = {W2558748708},
  pages         = {18--42},
  pdf           = {https://arxiv.org/abs/1611.08097},
  publisher     = {IEEE},
  title         = {Geometric Deep Learning: Going beyond Euclidean Data},
  volume        = {34},
  year          = {2017}
}

@inproceedings{Dwivedi2021GraphTransformer,
  abstract      = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
  author        = {Vijay Prakash Dwivedi and Xavier Bresson},
  booktitle     = {AAAI Workshop on Deep Learning on Graphs: Methods and Applications (DLG-AAAI'21)},
  doi           = {10.48550/arXiv.2012.09699},
  eprint        = {2012.09699},
  eprinttype    = {arXiv},
  month         = {2},
  openalex      = {W3113177135},
  pages         = {--},
  pdf           = {https://arxiv.org/pdf/2012.09699.pdf},
  primaryclass  = {cs.LG},
  title         = {A Generalization of Transformer Networks to Graphs},
  year          = {2021}
}

@inproceedings{Rampasek2022GraphGPS,
  abstract      = {We propose a recipe on how to build a general, powerful, scalable (GPS) Graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being local, global or relative. Further, GTs are constrained to small graphs with a few hundred nodes, and we show how to scale them to thousands of nodes. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We ablate recent Graph Transformer variants and show that the most important design choice is to use a hybrid approach that combines the local inductive bias of MPNNs with the global connectivity of attention. Empirically, on 16 benchmarks, GPS achieves significantly better results compared to previous Graph Transformer variants (2nd best on average), and is competitive with the state-of-the-art in each respective domain: graph-level tasks, node classification, and link prediction.},
  author        = {Ladislav Rampášek and Michael Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4281706128},
  pages         = {14501--14515},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/5d4834a159f1547b267a05a4e2b7cf5e-Paper-Conference.pdf},
  series        = {NeurIPS '22},
  title         = {Recipe for a General, Powerful, Scalable Graph Transformer},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d4834a159f1547b267a05a4e2b7cf5e-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Frankle2019LotteryTicket,
  abstract      = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the ``lottery ticket hypothesis:'' dense, randomly-initialized, feed-forward networks contain subnetworks (``winning tickets'') that---when trained in isolation---reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective.},
  author        = {Jonathan Frankle and Michael Carbin},
  booktitle     = {Proceedings of the 7th International Conference on Learning Representations},
  month         = {5},
  openalex      = {W2963813662},
  pdf           = {https://openreview.net/pdf?id=rJl-b3RcF7},
  publisher     = {OpenReview.net},
  series        = {ICLR 2019},
  title         = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  url           = {https://openreview.net/forum?id=rJl-b3RcF7},
  year          = {2019}
}
