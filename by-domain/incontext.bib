@inproceedings{Brown2020,
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  author        = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D. Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4292779060},
  pages         = {1877--1901},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Language Models are Few-Shot Learners},
  volume        = {33},
  year          = {2020}
}

@inproceedings{Zhao2021,
  abstract      = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, particularly those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this bias, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as ``N/A''. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.},
  author        = {Zihao Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
  booktitle     = {Proceedings of the 38th International Conference on Machine Learning},
  openalex      = {W3132736064},
  pages         = {12697--12706},
  pdf           = {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  volume        = {139},
  year          = {2021}
}

@inproceedings{Min2022MetaICL,
  abstract      = {We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer. We find that the gains are particularly significant for target tasks that have domain shifts from the meta-training tasks, and that using a diverse set of the meta-training tasks is key to improvements. We also show that MetaICL approaches (and sometimes beats) the performance of models fully finetuned on the target task, and outperforms much bigger models with nearly 8x parameters. Finally, we show that MetaICL is complementary to human-written instructions, and the best performance can be achieved by combining both approaches.},
  author        = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle     = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  doi           = {10.18653/v1/2022.naacl-main.201},
  openalex      = {W3210277894},
  pages         = {2791--2809},
  pdf           = {https://aclanthology.org/2022.naacl-main.201.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {MetaICL: Learning to Learn In Context},
  year          = {2022}
}

@inproceedings{Wei2022Finetuned,
  abstract      = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  author        = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  openalex      = {W4286987939},
  pdf           = {https://openreview.net/pdf?id=gEZrGCozdqR},
  publisher     = {OpenReview.net},
  title         = {Finetuned Language Models Are Zero-Shot Learners},
  url           = {https://openreview.net/forum?id=gEZrGCozdqR},
  year          = {2022}
}

@inproceedings{Lu2022,
  abstract      = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are 'fantastic' and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.},
  address       = {Dublin, Ireland},
  author        = {Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
  booktitle     = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2022.acl-long.556},
  openalex      = {W3156470785},
  pages         = {8086--8098},
  pdf           = {https://aclanthology.org/2022.acl-long.556.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  url           = {https://aclanthology.org/2022.acl-long.556},
  year          = {2022}
}

@inproceedings{Xie2022,
  abstract      = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs, showing that in-context learning occurs implicitly via Bayesian inference of the latent concept. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling, sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  author        = {Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  month         = {4},
  openalex      = {W4286769130},
  pdf           = {https://openreview.net/pdf?id=RdJVFCHjUMI},
  publisher     = {OpenReview.net},
  title         = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  url           = {https://openreview.net/forum?id=RdJVFCHjUMI},
  year          = {2022}
}

@inproceedings{Chan2022,
  abstract      = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having a large number of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. Our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and indicate how future work might encourage both in-context and in-weights learning in domains beyond language.},
  author        = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4281481998},
  pages         = {1--14},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Wies2023,
  abstract      = {In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. By now, it is well-known that when provided with a few examples of a new task, large language models can ``learn'' to perform this task without any parameter updates. In-context learning is disruptive for many practical applications of large language models, however, this emergent learning paradigm is not well understood from a theoretical perspective. In this work, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to navigate the relationships between three key quantities: the number of pretraining tasks, the number of in-context examples, and the number of tasks on which the model will be evaluated. We present several theoretical results. First, we prove that in-context learning is more about identifying the task than about learning it, a result which is in line with a series of recent empirical findings. Second, we prove that ``task identification'' is only possible when the pretraining task distribution is sufficiently diverse. Third, we provide sample complexity bounds for the in-context learning setup. Crucially, these results imply that in certain settings, in-context learning is provably more sample efficient than explicitly training a new model.},
  author        = {Noam Wies and Yoav Levine and Amnon Shashua},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4327525504},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/73950f0eb4ac0925dc71ba2406893320-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {The Learnability of In-Context Learning},
  volume        = {36},
  year          = {2023}
}

@inproceedings{vonOswald2023,
  abstract      = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by this construction, we show empirically that when training self-attention-only Transformers on simple regression tasks, the models learned by GD exhibit great similarity to our construction. Thus, we show how trained Transformers become mesa-optimizers, i.e., learn to perform gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.},
  author        = {Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4311726128},
  pages         = {35151--35174},
  pdf           = {https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Transformers Learn In-Context by Gradient Descent},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Dai2023,
  abstract      = {Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. This paper explains language models as meta-optimizers and understands in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent based on gradient descent on attention. ICL works by producing meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective.},
  address       = {Toronto, Canada},
  author        = {Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei},
  booktitle     = {Findings of the Association for Computational Linguistics: ACL 2023},
  doi           = {10.18653/v1/2023.findings-acl.247},
  month         = {7},
  openalex      = {W4312107248},
  pages         = {4005--4019},
  pdf           = {https://aclanthology.org/2023.findings-acl.247.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
  url           = {https://aclanthology.org/2023.findings-acl.247},
  year          = {2023}
}

@inproceedings{Garg2022,
  abstract      = {In-context learning is the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To investigate this, we consider the problem of training a model to in-context learn a function class (e.g., linear functions): given data derived from some functions in the class, can we train a model (e.g., a Transformer) to in-context learn most functions from that class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions---that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the Transformer and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes: sparse linear functions where the model outperforms least squares and nearly matches the performance of Lasso, and two-layer neural networks where the model performs comparably to neural networks trained on in-context examples using gradient descent.},
  author        = {Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4320516905},
  pages         = {30378--30391},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  volume        = {35},
  year          = {2022}
}

@misc{Olsson2022,
  abstract      = {"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] $ o$ [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. The evidence quality varies: for small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.},
  archiveprefix = {arXiv},
  author        = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  doi           = {10.48550/arXiv.2209.11895},
  eprint        = {2209.11895},
  month         = {9},
  openalex      = {W4297412003},
  pdf           = {https://arxiv.org/pdf/2209.11895.pdf},
  primaryclass  = {cs.LG},
  title         = {In-context Learning and Induction Heads},
  url           = {https://arxiv.org/abs/2209.11895},
  year          = {2022}
}

@inproceedings{Wei2022Chain,
  abstract      = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc V. Le and Denny Zhou},
  booktitle     = {Advances in Neural Information Processing Systems 35 (NeurIPS 2022)},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  openalex      = {W4221143046},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  year          = {2022}
}

@inproceedings{Zhou2023,
  abstract      = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  address       = {Kigali, Rwanda},
  author        = {Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V. Le and Ed H. Chi},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4281483047},
  pdf           = {https://openreview.net/pdf?id=WZH7099tgfM},
  publisher     = {OpenReview.net},
  title         = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  url           = {https://openreview.net/forum?id=WZH7099tgfM},
  year          = {2023}
}

@inproceedings{Yao2023,
  abstract      = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%.},
  author        = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  openalex      = {W4377130677},
  pages         = {1--14},
  pdf           = {https://neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Zhang2023AutoCoT,
  abstract      = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations.},
  author        = {Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  note          = {Code available at r̆lhttps://github.com/amazon-research/auto-cot},
  openalex      = {W4304194220},
  pdf           = {https://openreview.net/pdf?id=5NTt8GFjUHkr},
  publisher     = {OpenReview.net},
  title         = {Automatic Chain of Thought Prompting in Large Language Models},
  url           = {https://openreview.net/forum?id=5NTt8GFjUHkr},
  year          = {2023}
}

@inproceedings{Min2022Rethinking,
  abstract      = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choice tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  address       = {Abu Dhabi, United Arab Emirates},
  author        = {Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  booktitle     = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  doi           = {10.18653/v1/2022.emnlp-main.759},
  openalex      = {W4385567149},
  pages         = {11048--11064},
  pdf           = {https://aclanthology.org/2022.emnlp-main.759.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  url           = {https://aclanthology.org/2022.emnlp-main.759},
  year          = {2022}
}

@inproceedings{Liu2022,
  abstract      = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its powerful and versatile in-context few-shot learning ability. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's few-shot capabilities. Inspired by the recent success of leveraging a retrieval module to augment large-scale neural network models, we propose to retrieve examples that are semantically-similar to a test sample to formulate its corresponding prompt. Intuitively, the in-context examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's extensive knowledge. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (41.9% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset). We hope our investigation could help understand the behaviors of GPT-3 and large-scale pre-trained LMs in general and enhance their few-shot capabilities.},
  address       = {Dublin, Ireland},
  author        = {Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
  booktitle     = {Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  doi           = {10.18653/v1/2022.deelio-1.10},
  month         = {5},
  openalex      = {W3122241445},
  pages         = {100--114},
  pdf           = {https://aclanthology.org/2022.deelio-1.10.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {What Makes Good In-Context Examples for GPT-3?},
  url           = {https://aclanthology.org/2022.deelio-1.10},
  year          = {2022}
}

@inproceedings{Wang2024LearningToRetrieve,
  abstract      = {Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes. The code and data are available at https://github.com/microsoft/LMOps/tree/main/llm_retriever.},
  address       = {St. Julian's, Malta},
  author        = {Liang Wang and Nan Yang and Furu Wei},
  booktitle     = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2024.eacl-long.105},
  month         = {3},
  openalex      = {W4411630208},
  pages         = {1752--1767},
  pdf           = {https://aclanthology.org/2024.eacl-long.105.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Learning to Retrieve In-Context Examples for Large Language Models},
  url           = {https://aclanthology.org/2024.eacl-long.105},
  year          = {2024}
}

@inproceedings{Zhang2022Active,
  abstract      = {With a handful of demonstration examples, large-scale language models show strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a $5.8%$ improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.},
  address       = {Abu Dhabi, United Arab Emirates},
  author        = {Yiming Zhang and Shi Feng and Chenhao Tan},
  booktitle     = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  doi           = {10.18653/v1/2022.emnlp-main.622},
  openalex      = {W4385573504},
  pages         = {9134--9148},
  pdf           = {https://aclanthology.org/2022.emnlp-main.622.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Active Example Selection for In-Context Learning},
  url           = {https://aclanthology.org/2022.emnlp-main.622},
  year          = {2022}
}

@misc{Hao2022,
  abstract      = {Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. To address this limitation, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. Our approach allows scaling the number of exemplars with linear complexity instead of quadratic complexity with respect to length, enabling in-context learning with thousands of examples while maintaining computational efficiency.},
  archiveprefix = {arXiv},
  author        = {Yaru Hao and Yutao Sun and Li Dong and Zhixiong Han and Yuxian Gu and Furu Wei},
  eprint        = {2212.06713},
  month         = {12},
  openalex      = {W4311557185},
  pdf           = {https://arxiv.org/pdf/2212.06713},
  primaryclass  = {cs.CL},
  title         = {Structured Prompting: Scaling In-Context Learning to 1,000 Examples},
  url           = {https://arxiv.org/abs/2212.06713},
  year          = {2022}
}

@misc{Anonymous2024Bayesian,
  abstract      = {In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a family of novel Bayesian scaling laws for ICL. In experiments with GPT-2 models of different sizes, our scaling laws match existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then experiment on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause the suppressed behavior to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.},
  author        = {Aryaman Arora and Dan Jurafsky and Christopher Potts and Noah D. Goodman},
  howpublished  = {arXiv preprint arXiv:2410.16531},
  month         = {10},
  note          = {Submitted to ICLR 2025},
  openalex      = {W4404349999},
  pdf           = {https://arxiv.org/pdf/2410.16531.pdf},
  title         = {Bayesian Scaling Laws for In-Context Learning},
  url           = {https://arxiv.org/abs/2410.16531},
  year          = {2024}
}

@misc{gu2025semisupervised,
  abstract      = {The high cost of obtaining high-quality annotated data for in-context learning (ICL) has motivated the development of methods that use self-generated annotations in place of ground-truth labels. While these approaches have shown promising results in few-shot settings, they generally do not scale to many-shot scenarios. In this work, we study ICL with self-generated examples using a framework analogous to traditional semi-supervised learning, consisting of annotation generation, demonstration selection, and in-context inference. Within this framework, we propose a simple baseline that outperforms ground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we observe a scaling law with this baseline, where optimal performance is achieved with more than 1,000 demonstrations. To fully exploit the many-shot capabilities of semi-supervised ICL, we introduce IterPSD, an iterative annotation approach that integrates iterative refinement and curriculum pseudo-labeling techniques from semi-supervised learning, yielding up to 6.8% additional gains on classification tasks.},
  archiveprefix = {arXiv},
  author        = {Zhengyao Gu and Henry Peng Zou and Yankai Chen and Aiwei Liu and Weizhi Zhang and Philip S. Yu},
  eprint        = {2503.03062},
  howpublished  = {arXiv preprint arXiv:2503.03062},
  month         = {3},
  pdf           = {https://arxiv.org/pdf/2503.03062.pdf},
  primaryclass  = {cs.CL},
  title         = {Semi-Supervised In-Context Learning: A Baseline Study},
  url           = {https://arxiv.org/abs/2503.03062},
  year          = {2025}
}

@misc{shi2025explaining,
  abstract      = {Long Context Language Models have drawn great attention in the past few years. There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impact Language Modeling. In this work, we: (1) propose a clean and effective theoretical framework on explaining the impact of context length to Language Modeling, from an Intrinsic Space perspective; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain case. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models.},
  archiveprefix = {arXiv},
  author        = {Jingzhe Shi and Qinwei Ma and Hongyi Liu and Hang Zhao and Jenq-Neng Hwang and Serge Belongie and Lei Li},
  doi           = {10.48550/arXiv.2502.01481},
  eprint        = {2502.01481},
  howpublished  = {arXiv preprint arXiv:2502.01481},
  month         = {2},
  openalex      = {W4407174629},
  pdf           = {https://arxiv.org/pdf/2502.01481.pdf},
  primaryclass  = {cs.LG},
  title         = {Explaining Context Length Scaling and Bounds for Language Models},
  url           = {https://arxiv.org/abs/2502.01481},
  year          = {2025}
}

@inproceedings{Shi2024,
  abstract      = {Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show In-Context Pretraining offers a simple and scalable approach to significantly enhance LMs' performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).},
  author        = {Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and Margaret Li and Gergely Szilvasy and Rich James and Xi Victoria Lin and Noah A. Smith and Luke Zettlemoyer and Wen-tau Yih and Mike Lewis},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  openalex      = {W4387725603},
  pdf           = {https://arxiv.org/abs/2310.10638},
  title         = {In-Context Pretraining: Language Modeling Beyond Document Boundaries},
  url           = {https://openreview.net/forum?id=M1tblHQXzu},
  year          = {2024}
}

@inproceedings{Schoch2025,
  abstract      = {Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning). This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.},
  address       = {Albuquerque, New Mexico, USA},
  author        = {Stephanie Schoch and Yangfeng Ji},
  booktitle     = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  doi           = {10.48550/arxiv.2502.06653},
  month         = {April},
  openalex      = {W4407359228},
  pages         = {TBD},
  pdf           = {https://arxiv.org/pdf/2502.06653.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {In-Context Learning (and Unlearning) of Length Biases},
  url           = {https://arxiv.org/abs/2502.06653},
  year          = {2025}
}

@misc{Anonymous2025Supervised,
  abstract      = {In-Context Learning (ICL) allows Large Language Models (LLM) to adapt to new tasks with just a few examples, but their predictions often suffer from systematic biases, leading to unstable performances in classification. While calibration techniques are proposed to mitigate these biases, we show that, in the logit space, many of these methods are equivalent to merely shifting the LLM's decision boundary without having the ability to alter its orientation. This proves inadequate when biases cause the LLM to be severely misdirected. We propose Supervised Calibration (SC), a novel framework that learns an optimal, per-class affine transformation of LLM's predictive probabilities. SC can alter and potentially reverse the LLM's decision boundary, and integrates context-invariance and directional trust-region regularizers. SC delivers state-of-the-art performance across multiple language models and datasets.},
  author        = {Korel Gundem and Juncheng Dong and Dennis Zhang and Vahid Tarokh and Zhengling Qi},
  howpublished  = {arXiv preprint arXiv:2505.23783},
  note          = {Primary Subject: Machine Learning (stat.ML); Secondary Subjects: Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG)},
  title         = {Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning},
  url           = {https://arxiv.org/abs/2505.23783},
  year          = {2024}
}

@misc{Anonymous2024LongPIBench,
  abstract      = {Positional bias in large language models (LLMs) hinders their ability to effectively process long inputs. A prominent example is the 'lost in the middle' phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LongPiBench, a benchmark designed to assess positional bias involving multiple pieces of relevant information. Thorough experiments are conducted with five commercial and six open-source models. These experiments reveal that while most current models are robust against the 'lost in the middle' issue, there exist significant biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases to advance LLM's capabilities.},
  author        = {Runchu Tian and Yanghao Li and Yuepeng Fu and Siyang Deng and Qinyu Luo and Cheng Qian and Shuo Wang and Xin Cong and Zhong Zhang and Yesai Wu and Yankai Lin and Huadong Wang and Xiaojiang Liu},
  howpublished  = {arXiv preprint arXiv:2410.14641},
  month         = {10},
  note          = {Submitted to ACL 2025 Findings},
  openalex      = {W4403996099},
  pdf           = {https://arxiv.org/pdf/2410.14641.pdf},
  title         = {Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs},
  url           = {https://arxiv.org/abs/2410.14641},
  year          = {2024}
}

@inproceedings{Mueller2024,
  abstract      = {In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax - a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.},
  address       = {Mexico City, Mexico},
  author        = {Aaron Mueller and Albert Webson and Jackson Petty and Tal Linzen},
  booktitle     = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2024.naacl-long.267},
  month         = {6},
  openalex      = {W4401042367},
  pages         = {4761--4779},
  pdf           = {https://aclanthology.org/2024.naacl-long.267.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax},
  url           = {https://aclanthology.org/2024.naacl-long.267},
  year          = {2024}
}

@inproceedings{JimenezGutierrez2022,
  abstract      = {Large pre-trained language models (PLMs) have demonstrated remarkable few-shot learning capabilities on a wide range of tasks. However, we observe that the few-shot performance of GPT-3 can be highly unstable across different subsets of the training set, often not outperforming much smaller models. This instability makes it difficult to determine whether GPT-3 has truly learned a task or merely relies on spurious statistical patterns. We present the first systematic and comprehensive study comparing GPT-3's few-shot in-context learning with fine-tuning smaller PLMs on biomedical information extraction tasks, including named entity recognition and relation extraction. Our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM, and this performance gap remains substantial even when more training data becomes available. Our findings provide important guidance for biomedical researchers and practitioners toward more promising and cost-effective approaches to biomedical NLP.},
  address       = {Abu Dhabi, United Arab Emirates},
  author        = {Bernal Jiménez Gutiérrez and Nikolas McNeal and Clayton Washington and You Chen and Lang Li and Huan Sun and Yu Su},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  doi           = {10.18653/v1/2022.findings-emnlp.329},
  openalex      = {W4385573954},
  pages         = {4497--4512},
  pdf           = {https://aclanthology.org/2022.findings-emnlp.329.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again},
  url           = {https://aclanthology.org/2022.findings-emnlp.329},
  year          = {2022}
}

@inproceedings{Anonymous2025Sufficient,
  abstract      = {In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on the established benchmark MT-Bench, especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding high-quality, potentially carefully selected via greedy search, demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime, where ICL can be a viable alternative to IFT. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT. We provide our code at https://github.com/tml-epfl/icl-alignment.},
  author        = {Hao Zhao and Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
  booktitle     = {Proceedings of the Thirteenth International Conference on Learning Representations},
  month         = {1},
  openalex      = {W4399252501},
  pdf           = {https://openreview.net/pdf?id=STEEDDv3zI},
  publisher     = {ICLR},
  title         = {Is In-Context Learning Sufficient for Instruction Following in LLMs?},
  url           = {https://openreview.net/forum?id=STEEDDv3zI},
  year          = {2025}
}

@misc{Anonymous2025ContextTuning,
  abstract      = {We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the model's inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. We propose two variants: CT-Prompt, which applies Prompt Tuning to a soft prompt initialized from few-shot examples, and CT-KV, which applies Prefix Tuning to optimize the key-value (KV) cache derived from those same examples. CT-KV achieves linear training time complexity while also outperforming CT-Prompt and achieving competitive performance to TTT, thanks to the efficiency and per-layer conditioning of the KV cache. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency.},
  author        = {Jack Lu and Ryan Teehan and Zhenbang Yang and Mengye Ren},
  eprint        = {2507.04221},
  eprinttype    = {arXiv},
  howpublished  = {arXiv preprint},
  month         = {7},
  pdf           = {https://arxiv.org/pdf/2507.04221.pdf},
  title         = {Context Tuning for In-Context Optimization},
  url           = {https://arxiv.org/abs/2507.04221},
  year          = {2025}
}

@inproceedings{Choi2025TeachingLLMs,
  abstract      = {Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, 'can prompting help us teach LLMs how to learn'. In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model's interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.},
  author        = {Younwoo Choi and Muhammad Adil Asif and Ziwen Han and John Willes and Rahul G. Krishnan},
  booktitle     = {Proceedings of the Thirteenth International Conference on Learning Representations},
  pdf           = {https://arxiv.org/pdf/2503.09032.pdf},
  series        = {ICLR 2025},
  title         = {Teaching LLMs How to Learn with Contextual Fine-Tuning},
  url           = {https://arxiv.org/abs/2503.09032},
  year          = {2025}
}

@misc{Anonymous2025PARA,
  abstract      = {In the realm of parameter-efficient fine-tuning (PEFT) methods, while options like LoRA are available, there is a persistent demand in the industry for a PEFT approach that excels in both efficiency and performance within the context of single-backbone multi-tenant applications. This paper introduces a new and straightforward PEFT technique, termed Prompt Aware Representation Adjustment (PARA). The core of our proposal is to integrate a lightweight vector generator within each Transformer layer. This generator produces vectors that are responsive to input prompts, thereby adjusting the hidden representations accordingly. Our extensive experimentation across diverse tasks has yielded promising results. Firstly, the PARA method has been shown to surpass current PEFT benchmarks in terms of performance, despite having a similar number of adjustable parameters. Secondly, it has proven to be more efficient than LoRA in the single-backbone multi-tenant scenario, highlighting its significant potential for industrial adoption.},
  archiveprefix = {arXiv},
  author        = {Zequan Liu and Yi Zhao and Ming Tan and Wei Zhu and Aaron Xuxiang Tian},
  eprint        = {2502.01033},
  howpublished  = {arXiv preprint},
  month         = {2},
  openalex      = {W4407128741},
  pdf           = {https://arxiv.org/pdf/2502.01033.pdf},
  primaryclass  = {cs.CL},
  title         = {PARA: Parameter-Efficient Fine-tuning with Prompt Aware Representation Adjustment},
  year          = {2025}
}

@misc{Moeini2025,
  abstract      = {Reinforcement learning (RL) agents typically optimize their policies by performing expensive backward passes to update their network parameters. However, some agents can solve new tasks without updating any parameters by simply conditioning on additional context such as their action-observation histories. This paper surveys work on such behavior, known as in-context reinforcement learning.},
  archiveprefix = {arXiv},
  author        = {Amir Moeini and Jiuqi Wang and Jacob Beck and Ethan Blaser and Shimon Whiteson and Rohan Chandra and Shangtong Zhang},
  eprint        = {2502.07978},
  howpublished  = {arXiv preprint arXiv:2502.07978},
  month         = {2},
  pdf           = {https://arxiv.org/pdf/2502.07978.pdf},
  primaryclass  = {cs.LG},
  title         = {A Survey of In-Context Reinforcement Learning},
  url           = {https://arxiv.org/abs/2502.07978},
  year          = {2025}
}

@inproceedings{Wang2023PromptDiffusion,
  abstract      = {We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly on six different tasks using these prompts. The resulting Prompt Diffusion model becomes the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation for the trained tasks and effectively generalizes to new, unseen vision tasks using their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion.},
  address       = {Red Hook, NY, USA},
  author        = {Zhendong Wang and Yifan Jiang and Yadong Lu and Yelong Shen and Pengcheng He and Weizhu Chen and Zhangyang Wang and Mingyuan Zhou},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4367859966},
  pages         = {72769},
  pdf           = {https://papers.nips.cc/paper_files/paper/2023/file/1b3750390ca8b931fb9ca988647940cb-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {In-Context Learning Unlocked for Diffusion Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b3750390ca8b931fb9ca988647940cb-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Qu2025,
  abstract      = {The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While the very recent TabPFNv2 foundation model (2025) excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 56 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Inference code and pre-trained models are available at https://github.com/soda-inria/tabicl.},
  author        = {Jingang Qu and David Holzmüller and Gaël Varoquaux and Marine Le Morvan},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  openalex      = {W4407385137},
  pdf           = {https://arxiv.org/pdf/2502.05564.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {TabICL: A Tabular Foundation Model for In-Context Learning on Large Data},
  url           = {https://arxiv.org/abs/2502.05564},
  year          = {2025}
}

@inproceedings{Nulli2024,
  abstract      = {Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this work, we investigate the reasons for such a lack of capability by performing an extensive bench-marking of compositional understanding in VLMs. We compare contrastive models with generative ones and analyze their differences in architecture, pre-training data, and training tasks and losses. Furthermore, we leverage In-Context Learning (ICL) as a way to improve the ability of VLMs to perform more complex reasoning and understanding given an image.},
  address       = {Vienna, Austria},
  author        = {Matteo Nulli and Anesa Ibrahimi and Avik Pal and Hoshe Lee and Ivona Najdenkoska},
  booktitle     = {Proceedings of the 1st ICML Workshop on In-Context Learning},
  doi           = {10.48550/arXiv.2407.15487},
  month         = {7},
  title         = {In-Context Learning Improves Compositional Understanding of Vision-Language Models},
  url           = {https://arxiv.org/abs/2407.15487},
  year          = {2024}
}

@inproceedings{Anonymous2024Impact,
  author        = {Anonymous},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  note          = {Submission or unpublished work},
  title         = {The Impact of Demonstrations on Multimodal In-Context Learning},
  year          = {2024}
}

@inproceedings{Do2024,
  abstract      = {We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings.},
  address       = {Bangkok, Thailand},
  author        = {Xuan Long Do and Yiran Zhao and Hannah Brown and Yuxi Xie and James Xu Zhao and Nancy F. Chen and Kenji Kawaguchi and Michael Shieh and Junxian He},
  booktitle     = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2024.acl-long.395},
  openalex      = {W4402671583},
  pages         = {7308--7327},
  pdf           = {https://aclanthology.org/2024.acl-long.395.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Prompt Optimization via Adversarial In-Context Learning},
  url           = {https://aclanthology.org/2024.acl-long.395},
  year          = {2024}
}

@misc{Wang2024RDES,
  abstract      = {Diversity in demonstration selection is critical for enhancing model generalization by enabling broader coverage of structures and concepts. Constructing appropriate demonstration sets remains a key research challenge. This paper introduces the Relevance-Diversity Enhanced Selection (RDES), an innovative approach that leverages reinforcement learning (RL) frameworks to optimize the selection of diverse reference demonstrations for tasks amenable to in-context learning (ICL), particularly text classification and reasoning, in few-shot prompting scenarios. RDES employs frameworks like Q-learning and a PPO-based variant to dynamically identify demonstrations that maximize both diversity (quantified by label distribution) and relevance to the task objective. This strategy ensures a balanced representation of reference data, leading to improved accuracy and generalization. Through extensive experiments on multiple benchmark datasets, including diverse reasoning tasks, and involving 14 closed-source and open-source LLMs, we demonstrate that RDES significantly enhances performance compared to ten established baselines. Our evaluation includes analysis of performance across varying numbers of demonstrations on selected datasets. Furthermore, we investigate incorporating Chain-of-Thought (CoT) reasoning, which further boosts predictive performance. The results highlight the potential of RL for adaptive demonstration selection and addressing challenges in ICL.},
  archiveprefix = {arXiv},
  author        = {Xubin Wang and Jianfei Wu and Yichen Yuan and Deyu Cai and Mingzhe Li and Weijia Jia},
  eprint        = {2412.03966},
  howpublished  = {arXiv preprint arXiv:2412.03966},
  month         = {12},
  openalex      = {W4405094800},
  primaryclass  = {cs.AI},
  title         = {Demonstration Selection for In-Context Learning via Reinforcement Learning},
  url           = {https://arxiv.org/abs/2412.03966},
  year          = {2024}
}

@inproceedings{Anonymous2024Where,
  abstract      = {Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task. Through a series of layer-wise context-masking experiments on GPTNeo2.7B, Bloom3B, Starcoder2-7B, Llama3.1-8B, Llama3.1-8B-Instruct, on Machine Translation and Code generation, we demonstrate evidence of a 'task recognition' point where the task is encoded into the input representations and attention to context is no longer necessary. Taking advantage of this redundancy results in 45% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation. Our findings also have implications for resource and parameter efficient fine-tuning; we observe a correspondence between strong fine-tuning performance of individual LoRA layers and the task recognition layers.},
  author        = {Suzanna Sia and David Mueller and Kevin Duh},
  booktitle     = {Advances in Neural Information Processing Systems},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/3979818cdc7bc8dbeec87170c11ee340-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Where does In-context Learning Happen in Large Language Models?},
  url           = {https://openreview.net/forum?id=LLuSjg59an},
  volume        = {37},
  year          = {2024}
}

@inproceedings{Park2025,
  abstract      = {Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy 'graph tracing' task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.},
  author        = {Core Francisco Park and Andrew Lee and Ekdeep Singh Lubana and Yongyi Yang and Maya Okawa and Kento Nishi and Martin Wattenberg and Hidenori Tanaka},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  pdf           = {https://openreview.net/pdf?id=pXlmOmlHJZ},
  title         = {ICLR: In-Context Learning of Representations},
  url           = {https://openreview.net/forum?id=pXlmOmlHJZ},
  year          = {2025}
}

@inproceedings{Anonymous2023Meta,
  abstract      = {Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.},
  author        = {Julian Coda-Forno and Marcel Binz and Zeynep Akata and Matthew Botvinick and Jane X. Wang and Eric Schulz},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4377864916},
  pages         = {70239},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/cda04d7ea67ea1376bf8c6962d8541e0-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Meta-in-context learning in large language models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Anonymous2025Induction,
  author        = {Anonymous},
  booktitle     = {Proceedings of the International Conference on Machine Learning},
  title         = {From Induction Heads to Function Vectors: A Deeper Look at the Mechanisms of In-Context Learning},
  year          = {2025}
}

@inproceedings{Huang2025,
  abstract      = {Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on an in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. With our technique, we also show that looped transformers significantly improve final performance compared to transformers without looping in the in-context learning of linear regression. Empirically, we demonstrate that CoT prompting yields substantial performance improvements.},
  author        = {Jianhao Huang and Zixuan Wang and Jason D. Lee},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  note          = {Spotlight presentation},
  pdf           = {https://openreview.net/pdf?id=r3DF5sOo5B},
  title         = {Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought},
  url           = {https://openreview.net/forum?id=r3DF5sOo5B},
  year          = {2025}
}

@inproceedings{Anonymous2024Mixture,
  abstract      = {In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle various tasks by providing input-output examples as additional inputs, referred to as demonstrations. Nevertheless, the performance of ICL could be easily impacted by the quality of selected demonstrations. Existing efforts generally learn a retriever model to score each demonstration for selecting suitable demonstrations, however, the effect is suboptimal due to the large search space and the noise from unhelpful demonstrations. In this study, we introduce MoD, which partitions the demonstration pool into groups, each governed by an expert to reduce search space. We further design an expert-wise training strategy to alleviate the impact of unhelpful demonstrations when optimizing the retriever model. During inference, experts collaboratively retrieve demonstrations for the input query to enhance the ICL performance. We validate MoD via experiments across a range of NLP datasets and tasks, demonstrating its state-of-the-art performance and shedding new light on the future design of retrieval methods for ICL.},
  author        = {Song Wang and Zihan Chen and Chengshuai Shi and Cong Shen and Jundong Li},
  booktitle     = {Advances in Neural Information Processing Systems},
  pages         = {--},
  pdf           = {https://papers.nips.cc/paper_files/paper/2024/file/a0da098e0031f58269efdcba40eedf47-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Mixture of Demonstrations for In-Context Learning},
  url           = {https://openreview.net/forum?id=uqxSLoCw3K},
  volume        = {37},
  year          = {2024}
}

@misc{Anonymous2025Factored,
  abstract      = {In this paper, we propose a novel factored agent architecture designed to overcome the limitations of traditional single-agent systems in agentic AI. Our approach decomposes the agent into two specialized components: (1) a large language model (LLM) that serves as a high level planner and in-context learner, which may use dynamically available information in user prompts, (2) a smaller language model which acts as a memorizer of tool format and output. This decoupling addresses prevalent issues in monolithic designs, including malformed, missing, and hallucinated API fields, as well as suboptimal planning in dynamic environments. Empirical evaluations demonstrate that our factored architecture significantly improves planning accuracy and error resilience, while elucidating the inherent trade-off between in-context learning and static memorization. These findings suggest that a factored approach is a promising pathway for developing more robust and adaptable agentic AI systems.},
  author        = {Nicholas Roth and Christopher Hidey and Lucas Spangher and William F. Arnold and Chang Ye and Nick Masiewicki and Jinoo Baek and Peter Grabowski and Eugene Ie},
  eprint        = {2503.22931},
  howpublished  = {arXiv preprint arXiv:2503.22931},
  month         = {3},
  pdf           = {https://arxiv.org/pdf/2503.22931.pdf},
  primaryclass  = {cs.AI},
  title         = {Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use},
  url           = {https://arxiv.org/abs/2503.22931},
  year          = {2025}
}

@article{Maity2025Leveraging,
  abstract      = {Question generation in education is a time-consuming and cognitively demanding task, as it requires creating questions that are both contextually relevant and pedagogically sound. Current automated question generation methods often generate questions that are out of context. In this work, we explore advanced techniques for automated question generation in educational contexts, focusing on In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and a novel Hybrid Model that merges both methods. We implement GPT-4 for ICL using few-shot examples and BART with a retrieval module for RAG. The Hybrid Model combines RAG and ICL to address these issues and improve question quality. Evaluation is conducted using automated metrics, followed by human evaluation metrics. Our results show that both the ICL approach and the Hybrid Model consistently outperform other methods, including baseline models, by generating more contextually accurate and relevant questions.},
  archiveprefix = {arXiv},
  author        = {Subhankar Maity and Aniket Deroy and Sudeshna Sarkar},
  eprint        = {2501.17397},
  journal       = {arXiv preprint arXiv:2501.17397},
  month         = {1},
  openalex      = {W4406975388},
  pdf           = {https://arxiv.org/pdf/2501.17397.pdf},
  primaryclass  = {cs.CL},
  title         = {Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic Question Generation in Educational Domains},
  url           = {https://arxiv.org/abs/2501.17397},
  year          = {2025}
}

@misc{Anonymous2025DeepSeek,
  abstract      = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  author        = {DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and others},
  howpublished  = {arXiv preprint arXiv:2501.12948},
  month         = {1},
  openalex      = {W4406779522},
  pdf           = {https://arxiv.org/pdf/2501.12948},
  title         = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  url           = {https://arxiv.org/abs/2501.12948},
  year          = {2025}
}
