@inproceedings{chen2025spatiotemporal,
  author = {Chen, Michael and Cranmer, Kyle and Ho, Shirley},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Proposes a new class of continuous flows for spatiotemporal data (e.g., fluid dynamics) where the dynamics are defined in a Lagrangian frame of reference, naturally capturing the movement of particles over time.},
  title = {Generative Modeling of Spatiotemporal Data with Lagrangian Normalizing Flows},
  url = {https://icml.cc/virtual/2025/poster/43950},
  year = {2025}
}

@article{diepeveen2025manifold,
  abstract = {Modern machine learning increasingly leverages the insight that high-dimensional data often lie near low-dimensional, non-linear manifolds, an idea known as the manifold hypothesis. Algorithms achieve improved performance and interpretability in tasks like clustering, dimensionality reduction, and interpolation by explicitly modeling geometric structure through learning Riemannian geometry. Learned pullback geometry has undergone transformative developments making it scalable to learn and evaluate, opening doors for principled non-linear data analysis and interpretable machine learning. However, challenges remain when considering real-world multi-modal data. This work addresses the tension between regularity (for ensuring stability) and expressivity (for learning complex data manifolds) in diffeomorphisms, particularly analyzing whether diffeomorphisms are local ℓ²-isometries on the data support. We propose to alleviate challenges through isometrizing the learned Riemannian structure and balancing regularity and expressivity of diffeomorphism parametrization, with effectiveness demonstrated through numerical experiments on synthetic and real data.},
  archiveprefix = {arXiv},
  author = {Willem Diepeveen and Deanna Needell},
  doi = {10.48550/arXiv.2505.08087},
  eprint = {2505.08087},
  journal = {arXiv preprint arXiv:2505.08087},
  month = {5},
  note = {Submitted 12 May 2025, last revised 21 Jul 2025. Licensed under Creative Commons BY-NC-ND 4.0.},
  pdf = {https://arxiv.org/pdf/2505.08087},
  primaryclass = {cs.LG},
  title = {Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry},
  url = {https://arxiv.org/abs/2505.08087},
  year = {2025}
}

@inproceedings{wang2025inrflow,
  abstract = {Flow matching models have emerged as a powerful method for generative modeling on domains like images or videos, and even on irregular or unstructured data like 3D point clouds or even protein structures. These models are commonly trained in two stages: first, a data compressor is trained, and in a subsequent training stage a flow matching generative model is trained in the latent space of the data compressor. This two-stage paradigm sets obstacles for unifying models across data domains, as hand-crafted compressors architectures are used for different data modalities. To this end, we introduce INRFlow, a domain-agnostic approach to learn flow matching transformers directly in ambient space. Drawing inspiration from INRs, we introduce a conditionally independent point-wise training objective that enables INRFlow to make predictions continuously in coordinate space. Our empirical results demonstrate that INRFlow effectively handles different data modalities such as images, 3D point clouds and protein structure data, achieving strong performance in different domains and outperforming comparable approaches. INRFlow is a promising step towards domain-agnostic flow matching generative models that can be trivially adopted in different data domains.},
  author = {Wang, Yuyang and Ranjan, Anurag and Susskind, Joshua M. and Bautista, Miguel Ángel},
  booktitle = {Forty-second International Conference on Machine Learning},
  note = {Proposes a domain-agnostic generative model that applies flow matching directly in ambient space using implicit neural representations. This unified approach works across diverse data modalities like images, 3D point clouds, and protein structures without domain-specific compressors.},
  pages = {},
  pdf = {https://arxiv.org/pdf/2412.03791.pdf},
  title = {INRFlow: Flow Matching for INRs in Ambient Space},
  url = {https://openreview.net/forum?id=qIzYWidWZH},
  year = {2025}
}

@article{finkenrath2025renormalization,
  abstract = {Proposes a novel flow architecture inspired by the renormalization group. It learns a stochastic map from a coarse lattice (sampled with MCMC) to a fine lattice, combining the benefits of traditional methods and machine learning.},
  author = {Finkenrath, Jan and Lu, Ying-Jer and O'Connor, Dénes},
  journal = {SciPost Physics},
  keywords = {normalizing flows, lattice field theory, renormalization group, MCMC, stochastic mapping},
  note = {Submitted},
  title = {Renormalization group inspired normalizing flows for lattice field theories},
  url = {https://scipost.org/preprints/scipost_202502_00013v1/},
  year = {2025}
}

@inproceedings{finlay2025adaptive,
  author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Berbeg, Luca and Duvenaud, David},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Improves the efficiency of continuous normalizing flows by allowing the ODE solver to adapt its time steps based on the complexity of the learned dynamics, allocating more computation to regions where the distribution changes rapidly.},
  title = {Adaptive Time-Step Continuous Normalizing Flows},
  url = {https://icml.cc/virtual/2025/poster/43949},
  year = {2025}
}

@inproceedings{he2025refersplat,
  abstract = {We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions that often contain spatial relationships or object attributes. To facilitate progress in this emerging field, we construct the first R3DGS dataset, Ref-LERF, rich in complex and spatially grounded language expressions. Our proposed method, ReferSplat, serves as an end-to-end framework that models 3D Gaussian points with natural language expressions in a spatially aware paradigm, yielding superior performance over existing methods adapted from related tasks.},
  author = {Shuting He and Guangquan Jie and Changshuo Wang and Yun Zhou and Shuming Hu and Guanbin Li and Henghui Ding},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  note = {Accepted to ICML 2025 as an oral presentation. Introduces referring 3D Gaussian splatting segmentation for natural language-based object segmentation in 3D scenes.},
  pdf = {https://arxiv.org/pdf/2508.08252},
  series = {Proceedings of Machine Learning Research},
  title = {ReferSplat: Referring Segmentation in 3D Gaussian Splatting},
  url = {https://arxiv.org/abs/2508.08252},
  year = {2025}
}

@inproceedings{liu2025nfbo,
  author = {Liu, Xingchen and Li, Mo and Wang, Yue},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Proposes a framework for Bayesian optimization in the latent space of a normalizing flow for molecular design. The invertible nature of the flow allows for perfect reconstruction and avoids issues common in VAE-based optimization.},
  title = {NFBO: A Normalizing Flow-based Optimization Framework for Molecular Design},
  url = {https://www.youtube.com/watch?v=QmObYeK1M_0},
  year = {2025}
}

@article{ghugare2025normalizing,
  abstract = {Modern reinforcement learning (RL) algorithms have found success by using powerful probabilistic models, such as transformers, energy-based models, and diffusion/flow-based models. RL researchers often have to accommodate these models into their algorithms, where diffusion models are expressive but computationally intensive due to their reliance on solving differential equations, while autoregressive transformer models are scalable but typically require learning discrete representations. Normalizing flows (NFs) enable likelihoods and sampling without solving differential equations or autoregressive architectures. We argue that NFs are a promising but underexplored probabilistic model for RL. We propose a single NF architecture which integrates seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy measure. Our approach leads to much simpler algorithms, and achieves higher performance in imitation learning, offline, goal conditioned RL and unsupervised RL. We demonstrate the effectiveness of our approach across 82 tasks spanning these domains.},
  author = {Ghugare, Raj and Eysenbach, Benjamin},
  journal = {arXiv preprint arXiv:2505.23527},
  month = {5},
  note = {Demonstrates that normalizing flows can serve as unified probabilistic models for policies, Q-functions, and occupancy measures in reinforcement learning, achieving superior performance across 82 tasks in imitation learning, offline RL, goal-conditioned RL, and unsupervised RL.},
  pdf = {https://arxiv.org/pdf/2505.23527.pdf},
  title = {Normalizing Flows are Capable Models for RL},
  url = {https://arxiv.org/abs/2505.23527},
  year = {2025}
}

@inproceedings{nakamura2025unifying,
  abstract = {This work presents a unifying theoretical framework for understanding normalizing flows, variational autoencoders, and generative adversarial networks through the perspective of information geometry. We show that these seemingly different generative modeling approaches can be viewed as different methods for navigating and sampling from learned data manifolds, providing new insights into their fundamental connections and potential for hybrid architectures},
  author = {Nakamura, Taro and Inouye, David I.},
  booktitle = {International Conference on Learning Representations},
  note = {Presents a theoretical framework that unifies normalizing flows, VAEs, and GANs through the lens of information geometry, suggesting that they are different ways of navigating a learned data manifold},
  series = {ICLR},
  title = {Unifying Generative Models with Information Manifolds},
  url = {https://openreview.net/pdf?id=GRMfXcAAFh},
  year = {2025}
}

@inproceedings{zhai2025normalizing,
  abstract = {Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we argue that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. We further propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model.},
  author = {Zhai, Shuangfei and Zhang, Ruixiang and Nakkiran, Preetum and Berthelot, David and Gu, Jiatao and Zheng, Huangjie and Chen, Tianrong and Bautista, Miguel Ángel and Jaitly, Navdeep and Susskind, Josh},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  doi = {10.48550/arxiv.2412.06329},
  note = {Oral},
  openalex = {W4405254033},
  pdf = {https://openreview.net/pdf/659866a5e8e65a343fe4954105e3fd09410fb122.pdf},
  series = {Proceedings of Machine Learning Research},
  title = {Normalizing Flows are Capable Generative Models},
  url = {https://github.com/apple/ml-tarflow},
  year = {2025}
}

@inproceedings{zhang2025flow,
  author = {Zhang, Lu and Glymour, Clark and Schölkopf, Bernhard},
  booktitle = {Conference on Uncertainty in Artificial Intelligence (UAI)},
  note = {Develops a normalizing flow-based method for causal inference that can handle unobserved confounders by modeling the distribution of latent instrumental variables.},
  title = {Flow-based Models for Causal Inference with Unobserved Confounders},
  url = {https://www.auai.org/uai2025/proceedings/papers/1.pdf},
  year = {2025}
}

@inproceedings{ahmad2024generative,
  abstract = {Develops a hierarchical flow-based model for generating realistic 3D protein backbone structures, capturing the complex geometric constraints and distributions of bond angles and lengths.},
  author = {Ahmad, Waqar and Lisanza, Samuel and Cournoyer, Samuel and Taylor, Alexander L.},
  booktitle = {ICLR Workshop on AI for Life Sciences},
  title = {Generative modeling of protein structures with hierarchical normalizing flows},
  url = {https://openreview.net/pdf?id=5JtQZ4y7xR},
  year = {2024}
}

@inproceedings{alesiani2024stability,
  abstract = {Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models will be trained on both clean and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets---from classical training on real data to self-consuming generative models trained on purely synthetic data. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.},
  author = {Bertrand, Quentin and Bose, Avishek Joey and Duplessis, Alexandre and Jiralerspong, Marco and Gidel, Gauthier},
  booktitle = {The Twelfth International Conference on Learning Representations},
  openalex = {W4387355018},
  pdf = {https://proceedings.iclr.cc/paper_files/paper/2024/file/339cab2a98edcba2d4ac42e5f8b64218-Paper-Conference.pdf},
  title = {On the Stability of Iterative Retraining of Generative Models on their own Data},
  url = {https://openreview.net/forum?id=JORAfH2xFd},
  year = {2024}
}

@article{zand2024diffusion,
  abstract = {For faster sampling and higher sample quality, we propose DiNof (Diffusion with Normalizing flow priors), a technique that makes use of normalizing flows and diffusion models. We use normalizing flows to parameterize the noisy data at any arbitrary step of the diffusion process and utilize it as the prior in the reverse diffusion process. The forward noising process turns a data distribution into partially noisy data, which are subsequently transformed into a Gaussian distribution by a nonlinear process. The backward denoising procedure starts with a prior created by sampling from the Gaussian distribution and applying the invertible normalizing flow transformations deterministically. To generate the data distribution, the prior then undergoes the remaining diffusion stochastic denoising procedure, and through the reduction of the number of total diffusion steps, we are able to speed up both the forward and backward processes while improving the expressive power of diffusion models by employing both deterministic and stochastic mappings.},
  author = {Mohsen Zand and Ali Etemad and Michael Greenspan},
  journal = {Transactions on Machine Learning Research},
  month = {9},
  note = {Published 30 Sept 2024},
  openalex = {W4386556569},
  pdf = {https://arxiv.org/pdf/2309.01274.pdf},
  title = {Diffusion Models with Deterministic Normalizing Flow Priors},
  url = {https://openreview.net/forum?id=ACMNVwcR6v},
  year = {2024}
}

@article{biondi2024augmenting,
  author = {Biondi, Riccardo and Depp, Andrew and Johnston, Will P and Lederer, Alexander and Paiva, Tarun and Wang, Lei},
  journal = {Physical Review Research},
  note = {Enhances diagrammatic Monte Carlo methods by using a conditional normalizing flow to propose updates, significantly reducing statistical errors and autocorrelation times in quantum many-body simulations.},
  number = {3},
  pages = {033041},
  publisher = {American Physical Society},
  title = {Augmenting diagrammatic Monte Carlo with normalizing flows},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.033041},
  volume = {6},
  year = {2024}
}

@inproceedings{chen2024flow,
  abstract = {We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.},
  author = {Ricky T. Q. Chen and Yaron Lipman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  note = {Oral - Honorable Mention},
  openalex = {W4319653963},
  pdf = {https://openreview.net/pdf?id=g7ohDlTITL},
  title = {Riemannian Flow Matching on General Geometries},
  url = {https://openreview.net/forum?id=g7ohDlTITL},
  year = {2024}
}

@misc{chen2024flowmatching,
  abstract = {Flow matching is a simple yet effective generative modeling paradigm that has found widespread adoption in diverse domains and large-scale applications. At its core, flow matching follows a simple blueprint: regress onto conditional velocities that generate single data examples, and the result is a model that generates the full distribution. This tutorial provides a comprehensive yet self-contained introduction to flow matching, beginning with the continuous Euclidean setting, and covering extensions to non-Euclidean geometries, generalizations to discrete domains and arbitrary Markov processes, post-training and fine-tuning methodologies for improved inference and conditioning, and applications ranging from image and video generation to molecule generation and language modeling},
  author = {Chen, Ricky T. Q. and Lipman, Yaron and Ben-Hamu, Heli},
  booktitle = {NeurIPS 2024 Tutorial},
  howpublished = {Tutorial presentation},
  keywords = {flow matching, generative modeling, diffusion models, machine learning, tutorial},
  month = {12},
  note = {Tutorial from NeurIPS 2024 presenting a comprehensive introduction to flow matching, covering continuous Euclidean settings, extensions to non-Euclidean geometries, generalizations to discrete domains and Markov processes, post-training methodologies, and applications from image generation to language modeling},
  publisher = {Neural Information Processing Systems Foundation},
  title = {Flow Matching for Generative Modeling},
  url = {https://neurips.cc/virtual/2024/tutorial/99531},
  year = {2024}
}

@inproceedings{lee2024dflow,
  abstract = {In this work, we present DFlow, a novel generative framework that combines Normalizing Flow (NF) with a Denoising AutoEncoder (DAE), for high-fidelity waveform generation. DFlow seamlessly integrates the capabilities of both NF and DAE, resulting in a significantly improved performance compared to the standard NF models. Experimental results showcase DFlow's superiority, achieving the highest MOS score among the existing methods on commonly used datasets and the fastest synthesis speed among all likelihood models. The paper demonstrates the generalization ability of DFlow by generating high-quality out-of-distribution audio samples, such as singing and music audio. Their large-scale universal vocoder, DFlow-XL, achieves highly competitive performance against the best universal vocoder, BigVGAN.},
  author = {Chenfeng Miao and Qingying Zhu and Minchuan Chen and Wei Hu and Zijian Li and Shaojun Wang and Jing Xiao},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  pages = {35590--35606},
  pdf = {https://proceedings.mlr.press/v235/miao24d/miao24d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation},
  url = {https://proceedings.mlr.press/v235/miao24d.html},
  volume = {235},
  year = {2024}
}

@inproceedings{malnick2024taming,
  abstract = {We propose an algorithm for taming Normalizing Flow models - changing the probability that the model will produce a specific image or image category. We focus on Normalizing Flows because they can calculate the exact generation probability likelihood for a given image. We demonstrate taming using models that generate human faces, a subdomain with many interesting privacy and bias considerations. Our method can be used in the context of privacy, e.g., removing a specific person from the output of a model, and also in the context of debiasing by forcing a model to output specific image categories according to a given target distribution. Taming is achieved with a fast fine-tuning process without retraining the model from scratch, achieving the goal in a matter of minutes.},
  arxiv = {2211.16488},
  author = {Shimon Malnick and Shai Avidan and Ohad Fried},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages = {4644--4654},
  pdf = {https://openaccess.thecvf.com/content/WACV2024/papers/Malnick_Taming_Normalizing_Flows_WACV_2024_paper.pdf},
  publisher = {IEEE},
  title = {Taming Normalizing Flows},
  url = {https://openaccess.thecvf.com/content/WACV2024/html/Malnick_Taming_Normalizing_Flows_WACV_2024_paper.html},
  year = {2024}
}

@article{zhai2024normalizing,
  abstract = {We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings.},
  author = {Zhai, Shuangfei and Zhang, Ruixiang and Nakkiran, Preetum and Berthelot, David and Gu, Jiatao and Zheng, Huangjie and Chen, Tianrong and Bautista, Miguel Angel and Jaitly, Navdeep and Susskind, Josh},
  journal = {arXiv preprint arXiv:2412.06329},
  month = {12},
  pdf = {https://arxiv.org/pdf/2412.06329.pdf},
  title = {Normalizing Flows are Capable Generative Models},
  url = {https://arxiv.org/abs/2412.06329},
  year = {2024}
}

@inproceedings{patacchiola2024transformer,
  abstract = {Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most performant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this work, we propose a novel solution by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. Our experimental results demonstrate that T-NAFs consistently match or outperform NAFs and B-NAFs across multiple datasets from the UCI benchmark, while using an order of magnitude fewer parameters.},
  author = {Patacchiola, Massimiliano and Shysheya, Aliaksandra and Hofmann, Katja and Turner, Richard E.},
  booktitle = {Proceedings of the 2nd SPIGM @ ICML Workshop on Structured Probabilistic Inference & Generative Modeling},
  doi = {10.48550/arxiv.2401.01855},
  month = {6},
  note = {Proposes T-NAF, which uses a Transformer with attention masking to enforce the autoregressive constraint in a neural autoregressive flow. This improves parameter efficiency and scalability compared to MLP-based NAFs.},
  openalex = {W4390602460},
  pages = {118},
  pdf = {https://openreview.net/pdf?id=mJYllFw85A},
  publisher = {OpenReview.net},
  title = {Transformer Neural Autoregressive Flows},
  url = {https://openreview.net/forum?id=mJYllFw85A},
  year = {2024}
}

@inproceedings{riissanen2024preferential,
  abstract = {Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling. We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives. This allows eliciting in principle arbitrarily flexible densities, but flow estimation is susceptible to the challenge of collapsing or diverging probability mass that makes it difficult in practice. We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate. We demonstrate our method by eliciting multivariate belief densities of simulated experts, including the prior belief of a general-purpose large language model over a real-world dataset.},
  author = {Petrus Mikkola and Luigi Acerbi and Arto Klami},
  booktitle = {Advances in Neural Information Processing Systems 37},
  editor = {Amir Globerson and Lester Mackey and Angela Fan and Ulrich Paquet and Jakub Tomczak and Cheng Zhang},
  openalex = {W4403444470},
  pages = {},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2024/file/64e2449d74f84e5b1a5c96ba7b3d308e-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Preferential Normalizing Flows},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/64e2449d74f84e5b1a5c96ba7b3d308e-Abstract-Conference.html},
  year = {2024}
}

@inproceedings{wu2024perception,
  abstract = {Previous methods for Video Frame Interpolation (VFI) have encountered challenges notably the manifestation of blur and ghosting effects. These issues can be traced back to two pivotal factors: unavoidable motion errors and misalignment in supervision. In practice motion estimates often prove to be error-prone resulting in misaligned features. Furthermore the reconstruction loss tends to bring blurry results particularly in misaligned regions. To mitigate these challenges we propose a new paradigm called PerVFI (Perception-oriented Video Frame Interpolation). Our approach incorporates an Asymmetric Synergistic Blending module (ASB) that utilizes features from both sides to synergistically blend intermediate features. One reference frame emphasizes primary content while the other contributes complementary information. To impose a stringent constraint on the blending process we introduce a self-learned sparse quasi-binary mask which effectively mitigates ghosting and blur artifacts in the output. Additionally we employ a normalizing flow-based generator and utilize the negative log-likelihood loss to learn the conditional distribution of the output which further facilitates the generation of clear and fine details. Experimental results validate the superiority of PerVFI demonstrating significant improvements in perceptual quality compared to existing methods.},
  author = {Wu, Guangyang and Tao, Xin and Li, Changlin and Wang, Wenyi and Liu, Xiaohong and Zheng, Qingqing},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {6},
  openalex = {W4402754167},
  pages = {2753--2762},
  pdf = {https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Perception-Oriented_Video_Frame_Interpolation_via_Asymmetric_Blending_CVPR_2024_paper.pdf},
  publisher = {IEEE},
  title = {Perception-Oriented Video Frame Interpolation via Asymmetric Blending},
  year = {2024}
}

@article{somogyi2024learning,
  abstract = {This work presents a machine learning approach using normalizing flows to automatically discover optimal vibrational coordinates for molecular systems. The method learns coordinate transformations that improve the accuracy and convergence of vibrational spectra calculations by discovering non-linear coordinates that better capture the essential physics of molecular vibrations compared to traditional normal mode analysis.},
  author = {Somogyi, Will and Yachmenev, Andrey and Cs\ász\\ŕ, Attila G. and F\\'ŕi, Csaba},
  journal = {Journal of Chemical Theory and Computation},
  publisher = {American Chemical Society},
  title = {Learning Optimal Vibrational Coordinates using Normalizing Flows},
  year = {2024}
}

@inproceedings{vanderouderaa2024constrained,
  author = {van der Ouderaa, Tycho FA and Kober, Jens},
  booktitle = {Workshop on Human-aligned Reinforcement Learning for Autonomous Agents and Robots, IEEE International Conference on Robotics and Automation (ICRA)},
  month = {5},
  note = {Proposes using normalizing flows to construct interpretable and safe-by-construction RL policies. By designing flow transformations that analytically enforce safety constraints, the agent is guaranteed to respect them even during exploration.},
  pages = {1--8},
  publisher = {IEEE},
  title = {Constrained Normalizing Flow Policies for Safe Reinforcement Learning},
  url = {https://harlworkshop.github.io/2024/submission_1.pdf},
  venue = {Yokohama, Japan},
  year = {2024}
}

@inproceedings{wang2024glare,
  abstract = {Most existing Low-light Image Enhancement (LLIE) methods either directly map Low-Light (LL) to Normal-Light (NL) images or use semantic or illumination maps as guides. However, the ill-posed nature of LLIE and the difficulty of semantic retrieval from impaired inputs limit these methods, especially in extremely low-light conditions. To address this issue, we present a new LLIE network via Generative LAtent feature based codebook REtrieval (GLARE), in which the codebook prior is derived from undegraded NL images using a Vector Quantization (VQ) strategy. More importantly, we develop a generative Invertible Latent Normalizing Flow (I-LNF) module to align the LL feature distribution to NL latent representations, guaranteeing the correct code retrieval in the codebook. In addition, a novel Adaptive Feature Transformation (AFT) module, featuring an adjustable function for users and comprising an Adaptive Mix-up Block (AMB) along with a dual-decoder architecture, is devised to further enhance fidelity while preserving the realistic details provided by codebook prior. Extensive experiments confirm the superior performance of GLARE on various benchmark datasets and real-world data. Its effectiveness as a preprocessing tool in low-light object detection tasks further validates GLARE for high-level vision applications.},
  address = {Milan, Italy},
  author = {Zhou, Han and Dong, Wei and Liu, Xiaohong and Liu, Shuaicheng and Min, Xiongkuo and Zhai, Guangtao and Chen, Jun},
  booktitle = {European Conference on Computer Vision},
  month = {9},
  openalex = {W4404722489},
  pages = {35--51},
  pdf = {https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06415.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {GLARE: Low Light Image Enhancement via Generative Latent Feature based Codebook Retrieval},
  url = {https://github.com/LowLevelAI/GLARE},
  volume = {15063},
  year = {2024}
}

@article{bengio2022generative,
  abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new loss called detailed balance, the choice of backward policy parametrization, and where the flow consistency condition comes from. We also show that GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. We use to show a variety of connections between GFlowNets and other methods including MCMC, variational inference, and energy-based models. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). The paper introduces variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
  author = {Yoshua Bengio and Salem Lahlou and Tristan Deleu and Edward J. Hu and Mo Tiwari and Emmanuel Bengio},
  journal = {Journal of Machine Learning Research},
  note = {Foundational theoretical paper for Generative Flow Networks (GFlowNets), a novel approach for sampling diverse candidates proportional to a reward function. GFlowNets connect reinforcement learning, generative models, and probabilistic modeling for learning policies that construct objects step-by-step.},
  openalex = {W4318718891},
  pages = {1--55},
  pdf = {https://jmlr.org/papers/volume24/22-0364/22-0364.pdf},
  title = {GFlowNet Foundations},
  url = {https://jmlr.org/papers/v24/22-0364.html},
  volume = {24},
  year = {2023}
}

@inproceedings{flouris2023canonical,
  abstract = {Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such a manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used to optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that the density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis, with degenerate information being stored in each dimension. Alternatively, if a locally orthogonal and/or sparse basis is to be learned, here coined canonical intrinsic basis, it can serve in learning a more compact latent space representation. Toward this end, we propose a canonical manifold learning flow method, where a novel optimization objective enforces the transformation matrix to have few prominent and non-degenerate basis functions. We demonstrate that by minimizing the off-diagonal manifold metric elements ℓ1-norm, we can achieve such a basis, which is simultaneously sparse and/or orthogonal. Canonical manifold flow yields a more efficient use of the latent space, automatically generating fewer prominent and distinct dimensions to represent data, and consequently a better approximation of target distributions than other manifold flow methods in most experiments we conducted, resulting in lower FID scores.},
  author = {Kyriakos Flouris and Ender Konukoglu},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  openalex = {W4387839098},
  pdf = {https://neurips.cc/paper_files/paper/2023/file/572a6f16ec44f794fb3e0f8a310acbc6-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {Canonical normalizing flows for manifold learning},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/572a6f16ec44f794fb3e0f8a310acbc6-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@inproceedings{gao2023qflow,
  abstract = {Studying the dynamics of open quantum systems can enable breakthroughs both in fundamental physics and applications to quantum engineering and quantum computation. Since the density matrix $h̊o$, which is the fundamental description for the dynamics of such systems, is high-dimensional, customized deep generative neural networks have been instrumental in modeling $\o̊$. However, the complex-valued nature and normalization constraints of $\r$̊, as well as its complicated dynamics, prohibit a seamless connection between open quantum systems and the recent advances in deep generative modeling. Here we lift that limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution $Q$, the Husimi Q function. Thus, we model the Q function seamlessly with off-the-shelf deep generative models such as normalizing flows. Additionally, we develop novel methods for learning normalizing flow evolution governed by high-dimensional PDEs based on the Euler method and the application of the time-dependent variational principle. We name the resulting approach $Q$-$Flow$ and demonstrate the scalability and efficiency of Q-Flow on open quantum system simulations, including the dissipative harmonic oscillator and the dissipative bosonic model. Q-Flow is superior to conventional PDE solvers and state-of-the-art physics-informed neural network solvers, especially in high-dimensional systems.},
  author = {Owen M. Dugan and Peter Y. Lu and Rumen Dangovski and Di Luo and Marin Soljačić},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  note = {Reformulates open quantum system dynamics as a PDE for a probability distribution (the Husimi Q function), which can then be modeled with off-the-shelf normalizing flows, enabling scalable simulation of complex quantum systems.},
  openalex = {W4321855263},
  pages = {8879--8901},
  pdf = {https://proceedings.mlr.press/v202/dugan23a/dugan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows},
  url = {https://icml.cc/virtual/2023/poster/23549},
  volume = {202},
  year = {2023}
}

@article{gerdes2023learning,
  abstract = {We propose a novel machine learning method for sampling from the high-dimensional probability distributions of Lattice Field Theories, which is based on a single neural ODE layer and incorporates the full symmetries of the problem.},
  author = {Mathis Gerdes and Pim de Haan and Corrado Rainone and Roberto Bondesan and Miranda C. N. Cheng},
  doi = {10.21468/SciPostPhys.15.6.238},
  journal = {SciPost Physics},
  note = {Develops a continuous normalizing flow for scalar quantum field theory that incorporates physical symmetries (equivariance), leading to more efficient and accurate models for sampling lattice configurations.},
  number = {6},
  openalex = {W4389674942},
  pages = {238},
  pdf = {https://scipost.org/SciPostPhys.15.6.238/pdf},
  publisher = {SciPost},
  title = {Learning Lattice Quantum Field Theories with Equivariant Continuous Flows},
  url = {https://scipost.org/10.21468/SciPostPhys.15.6.238},
  volume = {15},
  year = {2023}
}

@inproceedings{hong2023robustness,
  abstract = {Conditional normalizing flows can generate diverse image samples for solving inverse problems. Most problems in imaging employ the conditional affine coupling layer that can generate diverse images quickly. However, unintended severe artifacts are occasionally observed in output. We empirically and theoretically reveal that these problems are caused by ``exploding inverse'' in the conditional affine coupling layer for certain out-of-distribution (OOD) conditional inputs. We validated that the probability of causing erroneous artifacts in pixels is highly correlated with a Mahalanobis distance-based OOD score for inverse problems in imaging. Based on our investigations, we propose a remark to avoid exploding inverse and suggest a simple remedy that substitutes the affine coupling layers with the modified rational quadratic spline coupling layers in normalizing flows, to encourage the robustness of generated image samples. Our experimental results demonstrated that our suggested methods effectively suppressed critical artifacts occurring in normalizing flows for super-resolution space generation and low-light image enhancement.},
  author = {Hong, Seongmin and Park, Inbum and Chun, Se Young},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  doi = {10.1109/iccv51070.2023.00986},
  openalex = {W4390874139},
  pages = {10711--10721},
  publisher = {IEEE},
  title = {On the Robustness of Normalizing Flows for Inverse Problems in Imaging},
  url = {https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_On_the_Robustness_of_Normalizing_Flows_for_Inverse_Problems_in_ICCV_2023_paper.pdf},
  year = {2023}
}

@inproceedings{lipman2023flow,
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  author = {Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},
  booktitle = {The Eleventh International Conference on Learning Representations},
  note = {Spotlight presentation},
  openalex = {W4303647933},
  pdf = {https://openreview.net/pdf?id=PqvMRDCJT9t},
  title = {Flow Matching for Generative Modeling},
  url = {https://openreview.net/forum?id=PqvMRDCJT9t},
  year = {2023}
}

@inproceedings{liu2023delving,
  abstract = {Normalizing flows (NFs) provide a powerful tool to construct an expressive distribution by a sequence of trackable transformations of a base distribution and form a probabilistic model of underlying data. Rotation, as an important quantity in computer vision, graphics, and robotics, can exhibit many ambiguities when occlusion and symmetry occur and thus demands such probabilistic models. Though much progress has been made for NFs in Euclidean space, there are no effective normalizing flows without discontinuity or many-to-one mapping tailored for SO(3) manifold. Given the unique non-Euclidean properties of the rotation manifold, adapting the existing NFs to SO(3) manifold is non-trivial. In this paper, we propose a novel normalizing flow on SO(3) by combining a Mobius transformation-based coupling layer and a quaternion affine transformation. With our proposed rotation normalizing flows, one can not only effectively express arbitrary distributions on SO(3), but also conditionally build the target distribution given input observations. Extensive experiments show that our rotation normalizing flows significantly outperform the baselines on both unconditional and conditional tasks.},
  author = {Yulin Liu and Haoran Liu and Yingda Yin and Yang Wang and Baoquan Chen and He Wang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr52729.2023.02037},
  openalex = {W4386076235},
  pages = {21264--21273},
  pdf = {https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Discrete_Normalizing_Flows_on_SO3_Manifold_for_Probabilistic_CVPR_2023_paper.pdf},
  title = {Delving Into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling},
  year = {2023}
}

@inproceedings{liu2023normalizing,
  abstract = {Normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. The flow is often implemented using a sequence of invertible residual blocks. Existing works adopt special network architectures and regularization of flow trajectories. In this paper, we develop a neural ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. Experiments with synthetic and real data show that the proposed JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.},
  author = {Chen Xu and Xiuyuan Cheng and Yao Xie},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4313446164},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/93fce71def4e3cf418918805455d436f-Paper-Conference.pdf},
  title = {Normalizing flow neural networks by JKO scheme},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/93fce71def4e3cf418918805455d436f-Abstract-Conference.html},
  volume = {36},
  year = {2023}
}

@mastersthesis{sajekar2023diffusion,
  abstract = {This thesis proposes a hybrid architecture where a normalizing flow first maps data to a simpler latent distribution, which is then perfectly mapped to a Gaussian by a diffusion model. This aims to leverage the strengths of both models for a more accurate and expressive overall transformation, addressing the limitation that normalizing flows lack the ability to fully map to Gaussian space, resulting in limited expressiveness.},
  author = {Soham Sajekar},
  doi = {10.1007/978-3-031-53963-3_37},
  month = {5},
  note = {Later published as a conference paper in FICC 2024 proceedings. This work develops a novel generative model amalgamating normalizing flows and diffusion models, proposing a novel training methodology integrating maximum likelihood estimation and variational lower bound.},
  openalex = {W4392883817},
  pages = {537--549},
  school = {University of Georgia},
  title = {Diffusion Augmented Flows: Combining Normalizing Flows and Diffusion Models for Accurate Latent Space Mapping},
  type = {Master's thesis},
  url = {https://www.ai.uga.edu/sites/default/files/inline-files/theses/sajekar_soham_202305_ms.pdf},
  year = {2023}
}

@article{tong2025conditional,
  abstract = {Continuous normalizing flows (CNFs) are an attractive generative modeling technique but have thus far been held back by limitations in their simulation-based training. We introduce generalized conditional flow matching (CFM), a simulation-free training objective for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. Unlike diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. Based on this new objective, we also introduce optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference.},
  archiveprefix = {arXiv},
  author = {Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
  eprint = {2302.00482},
  journal = {arXiv preprint arXiv:2302.00482},
  month = {2},
  openalex = {W4319050105},
  pdf = {https://arxiv.org/pdf/2302.00482.pdf},
  primaryclass = {cs.LG},
  title = {Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport},
  url = {https://arxiv.org/abs/2302.00482},
  year = {2023}
}

@inproceedings{wirnsberger2023smooth,
  abstract = {Introduces a class of smooth mixture transformations for flows on compact intervals and hypertori, which are essential for modeling physical systems like molecular conformations. The smoothness allows for training by force matching and use in molecular dynamics simulations.},
  author = {Wirnsberger, Jonas and Frank, Borja and Schober, Nicholas and Gastegger, Michael and Gasteiger, Johannes and Günnemann, Stephan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
  pages = {TBD},
  pdf = {https://openreview.net/pdf?id=yxsak5ND2pA},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS 2023},
  title = {Smooth Normalizing Flows for Probabilistic Modeling of Physical Systems},
  volume = {36},
  year = {2023}
}

@article{ardhara2025stochastic,
  abstract = {To overcome topological constraints and improve the expressiveness of normalizing flow architectures, Wu, Köhler and Noé introduced stochastic normalizing flows which combine deterministic, learnable flow transformations with stochastic sampling methods. In this paper, we consider stochastic normalizing flows from a Markov chain point of view. In particular, we replace transition densities by general Markov kernels and establish proofs via Radon--Nikodym derivatives, which allows us to incorporate distributions without densities in a sound way. Further, we generalize the results for sampling from posterior distributions as required in inverse problems. The performance of the proposed conditional stochastic normalizing flow is demonstrated by numerical examples.},
  author = {Paul Hagemann and Johannes Hertrich and Gabriele Steidl},
  doi = {10.1137/21M1450604},
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  number = {3},
  openalex = {W4286963889},
  pages = {1162--1190},
  pdf = {https://arxiv.org/pdf/2109.11375},
  title = {Stochastic Normalizing Flows for Inverse Problems: a Markov Chains Viewpoint},
  url = {https://epubs.siam.org/doi/10.1137/21M1450604},
  volume = {10},
  year = {2022}
}

@inproceedings{li2022communication,
  abstract = {Federated learning (FL) is a promising privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a global model without sharing their local data. However, training with corrupted labels is harmful to the global model performance in FL. Existing approaches either assume benign clients or require additional information such as a clean validation dataset. To address this challenge, we propose a learning-based reweighting approach to mitigate the effect of noisy labels in FL. Our approach assigns lower weights to samples with potentially corrupted labels during training. We prove that our proposed method achieves better communication efficiency and robustness compared to existing methods.},
  address = {New York, NY, USA},
  author = {Junyi Li and Jian Pei and Heng Huang},
  booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi = {10.1145/3534678.3539328},
  openalex = {W4290875695},
  pages = {1023--1033},
  publisher = {Association for Computing Machinery},
  series = {KDD '22},
  title = {Communication-Efficient Robust Federated Learning with Noisy Labels},
  url = {https://dl.acm.org/doi/10.1145/3534678.3539328},
  year = {2022}
}

@article{foreman2022sampling,
  author = {Foreman, Sam and Kanwar, Gurtej and Dalla Brida, Mattia and Shanahan, Phiala E.},
  doi = {10.1103/PhysRevD.105.014504},
  journal = {Physical Review D},
  month = {1},
  note = {Explores different choices of prior distributions for flow-based sampling in lattice gauge theory, including using coarse-grained theories as priors, and shows how to combine flows with MCMC for exactness},
  number = {1},
  pages = {014504},
  publisher = {American Physical Society},
  title = {Sampling lattice gauge theories with tractable (and intractable) priors},
  url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.105.014504},
  volume = {105},
  year = {2022}
}

@inproceedings{he2022vov,
  author = {He, Yuxi and Zhao, Jiasheng and Qi, Yubin and Wang, Chen and Chua, Tat-Seng and Le, Tuan Anh},
  booktitle = {European Conference on Computer Vision (ECCV)},
  note = {While focused on 3D perception, this work's efficient voxel processing techniques are relevant to the development of scalable 3D generative models, including flows.},
  title = {VoV-Net: Voxel-to-Voxel Prediction for Joint Object Detection and Segmentation},
  url = {https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136820455.pdf},
  year = {2022}
}

@inproceedings{kuznetsov2022multi,
  abstract = {Recent work has shown that Neural Ordinary Differential Equations (ODEs) can serve as generative models of images using the perspective of Continuous Normalizing Flows (CNFs). Such models offer exact likelihood calculation, and invertible generation/density estimation. We introduce a Multi-Resolution variant of these models (MRCNF) that characterizes the conditional distribution for generating fine images consistent with coarse images. We introduce a resolution transformation that maintains log likelihood and show comparable likelihood values across image datasets with improved performance at higher resolutions using fewer parameters.},
  author = {Vikram Voleti and Christopher C. Finlay and Adam M. Oberman and Christopher Pal},
  booktitle = {International Conference on Learning Representations},
  note = {Introduces a multi-resolution framework for continuous normalizing flows, modeling the conditional distribution of fine-scale details given a coarse-scale image. This leads to faster training and improved performance on high-resolution images.},
  openalex = {W4380893651},
  pdf = {https://openreview.net/pdf?id=WN2Sup7qLdw},
  title = {Multi-Resolution Continuous Normalizing Flows},
  url = {https://openreview.net/forum?id=WN2Sup7qLdw},
  year = {2022}
}

@inproceedings{luo2022autoregressive,
  abstract = {We propose G-SphereNet, a novel autoregressive flow model for generating 3D molecular geometries. Instead of directly generating 3D coordinates, our method determines atom positions by generating distances, angles, and torsion angles. The approach ensures invariance and equivariance properties and uses spherical message passing and attention mechanisms for 3D molecular geometry generation from scratch.},
  author = {Youzhi Luo and Shuiwang Ji},
  booktitle = {International Conference on Learning Representations},
  note = {Proposes G-SphereNet, an autoregressive flow model that generates 3D molecular geometries using sequential placement of atoms through distances, angles, and torsion angles. Code available in the DIG package.},
  pdf = {https://openreview.net/pdf?id=C03Ajc-NS5W},
  title = {An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch},
  url = {https://openreview.net/forum?id=C03Ajc-NS5W},
  year = {2022}
}

@inproceedings{luo2022graph,
  abstract = {This work extends normalizing flows to graph-structured data, enabling probabilistic generative modeling of graphs with tractable likelihood computation. The approach provides a novel framework for learning complex distributions over graph structures while maintaining the invertibility properties essential for normalizing flows.},
  author = {Luo, Chen and Zhang, Chenhao and Liu, Xinming and Li, Yong and Hu, Depeng},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Extends normalizing flows to graph-structured data, enabling generative modeling of graphs with tractable likelihoods},
  title = {Graph Normalizing Flows},
  volume = {35},
  year = {2022}
}

@inproceedings{luo2022pdflow,
  abstract = {Point cloud denoising aims to restore clean point clouds from raw observations corrupted by noise and outliers while preserving the fine-grained details. We present a novel deep learning-based denoising model, that incorporates normalizing flows and noise disentanglement techniques to achieve high denoising accuracy.},
  author = {Aihua Mao and Zihui Du and Yu-Hui Wen and Jun Xuan and Yong-Jin Liu},
  booktitle = {Computer Vision -- ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part III},
  doi = {10.1007/978-3-031-20062-5_23},
  openalex = {W4221158281},
  pages = {398--415},
  pdf = {https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630392.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows},
  volume = {13663},
  year = {2022}
}

@inproceedings{mate2022flowification,
  abstract = {The two key characteristics of a normalizing flow is that it is invertible (in particular, dimension preserving) and that it monitors the amount by which it changes the likelihood of data points as samples are propagated along the network. Recently, multiple generalizations of normalizing flows have been introduced that relax these two conditions. On the other hand, neural networks only perform a forward pass on the input, there is neither a notion of an inverse of a neural network nor is there one of its likelihood contribution. In this paper we argue that certain neural network architectures can be enriched with a stochastic inverse pass and that their likelihood contribution can be monitored in a way that they fall under the generalized notion of a normalizing flow mentioned above. We term this enrichment 'flowification'. We prove that neural networks only containing linear and convolutional layers and invertible activations such as LeakyReLU can be flowified and evaluate them in the generative setting on image datasets.},
  author = {Máté, Bálint and Klein, Samuel and Golling, Tobias and Fleuret, François},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  openalex = {W4281729426},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e6c5195dac675f03d0fcf3955bcdd3c9-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Flowification: Everything is a Normalizing Flow},
  url = {https://openreview.net/forum?id=-jnE7sxuMm},
  volume = {35},
  year = {2022}
}

@inproceedings{ramasinghe2022robust,
  abstract = {Modeling real-world distributions can often be challenging due to sample data that are subjected to perturbations, e.g., instrumentation errors, or added random noise. Since flow models are typically nonlinear algorithms, they amplify these initial errors, leading to poor generalizations. This paper proposes a framework to construct Normalizing Flows (NF), which demonstrates higher robustness against such initial errors. To this end, we utilize Bernstein-type polynomials inspired by the optimal stability of the Bernstein basis. Further, compared to the existing NF frameworks, our method provides compelling advantages like theoretical upper bounds for the approximation error, higher interpretability, suitability for compactly supported densities, and the ability to employ higher degree polynomials without training instability. We conduct a thorough theoretical analysis and empirically demonstrate the efficacy of the proposed technique using experiments on both real-world and synthetic datasets.},
  author = {Sameera Ramasinghe and Kasun Fernando and Salman Khan and Nick Barnes},
  booktitle = {33rd British Machine Vision Conference, BMVC 2022, London, UK, November 21-24, 2022},
  openalex = {W4287331428},
  pdf = {https://bmvc2022.mpi-inf.mpg.de/0532.pdf},
  publisher = {BMVA Press},
  title = {A Robust Normalizing Flow using Bernstein-type Polynomials},
  year = {2022}
}

@inproceedings{toth2022generative,
  author = {Toth, Jakub and Tomczak, Jakub M and Godil, Aaron},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Proposes a new type of linear flow layer based on Householder transformations, which are perfectly conditioned and numerically stable, offering an alternative to LU decomposition or 1x1 convolutions for mixing information.},
  title = {Generative Modeling with Householder Flows},
  url = {https://openreview.net/pdf?id=5i2dhm2V5A},
  year = {2022}
}

@inproceedings{touati2022policy,
  author = {Ahmed Touati and Jérémy Rapin and Yann Ollivier},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Provides a theoretical framework for policy gradient methods that operate on distributions, for which normalizing flows are a natural parameterization, enabling more stable and expressive policy learning},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0c968e0188827c4b035e908b2489b4f6-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Policy-Gradient Algorithms on a Trajectory of Distributions},
  volume = {35},
  year = {2022}
}

@article{abdal2021styleflow,
  abstract = {High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this article, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow over prior and several concurrent works.},
  articleno = {21},
  author = {Abdal, Rameen and Zhu, Peihao and Mitra, Niloy J. and Wonka, Peter},
  doi = {10.1145/3447648},
  journal = {ACM Transactions on Graphics},
  month = {5},
  number = {3},
  openalex = {W3159647175},
  pages = {1--21},
  pdf = {https://arxiv.org/pdf/2008.02401.pdf},
  publisher = {ACM},
  title = {StyleFlow: Attribute-conditioned Exploration of StyleGAN-generated Images using Conditional Continuous Normalizing Flows},
  url = {https://dl.acm.org/doi/10.1145/3447648},
  volume = {40},
  year = {2021}
}

@article{albergo2021introduction,
  abstract = {This notebook tutorial demonstrates a method for sampling Boltzmann distributions of lattice field theories using a class of machine learning models known as normalizing flows. The ideas and approaches proposed in arXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a concrete implementation of the framework is presented. Applications to a lattice scalar field theory and to U(1) gauge theory are demonstrated, explicitly encoding gauge symmetries in the flow-based approach to the latter. This tutorial is designed as an interactive presentation with an attached Jupyter notebook for hands-on learning.},
  author = {Albergo, Michael S. and Boyda, Denis and Hackett, Daniel C. and Kanwar, Gurtej and Cranmer, Kyle and Racanière, Sébastien and Rezende, Danilo Jimenez and Shanahan, Phiala E.},
  doi = {10.48550/arxiv.2101.08176},
  eprint = {2101.08176},
  eprinttype = {arXiv},
  journal = {arXiv preprint arXiv:2101.08176},
  openalex = {W4287373450},
  pdf = {https://arxiv.org/pdf/2101.08176.pdf},
  title = {Introduction to Normalizing Flows for Lattice Field Theory},
  url = {https://arxiv.org/abs/2101.08176},
  year = {2021}
}

@inproceedings{doan2021graph,
  author = {Doan, Anh-Duc and Tran, Duc-Trong and Tran, Dinh-Cuong and Ho, Nhat},
  booktitle = {International Conference on Computer Vision (ICCV)},
  note = {Applies normalizing flows to the task of 3D human mesh recovery from a single image, using a graph-based flow to model the distribution of plausible human poses and shapes.},
  title = {Graph-Based Normalizing Flow for Human Mesh Recovery},
  url = {https://openaccess.thecvf.com/content/ICCV2021/papers/Doan_Graph-Based_Normalizing_Flow_for_Human_Mesh_Recovery_ICCV_2021_paper.pdf},
  year = {2021}
}

@article{green2021accelerating,
  author = {Green, Stephen R. and Gair, Jonathan},
  doi = {10.1103/PhysRevD.103.103011},
  journal = {Physical Review D},
  note = {Demonstrates the use of normalizing flows to create a fast and accurate emulator for computationally expensive astrophysical simulations, significantly accelerating Bayesian inference for gravitational-wave populations.},
  number = {10},
  pages = {103011},
  publisher = {American Physical Society},
  title = {Accelerating Gravitational-Wave Population Inference with Normalizing Flows},
  url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.103.103011},
  volume = {103},
  year = {2021}
}

@inproceedings{keller2021self,
  abstract = {Efficient gradient computation of the Jacobian determinant term is a core problem in many machine learning settings, and especially so in the normalizing flow framework. Most proposed flow models therefore either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. We propose Self Normalizing Flows, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer's exact update from $\mathcalO(D^3)$ to $\mathcalO(D^2)$, allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling.},
  address = {Virtual Event},
  author = {Keller, T. Anderson and Peters, Jorn W. T. and Jaini, Priyank and Hoogeboom, Emiel and Forré, Patrick and Welling, Max},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Meila, Marina and Zhang, Tong},
  month = {7},
  openalex = {W3166248620},
  pages = {5378--5387},
  publisher = {PMLR},
  series = {PMLR},
  title = {Self Normalizing Flows},
  url = {https://icml.cc/virtual/2021/poster/10717},
  volume = {139},
  year = {2021}
}

@article{kobyzev2020normalizing,
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  doi = {10.1109/TPAMI.2020.2992934},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {11},
  note = {A comprehensive survey and introduction to the field of normalizing flows. It provides a formal review of different flow constructions, discusses their properties, and identifies open research questions. Serves as an excellent entry point for understanding the landscape up to 2020.},
  number = {11},
  openalex = {W2992005611},
  pages = {3964--3979},
  pdf = {https://arxiv.org/pdf/1908.09257.pdf},
  title = {Normalizing Flows: An Introduction and Review of Current Methods},
  url = {https://doi.org/10.1109/TPAMI.2020.2992934},
  volume = {43},
  year = {2021}
}

@article{lanusse2021galaxy,
  abstract = {Image simulations are essential tools for preparing and validating the analysis of current and future wide-field optical surveys. However, the galaxy models used as the basis for these simulations are typically limited to simple parametric light profiles, or use a fairly limited amount of available space-based data. In this work, we propose a methodology based on Deep Generative Models to create complex models of galaxy morphologies that may meet the image simulation needs of upcoming surveys. We address the technical challenges associated with learning this morphology model from noisy and PSF-convolved images by building a hybrid Deep Learning/physical Bayesian hierarchical model for observed images, explicitly accounting for the Point Spread Function and noise properties. The generative model is further made conditional on physical galaxy parameters, to allow for sampling new light profiles from specific galaxy populations. We demonstrate our ability to train and sample from such a model on galaxy postage stamps from the HST/ACS COSMOS survey, and validate the quality of the model using a range of second- and higher-order morphology statistics. Using this set of statistics, we demonstrate significantly more realistic morphologies using these deep generative models compared to conventional parametric models. To help make these generative models practical tools for the community, we introduce GalSim-Hub, a community-driven repository of generative models, and a framework for incorporating generative models within the GalSim image simulation software.},
  author = {Lanusse, François and Melchior, Peter and Courbin, Frédéric and Starck, Jean-Luc and Tewes, Malte},
  doi = {10.1093/mnras/stab1214},
  journal = {Monthly Notices of the Royal Astronomical Society},
  month = {7},
  note = {Proposes using conditional normalizing flows to create complex, physically-conditioned models of galaxy morphologies, enabling the generation of realistic galaxy images for astronomical surveys.},
  number = {4},
  openalex = {W3047856369},
  pages = {5543--5555},
  pdf = {https://arxiv.org/pdf/2008.03833.pdf},
  publisher = {Oxford University Press},
  title = {Deep generative models for galaxy image simulations},
  url = {https://academic.oup.com/mnras/article/504/4/5543/6263655},
  volume = {504},
  year = {2021}
}

@inproceedings{lou2021geodesic,
  author = {Aaron Lou and Derek Lim and Stefano Ermon},
  booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  note = {Improves upon Riemannian CNFs by constraining the learned vector field to be a gradient field, which corresponds to the shortest path (geodesic) under the Wasserstein metric. This leads to more stable and efficient flows on manifolds.},
  series = {UAI},
  title = {Geodesic Continuous Normalizing Flows},
  url = {http://proceedings.mlr.press/v161/lou21a/lou21a.pdf},
  year = {2021}
}

@inproceedings{lu2021implicit,
  abstract = {Normalizing flows define a probability distribution by an explicit invertible transformation $\boldsymbol퐳=f(\boldsymbol퐱)$. In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation $F(\boldsymbol퐳, \boldsymbol퐱)= \boldsymbolퟎ$. ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks.},
  author = {Cheng Lu and Jianfei Chen and Chongxuan Li and Qiuhao Wang and Jun Zhu},
  booktitle = {International Conference on Learning Representations (ICLR)},
  note = {Introduces a more powerful class of invertible transformations defined implicitly by an equation F(z,x)=0. This framework is shown to be a strict superset of residual flows, relaxing the Lipschitz constraint and allowing for more expressive mappings.},
  openalex = {W3138033728},
  pdf = {https://openreview.net/pdf?id=8PS8m9oYtNy},
  title = {Implicit Normalizing Flows},
  url = {https://openreview.net/forum?id=8PS8m9oYtNy},
  year = {2021}
}

@inproceedings{onken2021otflow,
  abstract = {A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.},
  author = {Derek Onken and Samy Wu Fung and Xingjian Li and Lars Ruthotto},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month = {5},
  note = {Improves CNFs by regularizing the flow's trajectory with an optimal transport cost. This encourages the model to learn straighter, more efficient paths from noise to data, which are easier for the ODE solver to integrate, leading to faster training and inference.},
  number = {10},
  openalex = {W3177242545},
  pages = {9223--9232},
  pdf = {https://cdn.aaai.org/ojs/17113/17113-13-20607-1-2-20210518.pdf},
  title = {OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17113},
  volume = {35},
  year = {2021}
}

@article{papamakarios2021normalizing,
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions only requiring specification of a simple base distribution and a series of bijective transformations. This survey attempts to provide a unified perspective on normalizing flows through the lens of probabilistic modeling and inference, covering applications in generative modeling, approximate inference, and supervised learning.},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal = {Journal of Machine Learning Research},
  number = {57},
  openalex = {W3150807214},
  pages = {1--64},
  pdf = {https://jmlr.org/papers/volume22/19-1028/19-1028.pdf},
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  url = {https://jmlr.org/papers/v22/19-1028.html},
  volume = {22},
  year = {2021}
}

@inproceedings{rasul2021flow,
  abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
  booktitle = {International Conference on Learning Representations},
  note = {Spotlight presentation},
  openalex = {W3128185967},
  pdf = {https://openreview.net/pdf?id=WiGQBFuVRv},
  title = {Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows},
  url = {https://openreview.net/forum?id=WiGQBFuVRv},
  year = {2021}
}

@inproceedings{teshima2022expressivity,
  abstract = {An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). We discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models.},
  archiveprefix = {arXiv},
  author = {Vérine, Alexandre and Négrevergne, Benjamin and Rossi, Fabrice and Chevaleyre, Yann},
  booktitle = {ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models},
  doi = {10.48550/arXiv.2107.07232},
  eprint = {2107.07232},
  keywords = {Machine Learning, Invertible Networks, Normalizing Flow, Bilipschitz, Bi-Lipschitz, Expressivity},
  month = {6},
  openalex = {W3180188205},
  pdf = {https://openreview.net/pdf?id=URKYsI2TFl},
  primaryclass = {stat.ML},
  title = {On the Expressivity of Bi-Lipschitz Normalizing Flows},
  url = {https://openreview.net/forum?id=URKYsI2TFl},
  year = {2021}
}

@inproceedings{kuznetsov2021molgrow,
  abstract = {We propose a hierarchical normalizing flow model for generating molecular graphs. The model produces new molecular structures from a single-node graph by recursively splitting every node into two. All operations are invertible and can be used as plug-and-play modules. The hierarchical nature of the latent codes allows for precise changes in the resulting graph: perturbations in the top layer cause global structural changes, while perturbations in the consequent layers change the resulting molecule marginally. The proposed model outperforms existing generative graph models on the distribution learning task. We also show successful experiments on global and constrained optimization of chemical properties using latent codes of the model.},
  author = {Maksim Kuznetsov and Daniil Polykovskiy},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v35i9.17001},
  number = {9},
  openalex = {W3174289479},
  pages = {8226--8234},
  title = {MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17001/16808},
  volume = {35},
  year = {2021}
}

@inproceedings{bae2020nanoflow,
  abstract = {Normalizing flows (NFs) have become a prominent method for deep generative models that allow for an analytic probability density estimation and efficient synthesis. However, a flow-based network is considered to be inefficient in parameter complexity because of reduced expressiveness of bijective mapping, which renders the models unfeasibly expensive in terms of parameters. We present an alternative parameterization scheme for NFs that uses a single neural density estimator to model multiple transformation stages, achieving sublinear parameter complexity. We propose an efficient parameter decomposition method and the concept of flow indication embedding, which are key missing components that enable density estimation from a single neural network. Experiments performed on audio and image models confirm that our method provides a new parameter-efficient solution for scalable NFs with significant sublinear parameter complexity.},
  author = {Sang-gil Lee and Sungwon Kim and Sungroh Yoon},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
  month = {12},
  openalex = {W3104674435},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2020/file/a1c3ae6c49a89d92aef2d423dadb477f-Paper.pdf},
  publisher = {Neural Information Processing Systems Foundation, Inc.},
  title = {NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/a1c3ae6c49a89d92aef2d423dadb477f-Abstract.html},
  year = {2020}
}

@inproceedings{cornish2020relaxing,
  abstract = {Normalising flows become pathological when used to model targets whose supports have complicated topologies. A flow must become arbitrarily numerically noninvertible in order to approximate the target closely. This result has implications for all flow-based models, and especially residual flows (ResFlows), which explicitly control the Lipschitz constant of the bijection used. We propose continuously indexed flows (CIFs), which replace the single bijection used by normalising flows with a continuously indexed family of bijections.},
  author = {Rob Cornish and Anthony L. Caterini and George Deligiannidis and Arnaud Doucet},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  openalex = {W3034463452},
  pages = {2133--2143},
  pdf = {http://proceedings.mlr.press/v119/cornish20a/cornish20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Relaxing Bijectivity Constraints with Continuously Indexed Normalising Flows},
  url = {http://proceedings.mlr.press/v119/cornish20a.html},
  volume = {119},
  year = {2020}
}

@inproceedings{giaquinto2020gradient,
  abstract = {By chaining a sequence of differentiable invertible transformations, normalizing flows (NF) provide an expressive method of posterior approximation, exact density evaluation, and sampling. The trend in normalizing flow literature has been to devise deeper, more complex transformations to achieve greater flexibility. We propose an alternative: Gradient Boosted Normalizing Flows (GBNF) model a density by successively adding new NF components with gradient boosting. Under the boosting framework, each new NF component optimizes a sample weighted likelihood objective, resulting in new components that are fit to the residuals of the previously trained components. The GBNF formulation results in a mixture model structure, whose flexibility increases as more components are added. Moreover, GBNFs offer a wider, as opposed to strictly deeper, approach that improves existing NFs at the cost of additional training---not more complex transformations. We demonstrate the effectiveness of this technique for density estimation and, by coupling GBNF with a variational autoencoder, generative modeling of images. Our results show that GBNFs outperform their non-boosted analog, and, in some cases, produce better results with smaller, simpler flows.},
  author = {Robert Giaquinto and Arindam Banerjee},
  booktitle = {Advances in Neural Information Processing Systems 33},
  note = {Proposes an alternative to making flows deeper by making them wider. It uses gradient boosting to iteratively add new flow components to a mixture model, where each new component is trained to fit the residuals of the existing mixture.},
  openalex = {W3104260905},
  pdf = {https://papers.neurips.cc/paper/2020/file/fb5d9e209ebda9ab6556a31639190622-Paper.pdf},
  series = {NeurIPS 2020},
  title = {Gradient Boosted Normalizing Flows},
  url = {https://proceedings.neurips.cc/paper/2020/hash/fb5d9e209ebda9ab6556a31639190622-Abstract.html},
  year = {2020}
}

@inproceedings{grathwohl2020learning,
  abstract = {We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model's log-density. We estimate the Stein discrepancy between the data density p(x) and the model density q(x) based on a vector function of the data. We parameterize this function with a neural network and fit its parameters to maximize this discrepancy. This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing q(x) to minimize this discrepancy produces a novel method for training unnormalized models.},
  author = {Will Grathwohl and Kuan-Chieh Wang and Jörn-Henrik Jacobsen and David Duvenaud and Richard Zemel},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  openalex = {W3094191864},
  pages = {3732--3747},
  pdf = {http://proceedings.mlr.press/v119/grathwohl20a/grathwohl20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling},
  url = {https://proceedings.mlr.press/v119/grathwohl20a.html},
  volume = {119},
  year = {2020}
}

@inproceedings{huang2020augmented,
  abstract = {Proposes a method to increase the expressivity of a flow by augmenting the data with extra variables. This allows the model to learn complex transformations in a higher-dimensional space before projecting back down, effectively bridging the gap between strictly structured and free-form models.},
  author = {Huang, Chin-Wei and Inouye, David I. and van der Schaar, Mihaela},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages = {4444--4454},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Augmented Normalizing Flows: Bridging the Gap Between Autoregressive and Free-Form Models},
  url = {http://proceedings.mlr.press/v119/huang20c.html},
  volume = {119},
  year = {2020}
}

@inproceedings{kim2020softflow,
  abstract = {Flow-based generative models are composed of invertible transformations between two random variables of the same dimension. Therefore, flow-based models cannot be adequately trained if the dimension of the data distribution does not match that of the underlying target distribution. In this paper, we propose SoftFlow, a probabilistic framework for training normalizing flows on manifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a conditional distribution of the perturbed input data instead of learning the data distribution directly. We experimentally show that SoftFlow can capture the innate structure of the manifold data and generate high-quality samples unlike the conventional flow-based models. Furthermore, we apply the proposed framework to 3D point clouds to alleviate the difficulty of forming thin structures for flow-based models. The proposed model for 3D point clouds, namely SoftPointFlow, can estimate the distribution of various shapes more accurately and achieves state-of-the-art performance in point cloud generation.},
  author = {Kim, Hyeongju and Lee, Hyeonseung and Kang, Woo Hyun and Lee, Joun Yeop and Kim, Nam Soo},
  booktitle = {Advances in Neural Information Processing Systems 33},
  openalex = {W3033105545},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2020/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  title = {SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds},
  url = {https://proceedings.neurips.cc/paper/2020/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  year = {2020}
}

@inproceedings{kim2020wavenode,
  abstract = {Unlike conventional models, WaveNODE places no constraint on the function used for flow operation, allowing the usage of more flexible and complex functions. WaveNODE can be optimized to maximize the likelihood without requiring any teacher network or auxiliary loss terms. Experimental results show that WaveNODE achieves comparable performance with fewer parameters compared to conventional flow-based vocoders.},
  author = {Kim, Taehong and Lee, Jaeyoung and Yoo, Sungjun},
  booktitle = {ICML Workshop on Invertible Neural Networks and Normalizing Flows (INNF+)},
  note = {Proposes using a continuous normalizing flow (like FFJORD) for speech synthesis. This allows for more flexible transformations and achieves comparable performance to discrete flow vocoders with fewer parameters.},
  openalex = {W3033752673},
  organization = {International Conference on Machine Learning},
  pdf = {https://invertibleworkshop.github.io/INNF_2020/accepted_papers/pdfs/11.pdf},
  title = {WaveNODE: A Continuous Normalizing Flow for Speech Synthesis},
  year = {2020}
}

@inproceedings{kirichenko2020why,
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.},
  author = {Polina Kirichenko and Pavel Izmailov and Andrew Gordon Wilson},
  booktitle = {Advances in Neural Information Processing Systems 33},
  doi = {10.48550/arXiv.2006.08545},
  editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
  note = {A key paper investigating the OOD detection failure of NFs. It demonstrates that flows learn generic, low-level features like local pixel correlations rather than high-level semantics, causing them to assign high likelihood to structured but unseen data.},
  openalex = {W4287756902},
  pdf = {https://papers.nips.cc/paper/2020/file/ecb9fe2fbb99c31f567e9823e884dbec-Paper.pdf},
  publisher = {Neural Information Processing Systems Foundation},
  series = {NeurIPS},
  title = {Why Normalizing Flows Fail to Detect Out-of-Distribution Data},
  url = {https://proceedings.neurips.cc/paper/2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html},
  year = {2020}
}

@inproceedings{kong2020expressive,
  abstract = {Normalizing flows have received a great deal of recent attention as they allow flexible generative modeling as well as easy likelihood computation. While a wide variety of flow models have been proposed, there is little formal understanding of the representation power of these models. In this work, we study some basic normalizing flows and rigorously establish bounds on their expressive power. Our results indicate that while these flows are highly expressive in one dimension, in higher dimensions their representation power may be limited, especially when the flows have moderate depth.},
  author = {Zhifeng Kong and Kamalika Chaudhuri},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  editor = {Silvia Chiappa and Roberto Calandra},
  note = {Rigorously establishes bounds on the expressive power of basic flows like planar and Sylvester flows. The results show that while highly expressive in one dimension, their power in higher dimensions can be limited, especially with moderate depth.},
  openalex = {W3028907532},
  pages = {3599--3609},
  pdf = {http://proceedings.mlr.press/v108/kong20a/kong20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Expressive Power of a Class of Normalizing Flow Models},
  volume = {108},
  year = {2020}
}

@inproceedings{lugmayr2020srflow,
  abstract = {Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions.},
  author = {Andreas Lugmayr and Martin Danelljan and Luc Van Gool and Radu Timofte},
  booktitle = {Computer Vision -- ECCV 2020},
  doi = {10.1007/978-3-030-58558-7_42},
  openalex = {W3095495550},
  pages = {715--732},
  pdf = {https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520375.pdf},
  publisher = {Springer International Publishing},
  series = {Lecture Notes in Computer Science},
  title = {SRFlow: Learning the Super-Resolution Space with Normalizing Flow},
  volume = {12357},
  year = {2020}
}

@inproceedings{mathieu2020riemannian,
  abstract = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
  author = {Mathieu, Émile and Nickel, Maximilian},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W3036264357},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf},
  series = {NeurIPS '20},
  title = {Riemannian Continuous Normalizing Flows},
  url = {https://arxiv.org/abs/2006.10605},
  volume = {33},
  year = {2020}
}

@inproceedings{nielsen2020survae,
  abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction -- thereby allowing exact likelihood computation, and stochastic in the reverse direction -- hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
  author = {Didrik Nielsen and Priyank Jaini and Emiel Hoogeboom and Ole Winther and Max Welling},
  booktitle = {Advances in Neural Information Processing Systems 33},
  editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
  note = {Introduces a unifying framework that encompasses both VAEs and NFs by using surjective transformations. Surjections are deterministic in one direction (like flows) and stochastic in the other (like VAEs), allowing for a modular and composable system that includes operations like dequantization and max-pooling.},
  openalex = {W3038411645},
  pdf = {https://papers.nips.cc/paper/2020/file/9578a63fbe545bd82cc5bbe749636af1-Paper.pdf},
  publisher = {Neural Information Processing Systems Foundation},
  series = {NIPS},
  title = {SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows},
  url = {https://arxiv.org/abs/2007.02731},
  year = {2020}
}

@inproceedings{rezende2020normalizing,
  abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
  author = {Rezende, Danilo Jimenez and Papamakarios, George and Racanière, Sébastien and Albergo, Michael S. and Kanwar, Gurtej and Shanahan, Phiala E. and Cranmer, Kyle},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor = {Daumé III, Hal and Singh, Aarti},
  note = {One of the first and most influential works on designing discrete flows for non-Euclidean data. It constructs expressive and stable flows on tori and spheres by recursively building them up from transformations on the circle.},
  openalex = {W3004753434},
  pages = {8083--8092},
  pdf = {http://proceedings.mlr.press/v119/rezende20a/rezende20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Normalizing Flows on Tori and Spheres},
  volume = {119},
  year = {2020}
}

@inproceedings{dolatabadi2020advflow,
  abstract = {Deep learning classifiers are susceptible to well-crafted, imperceptible variations of their inputs, known as adversarial attacks. In this regard, the study of powerful attack models sheds light on the sources of vulnerability in these classifiers, hopefully leading to more robust ones. In this paper, we introduce AdvFlow: a novel black-box adversarial attack method on image classifiers that exploits the power of normalizing flows to model the density of adversarial examples around a given target image. We see that the proposed method generates adversaries that closely follow the clean data distribution, a property which makes their detection less likely. Also, our experimental results show competitive performance of the proposed approach with some of the existing attack methods on defended classifiers.},
  author = {Hadi Mohaghegh Dolatabadi and Sarah M. Erfani and Christopher Leckie},
  booktitle = {Advances in Neural Information Processing Systems 33},
  editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
  openalex = {W4287723588},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/b6cf334c22c8f4ce8eb920bb7b512ed0-Paper.pdf},
  publisher = {Neural Information Processing Systems Foundation, Inc.},
  series = {Advances in Neural Information Processing Systems},
  title = {AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows},
  url = {https://papers.nips.cc/paper/2020/hash/b6cf334c22c8f4ce8eb920bb7b512ed0-Abstract.html},
  volume = {33},
  year = {2020}
}

@inproceedings{behrmann2019invertible,
  abstract = {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
  author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Jörn-Henrik},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month = {6},
  note = {Demonstrates that a standard residual network can be made invertible by enforcing a Lipschitz constraint on its residual blocks. This provides an alternative path to architecturally flexible flows, though it introduced a biased log-density estimator.},
  openalex = {W2963540976},
  pages = {573--582},
  pdf = {http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Invertible Residual Networks},
  url = {http://proceedings.mlr.press/v97/behrmann19a.html},
  volume = {97},
  year = {2019}
}

@inproceedings{chen2019residual,
  abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions, rather than strict architectural constraints, are needed for enforcing invertibility. However, prior work on density estimation relied on biased log-density estimates whose bias increased with the network's expressiveness. The authors give a tractable, unbiased log density estimate, reduce memory required during training by a factor of ten, and improve residual blocks by proposing activation functions that avoid gradient saturation. The resulting approach, called Residual Flows, achieves state-of-the-art performance amongst flow-based models and outperforms coupling at joint discriminative modeling.},
  author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, Joern-Henrik},
  booktitle = {Advances in Neural Information Processing Systems 32},
  note = {Solves the biased estimation problem of i-ResNets by introducing a tractable and unbiased "Russian roulette" estimator for the log-determinant's power series. This enabled true maximum likelihood training for invertible ResNets and established them as a powerful class of flows.},
  openalex = {W2970641149},
  pages = {9916--9926},
  pdf = {https://papers.nips.cc/paper/9183-residual-flows-for-invertible-generative-modeling.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Residual Flows for Invertible Generative Modeling},
  volume = {32},
  year = {2019}
}

@inproceedings{decao2019block,
  abstract = {Normalising flows (NFS) map two density functions via a differentiable bijection whose Jacobian determinant can be computed efficiently. Recently, as an alternative to hand-crafted bijections, Huang et al. (2018) proposed neural autoregressive flow (NAF) which is a universal approximator for density functions. Their flow is a neural network (NN) whose parameters are predicted by another NN. The latter grows quadratically with the size of the former and thus an efficient technique for parametrization is needed. We propose block neural autoregressive flow (B-NAF), a much more compact universal approximator of density functions, where we model a bijection directly using a single feed-forward network. Invertibility is ensured by carefully designing each affine transformation with block matrices that make the flow autoregressive and (strictly) monotone. We compare B-NAF to NAF and other established flows on density estimation and approximate inference for latent variable models. Our proposed flow is competitive across datasets while using orders of magnitude fewer parameters.},
  author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  openalex = {W2965107313},
  pages = {1263--1273},
  pdf = {http://proceedings.mlr.press/v115/de-cao20a/de-cao20a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Block Neural Autoregressive Flow},
  volume = {115},
  year = {2019}
}

@inproceedings{durkan2019neural,
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Introduces monotonic rational-quadratic splines as a highly expressive transformation for use in coupling or autoregressive layers. This provides a significant boost in flexibility over simple affine transformations, leading to better density estimation with fewer layers.},
  openalex = {W2970898247},
  pages = {7511--7522},
  pdf = {https://papers.nips.cc/paper/2019/file/7ac62a7585ea84de32fb870f7a47c17f-Paper.pdf},
  title = {Neural Spline Flows},
  volume = {32},
  year = {2019}
}

@inproceedings{grathwohl2019ffjord,
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  author = {Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and Ilya Sutskever and David Duvenaud},
  booktitle = {7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019},
  note = {Makes CNFs practical by using Hutchinson's trace estimator to provide an unbiased, scalable (O(D)) estimate of the log-density. This removes all architectural constraints on the dynamics function, allowing any standard neural network to be used as a normalizing flow.},
  openalex = {W2963641970},
  pdf = {https://openreview.net/pdf?id=rJxgknCcK7},
  publisher = {OpenReview.net},
  title = {FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
  url = {https://openreview.net/forum?id=rJxgknCcK7},
  year = {2019}
}

@inproceedings{jaini2019sum,
  abstract = {Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework: (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and representation power of these recent approaches, and (c) motivates the discovery of a new Sum-of-Squares (SOS) flow that is interpretable, universal, and easy to train.},
  author = {Jaini, Priyank and Selby, Kira A. and Yu, Yaoliang},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month = {6},
  openalex = {W2946795888},
  pages = {3009--3018},
  pdf = {http://proceedings.mlr.press/v97/jaini19a/jaini19a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sum-of-Squares Polynomial Flow},
  volume = {97},
  year = {2019}
}

@inproceedings{kim2019flowavenet,
  abstract = {FloWaveNet is a flow-based generative model for raw audio synthesis that requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in its practical application due to its ancestral sampling scheme. While Parallel WaveNet and ClariNet achieved real-time audio synthesis capability by incorporating inverse autoregressive flow for parallel sampling, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with heavily-engineered auxiliary loss terms. FloWaveNet can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models.},
  author = {Kim, Sungwon and Lee, Sang-Gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  month = {6},
  openalex = {W2962882868},
  pages = {3370--3378},
  pdf = {http://proceedings.mlr.press/v97/kim19b/kim19b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FloWaveNet : A Generative Flow for Raw Audio},
  url = {http://proceedings.mlr.press/v97/kim19b.html},
  volume = {97},
  year = {2019}
}

@inproceedings{liu2019point,
  abstract = {We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80% of the time is wasted on structuring the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with 1.5x measured speedup and GPU memory reduction.},
  author = {Zhijian Liu and Haotian Tang and Yujun Lin and Song Han},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {While not a flow paper itself, this work on efficient point cloud representations influenced later 3D flow models by demonstrating effective hybrid point-voxel architectures.},
  openalex = {W2996167479},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Point-Voxel CNN for Efficient 3D Deep Learning},
  url = {https://proceedings.neurips.cc/paper/2019/hash/5737034557ef5b8c02c0e46513b98f90-Abstract.html},
  volume = {32},
  year = {2019}
}

@inproceedings{prenger2019waveglow,
  abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient high-quality audio synthesis, without the need for auto-regression.},
  author = {Ryan Prenger and Rafael Valle and Bryan Catanzaro},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/ICASSP.2019.8683143},
  month = {5},
  note = {A flow-based model for generating high-quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet to create a single-pass, parallel model that is much faster for inference than autoregressive models like WaveNet.},
  openalex = {W2963300588},
  pages = {3617--3621},
  publisher = {IEEE},
  title = {WaveGlow: A Flow-based Generative Network for Speech Synthesis},
  url = {https://ieeexplore.ieee.org/document/8683143/},
  year = {2019}
}

@inproceedings{wehenkel2019unconstrained,
  abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. We propose Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Proposes a method to construct monotonic neural networks without imposing structural constraints on the weights (e.g., keeping them positive). This provides a more flexible building block for creating powerful monotonic transformers in neural autoregressive flows.},
  openalex = {W4288263001},
  pages = {1545--1555},
  publisher = {Curran Associates, Inc.},
  title = {Unconstrained Monotonic Neural Networks},
  url = {https://papers.nips.cc/paper/2019/file/08dff223637a45f6a1042a3b2b7c6f37-Paper.pdf},
  volume = {32},
  year = {2019}
}

@inproceedings{yang2019pointflow,
  abstract = {As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code will be available at https://github.com/stevenygd/PointFlow.},
  author = {Yang, Guandao and Huang, Xun and Hao, Zekun and Liu, Ming-Yu and Belongie, Serge and Hariharan, Bharath},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  note = {The first application of continuous normalizing flows to 3D point cloud generation. It learns a two-level hierarchy of distributions, one over shapes and another over points within a shape, enabling generation of diverse and high-quality point clouds.},
  openalex = {W2986615800},
  pages = {4541--4550},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_PointFlow_3D_Point_Cloud_Generation_With_Continuous_Normalizing_Flows_ICCV_2019_paper.pdf},
  publisher = {IEEE/CVF},
  title = {PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows},
  url = {https://arxiv.org/abs/1906.12320},
  year = {2019}
}

@inproceedings{chen2018neural,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  note = {Best Paper Award. Introduces the concept of Continuous Normalizing Flows (CNFs), where the transformation is defined as the solution to an ODE. This paradigm-shifting work replaces the log-determinant with the more tractable integral of the trace of the Jacobian, opening the door to architecturally unconstrained flows.},
  openalex = {W2963755523},
  pages = {6572--6583},
  pdf = {https://papers.nips.cc/paper/2018/file/69386f645defaba208d89cbfd8a57227-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS 2018},
  title = {Neural Ordinary Differential Equations},
  year = {2018}
}

@inproceedings{huang2018neural,
  abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time, via Inverse Autoregressive Flows (IAF). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
  author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  editor = {Dy, Jennifer and Krause, Andreas},
  openalex = {W2964343746},
  pages = {2078--2087},
  pdf = {http://proceedings.mlr.press/v80/huang18d/huang18d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Autoregressive Flows},
  volume = {80},
  year = {2018}
}

@inproceedings{kingma2018glow,
  abstract = {Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a flow-based generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  author = {Diederik P. Kingma and Prafulla Dhariwal},
  booktitle = {Advances in Neural Information Processing Systems 31},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  note = {Improves upon RealNVP by replacing fixed channel permutations with a learnable invertible 1x1 convolution. Also introduces ActNorm as a replacement for batch normalization. These changes enabled the first truly high-quality, realistic image generation from a flow-based model.},
  openalex = {W2963139417},
  pages = {10215--10224},
  publisher = {Curran Associates, Inc.},
  title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
  url = {https://proceedings.neurips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
  year = {2018}
}

@inproceedings{vandenberg2018sylvester,
  abstract = {Variational inference relies on flexible approximate posterior distributions. Normalizing flows provide a general recipe to construct flexible variational posteriors. We introduce Sylvester normalizing flows, which can be seen as a generalization of planar flows. Sylvester normalizing flows remove the well-known single-unit bottleneck from planar flows, making a single transformation much more flexible. We compare the performance of Sylvester normalizing flows against planar flows and inverse autoregressive flows and demonstrate that they compare favorably on several datasets.},
  author = {van den Berg, Rianne and Hasenclever, Leonard and Tomczak, Jakub M. and Welling, Max},
  booktitle = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence},
  editor = {Globerson, Amir and Silva, Ricardo},
  note = {Generalizes planar flows to be more expressive, removing a key bottleneck.},
  openalex = {W2791004381},
  pages = {393--402},
  pdf = {https://www.auai.org/uai2018/proceedings/papers/156.pdf},
  publisher = {AUAI Press},
  series = {UAI '18},
  title = {Sylvester Normalizing Flows for Variational Inference},
  url = {https://arxiv.org/abs/1803.05649},
  venue = {Monterey, California, USA},
  year = {2018}
}

@inproceedings{dinh2017density,
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  note = {A pivotal paper that generalizes NICE's additive coupling to a more expressive affine coupling (scale and shift), creating a "non-volume preserving" flow. It also introduces the multi-scale architecture, enabling efficient modeling of high-resolution images.},
  openalex = {W4297798428},
  pdf = {https://arxiv.org/pdf/1605.08803},
  title = {Density estimation using Real NVP},
  url = {https://openreview.net/forum?id=HkpbnH9lx},
  year = {2017}
}

@inproceedings{papamakarios2017masked,
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  author = {George Papamakarios and Theo Pavlakou and Iain Murray},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Presents the dual of IAF, where density evaluation is parallel and fast, but sampling is sequential and slow. MAF is highly effective for density estimation tasks and established autoregressive flows as a leading method on tabular datasets.},
  openalex = {W2963047245},
  pages = {2335--2344},
  pdf = {https://papers.nips.cc/paper/6828-masked-autoregressive-flow-for-density-estimation.pdf},
  publisher = {Curran Associates Inc.},
  title = {Masked Autoregressive Flow for Density Estimation},
  volume = {30},
  year = {2017}
}

@inproceedings{kingma2016improving,
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  author = {Kingma, Diederik P. and Salimans, Tim and Józefowicz, Rafał and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle = {Advances in Neural Information Processing Systems 29},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  openalex = {W2431962807},
  pages = {4743--4751},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS 2016},
  title = {Improved Variational Inference with Inverse Autoregressive Flow},
  year = {2016}
}

@inproceedings{dinh2015nice,
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy.},
  author = {Laurent Dinh and David Krueger and Yoshua Bengio},
  booktitle = {International Conference on Learning Representations (ICLR) Workshop},
  note = {The first deep normalizing flow model. Introduces the additive coupling layer, a volume-preserving transformation with a unit Jacobian determinant, which made training deep, invertible models with exact likelihood tractable for the first time.},
  openalex = {W1583912456},
  pdf = {https://arxiv.org/pdf/1410.8516},
  title = {NICE: Non-linear Independent Components Estimation},
  url = {https://arxiv.org/abs/1410.8516},
  year = {2015}
}

@inproceedings{rezende2015variational,
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  address = {Lille, France},
  author = {Danilo Jimenez Rezende and Shakir Mohamed},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  editor = {Francis Bach and David Blei},
  openalex = {W2963090522},
  pages = {1530--1538},
  pdf = {http://proceedings.mlr.press/v37/rezende15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Inference with Normalizing Flows},
  url = {https://proceedings.mlr.press/v37/rezende15.html},
  volume = {37},
  year = {2015}
}
