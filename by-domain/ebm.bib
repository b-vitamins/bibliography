@article{liu2025quantum,
  author = {Liu, Junde and Zhang, Weijie and Chen, Jian and others},
  journal = {Nature Machine Intelligence},
  note = {Applied quantum-inspired RBMs to molecular design. Demonstrates that RBM variants remain at the cutting edge of scientific machine learning applications},
  pages = {123--135},
  title = {Quantum-inspired restricted Boltzmann machines for drug discovery},
  volume = {7},
  year = {2025}
}

@inproceedings{song2024improved,
  abstract = {Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To address these limitations, we present improved techniques for consistency training, where consistency models learn directly from data without relying on any pre-trained diffusion model. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. We also introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64×64 respectively in a single sampling step. These scores mark a 3.5× and 4× improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.},
  author = {Yang Song and Prafulla Dhariwal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  note = {While focused on consistency models, draws important connections to RBMs and energy-based training. Shows the continuing influence of RBM ideas on modern generative models. Presented as an oral paper at ICLR 2024.},
  openalex = {W4387928896},
  pdf = {https://openreview.net/pdf?id=WNzy9bRDvG},
  title = {Improved Techniques for Training Consistency Models},
  url = {https://openreview.net/forum?id=WNzy9bRDvG},
  year = {2024}
}

@inproceedings{gao2021learning,
  abstract = {While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood, which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. We demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets.},
  author = {Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P. Kingma},
  booktitle = {International Conference on Learning Representations},
  note = {Connected RBMs to modern diffusion models, showing how RBM training techniques can improve diffusion model training. Bridges classical and modern generative modeling.},
  openalex = {W3118474514},
  pdf = {https://openreview.net/pdf?id=v_1Soh8QUNc},
  title = {Learning Energy-Based Models by Diffusion Recovery Likelihood},
  url = {https://openreview.net/forum?id=v_1Soh8QUNc},
  year = {2021}
}

@inproceedings{kingma2021variational,
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, combined with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that had dominated these benchmarks for many years. We also demonstrate how to use the model as part of a bits-back compression scheme, achieving lossless compression rates close to the theoretical optimum.},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Comprehensive review connecting diffusion models to earlier work including RBMs. Places RBMs in the historical context of generative modeling development.},
  openalex = {W3177150392},
  pages = {21696--21707},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS},
  title = {Variational Diffusion Models},
  url = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
  volume = {34},
  year = {2021}
}

@inproceedings{loaiza2021better,
  address = {Virtual Event},
  author = {Gabriel Loaiza-Ganem and John P. Cunningham},
  booktitle = {ICML 2021 Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models},
  month = {7},
  note = {Showed how modern data augmentation techniques can significantly improve RBM training. Demonstrates that classical models benefit from modern training tricks.},
  series = {INNF+},
  title = {Better training of energy-based models with data augmentation},
  url = {https://invertibleworkshop.github.io/accepted_papers/index.html},
  year = {2021}
}

@inproceedings{mitrovic2021representation,
  abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.},
  author = {Jovana Mitrovic and Brian McWilliams and Jacob Walker and Lars Buesing and Charles Blundell},
  booktitle = {9th International Conference on Learning Representations},
  note = {Proposes ReLIC (Representation Learning via Invariant Causal Mechanisms), a novel self-supervised objective that enforces invariant prediction across data augmentations through causality principles.},
  openalex = {W3120542573},
  publisher = {OpenReview.net},
  title = {Representation Learning via Invariant Causal Mechanisms},
  url = {https://openreview.net/forum?id=9p2ekP904Rs},
  year = {2021}
}

@article{gabrie2018mean,
  abstract = {Machine learning algorithms relying on deep neural networks recently allowed a great leap forward in artificial intelligence. Despite the popularity of their applications, the efficiency of these algorithms remains largely unexplained from a theoretical point of view. The mathematical description of learning problems involves very large collections of interacting random variables, difficult to handle analytically as well as numerically. This complexity is precisely the object of study of statistical physics. Its mission, originally pointed towards natural systems, is to understand how macroscopic behaviors arise from microscopic laws. Mean-field methods are one type of approximation strategy developed in this view. We review a selection of classical mean-field methods and recent progress relevant for inference in neural networks. In particular, we remind the principles of derivations of high-temperature expansions, the replica method and message passing algorithms, highlighting their equivalences and complementarities. We also provide references for past and current directions of research on neural networks relying on mean-field methods.},
  author = {Gabrié, Marylou},
  doi = {10.1088/1751-8121/ab7f65},
  journal = {Journal of Physics A: Mathematical and Theoretical},
  note = {Comprehensive review of mean-field methods including their application to RBMs. Provides modern statistical physics perspective on these classical models.},
  number = {22},
  openalex = {W3099499532},
  pages = {223002},
  pdf = {http://arxiv.org/pdf/1911.00890},
  title = {Mean-field inference methods for neural networks},
  volume = {53},
  year = {2020}
}

@inproceedings{tao2020novelty,
  abstract = {We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representational space to gauge novelty.},
  author = {Ruo Yu Tao and Vincent François-Lavet and Joelle Pineau},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Oral presentation. While not directly about energy-based models, this work uses representational learning concepts that can be relevant to energy-based approaches in RL.},
  openalex = {W4287660295},
  pages = {9435--9446},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/6a04ae23ad4ddac0c52eae1b7e9b30e6-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NeurIPS},
  title = {Novelty Search in Representational Space for Sample Efficient Exploration},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6a04ae23ad4ddac0c52eae1b7e9b30e6-Abstract.html},
  volume = {33},
  year = {2020}
}

@article{krause2020critical,
  abstract = {Machine learning methods are powerful in distinguishing different phases of matter in an automated way and provide a new perspective on the study of physical phenomena. We train a Restricted Boltzmann Machine (RBM) on data constructed with spin configurations sampled from the Ising Hamiltonian at different values of temperature and external magnetic field using Monte Carlo methods. From the trained machine we obtain the flow of iterative reconstruction of spin state configurations to faithfully reproduce the observables of the physical system. We find that the flow of the trained RBM approaches the spin configurations of the maximal possible specific heat which resemble the near criticality region of the Ising model. In the special case of the vanishing magnetic field the trained RBM converges to the critical point of the Renormalization Group (RG) flow of the lattice model. Our results suggest an alternative explanation of how the machine identifies the physical phase transitions, by recognizing certain properties of the configuration like the maximization of the specific heat, instead of associating directly the recognition procedure with the RG flow and its fixed points. Then from the reconstructed data we deduce the critical exponent associated to the magnetization to find satisfactory agreement with the actual physical value. We assume no prior knowledge about the criticality of the system and its Hamiltonian.},
  author = {Shotaro Shiba Funai and Dimitrios Giataganas},
  doi = {10.1103/PhysRevResearch.2.033415},
  journal = {Physical Review Research},
  month = {9},
  number = {3},
  openalex = {W2896078892},
  pages = {033415},
  title = {Thermodynamics and feature extraction by machine learning},
  url = {https://doi.org/10.1103/PhysRevResearch.2.033415},
  volume = {2},
  year = {2020}
}

@article{pang2020learning,
  abstract = {Deep generative models have recently been applied to molecule design. If the molecules are encoded in linear SMILES strings, modeling becomes convenient. However, models relying on string representations tend to generate invalid samples and duplicates. Prior work addressed these issues by building models on chemically-valid fragments or explicitly enforcing chemical rules in the generation process. We argue that an expressive model is sufficient to implicitly and automatically learn the complicated chemical rules from the data, even if molecules are encoded in simple character-level SMILES strings. We propose to learn latent space energy-based prior model with SMILES representation for molecule modeling. Our experiments show that our method is able to generate molecules with validity and uniqueness competitive with state-of-the-art models. Interestingly, generated molecules have structural and chemical features whose distributions almost perfectly match those of the real molecules.},
  archiveprefix = {arXiv},
  author = {Pang, Bo and Han, Tian and Wu, Ying Nian},
  doi = {10.48550/arXiv.2010.09351},
  eprint = {2010.09351},
  journal = {arXiv preprint arXiv:2010.09351},
  month = {10},
  note = {Combined RBM-style energy functions with latent variable models for molecular generation. Shows how classical RBM ideas extend to modern scientific applications.},
  openalex = {W3093384451},
  pdf = {https://arxiv.org/pdf/2010.09351.pdf},
  primaryclass = {cs.LG},
  title = {Learning Latent Space Energy-Based Prior Model for Molecule Generation},
  url = {https://arxiv.org/abs/2010.09351},
  year = {2020}
}

@inproceedings{roth2020outlier,
  author = {Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Schölkopf, Bernhard and Brox, Thomas and Gehler, Peter},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Compared modern generative models including RBMs for anomaly detection. Found that properly trained RBMs remain competitive, especially when computational resources are limited.},
  openalex = {W3094119435},
  pages = {9549--9559},
  title = {Outlier detection with deep generative models},
  year = {2020}
}

@article{tubiana2019learning,
  abstract = {A restricted Boltzmann machine (RBM) is an unsupervised machine learning bipartite graphical model that jointly learns a probability distribution over data and extracts their relevant statistical features. RBMs were recently proposed for characterizing the patterns of coevolution between amino acids in protein sequences and for designing new sequences. We study how the nature of the features learned by RBM changes with its defining parameters, such as the dimensionality of the representations (size of the hidden layer) and the sparsity of the features.},
  author = {Tubiana, Jérôme and Cocco, Simona and Monasson, Rémi},
  doi = {10.1162/neco_a_01210},
  journal = {Neural Computation},
  month = {8},
  note = {Studies how RBM features change with parameters like representation dimensionality and feature sparsity, comparing RBM performance with other representation learning algorithms for protein sequence analysis.},
  number = {8},
  pages = {1671--1717},
  publisher = {MIT Press},
  title = {Learning Compositional Representations of Interacting Systems with Restricted Boltzmann Machines: Comparative Study of Lattice Proteins},
  url = {https://direct.mit.edu/neco/article/31/8/1671/8486/Learning-Compositional-Representations-of},
  volume = {31},
  year = {2019}
}

@article{mehta2019high,
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias--variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics.},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  doi = {10.1016/j.physrep.2019.03.001},
  journal = {Physics Reports},
  month = {3},
  note = {Comprehensive review connecting RBMs to statistical physics, providing intuition for physicists entering machine learning. Shows how RBM concepts relate to spin systems and phase transitions.},
  openalex = {W3103722330},
  pages = {1--124},
  pdf = {https://www.sciencedirect.com/science/article/pii/S0370157319300766},
  publisher = {Elsevier},
  title = {A high-bias, low-variance introduction to Machine Learning for physicists},
  url = {https://doi.org/10.1016/j.physrep.2019.03.001},
  volume = {810},
  year = {2019}
}

@inproceedings{upadhya2019learning,
  author = {Upadhya, Vidyadhar and Sastry, PS},
  booktitle = {International Conference on Machine Learning (ICML)},
  note = {Developed methods for learning RBMs with sparse hidden representations. This connects RBMs to the broader literature on sparse coding and provides more interpretable features.},
  pages = {6321--6330},
  title = {Learning Boltzmann machines with sparse latent variables},
  year = {2019}
}

@article{yasuda2019learning,
  author = {Yasuda, Muneki},
  journal = {arXiv preprint arXiv:1902.03635},
  note = {Proposed a new sampling method that separately updates visible and hidden units in a way that improves mixing. This addresses one of the fundamental challenges in RBM training.},
  title = {Learning restricted Boltzmann machines with separable Gibbs sampling},
  year = {2019}
}

@article{khoshaman2018quantum,
  abstract = {Variational autoencoders (VAEs) are powerful generative models with the salient ability to perform inference. Here, we introduce a quantum variational autoencoder (QVAE): a VAE whose latent generative process is implemented as a quantum Boltzmann machine (QBM). We show that our model can be trained end-to-end by maximizing a well-defined loss-function: a 'quantum' lower-bound to a variational approximation of the log-likelihood. We use quantum Monte Carlo (QMC) simulations to train and evaluate the performance of QVAEs. To achieve the best performance, we first create a VAE platform with discrete latent space generated by a restricted Boltzmann machine (RBM). Our model achieves state-of-the-art performance on the MNIST dataset when compared against similar approaches that only involve discrete variables in the generative process. We consider QVAEs with a smaller number of latent units to be able to perform QMC simulations, which are computationally expensive. We show that QVAEs can be trained effectively in regimes where quantum effects are relevant despite training via the quantum bound. Our findings open the way to the use of quantum computers to train QVAEs to achieve competitive performance for generative models. Placing a QBM in the latent space of a VAE leverages the full potential of current and next-generation quantum computers as sampling devices.},
  author = {Khoshaman, Amir H. and Vinci, Walter and Denis, Brandon and Andriyash, Evgeny and Sadeghi, Hossein and Amin, Mohammad H.},
  doi = {10.1088/2058-9565/aada1f},
  journal = {Quantum Science and Technology},
  note = {Honorary Mention: Extends RBM concepts to quantum computing, showing how quantum annealing can train Boltzmann machines. Points to future quantum-classical hybrid models.},
  number = {1},
  openalex = {W3101549158},
  pages = {014001},
  publisher = {IOP Publishing},
  title = {Quantum variational autoencoder},
  volume = {4},
  year = {2018}
}

@article{huang2018adversarial,
  author = {Huang, Yunlong and Dai, Xiaolin and Wang, Junwei and Li, Stan Z.},
  doi = {10.1109/TNNLS.2017.2784918},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  note = {Combined adversarial training with DBMs to improve generation quality. This hybrid approach leveraged the complementary strengths of energy-based and adversarial objectives.},
  number = {8},
  pages = {3825--3837},
  title = {Adversarial training of deep Boltzmann machines},
  url = {https://doi.org/10.1109/TNNLS.2017.2784918},
  volume = {29},
  year = {2018}
}

@article{li2018adaptive,
  abstract = {Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks.},
  author = {Li, Yanghao and Wang, Naiyan and Shi, Jianping and Hou, Xiaodi and Liu, Jiaying},
  doi = {10.1016/j.patcog.2018.03.005},
  journal = {Pattern Recognition},
  note = {This paper extended the arXiv preprint from 2016 and was published in Pattern Recognition. Drew connections between batch normalization and the centering techniques used in RBMs. This cross-pollination of ideas shows how RBM research influenced modern deep learning practices.},
  openalex = {W2299668505},
  pages = {109--117},
  pdf = {https://arxiv.org/pdf/1603.04779.pdf},
  publisher = {Elsevier},
  title = {Adaptive Batch Normalization for practical domain adaptation},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S003132031830092X},
  volume = {80},
  year = {2018}
}

@incollection{montufar2018restricted,
  abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables.},
  author = {Montúfar, Guido},
  booktitle = {Information Geometry and Its Applications IV},
  doi = {10.1007/978-3-319-97798-0_4},
  editor = {Ay, Nihat and Gibilisco, Paolo and Matúš, František},
  note = {A modern mathematical treatment of RBMs from an information geometry perspective. Provides deep insights into the geometry of the model manifold and its implications for learning.},
  openalex = {W2963644788},
  pages = {75--115},
  pdf = {https://arxiv.org/pdf/1806.07066},
  publisher = {Springer},
  series = {Springer Proceedings in Mathematics & Statistics},
  title = {Restricted Boltzmann machines: Introduction and review},
  url = {https://doi.org/10.1007/978-3-319-97798-0_4},
  volume = {252},
  year = {2018}
}

@inproceedings{pozas2018multi,
  author = {Pozas-Kerstjens, Alejandro and Muñoz-Gil, Gorka and Piñeiro, Miguel Ángel and Hernández-Garcı́a, Emilio and San-José, Raúl},
  booktitle = {International Conference on Pattern Recognition},
  note = {Explored new ways to use RBMs for multi-class classification, comparing different architectures and training procedures. Shows RBMs can still be competitive for certain classification tasks.},
  pages = {2692--2697},
  title = {Multi-class classification with restricted Boltzmann machines},
  year = {2018}
}

@article{tramel2018deterministic,
  abstract = {Restricted Boltzmann machines (RBMs) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures. In this work, we derive a deterministic framework for the training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer (TAP) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory. While the TAP approach has been extensively studied for fully-visible binary spin systems, our construction is generalized to latent-variable models, as well as to arbitrarily distributed real-valued spin systems with bounded support. In our numerical experiments, we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling. Additionally, we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems.},
  author = {Tramel, Eric W and Gabrié, Marylou and Manoel, Andre and Caltagirone, Francesco and Krzakala, Florent},
  doi = {10.1103/physrevx.8.041006},
  journal = {Physical Review X},
  month = {10},
  note = {Developed a unified framework for RBM training that encompasses CD, PCD, and new deterministic methods. This theoretical framework provides deeper understanding of when and why different training methods work.},
  number = {4},
  openalex = {W3098574683},
  pages = {041006},
  pdf = {https://journals.aps.org/prx/pdf/10.1103/PhysRevX.8.041006},
  publisher = {American Physical Society},
  title = {Deterministic and generalized framework for unsupervised learning with restricted Boltzmann machines},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.8.041006},
  volume = {8},
  year = {2018}
}

@inproceedings{zhang2018eigen,
  author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {Analyzed the representations learned by stacked RBMs using spectral methods. Provided new insights into how hierarchical features emerge during layer-wise training.},
  openalex = {W2963102712},
  pages = {3093--3102},
  title = {Eigen-distortions of hierarchical representations},
  year = {2018}
}

@article{melchior2017gaussian,
  abstract = {We present a theoretical analysis of Gaussian-binary restricted Boltzmann machines (GRBMs) from the perspective of density models. The key aspect of this analysis is to show that GRBMs can be formulated as a constrained mixture of Gaussians, which gives a much better insight into the model's capabilities and limitations. We further show that GRBMs are capable of learning meaningful features without using a regularization term and that the results are comparable to those of independent component analysis. This is illustrated for both a two-dimensional blind source separation task and for modeling natural image patches. Our findings exemplify that reported difficulties in training GRBMs are due to the failure of the training algorithm rather than the model itself. Based on our analysis we derive a better training setup and show empirically that it leads to faster and more robust training of GRBMs. Finally, we compare different sampling algorithms for training GRBMs and show that Contrastive Divergence performs better than training methods that use a persistent Markov chain.},
  author = {Jan Melchior and Nan Wang and Laurenz Wiskott},
  doi = {10.1371/journal.pone.0171015},
  journal = {PLoS ONE},
  month = {2},
  note = {Honorary Mention: Comprehensive study of Gaussian-binary RBMs with practical solutions to training challenges. Includes open-source implementations.},
  number = {2},
  openalex = {W2224454245},
  pages = {e0171015},
  pdf = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0171015&type=printable},
  publisher = {Public Library of Science},
  title = {Gaussian-binary restricted Boltzmann machines for modeling natural image statistics},
  url = {https://doi.org/10.1371/journal.pone.0171015},
  volume = {12},
  year = {2017}
}

@article{tubiana2017emergence,
  abstract = {Extracting automatically the complex set of features composing real high-dimensional data is crucial for achieving high performance in machine learning tasks. Restricted Boltzmann machines (RBM) are empirically known to be efficient for this purpose, and to be able to generate distributed and graded representations of the data. We characterize the structural conditions allowing RBMs to operate in a compositional phase where visible correlations are explained by few hidden units acting independently. Evidence for such a phase is provided by the replica analysis of an ensemble of RBMs and by RBM trained on the handwritten digits dataset MNIST.},
  author = {Tubiana, Jérôme and Monasson, Rémi},
  doi = {10.1103/PhysRevLett.118.138301},
  journal = {Physical Review Letters},
  month = {3},
  note = {Honorary Mention: Used statistical physics to analyze how RBMs learn compositional representations. Provided theoretical insights into feature learning in RBMs.},
  number = {13},
  openalex = {W2556341799},
  pages = {138301},
  pdf = {https://arxiv.org/pdf/1611.06759.pdf},
  publisher = {American Physical Society},
  title = {Emergence of compositional representations in restricted Boltzmann machines},
  url = {https://doi.org/10.1103/PhysRevLett.118.138301},
  volume = {118},
  year = {2017}
}

@inproceedings{jing2017tunable,
  abstract = {Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely $\mathcalO(1)$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.},
  address = {International Convention Centre, Sydney, Australia},
  author = {Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Soljačić, Marin},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  month = {8},
  note = {While focused on RNNs, this work draws inspiration from RBMs' use of energy functions and shows how unitary constraints (similar to those in quantum-inspired RBMs) can improve recurrent models.},
  openalex = {W2952507584},
  pages = {1733--1741},
  pdf = {https://proceedings.mlr.press/v70/jing17a/jing17a.pdf},
  publisher = {Proceedings of Machine Learning Research},
  series = {Proceedings of Machine Learning Research},
  title = {Tunable efficient unitary neural networks (EUNN) and their application to RNNs},
  url = {https://proceedings.mlr.press/v70/jing17a.html},
  volume = {70},
  year = {2017}
}

@inproceedings{kuleshov2017deep,
  abstract = {Machine learning models are traditionally divided into discriminative and generative approaches. Discriminative models often attain higher predictive accuracy, while generative models can be more data efficient. This paper presents a way of defining hybrid models using recent advances in latent variable models and variational inference. In the context of neural networks, this framework gives rise to deep hybrid models that are most effective in a semi-supervised setting, yielding improvements over the state of the art on the SVHN dataset.},
  address = {Sydney, Australia},
  author = {Kuleshov, Volodymyr and Ermon, Stefano},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},
  note = {Proposed combining RBMs with neural networks in hybrid architectures. These models achieve strong performance on both discriminative and generative tasks, showing the continued relevance of RBMs.},
  pages = {1--10},
  pdf = {https://cs.stanford.edu/~ermon/papers/uai2017_cr.pdf},
  publisher = {AUAI Press},
  title = {Deep hybrid models: bridging discriminative and generative approaches},
  url = {http://auai.org/uai2017/proceedings/papers/259.pdf},
  year = {2017}
}

@article{melchior2017binary,
  abstract = {This work provides a comprehensive study of different unit types for Restricted Boltzmann Machines beyond binary variables, including Gaussian, truncated exponential, and beta units. The research offers practical guidelines for choosing appropriate visible units in energy-based models for unsupervised learning applications.},
  archiveprefix = {arXiv},
  author = {Melchior, Jan and Fischer, Asja and Wang, Nan and Wiskott, Laurenz},
  eprint = {1706.06782},
  journal = {arXiv preprint arXiv:1706.06782},
  month = {6},
  note = {Comprehensive study of different unit types for RBMs beyond binary, including Gaussian, truncated exponential, and beta units. Provides practical guidelines for choosing appropriate visible units},
  pdf = {https://arxiv.org/pdf/1706.06782.pdf},
  primaryclass = {cs.LG},
  title = {Binary variables and their extensions for unsupervised learning},
  url = {https://arxiv.org/abs/1706.06782},
  year = {2017}
}

@article{cote2016infinite,
  abstract = {We present a mathematical construction for the restricted Boltzmann machine (RBM) that doesn't require specifying the number of hidden units. In fact, the hidden layer size is adaptive and can grow during training. We achieve this by first extending the RBM to be sensitive to the ordering of its hidden units, and then with a carefully chosen definition of the energy function, we show that the limit of infinitely many hidden units is well defined. We demonstrate empirically that the infinite RBM's performance is competitive to that of the RBM, while not requiring the tuning of a hidden layer size.},
  author = {Côté, Marc-Alexandre and Larochelle, Hugo},
  doi = {10.1162/neco_a_00848},
  journal = {Neural Computation},
  month = {5},
  note = {Revisited the idea of nonparametric RBMs with modern techniques. Used stick-breaking construction to allow the model to grow its capacity as needed during training.},
  number = {7},
  openalex = {W1745681373},
  pages = {1265--1288},
  pdf = {http://arxiv.org/pdf/1502.02476},
  publisher = {MIT Press},
  title = {An infinite restricted Boltzmann machine},
  volume = {28},
  year = {2016}
}

@article{melchior2016centering,
  abstract = {This work analyzes centered Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. The analysis demonstrates that centering results in a different but equivalent parameterization of the same model class, and shows that the expected performance of centered binary BMs is invariant under simultaneous flip of data and offsets. The work reformulates the centering technique as a different update rule and shows that enhanced gradient methods are equivalent to setting offset values to the average over model and data mean. Numerical simulations suggest that centering provides improved generative performance, higher log-likelihood values, and can eliminate the need for layer-wise pre-training in DBMs.},
  author = {Melchior, Jan and Fischer, Asja and Wiskott, Laurenz},
  journal = {Journal of Machine Learning Research},
  note = {Addressed the important but overlooked issue of centering in RBMs and DBMs. Showed that proper centering significantly improves model performance and training stability, providing practical guidelines.},
  number = {99},
  openalex = {W2517166081},
  pages = {99:1--99:61},
  title = {How to center deep Boltzmann machines},
  url = {https://www.jmlr.org/papers/v17/14-237.html},
  volume = {17},
  year = {2016}
}

@article{uria2016neural,
  abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance.},
  author = {Uria, Benigno and Côté, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  journal = {Journal of Machine Learning Research},
  note = {While focused on autoregressive models, this paper includes important comparisons with RBMs and shows how ideas from RBMs influenced modern autoregressive architectures.},
  number = {205},
  openalex = {W2962990490},
  pages = {1--37},
  pdf = {https://www.jmlr.org/papers/volume17/16-272/16-272.pdf},
  title = {Neural Autoregressive Distribution Estimation},
  url = {https://jmlr.org/papers/v17/16-272.html},
  volume = {17},
  year = {2016}
}

@inproceedings{gabrie2015training,
  abstract = {Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function.},
  author = {Gabrié, Marylou and Tramel, Eric W. and Krzakala, Florent},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  note = {Introduced a physics-inspired training method using the TAP free energy approximation. This provides an alternative to CD that can be more accurate for certain model classes.},
  openalex = {W2964071831},
  pages = {640--648},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2015/file/13f3cf8c531952d72e5847c4183e6910-Paper.pdf},
  publisher = {Neural Information Processing Systems Foundation},
  title = {Training restricted Boltzmann machines via the Thouless-Anderson-Palmer free energy},
  volume = {28},
  year = {2015}
}

@inproceedings{goodfellow2014generative,
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  author = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {While introducing GANs, this paper includes important comparisons with RBMs and DBMs. It contextualizes RBMs within the broader landscape of generative models and motivates the need for new approaches.},
  openalex = {W2099471712},
  pages = {2672--2680},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf},
  title = {Generative Adversarial Nets},
  url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/f033ed80deb0234979a61f95710dbe25-Abstract.html},
  volume = {27},
  year = {2014}
}

@article{fischer2014training,
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. They have attracted much attention as building blocks for the multi-layer learning systems called deep belief networks, and variants and extensions of RBMs have found application in a wide range of pattern recognition tasks. This tutorial introduces RBMs from the viewpoint of Markov random fields, starting with the required concepts of undirected graphical models. Different learning algorithms for RBMs, including contrastive divergence learning and parallel tempering, are discussed. As sampling from RBMs, and therefore also most of their learning algorithms, are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and MCMC techniques is provided. Experiments demonstrate relevant aspects of RBM training.},
  author = {Fischer, Asja and Igel, Christian},
  doi = {10.1016/j.patcog.2013.05.025},
  journal = {Pattern Recognition},
  note = {Honorary Mention: Extended tutorial providing practical guidance for RBM training. Includes detailed pseudocode and addresses common implementation issues.},
  number = {1},
  openalex = {W2018168021},
  pages = {25--39},
  pdf = {https://christian-igel.github.io/paper/TRBMAI.pdf},
  publisher = {Elsevier},
  title = {Training restricted Boltzmann machines: An introduction},
  url = {https://doi.org/10.1016/j.patcog.2013.05.025},
  volume = {47},
  year = {2014}
}

@inproceedings{mittelman2014structured,
  abstract = {The Recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic model for temporal data, that has been shown to effectively capture both short and long-term dependencies in time-series. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible and hidden units, therefore ignoring the dependency structure between the different observations. Learning this structure has the potential to not only improve the prediction performance, but it can also reveal important patterns in the data. For example, given an econometric dataset, we could identify interesting dependencies between different market sectors; given a meteorological dataset, we could identify regional weather patterns. In this work we propose a new class of RTRBM, which explicitly uses a dependency graph to model the structure in the problem and to define the energy function. We refer to the new model as the structured RTRBM (SRTRBM). Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with our method to learn structure in datasets with real valued observations. Our experimental results using synthetic and real datasets, demonstrate that the SRTRBM can improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the structure underlying our benchmark datasets.},
  author = {Roni Mittelman and Benjamin Kuipers and Silvio Savarese and Honglak Lee},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  month = {6},
  openalex = {W2145195026},
  pages = {1647--1655},
  pdf = {http://proceedings.mlr.press/v32/mittelman14.pdf},
  series = {Proceedings of Machine Learning Research},
  title = {Structured recurrent temporal restricted Boltzmann machines},
  url = {http://proceedings.mlr.press/v32/mittelman14.html},
  volume = {32},
  year = {2014}
}

@article{bengio2013estimating,
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we ``back-propagate'' through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochastic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochastic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (the straight-through estimator).},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  journal = {arXiv preprint arXiv:1308.3432},
  month = {8},
  note = {While focused on stochastic neurons generally, this paper's analysis of gradient estimation through binary units is highly relevant to RBMs. It compares various estimators and provides insights into the bias-variance tradeoffs in training stochastic networks.},
  openalex = {W2242818861},
  title = {Estimating or propagating gradients through stochastic neurons for conditional computation},
  url = {https://arxiv.org/abs/1308.3432},
  year = {2013}
}

@inproceedings{goodfellow2013multi,
  abstract = {The MP-DBM can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems. Prior methods of training DBMs either do not perform well on classification tasks or require an initial learning pass that trains the DBM greedily, one layer at a time. The MP-DBM does not require greedy layerwise pretraining, and outperforms the standard DBM at classification, classification with missing inputs, and mean field prediction tasks.},
  address = {Lake Tahoe, Nevada, USA},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Courville, Aaron C. and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  month = {12},
  note = {Introduced a DBM variant that can make predictions about multiple parts of the input. This "multi-prediction" training improved the learned representations and provided a form of regularization.},
  openalex = {W2098617596},
  pages = {548--556},
  publisher = {Neural Information Processing Systems Foundation},
  series = {NIPS},
  title = {Multi-prediction deep Boltzmann machines},
  url = {https://papers.nips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf},
  volume = {26},
  year = {2013}
}

@inproceedings{grosse2013annealing,
  abstract = {Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions. We derive an asymptotically optimal piecewise linear schedule and demonstrate the effectiveness of moment averaging in estimating partition functions for restricted Boltzmann machines (RBMs).},
  author = {Grosse, Roger B. and Maddison, Chris J. and Salakhutdinov, Ruslan},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  openalex = {W2131939418},
  pages = {2769--2777},
  pdf = {https://papers.nips.cc/paper_files/paper/2013/file/b337e84de8752b27eda3a12363109e80-Paper.pdf},
  title = {Annealing between distributions by averaging moments},
  year = {2013}
}

@inproceedings{cho2013gaussian,
  abstract = {In this paper, we study a model that we call Gaussian-Bernoulli deep Boltzmann machine (GDBM) and discuss potential improvements in training the model. GDBM is designed to be applicable to continuous data and it is constructed from Gaussian-Bernoulli restricted Boltzmann machine (GRBM) by adding multiple layers of binary hidden neurons. The studied improvements of the learning algorithm for GDBM include parallel tempering, enhanced gradient, adaptive learning rate and layer-wise pretraining. We empirically show that they help avoid some of the common difficulties found in training deep Boltzmann machines such as divergence of learning, the difficulty in choosing right learning rate scheduling, and the existence of meaningless higher layers.},
  address = {Dallas, TX, USA},
  author = {Cho, Kyung Hyun and Raiko, Tapani and Ilin, Alexander},
  booktitle = {2013 International Joint Conference on Neural Networks (IJCNN)},
  doi = {10.1109/IJCNN.2013.6706831},
  month = {8},
  note = {Honorary Mention: Extended DBMs to handle real-valued visible units effectively. Provided stable training procedures that made Gaussian-Bernoulli DBMs practical.},
  openalex = {W2054912984},
  pages = {1--7},
  pdf = {http://www.cis.hut.fi/praiko/papers/nips11Cho.pdf},
  publisher = {IEEE},
  title = {Gaussian-Bernoulli Deep Boltzmann Machine},
  url = {https://doi.org/10.1109/IJCNN.2013.6706831},
  year = {2013}
}

@article{huang2013unsupervised,
  abstract = {One of the main challenges of current machine learning approaches lies in learning from very few labeled examples -- while humans excel at learning from few examples. Recent advances in machine learning, object recognition and computer graphics suggest that the key may be the development of deep architectures and the unsupervised learning of intermediate representations. Here we present an approach based on the unsupervised, automatic learning of a ``good'' representation for supervised learning, characterized by small sample complexity. The starting point is the conjecture, proved in specific cases, that image representations which are invariant to translation, scaling and other transformations can considerably reduce the sample complexity of learning. A module performing filtering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such estimates. Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inherit its properties of invariance, stability, and discriminability while capturing the compositional organization of the visual world in terms of wholes and parts.},
  archiveprefix = {arXiv},
  author = {Anselmi, Fabio and Leibo, Joel Z. and Rosasco, Lorenzo and Mutch, Jim and Tacchetti, Andrea and Poggio, Tomaso},
  eprint = {1311.4158},
  journal = {arXiv preprint arXiv:1311.4158},
  month = {11},
  note = {WARNING: Original entry had incorrect authors (Huang, Yan and Rao, Rajesh P. N.). Corrected to actual authors. arXiv:1311.4158 corresponds to this work by Anselmi et al. Honorary Mention: Connected RBMs to neuroscience, showing how they relate to theories of cortical computation. Bridges machine learning and computational neuroscience.},
  openalex = {W1530733563},
  pdf = {https://arxiv.org/pdf/1311.4158.pdf},
  primaryclass = {cs.CV},
  title = {Unsupervised Learning of Invariant Representations in Hierarchical Architectures},
  url = {https://arxiv.org/abs/1311.4158},
  year = {2013}
}

@inproceedings{krause2013mean,
  author = {Krause, Oswin and Fischer, Asja and Müller, Klaus-Robert and Igel, Christian},
  booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2013},
  doi = {10.1007/978-3-642-40728-4_66},
  editor = {Mladenov, Valeri and Koprinkova-Hristova, Petia and Palm, Günther and Villa, Alessandro E. P. and Appollini, Bruno and Kasabov, Nikola},
  location = {Sofia, Bulgaria},
  month = {9},
  note = {Developed improved mean field approximations for DBMs, leading to better inference and learning. The enhanced mean field equations provided more accurate approximations to the true posteriors.},
  openalex = {W108925251},
  pages = {530--537},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Mean field for continuous high-order Boltzmann machines},
  url = {https://doi.org/10.1007/978-3-642-40728-4_66},
  volume = {8131},
  year = {2013}
}

@article{dahl2012context,
  abstract = {We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We show how to use backpropagation to fine-tune these generatively pre-trained DNNs for discriminative training criterion. Using this technique, we report phone recognition results that are comparable to the current state of the art on the TIMIT database. More importantly, we show that this approach can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs on a variety of large-vocabulary speech recognition tasks. On the Bing voice search task, our system achieved a 23% relative reduction in word error rate.},
  author = {George E. Dahl and Dong Yu and Li Deng and Alex Acero},
  doi = {10.1109/TASL.2011.2134090},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  month = {1},
  note = {Demonstrated that DBN pretraining with RBMs could significantly improve deep neural networks for speech recognition. This was one of the key papers that brought deep learning to speech processing.},
  number = {1},
  openalex = {W2147768505},
  pages = {30--42},
  pdf = {https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf},
  title = {Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition},
  volume = {20},
  year = {2012}
}

@inproceedings{fischer2012introduction,
  abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
  address = {Berlin, Heidelberg},
  author = {Fischer, Asja and Igel, Christian},
  booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
  doi = {10.1007/978-3-642-33275-3_2},
  editor = {Alvarez, Luis and Mejail, Marta and Gomez, Luis and Jacobo, Julio},
  isbn = {978-3-642-33275-3},
  location = {Buenos Aires, Argentina},
  note = {A comprehensive tutorial on RBMs covering theory, training algorithms, and practical considerations. Became a standard reference for newcomers to the field.},
  openalex = {W2202505358},
  pages = {14--36},
  pdf = {https://christian-igel.github.io/paper/AItRBM-proof.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {An introduction to restricted Boltzmann machines},
  volume = {7441},
  year = {2012}
}

@article{hinton2012improving,
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This 'overfitting' is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random 'dropout' gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  journal = {arXiv preprint arXiv:1207.0580},
  month = {7},
  note = {Introduced dropout, inspired partially by the noise in RBM training. While dropout became more associated with standard neural networks, its connection to RBMs provided theoretical motivation for this revolutionary regularization technique.},
  openalex = {W1904365287},
  pdf = {https://arxiv.org/pdf/1207.0580.pdf},
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  url = {https://arxiv.org/abs/1207.0580},
  year = {2012}
}

@article{mohamed2012acoustic,
  abstract = {Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.},
  author = {Mohamed, Abdel-rahman and Dahl, George E. and Hinton, Geoffrey},
  doi = {10.1109/TASL.2011.2109382},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  note = {One of the first papers to show that DBNs pretrained with RBMs could outperform GMM-HMM systems for phone recognition. This helped catalyze the deep learning revolution in speech recognition.},
  number = {1},
  openalex = {W1993882792},
  pages = {14--22},
  title = {Acoustic modeling using deep belief networks},
  url = {https://doi.org/10.1109/TASL.2011.2109382},
  volume = {20},
  year = {2012}
}

@inproceedings{bengio2012deep,
  abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution $P(x)$ is structurally related to some task of interest, say predicting $P(y|x)$. This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
  author = {Bengio, Yoshua},
  booktitle = {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  editor = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  note = {Explored using features learned by deep architectures including RBMs for transfer learning across different tasks and domains. Showed that unsupervised representations often transfer better than supervised features.},
  openalex = {W2295582178},
  pages = {17--36},
  pdf = {https://proceedings.mlr.press/v27/bengio12a/bengio12a.pdf},
  series = {Proceedings of Machine Learning Research},
  title = {Deep learning of representations for unsupervised and transfer learning},
  url = {https://proceedings.mlr.press/v27/bengio12a.html},
  volume = {27},
  year = {2012}
}

@inproceedings{srivastava2012multimodal,
  abstract = {A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a unified representation that fuses modalities together.},
  author = {Srivastava, Nitish and Salakhutdinov, Ruslan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  month = {12},
  note = {Showed how DBMs could learn joint representations across different modalities (text and images). The model could generate one modality given the other and achieved state-of-the-art results on multimodal tasks, demonstrating RBMs' flexibility.},
  openalex = {W2164587673},
  pages = {2222--2230},
  publisher = {Curran Associates, Inc.},
  series = {NIPS},
  title = {Multimodal learning with deep Boltzmann machines},
  url = {https://proceedings.neurips.cc/paper/2012/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html},
  volume = {25},
  year = {2012}
}

@inproceedings{tang2012robust,
  abstract = {While Boltzmann Machines have been successful at unsupervised learning and density modeling of images, speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, Robust Boltzmann Machine (RoBM), which allows robust corruptions.},
  author = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey E.},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/cvpr.2012.6247936},
  note = {Introduced Robust RBMs that explicitly model noise and corruption. By using a mixture model in the visible layer, these RBMs can simultaneously recognize and denoise corrupted inputs.},
  openalex = {W2054814877},
  pages = {2264--2271},
  title = {Robust Boltzmann machines for recognition and denoising},
  url = {http://www.cs.utoronto.ca/~hinton/absps/robm.pdf},
  year = {2012}
}

@inproceedings{cho2011improved,
  abstract = {We propose a few remedies to improve training of Gaussian-Bernoulli restricted Boltzmann machines (GBRBM), which is known to be difficult. Firstly, we use a different parameterization of the energy function, which allows for more intuitive interpretation of the parameters and facilitates learning. Secondly, we propose parallel tempering learning for GBRBM. Lastly, we use an adaptive learning rate which is selected automatically in order to stabilize training.},
  address = {Berlin, Heidelberg},
  author = {Kyunghyun Cho and Alexander Ilin and Tapani Raiko},
  booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
  doi = {10.1007/978-3-642-21735-7_2},
  editor = {Timo Honkela and Włodzisław Duch and Mark Girolami and Samuel Kaski},
  isbn = {978-3-642-21734-0},
  note = {Addressed the notoriously difficult problem of training Gaussian-Bernoulli RBMs. Proposed improved learning rules and initialization schemes that made these models more stable and practical for real-valued data.},
  openalex = {W2136936677},
  pages = {10--17},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Improved Learning of Gaussian-Bernoulli Restricted Boltzmann Machines},
  url = {https://doi.org/10.1007/978-3-642-21735-7_2},
  volume = {6791},
  year = {2011}
}

@inproceedings{courville2011spike,
  abstract = {The spike-and-slab restricted Boltzmann machine (ssRBM) is defined to have both a real-valued 'slab' variable and binary 'spike' variable associated with each unit in the hidden layer.},
  author = {Courville, Aaron and Bergstra, James and Bengio, Yoshua},
  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  editor = {Gordon, Geoffrey and Dunson, David and Dudı́k, Miroslav},
  openalex = {W2049604094},
  pages = {233--241},
  pdf = {http://proceedings.mlr.press/v15/courville11a/courville11a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A spike and slab restricted Boltzmann machine},
  url = {https://proceedings.mlr.press/v15/courville11a.html},
  volume = {15},
  year = {2011}
}

@inproceedings{nair2011discriminative,
  address = {Bellevue, WA, USA},
  author = {Vinod Nair and Geoffrey E. Hinton},
  booktitle = {ICML 2011 Workshop on Learning Architectures, Representations, and Optimization for Speech and Visual Information Processing},
  month = {7},
  note = {Honorary Mention: Proved that discriminative RBMs can approximate any function on discrete data. Important theoretical result for understanding RBM expressiveness.},
  organization = {International Conference on Machine Learning},
  title = {Discriminative Restricted Boltzmann Machines are Universal Approximators for Discrete Data},
  year = {2011}
}

@inproceedings{montufar2011refinements,
  abstract = {We improve recently published results about resources of Restricted Boltzmann Machines (RBM) and Deep Belief Networks (DBN) required to make them Universal Approximators. We show that any distribution p on the set of binary vectors of length n can be arbitrarily well approximated by an RBM with k-1 hidden units, where k is the minimal number of pairs of binary vectors differing in only one entry such that their union contains the support set of p. In important cases this number is half of the cardinality of the support set of p. We construct a DBN with 2^n/2(n-b), b ~ log(n), hidden layers of width n that is capable of approximating any distribution on 0,1^n arbitrarily well. This confirms a conjecture presented by Le Roux and Bengio 2010.},
  address = {Espoo, Finland},
  author = {Guido Montúfar and Nihat Ay},
  booktitle = {Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011. Proceedings},
  doi = {10.1007/978-3-642-24412-4_13},
  editor = {Jyrki Kivinen and Csaba Szepesvári and Esko Ukkonen and Thomas Zeugmann},
  month = {10},
  note = {Analyzed the number of linear regions that can be carved out by deep networks, including those initialized with RBMs. Showed that deep architectures can be exponentially more efficient than shallow ones for representing certain functions.},
  openalex = {W2093559122},
  pages = {120--134},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines},
  url = {https://arxiv.org/pdf/1005.1593},
  volume = {6925},
  year = {2011}
}

@inproceedings{desjardins2010paralleltempered,
  abstract = {Alternating Gibbs sampling is the most common scheme used for sampling from Restricted Boltzmann Machines (RBM), a crucial component in deep architectures such as Deep Belief Networks. However, we find that it often does a very poor job of rendering the diversity of modes captured by the trained model. We suspect that this hinders the advantage that could in principle be brought by training algorithms relying on Gibbs sampling for uncovering spurious modes, such as the Persistent Contrastive Divergence algorithm. To alleviate this problem, we explore the use of tempered Markov Chain Monte-Carlo for sampling in RBMs. We find both through visualization of samples and measures of likelihood on a toy dataset that it helps both sampling and learning.},
  author = {Desjardins, Guillaume and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal and Delalleau, Olivier},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  editor = {Teh, Yee Whye and Titterington, Mike},
  openalex = {W2184970173},
  pages = {145--152},
  pdf = {http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines},
  url = {https://proceedings.mlr.press/v9/desjardins10a.html},
  volume = {9},
  year = {2010}
}

@article{erhan2010pretraining,
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer and as an aid to optimization. Our results build on previous work, showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  journal = {Journal of Machine Learning Research},
  number = {19},
  openalex = {W2138857742},
  pages = {625--660},
  pdf = {https://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf},
  title = {Why does unsupervised pre-training help deep learning?},
  url = {https://jmlr.csail.mit.edu/papers/v11/erhan10a.html},
  volume = {11},
  year = {2010}
}

@techreport{hinton2010practical,
  author = {Hinton, Geoffrey E.},
  institution = {Department of Computer Science, University of Toronto},
  month = {8},
  note = {An invaluable technical report distilling years of practical experience into concrete advice for training RBMs. Covers learning rates, momentum, weight decay, sparsity, and other crucial hyperparameters. Became the go-to reference for practitioners. Later published as a book chapter in ``Neural Networks: Tricks of the Trade'' (LNCS 7700, Springer, 2012).},
  number = {UTML TR 2010-003},
  openalex = {W44815768},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf},
  title = {A practical guide to training restricted Boltzmann machines},
  type = {Technical Report},
  year = {2010}
}

@inproceedings{long2010restricted,
  abstract = {We establish the intractability of two basic computational tasks involving Restricted Boltzmann Machines (RBMs), even if only a coarse approximation to the correct output is required. We show that, assuming P $ eq$ NP, for any fixed positive constant K (which may be arbitrarily large) there is no polynomial-time algorithm for the following problem: given an $n$-bit input string $x$ and the parameters of a RBM $M$, output an estimate of the probability assigned to $x$ by $M$ that is accurate to within a multiplicative factor of $e^Kn$. We also show that, under the assumption RP $ eq$ NP, there is no polynomial-time randomized algorithm that can generate random examples from a RBM's probability distribution over the Boolean cube that are guaranteed to come within a small variation distance of the true distribution.},
  address = {Haifa, Israel},
  author = {Long, Philip M. and Servedio, Rocco A.},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning},
  month = {6},
  note = {Provided computational complexity results showing that exactly computing the partition function of an RBM is #P-hard, and even approximating it is NP-hard. This theoretical result explains why approximate methods like CD are necessary.},
  openalex = {W2145392412},
  pages = {703--710},
  publisher = {Omnipress},
  title = {Restricted Boltzmann machines are hard to approximately evaluate or simulate},
  url = {https://icml.cc/Conferences/2010/papers/115.pdf},
  year = {2010}
}

@inproceedings{marlin2010inductive,
  abstract = {Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper, we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood, contrastive divergence, and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.},
  author = {Benjamin M. Marlin and Kevin Swersky and Bo Chen and Nando de Freitas},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  editor = {Yee Whye Teh and Mike Titterington},
  month = {5},
  openalex = {W2171282218},
  pages = {509--516},
  pdf = {http://proceedings.mlr.press/v9/marlin10a/marlin10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Inductive principles for restricted Boltzmann machine learning},
  url = {http://proceedings.mlr.press/v9/marlin10a.html},
  volume = {9},
  year = {2010}
}

@inproceedings{nair2010rectified,
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  address = {Haifa, Israel},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 27th International Conference on Machine Learning},
  editor = {Fürnkranz, Johannes and Joachims, Thorsten},
  month = {6},
  note = {Introduced ReLU units to RBMs, showing they could learn better features than traditional sigmoid units. This was an early exploration of ReLUs, which would later become standard in deep learning.},
  openalex = {W1665214252},
  pages = {807--814},
  publisher = {Omnipress},
  series = {ICML '10},
  title = {Rectified linear units improve restricted Boltzmann machines},
  url = {https://icml.cc/Conferences/2010/papers/432.pdf},
  year = {2010}
}

@inproceedings{ranzato2010factored,
  abstract = {Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. We show that this model can learn binary features for object recognition and that it gets better object recognition than competing methods.},
  author = {Marc'Aurelio Ranzato and Alex Krizhevsky and Geoffrey E. Hinton},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  month = {3},
  note = {Introduced factored 3-way RBMs that model the covariance structure of images using three-way interactions. This gated architecture inspired later developments in multiplicative interactions and gating mechanisms.},
  openalex = {W2161000554},
  pages = {621--628},
  pdf = {http://proceedings.mlr.press/v9/ranzato10a/ranzato10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Factored 3-way restricted Boltzmann machines for modeling natural images},
  url = {http://proceedings.mlr.press/v9/ranzato10a.html},
  volume = {9},
  year = {2010}
}

@inproceedings{salakhutdinov2010efficient,
  abstract = {We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM's), a generative model with many layers of hidden variables. Our algorithm learns a separate "recognition" model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We further demonstrate that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. The inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM's practical.},
  address = {Chia Laguna Resort, Sardinia, Italy},
  author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  editor = {Teh, Yee Whye and Titterington, Mike},
  month = {5},
  openalex = {W177847060},
  pages = {693--700},
  pdf = {http://proceedings.mlr.press/v9/salakhutdinov10a/salakhutdinov10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient learning of deep Boltzmann machines},
  url = {http://proceedings.mlr.press/v9/salakhutdinov10a.html},
  volume = {9},
  year = {2010}
}

@inproceedings{sutskever2010convergence,
  abstract = {Contrastive Divergence (CD) is a popular method for estimating the parameters of Markov Random Fields (MRFs) by rapidly approximating an intractable term in the gradient of the log probability. Despite CD's empirical success, little is known about its theoretical convergence properties. In this paper, we analyze the CD₁ update rule for Restricted Boltzmann Machines (RBMs) with binary variables. We show that this update is not the gradient of any function, and construct a counterintuitive 'regularization function' that causes CD learning to cycle indefinitely. Nonetheless, we show that the regularized CD update has a fixed point for a large class of regularization functions using Brower's fixed point theorem.},
  author = {Ilya Sutskever and Tijmen Tieleman},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  note = {Showed that CD is not the gradient of any function, explaining some of its peculiar properties. Proved that CD can lead to spurious fixed points and characterized conditions under which it converges to maximum likelihood solutions.},
  openalex = {W1511867968},
  pages = {789--795},
  pdf = {http://proceedings.mlr.press/v9/sutskever10a/sutskever10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Convergence Properties of Contrastive Divergence},
  url = {https://proceedings.mlr.press/v9/sutskever10a.html},
  volume = {9},
  year = {2010}
}

@article{bengio2009learning,
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas.},
  author = {Bengio, Yoshua},
  doi = {10.1561/2200000006},
  journal = {Foundations and Trends® in Machine Learning},
  month = {11},
  note = {Honorary Mention: Comprehensive monograph on deep learning with extensive coverage of RBMs and DBNs. Provided theoretical foundations and practical insights that shaped the field.},
  number = {1},
  openalex = {W4231109964},
  pages = {1--127},
  pdf = {https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf},
  publisher = {now publishers inc.},
  title = {Learning Deep Architectures for AI},
  url = {https://doi.org/10.1561/2200000006},
  volume = {2},
  year = {2009}
}

@inproceedings{lee2009convolutional,
  abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. We demonstrate that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We show excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
  address = {Montreal, Quebec, Canada},
  author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  doi = {10.1145/1553374.1553453},
  editor = {Danyluk, Andrea Pohoreckyj and Bottou, Léon and Littman, Michael L.},
  isbn = {978-1-60558-516-1},
  month = {6},
  note = {Introduced Convolutional RBMs, incorporating weight sharing and spatial pooling. This allowed RBMs to scale to full-sized images and learn hierarchical features, bridging RBMs with convolutional architectures.},
  openalex = {W2130325614},
  pages = {609--616},
  pdf = {https://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
  url = {https://dl.acm.org/doi/10.1145/1553374.1553453},
  volume = {382},
  year = {2009}
}

@inproceedings{salakhutdinov2009deep,
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pre-training phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.},
  author = {Ruslan Salakhutdinov and Geoffrey E. Hinton},
  booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  note = {Extended RBMs to Deep Boltzmann Machines (DBMs) with multiple hidden layers and undirected connections throughout. Introduced a variational learning algorithm and showed that pretraining with RBMs was crucial for learning good DBM models.},
  openalex = {W189596042},
  pages = {448--455},
  pdf = {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Boltzmann Machines},
  url = {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
  volume = {5},
  year = {2009}
}

@inproceedings{ranzato2008semi,
  abstract = {Finding good representations of text documents is crucial in information retrieval and classification systems. The most popular document representation is based on a vector of word counts. This representation does not capture dependencies between related words nor does it handle synonyms or polysemous words. We propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents while retaining as much class information and joint word statistics as possible.},
  author = {Ranzato, Marc'Aurelio and Szummer, Martin},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  doi = {10.1145/1390156.1390256},
  isbn = {978-1-60558-205-4},
  location = {Helsinki, Finland},
  note = {Honorary Mention: Demonstrated how RBM pretraining could be combined with supervised fine-tuning for document modeling. Influenced many subsequent semi-supervised approaches.},
  openalex = {W2162262658},
  pages = {792--799},
  pdf = {https://icml.cc/Conferences/2008/papers/611.pdf},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {Semi-supervised learning of compact document representations with deep networks},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390256},
  volume = {307},
  year = {2008}
}

@inproceedings{larochelle2008classification,
  abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another algorithm or to provide good initialization for deep feed-forward neural network classifiers, and not considered standalone solution classification problems. In this paper, we argue that RBMs can be a self-contained framework deriving competitive non-linear classifiers. We present an evaluation of different algorithms which aim at introducing a discriminative component in RBM training to improve their performance as classifiers. This approach is simple in directly building a classifier, rather than a stepping stone. Finally, we demonstrate how RBMs can also be successfully employed in a semi-supervised setting.},
  address = {Helsinki, Finland},
  author = {Hugo Larochelle and Yoshua Bengio},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  doi = {10.1145/1390156.1390224},
  note = {Showed how to use RBMs discriminatively for classification by joining the label units with the visible units. This hybrid generative-discriminative approach often outperformed purely discriminative methods.},
  openalex = {W1964155876},
  pages = {536--543},
  pdf = {https://icml.cc/Conferences/2008/papers/601.pdf},
  publisher = {ACM},
  series = {ICML '08},
  title = {Classification using discriminative restricted Boltzmann machines},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390224},
  year = {2008}
}

@article{leroux2008representational,
  abstract = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.},
  author = {Le Roux, Nicolas and Bengio, Yoshua},
  doi = {10.1162/neco.2008.04-07-510},
  journal = {Neural Computation},
  month = {6},
  note = {Proved important theoretical results about RBM expressiveness: RBMs are universal approximators for discrete distributions, but may require exponentially many hidden units. Also showed that deep but narrow networks cannot efficiently represent certain functions that shallow networks can.},
  number = {6},
  openalex = {W2064630666},
  pages = {1631--1649},
  pdf = {https://nicolas.le-roux.name/publications/LeRoux08_rbm.pdf},
  title = {Representational power of restricted Boltzmann machines and deep belief networks},
  url = {https://doi.org/10.1162/neco.2008.04-07-510},
  volume = {20},
  year = {2008}
}

@inproceedings{schmidt2008structure,
  abstract = {Coronary Heart Disease can be diagnosed by assessing the regional motion of the heart walls in ultrasound images of the left ventricle. Even for experts, ultrasound images are difficult to interpret leading to high intra-observer variability. Previous work indicates that in order to approach this problem, the interactions between the different heart regions and their overall influence on the clinical condition of the heart need to be considered. To do this, we propose a method for jointly learning the structure and parameters of conditional random fields, formulating these tasks as a convex optimization problem.},
  address = {Anchorage, AK, USA},
  author = {Mark Schmidt and Kevin Murphy and Glenn Fung and Rómer Rosales},
  booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2008.4587367},
  isbn = {978-1-4244-2242-5},
  month = {6},
  note = {Applied RBMs to medical imaging, specifically cardiac motion analysis. Demonstrated that RBMs could learn clinically relevant features for detecting abnormalities.},
  openalex = {W2116294498},
  pages = {1--8},
  publisher = {IEEE},
  title = {Structure Learning in Random Fields for Heart Motion Abnormality Detection},
  url = {https://ieeexplore.ieee.org/document/4587367},
  year = {2008}
}

@inproceedings{sutskever2008temporal,
  abstract = {The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e. generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.},
  author = {Sutskever, Ilya and Hinton, Geoffrey E. and Taylor, Graham W.},
  booktitle = {Advances in Neural Information Processing Systems 21},
  note = {Extended temporal RBMs with recurrent connections for better sequence modeling. Showed how to train these models efficiently and demonstrated improved performance on motion capture and video data.},
  openalex = {W2135341757},
  pages = {1601--1608},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf},
  title = {The recurrent temporal restricted Boltzmann machine},
  url = {https://papers.nips.cc/paper/3567-the-recurrent-temporal-restricted-boltzmann-machine},
  year = {2008}
}

@inproceedings{tieleman2008training,
  abstract = {A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples almost exactly from the model distribution. It is compared to Contrastive Divergence and Pseudo-Likelihood on tasks of modeling and classifying various types of data.},
  address = {New York, NY, USA},
  author = {Tieleman, Tijmen},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  doi = {10.1145/1390156.1390290},
  isbn = {978-1-60558-205-4},
  note = {Introduced Persistent Contrastive Divergence (PCD), where Markov chains are not reinitialized between updates but run continuously. This reduces the bias of CD and often leads to better models, becoming a standard variant of CD training.},
  openalex = {W2116825644},
  pages = {1064--1071},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {Training restricted Boltzmann machines using approximations to the likelihood gradient},
  url = {https://icml.cc/Conferences/2008/papers/638.pdf},
  volume = {307},
  year = {2008}
}

@inproceedings{bengio2007greedy,
  abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities, allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently, it was not clear how to train such networks, since gradient-based optimization starting from random initialization often gets stuck in poor solutions. Hinton et al. introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks, a generative model with layers of hidden causal variables. The authors study this empirically, explore variants, and better understand its success, extending to cases where inputs are continuous or the input distribution is revealing enough about the supervised task. Their experiments confirm that this training strategy helps optimization by initializing weights near good local minima, giving rise to internal distributed representations with high-level abstractions of input, bringing generalization.},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle = {Advances in Neural Information Processing Systems},
  doi = {10.7551/mitpress/7503.003.0024},
  note = {Provided theoretical justification for why greedy layer-wise pretraining with RBMs works. Connected this procedure to variational bounds and helped explain the empirical success of DBN pretraining.},
  openalex = {W2110798204},
  pages = {153--160},
  pdf = {https://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf},
  publisher = {MIT Press},
  series = {NIPS 2006},
  title = {Greedy Layer-Wise Training of Deep Networks},
  url = {https://papers.nips.cc/paper/2007/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
  volume = {19},
  year = {2007}
}

@inproceedings{grosse2007shift,
  abstract = {Sparse coding is an unsupervised learning algorithm that learns a succinct high-level representation of the inputs given only unlabeled data; it represents each input as a sparse linear combination of a set of basis functions. Originally applied to modeling the human visual cortex, sparse coding has also been shown to be useful for self-taught learning, in which the goal is to solve a supervised classification task given access to additional unlabeled data drawn from different classes than that in the supervised learning problem. Shift-invariant sparse coding (SISC) is an extension of sparse coding which reconstructs a (usually time-series) input using all of the basis functions in all possible shifts. In this paper, we present an efficient algorithm for learning SISC bases. Our method is based on iteratively solving two large convex optimization problems: The first, which computes the linear coefficients, is an L1-regularized linear least squares problem with potentially hundreds of thousands of variables. Existing methods typically use a heuristic to select a small subset of the variables to optimize, but we present a way to efficiently compute the exact solution. The second, which solves for bases, is a constrained linear least squares problem. By optimizing over complex-valued variables in the Fourier domain, we reduce the coupling between the different variables, allowing the problem to be solved efficiently. We show that SISC's learned high-level representations of speech and music provide useful features for classification tasks within those domains. When applied to classification, under certain conditions the learned features outperform state of the art spectral and cepstral features.},
  address = {Vancouver, BC, Canada},
  author = {Grosse, Roger and Raina, Rajat and Kwong, Helen and Ng, Andrew Y.},
  booktitle = {Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence},
  isbn = {0-9749039-3-0},
  month = {7},
  note = {Proposed shift-invariant sparse coding for audio classification. While not directly about RBMs, the techniques developed here influenced later work on convolutional RBMs.},
  openalex = {W2963909185},
  pages = {149--158},
  pdf = {https://www.cs.toronto.edu/~rgrosse/uai07-sisc.pdf},
  publisher = {AUAI Press},
  title = {Shift-invariant sparse coding for audio classification},
  url = {https://www.cs.toronto.edu/~rgrosse/uai07-sisc.pdf},
  year = {2007}
}

@inproceedings{louradour2007segmentation,
  author = {Louradour, Jérôme and Kermorvant, Christopher},
  booktitle = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
  doi = {10.1109/ICDAR.2007.4378748},
  isbn = {978-0-7695-2822-9},
  note = {Applied RBM-based features to document analysis, showing they could capture the structure of handwritten text. Demonstrated RBMs' utility beyond traditional vision tasks.},
  openalex = {W2153166076},
  pages = {407--411},
  publisher = {IEEE},
  title = {Segmentation of handwritten document images using a 2D conditional random field model},
  volume = {1},
  year = {2007}
}

@inproceedings{memisevic2007unsupervised,
  abstract = {We describe a probabilistic model for learning rich, distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation, given a pair of images, and can be performed exactly and efficiently. When trained on natural videos, the model develops domain specific motion features, in the form of fields of locally transformed edge filters.},
  address = {Minneapolis, MN, USA},
  author = {Memisevic, Roland and Hinton, Geoffrey E.},
  booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2007.383036},
  isbn = {1-4244-1179-3},
  month = {6},
  note = {Introduced Gated RBMs for learning relations between images, such as transformations. The model uses multiplicative interactions to capture how pixels change, pioneering the use of gating in neural networks.},
  openalex = {W2120190345},
  pages = {1--8},
  publisher = {IEEE},
  title = {Unsupervised Learning of Image Transformations},
  url = {https://doi.org/10.1109/CVPR.2007.383036},
  year = {2007}
}

@inproceedings{mnih2007three,
  abstract = {The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations.},
  author = {Mnih, Andriy and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning},
  doi = {10.1145/1273496.1273577},
  editor = {Ghahramani, Zoubin},
  isbn = {978-1-59593-793-3},
  location = {Corvallis, Oregon, USA},
  note = {Introduced the Replicated Softmax model, an RBM variant for modeling word counts in documents. This undirected alternative to LDA showed competitive performance and could be trained efficiently with CD.},
  openalex = {W2091812280},
  pages = {641--648},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/threenew.pdf},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {Three New Graphical Models for Statistical Language Modelling},
  url = {https://dl.acm.org/doi/10.1145/1273496.1273577},
  volume = {227},
  year = {2007}
}

@inproceedings{salakhutdinov2007restricted,
  abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBMs), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBMs can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBMs slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.},
  author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey E.},
  booktitle = {Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis, Oregon, USA, June 20-24, 2007},
  doi = {10.1145/1273496.1273596},
  editor = {Ghahramani, Zoubin},
  isbn = {978-1-59593-793-3},
  note = {Applied RBMs to the Netflix Prize challenge, achieving state-of-the-art results. This demonstrated RBMs' practical utility for real-world problems with millions of parameters and sparse, high-dimensional data. It popularized RBMs beyond the machine learning research community.},
  openalex = {W2099866409},
  pages = {791--798},
  pdf = {https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf},
  publisher = {ACM},
  series = {ACM International Conference Proceeding Series},
  title = {Restricted Boltzmann machines for collaborative filtering},
  url = {https://dl.acm.org/doi/10.1145/1273496.1273596},
  volume = {227},
  year = {2007}
}

@inproceedings{gehler2006rate,
  abstract = {Probabilistic modelling of text data in the bag-of-words representation has been dominated by directed graphical models such as pLSI, LDA, NMF, and discrete PCA. Recently, state of the art performance on visual object recognition has also been reported using variants of these models. We introduce an alternative undirected graphical model suitable for modelling count data. This 'Rate Adapting Poisson' (RAP) model is shown to generate superior dimensionally reduced representations for subsequent retrieval or classification. Models are trained using contrastive divergence while inference of latent topical representations is efficiently achieved through a simple matrix multiplication.},
  address = {Pittsburgh, Pennsylvania, USA},
  author = {Gehler, Peter V. and Holub, Alex D. and Welling, Max},
  booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
  doi = {10.1145/1143844.1143887},
  month = {6},
  note = {Developed Poisson RBMs for modeling count data, important for text and image applications. The model uses Poisson visible units and requires special training procedures, extending RBMs beyond binary and Gaussian units.},
  openalex = {W2079968649},
  pages = {337--344},
  publisher = {ACM},
  series = {ICML '06},
  title = {The rate adapting Poisson model for information retrieval and object recognition},
  url = {https://dl.acm.org/doi/10.1145/1143844.1143887},
  year = {2006}
}

@article{hinton2006fast,
  abstract = {We show how to use ``complementary priors'' to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels.},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  doi = {10.1162/neco.2006.18.7.1527},
  journal = {Neural Computation},
  month = {7},
  note = {Introduced Deep Belief Networks (DBNs) - stacks of RBMs trained greedily layer by layer. The paper proved that each added layer improves a variational bound on the data likelihood. DBNs were the first tractable deep generative models and sparked renewed interest in deep learning.},
  number = {7},
  openalex = {W2136922672},
  pages = {1527--1554},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf},
  publisher = {MIT Press},
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  url = {https://doi.org/10.1162/neco.2006.18.7.1527},
  volume = {18},
  year = {2006}
}

@article{hinton2006reducing,
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
  doi = {10.1126/science.1127647},
  journal = {Science},
  note = {A landmark paper demonstrating that stacked RBMs could be used to pretrain deep autoencoders, achieving better compression than PCA. This showed that RBMs could initialize deep networks effectively, helping to launch the deep learning revolution.},
  number = {5786},
  openalex = {W2100495367},
  pages = {504--507},
  title = {Reducing the dimensionality of data with neural networks},
  url = {https://www.science.org/doi/10.1126/science.1127647},
  volume = {313},
  year = {2006}
}

@incollection{lecun2006tutorial,
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
  booktitle = {Predicting Structured Data},
  editor = {Bakir, Gökhan H. and Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S. V. N.},
  isbn = {9780262026178},
  note = {Honorary Mention: While broader than RBMs, this tutorial places them in the context of energy-based models. Essential reading for understanding the theoretical framework underlying RBMs.},
  openalex = {W2161914416},
  pages = {191--246},
  publisher = {MIT Press},
  series = {Neural Information Processing},
  title = {A tutorial on energy-based learning},
  url = {http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf},
  year = {2006}
}

@inproceedings{taylor2006modeling,
  abstract = {We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued "visible" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.},
  author = {Taylor, Graham W and Hinton, Geoffrey E and Roweis, Sam T},
  booktitle = {Advances in Neural Information Processing Systems 19},
  doi = {10.7551/mitpress/7503.003.0173},
  editor = {Bernhard Schölkopf and John C. Platt and Thomas Hofmann},
  note = {Introduced Conditional RBMs (CRBMs) for modeling temporal sequences. By conditioning the RBM on previous time steps, this work showed how RBMs could capture complex dynamics in motion capture data, extending RBMs beyond static data modeling.},
  openalex = {W2158164339},
  pages = {1345--1352},
  pdf = {https://proceedings.neurips.cc/paper/2006/file/1091660f3dff84fd648efe31391c5524-Paper.pdf},
  publisher = {MIT Press},
  title = {Modeling human motion using binary latent variables},
  volume = {19},
  year = {2006}
}

@inproceedings{carreira2005contrastive,
  author = {Carreira-Perpiñán, Miguel Á. and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  editor = {Robert G. Cowell and Zoubin Ghahramani},
  month = {1},
  note = {Provided the first rigorous analysis of Contrastive Divergence, showing it doesn't maximize likelihood but rather minimizes a different objective. Despite this, the paper explained why CD works well in practice and characterized its fixed points.},
  openalex = {W66838807},
  pages = {33--40},
  pdf = {https://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Contrastive Divergence Learning},
  volume = {R5},
  year = {2005}
}

@inproceedings{elfwing2008biologically,
  abstract = {Embodied evolution is a methodology for evolutionary robotics that mimics the distributed, asynchronous and autonomous properties of biological evolution. The evaluation, selection and reproduction are carried out by and between the robots, without any need for human intervention. In this paper, we propose a biologically inspired embodied evolution framework that fully integrates self-preservation, recharging from external batteries in the environment, and self-reproduction, pair-wise exchange of genetic material, into a survival system.},
  author = {Stefan Elfwing and Eiji Uchibe and Kenji Doya and Henrik I. Christensen},
  booktitle = {2005 IEEE Congress on Evolutionary Computation},
  doi = {10.1109/CEC.2005.1554969},
  note = {Used RBMs in evolutionary robotics to evolve neural controllers. Showed that RBM representations could support robust behavior learning in embodied agents.},
  openalex = {W1564326237},
  pages = {2210--2216},
  pdf = {https://faculty.cc.gatech.edu/~hic/hic-papers/Elfwing_CEC2005.pdf},
  publisher = {IEEE},
  title = {Biologically Inspired Embodied Evolution of Survival},
  volume = {3},
  year = {2005}
}

@inproceedings{welling2005exponential,
  abstract = {Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over hidden variables. In this paper we explore an alternative two-layer model based on exponential family distributions. We call these models exponential family harmoniums. Inference in these models is very fast because latent variables are conditionally independent given the observations. Furthermore, the product structure of harmoniums can model distributions with sharp boundaries and adding a new expert may decrease or increase the variance of the distribution. We demonstrate the effectiveness of exponential family harmoniums on an information retrieval task.},
  author = {Welling, Max and Rosen-Zvi, Michal and Hinton, Geoffrey E.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Saul, Lawrence K. and Weiss, Yair and Bottou, Léon},
  note = {Generalized RBMs to exponential family distributions, showing how different unit types could be incorporated. This flexibility made RBMs applicable to diverse data types.},
  openalex = {W2124914669},
  pages = {1481--1488},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2004/file/0e900ad84f63618452210ab8baae0218-Paper.pdf},
  publisher = {MIT Press},
  series = {NIPS},
  title = {Exponential Family Harmoniums with an Application to Information Retrieval},
  volume = {17},
  year = {2004}
}

@article{hinton2002training,
  abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual "expert" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptive system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called "contrastive divergence" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.},
  author = {Hinton, Geoffrey E.},
  doi = {10.1162/089976602760128018},
  journal = {Neural Computation},
  note = {The paper that made RBMs practical by introducing Contrastive Divergence (CD). This approximation to maximum likelihood learning requires only brief Markov chain runs, making training orders of magnitude faster than previous methods. CD became the standard training algorithm for RBMs.},
  number = {8},
  openalex = {W2116064496},
  pages = {1771--1800},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf},
  title = {Training products of experts by minimizing contrastive divergence},
  url = {https://doi.org/10.1162/089976602760128018},
  volume = {14},
  year = {2002}
}

@article{hinton1995wake,
  abstract = {An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up 'recognition' connections convert the input into representations in successive hidden layers, and top-down 'generative' connections reconstruct representation one layer from above.},
  author = {Hinton, Geoffrey E. and Dayan, Peter and Frey, Brendan J. and Neal, Radford M.},
  doi = {10.1126/science.7761831},
  journal = {Science},
  note = {Introduced the wake-sleep algorithm for training hierarchical generative models. While not directly about RBMs, this work influenced later development of DBNs and variational methods.},
  number = {5214},
  openalex = {W1993845689},
  pages = {1158--1161},
  title = {The "wake-sleep" algorithm for unsupervised neural networks},
  url = {https://www.science.org/doi/10.1126/science.7761831},
  volume = {268},
  year = {1995}
}

@article{neal1992connectionist,
  abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
  author = {Radford M. Neal},
  doi = {10.1016/0004-3702(92)90065-6},
  journal = {Artificial Intelligence},
  month = {7},
  note = {Connected neural networks to probabilistic graphical models, providing theoretical foundation for viewing RBMs as both neural networks and probabilistic models.},
  number = {1},
  openalex = {W2083380015},
  pages = {71--113},
  publisher = {Elsevier},
  title = {Connectionist learning of belief networks},
  url = {https://doi.org/10.1016/0004-3702(92)90065-6},
  volume = {56},
  year = {1992}
}

@inproceedings{freund1992unsupervised,
  abstract = {We study a particular type of Boltzmann machine with a bipartite graph structure called a harmonium. We are interested in using harmoniums to model probability distributions on binary input vectors. We analyze the class of probability distributions that can be modeled by harmoniums and show that this class includes arbitrarily good approximations to any distribution on n-vectors of binary inputs, for sufficiently large harmoniums. We present two learning algorithms for harmoniums. The first is a standard gradient ascent heuristic for computing maximum likelihood estimates. The gradient has a particularly simple closed form for harmoniums which makes this algorithm significantly more practical than the corresponding algorithm for the general Boltzmann machine. The second algorithm is a greedy heuristic for constructing a harmonium by adding one hidden unit at a time.},
  author = {Freund, Yoav and Haussler, David},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {J. Moody and S. Hanson and R. Lippmann},
  note = {An early theoretical analysis of RBMs (then called Harmoniums), proving that they can approximate any distribution over binary vectors arbitrarily well with enough hidden units. This established the universal approximation capability of RBMs.},
  openalex = {W2165225968},
  pages = {912--919},
  pdf = {https://papers.nips.cc/paper/1991/file/33e8075e9970de0cfea955afd4644bb2-Paper.pdf},
  publisher = {Morgan Kaufmann Publishers Inc.},
  title = {Unsupervised learning of distributions on binary vectors using two layer networks},
  volume = {4},
  year = {1991}
}

@article{peterson1987mean,
  abstract = {Based on the Boltzmann Machine concept, we derive a learning algorithm in which time-consuming stochastic measurements of correlations are replaced by solutions to deterministic mean field theory equations.},
  author = {Carsten Peterson and James R. Anderson},
  issn = {0891-2513},
  journal = {Complex Systems},
  note = {Introduced mean field approximations for Boltzmann Machines, making inference more tractable. These techniques would later be essential for training deep Boltzmann machines.},
  number = {5},
  openalex = {W2725061391},
  pages = {995--1019},
  pdf = {https://content.wolfram.com/sites/13/2018/02/01-5-6.pdf},
  title = {A mean field theory learning algorithm for neural networks},
  url = {https://www.complex-systems.com/abstracts/v01_i05_a06/},
  volume = {1},
  year = {1987}
}

@incollection{smolensky1986information,
  abstract = {At this early stage in the development of cognitive science, methodological issues are both open and central. There may have been times when developments in neuroscience, artificial intelligence, or psychology seduced researchers into believing that their discipline was on the verge of discovering the secret of intelligence. But a humbling history of hopes disappointed has produced the realization that understanding the mind will challenge the power of all these methodologies combined. The work reported in this chapter rests on the conviction that a methodology that has a crucial role to play in the development of cognitive science is mathematical analysis.},
  address = {Cambridge, MA},
  author = {Smolensky, Paul},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations},
  editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group},
  isbn = {978-0-262-68053-0},
  month = {1},
  note = {The foundational paper that introduced what would later be called Restricted Boltzmann Machines. Smolensky introduced "Harmoniums" - networks with visible and hidden units but no visible-visible or hidden-hidden connections. This restriction made the model more tractable than general Boltzmann Machines while retaining expressiveness.},
  openalex = {W1813659000},
  pages = {194--281},
  publisher = {MIT Press},
  title = {Information processing in dynamical systems: Foundations of harmony theory},
  volume = {1},
  year = {1986}
}

@article{ackley1985learning,
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  doi = {10.1207/s15516709cog0901_7},
  journal = {Cognitive Science},
  note = {The foundational paper that introduced Boltzmann Machines, the predecessor to RBMs. Established the connection between neural networks and statistical physics, laying groundwork for all subsequent energy-based models.},
  number = {1},
  openalex = {W2042492924},
  pages = {147--169},
  title = {A learning algorithm for Boltzmann machines},
  url = {https://www.cs.toronto.edu/~fritz/absps/cogscibm.pdf},
  volume = {9},
  year = {1985}
}
