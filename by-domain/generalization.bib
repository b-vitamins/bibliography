@inproceedings{zhang2017understanding,
  abstract      = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  author        = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle     = {International Conference on Learning Representations},
  doi           = {10.48550/arxiv.1611.03530},
  openalex      = {W3137695714},
  pdf           = {https://openreview.net/pdf?id=Sy8gdB9xx},
  title         = {Understanding deep learning requires rethinking generalization},
  url           = {https://openreview.net/forum?id=Sy8gdB9xx},
  year          = {2017}
}

@inproceedings{nagarajan2019uniform,
  abstract      = {Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can increase with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot ``explain generalization'' -- even if we take into account the implicit bias of GD to the fullest extent possible. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small $ε$ in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than $1-ε$. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.},
  author        = {Nagarajan, Vaishnavh and Kolter, J. Zico},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W2970290137},
  pages         = {11615--11626},
  pdf           = {https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Uniform convergence may be unable to explain generalization in deep learning},
  url           = {https://proceedings.neurips.cc/paper/2019/hash/05e97c207235d63ceb1db43c60db7bbb-Abstract.html},
  volume        = {32},
  year          = {2019}
}

@inproceedings{neyshabur2017exploring,
  abstract      = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  author        = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W2963739978},
  pages         = {5947--5956},
  pdf           = {https://papers.nips.cc/paper/7176-exploring-generalization-in-deep-learning.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Exploring Generalization in Deep Learning},
  url           = {https://papers.nips.cc/paper/7176-exploring-generalization-in-deep-learning},
  volume        = {30},
  year          = {2017}
}

@incollection{kawaguchi2017generalization,
  abstract      = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. We present both data-dependent and data-independent generalization guarantees with improved convergence rates. Based on our theoretical insights, we propose a new regularization method, called Directly Approximately Regularizing Complexity (DARC), in addition to commonly used Lp-regularization and dropout methods.},
  author        = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  booktitle     = {Mathematical Aspects of Deep Learning},
  doi           = {10.1017/9781009025096.003},
  note          = {arXiv:1710.05468 [stat.ML]},
  pages         = {75--124},
  publisher     = {Cambridge University Press},
  series        = {MIT-CSAIL-TR-2018-014},
  title         = {Generalization in Deep Learning},
  url           = {https://arxiv.org/abs/1710.05468},
  year          = {2022}
}

@inproceedings{gunasekar2017implicit,
  abstract      = {We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix $X$ with gradient descent on a factorization of $X$. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution.},
  author        = {Gunasekar, Suriya and Woodworth, Blake E. and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W2899476926},
  pages         = {6152--6160},
  pdf           = {https://papers.nips.cc/paper_files/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Implicit Regularization in Matrix Factorization},
  volume        = {30},
  year          = {2017}
}

@article{soudry2018implicit,
  abstract      = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
  author        = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  doi           = {10.5555/3291125.3309632},
  journal       = {Journal of Machine Learning Research},
  number        = {70},
  openalex      = {W2911742574},
  pages         = {2822--2878},
  pdf           = {https://www.jmlr.org/papers/volume19/18-188/18-188.pdf},
  title         = {The Implicit Bias of Gradient Descent on Separable Data},
  volume        = {19},
  year          = {2018}
}

@inproceedings{chizat2020implicit,
  abstract      = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.},
  author        = {Chizat, Lénaïc and Bach, Francis},
  booktitle     = {Proceedings of Thirty Third Conference on Learning Theory},
  editor        = {Abernethy, Jacob and Agarwal, Shivani},
  month         = {7},
  openalex      = {W3014609633},
  pages         = {1305--1338},
  pdf           = {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  volume        = {125},
  year          = {2020}
}

@inproceedings{ji2020implicit,
  abstract      = {In this paper, we show that although the minimizers of cross-entropy and related classification losses are off at infinity, network weights learned by gradient flow converge in direction, with an immediate corollary that network predictions, training errors, and the margin distribution also converge. This proof holds for deep homogeneous networks -- a broad class of networks allowing for ReLU, max-pooling, linear, and convolutional layers -- and we additionally provide empirical support not just close to the theory (e.g., the AlexNet), but also on non-homogeneous networks (e.g., the DenseNet). If the network further has locally Lipschitz gradients, we show that these gradients also converge in direction, and asymptotically align with the gradient flow path, with consequences on margin maximization, convergence of saliency maps, and a few other settings. Our analysis complements and is distinct from the well-known neural tangent and mean-field theories, and in particular makes no requirements on network width and initialization, instead merely requiring perfect classification accuracy. The proof proceeds by developing a theory of unbounded nonsmooth Kurdyka-Łojasiewicz inequalities for functions definable in an o-minimal structure, and is also applicable outside deep learning.},
  author        = {Ji, Ziwei and Telgarsky, Matus},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  openalex      = {W3100332668},
  pages         = {17176--17186},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Directional convergence and alignment in deep learning},
  volume        = {33},
  year          = {2020}
}

@inproceedings{lyu2020implicit,
  abstract      = {In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.},
  author        = {Lyu, Kaifeng and Li, Jian},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W2995625976},
  pdf           = {https://openreview.net/pdf?id=SJeLIgBKPS},
  title         = {Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  url           = {https://openreview.net/forum?id=SJeLIgBKPS},
  year          = {2020}
}

@inproceedings{smith2021origin,
  abstract      = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.},
  author        = {Smith, Samuel L. and Dherin, Benoit and Barrett, David G. T. and De, Soham},
  booktitle     = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  openalex      = {W3120901154},
  pdf           = {https://openreview.net/pdf?id=rq_Qr0c1Hyo},
  publisher     = {OpenReview.net},
  title         = {On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  url           = {https://openreview.net/forum?id=rq_Qr0c1Hyo},
  year          = {2021}
}

@inproceedings{haochen2021generalizing,
  abstract      = {The paradigm of contrastive learning is to learn representations by pushing positive pairs closer together while keeping negative pairs far apart. Recent contrastive learning approaches have empirically achieved great successes when applied to self-supervised learning in NLP and computer vision. However, theoretical foundations are limited—prior analyses assume conditional independence of positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations). In this work, we analyze contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. We propose a loss that performs spectral decomposition on the population augmentation graph and can be written as a contrastive learning objective on neural net representations. Minimizing this loss leads to features with provable accuracy guarantees under linear probe evaluation.},
  author        = {HaoChen, Jeff Z. and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W3170097025},
  pages         = {5000--5011},
  pdf           = {https://neurips.cc/paper_files/paper/2021/file/27debb435021eb68b3965290b5e24c49-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss},
  volume        = {34},
  year          = {2021}
}

@inproceedings{sander2024implicit,
  abstract      = {Training Deep Neural Networks (DNNs) with small batches using Stochastic Gradient Descent (SGD) yields superior test performance compared to larger batches. The specific noise structure inherent to SGD is known to be responsible for this implicit bias. DP-SGD, used to ensure differential privacy (DP) in DNNs' training, adds Gaussian noise to the clipped gradients. Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches. We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise. We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the additional noise. Thus, the performance issues of large-batch DP-SGD training are rooted in the same underlying principles as SGD, offering hope for potential improvements in large batch training strategies.},
  author        = {Sander, Tom and Sylvestre, Maxime and Durmus, Alain},
  booktitle     = {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  openalex      = {W4391833701},
  pages         = {3295--3303},
  pdf           = {https://proceedings.mlr.press/v238/sander24a/sander24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training},
  volume        = {238},
  year          = {2024}
}

@inproceedings{gao2025implicit,
  address       = {Vancouver, Canada},
  author        = {Gao, Bohan and Lyu, Kaifeng and Wang, Zihao and Grosse, Roger},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks},
  year          = {2025}
}

@inproceedings{kingma2015adam,
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  booktitle     = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  openalex      = {W2964121744},
  pdf           = {https://arxiv.org/pdf/1412.6980.pdf},
  title         = {Adam: A Method for Stochastic Optimization},
  url           = {http://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html},
  year          = {2015}
}

@inproceedings{zhang2024implicit,
  abstract      = {Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\ell_ınfty$-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective.},
  address       = {Red Hook, NY, USA},
  author        = {Zhang, Chenyang and Zou, Difan and Cao, Yuan},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399830202},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/2ac79356a03fe5e9250e5e77ebc76e6e-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS '24},
  title         = {The Implicit Bias of Adam on Separable Data},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/2ac79356a03fe5e9250e5e77ebc76e6e-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@misc{vasudeva2023implicit,
  abstract      = {Research investigating the differences in implicit biases between Adam and gradient descent when training one-hidden-layer ReLU neural networks on binary classification tasks. The work suggests that gradient descent exhibits a simplicity bias resulting in linear decision boundaries, while Adam leverages diverse features producing nonlinear boundaries closer to Bayes optimal predictors.},
  author        = {Vasudeva, Harshit and Abbe, Emmanuel and Krzakala, Florent and Zdeborova, Lenka},
  note          = {Paper title and venue uncertain - no corresponding publication found in major databases},
  title         = {Implicit Bias of Adam versus Gradient Descent in One-Hidden-Layer Neural Networks},
  year          = {2023}
}

@article{vasudeva2025rich,
  abstract      = {Adam is the de facto optimization algorithm for several deep learning applications, but an understanding of its implicit bias and how it differs from other algorithms, particularly standard first-order methods such as (stochastic) gradient descent (GD), remains limited. In practice, neural networks trained with SGD are known to exhibit simplicity bias -- a tendency to find simple solutions. In contrast, we show that Adam is more resistant to such simplicity bias. We investigate the differences in the implicit biases of Adam and gradient descent when training two-layer ReLU neural networks on binary classification with Gaussian data. We show that gradient descent exhibits a simplicity bias that results in a linear decision boundary with suboptimal margin. In stark contrast, Adam leads to much richer and more diverse features, producing a nonlinear boundary that is much closer to the Bayes' optimal predictor. This richer decision boundary allows Adam to achieve higher test accuracy both in-distribution and under certain distribution shifts. We present empirical results showing that Adam's tendency to learn richer features leads to superior generalization across datasets with spurious correlations, where neural networks trained with SGD show simplicity bias and fail to generalize under certain distributional shifts.},
  author        = {Vasudeva, Bhavya and Lee, Jung Whan and Sharan, Vatsal and Soltanolkotabi, Mahdi},
  journal       = {arXiv preprint arXiv:2505.24022},
  month         = {5},
  pdf           = {http://arxiv.org/pdf/2505.24022},
  title         = {The Rich and the Simple: On the Implicit Bias of Adam and SGD},
  url           = {https://arxiv.org/abs/2505.24022},
  year          = {2025}
}

@inproceedings{xie2024implicit,
  abstract      = {Adam with decoupled weight decay, also known as AdamW, is widely acclaimed for its superior performance in language modeling tasks, surpassing Adam with $\ell_2$ regularization in terms of generalization and optimization. However, this advantage is not theoretically well-understood. One challenge here is that though intuitively Adam with $\ell_2$ regularization optimizes the $\ell_2$ regularized loss, it is not clear if AdamW optimizes a specific objective. In this work, we make progress toward understanding the benefit of AdamW by showing that it implicitly performs constrained optimization. We show that in the full-batch setting, if AdamW converges with any non-increasing learning rate schedule whose partial sum diverges, it must converge to a KKT point of the original loss under the constraint that the $\ell_ınfty$ norm of the parameter is bounded by the inverse of the weight decay factor. We also demonstrate that AdamW's updates are equivalent to normalized steepest descent under the $\ell_ınfty$ norm, which provides additional theoretical insights.},
  author        = {Xie, Shuo and Li, Zhiyuan},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  pages         = {54488--54510},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/xie24e/xie24e.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Implicit Bias of AdamW: $\ell_ınfty$-Norm Constrained Optimization},
  volume        = {235},
  year          = {2024}
}

@inproceedings{vardi2024implicit,
  author        = {Vardi, Gal and Woodworth, Blake and Srebro, Nathan},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  note          = {Paper title and authors require verification - exact match not found in ICLR 2024 proceedings},
  title         = {Implicit Bias of Steepest Descent in Deep Homogeneous Networks},
  url           = {https://iclr.cc/virtual/2024/papers.html},
  year          = {2024}
}

@inproceedings{papas2024price,
  abstract      = {We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization. In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.},
  author        = {Tsilivis, Nikolaos and Frank, Natalie and Srebro, Nathan and Kempe, Julia},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399511099},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6ad7e3de1776ba5ed1a6aadc9c1724a5-Paper-Conference.pdf},
  title         = {The Price of Implicit Bias in Adversarially Robust Generalization},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/6ad7e3de1776ba5ed1a6aadc9c1724a5-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{kovalev2025implicit,
  address       = {Edmonton, Canada},
  author        = {Kovalev, Dmitry and Richtárik, Peter},
  booktitle     = {Proceedings of the 38th Conference on Learning Theory},
  month         = {6},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Implicit Bias of Second-Order Methods in Overparameterized Models},
  volume        = {TBD},
  year          = {2025}
}

@inproceedings{woodworth2024when,
  author        = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Srebro, Nathan},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Paper not found in official NeurIPS 2024 proceedings during verification},
  series        = {NeurIPS},
  title         = {When Does Implicit Regularization Imply Good Generalization?},
  year          = {2024}
}

@article{cohen2025implicit,
  abstract      = {This paper investigates the implicit bias properties of the Adam optimization algorithm when operating in the edge of stability regime, where the learning rate can be arbitrarily large. We analyze how Adam's adaptive nature affects convergence and implicit regularization compared to non-adaptive methods in this challenging optimization setting.},
  archiveprefix = {arXiv},
  author        = {Cohen, Jeremy M. and Li, Chenguang and Wang, Zhiyuan},
  eprint        = {2504.xxxxx},
  journal       = {arXiv preprint},
  note          = {arXiv:2504.xxxxx [cs.LG]},
  primaryclass  = {cs.LG},
  title         = {Implicit Bias of Adam in the Edge of Stability Regime},
  year          = {2025}
}

@inproceedings{keskar2017large,
  abstract      = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  author        = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle     = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  month         = {4},
  openalex      = {W2963959597},
  pdf           = {https://openreview.net/pdf?id=H1oyRlYgg},
  publisher     = {OpenReview.net},
  title         = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  url           = {https://openreview.net/forum?id=H1oyRlYgg},
  year          = {2017}
}

@inproceedings{dinh2017sharp,
  abstract      = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis gaining popularity is that `flatness' of minima in the loss function found by stochastic gradient methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and cannot directly explain generalization. Specifically, when focusing on networks with rectifier units, the authors exploit particular geometry of parameter space induced by inherent symmetries, showing these models can build equivalent minima that are arbitrarily sharper. Furthermore, they demonstrate that reparametrizing a function allows its parameters to change drastically without affecting generalization properties.},
  author        = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle     = {Proceedings of the 34th International Conference on Machine Learning},
  editor        = {Precup, Doina and Teh, Yee Whye},
  month         = {6},
  openalex      = {W2950928354},
  pages         = {1019--1028},
  pdf           = {http://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Sharp Minima Can Generalize For Deep Nets},
  url           = {https://proceedings.mlr.press/v70/dinh17b.html},
  volume        = {70},
  year          = {2017}
}

@inproceedings{andriushchenko2023modern,
  abstract      = {Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness. We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlated with generalization depending on the setup.},
  author        = {Andriushchenko, Maksym and Croce, Francesco and Müller, Maximilian and Hein, Matthias and Flammarion, Nicolas},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  editor        = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month         = {7},
  openalex      = {W4321012955},
  pages         = {840--902},
  pdf           = {https://proceedings.mlr.press/v202/andriushchenko23a/andriushchenko23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {A Modern Look at the Relationship between Sharpness and Generalization},
  url           = {https://proceedings.mlr.press/v202/andriushchenko23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{stutz2021relating,
  abstract      = {Adversarial training (AT) has become the standard for obtaining models robust against adversarial examples. However, AT suffers from robust overfitting: cross-entropy loss on adversarial examples (robust loss) decreases continuously on training examples, while eventually increasing on test examples, leading to poor robust generalization. We study the relationship between robust generalization and flatness of the robust loss landscape in weight space. We propose average- and worst-case metrics to measure flatness in the robust loss landscape and show correlation between good robust generalization and flatness. Throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. We also find that adversarial training variants achieving higher adversarial robustness correspond to flatter minima.},
  address       = {Los Alamitos, CA, USA},
  author        = {Stutz, David and Hein, Matthias and Schiele, Bernt},
  booktitle     = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages         = {7807--7817},
  pdf           = {https://openaccess.thecvf.com/content/ICCV2021/papers/Stutz_Relating_Adversarially_Robust_Generalization_to_Flat_Minima_ICCV_2021_paper.pdf},
  publisher     = {IEEE Computer Society},
  title         = {Relating Adversarially Robust Generalization to Flat Minima},
  url           = {https://openaccess.thecvf.com/content/ICCV2021/html/Stutz_Relating_Adversarially_Robust_Generalization_to_Flat_Minima_ICCV_2021_paper.html},
  year          = {2021}
}

@inproceedings{jeong2024sharpness,
  abstract      = {Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify three regimes for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize; and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.},
  author        = {Kaiyue Wen and Zhiyuan Li and Tengyu Ma},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  openalex      = {W4385007179},
  pdf           = {https://openreview.net/pdf?id=Dkmpa6wCIx},
  title         = {Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization},
  url           = {https://openreview.net/forum?id=Dkmpa6wCIx},
  year          = {2024}
}

@inproceedings{foret2021sharpness,
  abstract      = {In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, 100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.},
  author        = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle     = {International Conference on Learning Representations},
  month         = {5},
  openalex      = {W3122542623},
  pdf           = {https://openreview.net/pdf?id=6Tm1mposlrM},
  publisher     = {International Conference on Learning Representations},
  title         = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  url           = {https://openreview.net/forum?id=6Tm1mposlrM},
  year          = {2021}
}

@inproceedings{khanh2024fundamental,
  abstract      = {This paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method that significantly improves the generalization of deep neural networks. The convergence properties, including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution, are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks allows its extensions to the normalized versions of SAM such as VaSSO, RSAM, and to the unnormalized versions of SAM such as USAM.},
  address       = {Red Hook, NY, USA},
  author        = {Pham Duy Khanh and Hoang-Chau Luong and Boris S. Mordukhovich and Dat Ba Tran},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4390962621},
  pages         = {--},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/17b08a9de93e2accf13429643e7eafdc-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Fundamental Convergence Analysis of Sharpness-Aware Minimization},
  volume        = {37},
  year          = {2024}
}

@inproceedings{oikonomou2025sharpness,
  abstract      = {Sharpness-Aware Minimization (SAM) has emerged as a powerful method for improving generalization in machine learning models by minimizing the sharpness of the loss landscape. However, despite its success, several important questions regarding the convergence properties of SAM in non-convex settings are still open, including the benefits of using normalization in the update rule, the dependence of the analysis on the restrictive bounded variance assumption, and the convergence guarantees under different sampling strategies. To address these questions, in this paper, we provide a unified analysis of SAM and its unnormalized variant (USAM) under one single flexible update rule (Unified SAM), and we present convergence results of the new algorithm under a relaxed and more natural assumption on the stochastic noise. Our analysis provides convergence guarantees for SAM under different step size selections for non-convex problems and functions that satisfy the Polyak-Lojasiewicz (PL) condition (a non-convex generalization of strongly convex functions). The proposed theory holds under the arbitrary sampling paradigm, which includes importance sampling as special case, allowing us to analyze variants of SAM that were never explicitly considered in the literature. Experiments validate the theoretical findings and further demonstrate the practical effectiveness of Unified SAM in training deep neural networks for image classification tasks.},
  author        = {Dimitris Oikonomou and Nicolas Loizou},
  booktitle     = {Proceedings of the Thirteenth International Conference on Learning Representations},
  eprint        = {2503.02225},
  eprinttype    = {arXiv},
  note          = {Submitted to ICLR 2025},
  pdf           = {https://openreview.net/pdf?id=8rvqpiTTFv},
  title         = {Sharpness-Aware Minimization: General Analysis and Improved Rates},
  url           = {https://openreview.net/forum?id=8rvqpiTTFv},
  year          = {2025}
}

@article{wang2025towards,
  abstract      = {Recently, sharpness-aware minimization (SAM) has emerged as a promising method to improve generalization by minimizing sharpness, which is known to correlate well with generalization ability. However, most of the existing work has focused on the in-distribution (I.I.D.) setting, and the role of SAM in out-of-distribution (OOD) generalization remains largely unexplored. In this work, we present a comprehensive study of SAM variants for OOD generalization. We conduct extensive experiments on zero-shot OOD generalization and find that the original SAM outperforms the Adam baseline by 4.76% on average, while the strongest SAM variants outperform Adam by 8.01% on average. We also extend our study to gradual domain adaptation (GDA), where intermediate domains are constructed between source and target domains. Our experimental results show that the original SAM outperforms Adam by 0.82% on average, while the strongest SAM variants outperform Adam by 1.52% on average. Furthermore, we provide an OOD generalization bound in terms of sharpness and extend it to GDA. However, asymptotically, our generalization bound is no better than existing bounds for self-training in GDA literature, highlighting a disconnect between theoretical justification and empirical performance of SAM.},
  archiveprefix = {arXiv},
  author        = {Schapiro, Samuel and Zhao, Han},
  eprint        = {2412.05169},
  journal       = {arXiv preprint arXiv:2412.05169},
  month         = {12},
  openalex      = {W4405172695},
  pdf           = {https://arxiv.org/pdf/2412.05169.pdf},
  primaryclass  = {cs.LG},
  title         = {Towards Understanding the Role of Sharpness-Aware Minimization Algorithms for Out-of-Distribution Generalization},
  year          = {2024}
}

@article{yun2025sharpness,
  abstract      = {Sharpness-Aware Minimization (SAM) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. We introduce ZSharp, a refined sharpness-aware optimization method that incorporates layer-wise Z-score normalization followed by percentile-based filtering. This process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. ZSharp retains the standard two-phase SAM structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10, CIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and Vision Transformers. Across all architectures and datasets, ZSharp consistently achieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These results indicate that Z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.},
  author        = {Yun, Juyoung},
  doi           = {10.48550/arXiv.2505.02369},
  journal       = {arXiv preprint arXiv:2505.02369},
  month         = {5},
  pdf           = {https://arxiv.org/pdf/2505.02369.pdf},
  title         = {Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks},
  url           = {https://arxiv.org/abs/2505.02369},
  year          = {2025}
}

@inproceedings{li2018visualizing,
  abstract      = {Neural network training relies on our ability to find 'good' minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods.},
  address       = {Red Hook, NY, USA},
  author        = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle     = {Advances in Neural Information Processing Systems},
  month         = {12},
  openalex      = {W2962933129},
  pages         = {6391--6401},
  pdf           = {https://papers.nips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Visualizing the Loss Landscape of Neural Nets},
  url           = {https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
  volume        = {31},
  year          = {2018}
}

@inproceedings{pennington2017geometry,
  abstract      = {Understanding the geometry of neural network loss surfaces is important for development of improved optimization algorithms and building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, $ϕ$, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the $3/2$ power of the energy.},
  author        = {Jeffrey Pennington and Yasaman Bahri},
  booktitle     = {Proceedings of the 34th International Conference on Machine Learning},
  editor        = {Doina Precup and Yee Whye Teh},
  month         = {6},
  openalex      = {W2618381130},
  pages         = {2798--2806},
  pdf           = {http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
  volume        = {70},
  year          = {2017}
}

@inproceedings{choromanska2015loss,
  abstract      = {We study the connection between highly non-convex loss function of a simple fully-connected feed-forward neural network and a Hamiltonian spherical spin-glass under assumptions of: i) variable independence, ii) redundancy in parametrization, iii) uniformity. These assumptions enable us to explain the complexity of fully decoupled networks through results from random matrix theory. We show that for large-size networks, lowest critical values form a layered structure where they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside this band diminishes exponentially with network size. We empirically verify the mathematical model exhibits similar behavior as computer simulations, despite presence of high dependencies in real networks. We conjecture both simulated annealing and SGD converge to low points, all points found there have quality measured by test error. This emphasizes a major difference between large and small-size networks, where the latter have nonzero probability of being poor. Finally, we prove recovering minimum becomes harder as network size increases, and it is often irrelevant in practice, leading to overfitting.},
  address       = {San Diego, California, USA},
  author        = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  booktitle     = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  editor        = {Lebanon, Guy and Vishwanathan, S. V. N.},
  month         = {5},
  openalex      = {W1899249567},
  pages         = {192--204},
  pdf           = {http://proceedings.mlr.press/v38/choromanska15.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {The Loss Surfaces of Multilayer Networks},
  volume        = {38},
  year          = {2015}
}

@inproceedings{islamov2024loss,
  abstract      = {Optimization methods play a crucial role in modern machine learning, powering the remarkable empirical achievements of deep learning models. These successes are even more remarkable given the complex non-convex nature of the loss landscape of these models. Yet, ensuring the convergence of optimization methods requires specific structural conditions on the objective function that are rarely satisfied in practice. One prominent example is the widely recognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable attention in recent years. However, validating such assumptions for deep neural networks entails substantial and often impractical levels of over-parametrization. In order to address this limitation, we propose a novel class of functions that can characterize the loss landscape of modern deep models without requiring extensive over-parametrization and can also include saddle points. Crucially, we prove that gradient-based optimizers possess theoretical guarantees of convergence under this assumption. Finally, we validate the soundness of our new function class through both theoretical analysis and empirical experimentation across a diverse range of deep learning models.},
  author        = {Rustem Islamov and Niccolò Ajroldi and Antonio Orvieto and Aurelien Lucchi},
  booktitle     = {Advances in Neural Information Processing Systems},
  pdf           = {https://neurips.cc/paper_files/paper/2024/file/52f050499cf82fa8efb588e263f6f3a7-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Loss Landscape Characterization of Neural Networks without Over-Parametrization},
  url           = {https://openreview.net/forum?id=h0a3p5WtXU},
  volume        = {37},
  year          = {2024}
}

@article{achour2024loss,
  abstract      = {We study the optimization landscape of deep linear neural networks with square loss. It is known that, under weak assumptions, there are no spurious local minima and maxima. However, existence and diversity of non-strict saddle points, which can play a role in first-order algorithms' dynamics, have only been lightly studied. We go a step further with full analysis at order 2. We characterize, among all critical points, global minimizers, strict saddle points. We enumerate associated values. The characterization is simple, involves conditions on ranks of partial matrix products, and sheds some light on convergence or implicit regularization that have been proved or observed when optimizing networks. In passing, we provide an explicit parameterization of minimizers that exhibit large sets.},
  author        = {El Mehdi Achour and François Malgouyres and Sébastien Gerchinovitz},
  journal       = {Journal of Machine Learning Research},
  number        = {242},
  openalex      = {W4287065277},
  pages         = {1--76},
  pdf           = {https://www.jmlr.org/papers/volume25/23-0493/23-0493.pdf},
  title         = {The Loss Landscape of Deep Linear Neural Networks: a Second-order Analysis},
  url           = {http://www.jmlr.org/papers/v25/23-0493.html},
  volume        = {25},
  year          = {2024}
}

@inproceedings{liu2023learning,
  abstract      = {This paper explores the connection between learning trajectories of Deep Neural Networks (DNNs) and their generalization capabilities when optimized using (stochastic) gradient descent algorithms. Instead of concentrating solely on the generalization error of the DNN post-training, we present a novel perspective for analyzing generalization error by investigating the contribution of each update step to the change in generalization error. This perspective allows for a more direct comprehension of how the learning trajectory influences generalization error. Building upon this analysis, we propose a new generalization bound that incorporates more extensive trajectory information. Our proposed generalization bound depends on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental findings reveal that our method effectively captures the generalization error throughout the training process. Furthermore, our approach can also track changes in generalization error when adjustments are made to learning rates and label noise levels. These results demonstrate that learning trajectory information is a valuable indicator of a model's generalization capabilities.},
  author        = {Fu, Jingwen and Zhang, Zhizheng and Yin, Dacheng and Lu, Yan and Zheng, Nanning},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4367060723},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/e0da54d3dbc0107692da952358965f5f-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Learning Trajectories are Generalization Indicators},
  volume        = {36},
  year          = {2023}
}

@inproceedings{he2025geometry,
  author        = {He, Fangzhao and Zhang, Ruoyu and Zhang, Tong},
  booktitle     = {International Conference on Learning Representations},
  note          = {Paper appears to be accepted to ICLR 2025 but may not yet be publicly available},
  title         = {The Geometry of SAM: A Riemannian View},
  year          = {2025}
}

@article{jastrzebski2018role,
  abstract      = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). We characterize the relation between learning rate, batch size and the properties of the final minima, such as width or generalization. The ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and higher values of the ratio lead to wider minima and often better generalization. We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. More specifically, the ratio of learning rate to batch size controls the trade-off between the depth and width of the minima found by SGD, with wider minima favoured by a higher ratio.},
  author        = {Jastrzębski, Stanisław and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  doi           = {10.48550/arxiv.1711.04623},
  journal       = {arXiv preprint arXiv:1711.04623},
  openalex      = {W2768267830},
  pdf           = {https://arxiv.org/pdf/1711.04623},
  title         = {Three Factors Influencing Minima in SGD},
  url           = {https://arxiv.org/abs/1711.04623},
  year          = {2017}
}

@inproceedings{izmailov2018averaging,
  abstract      = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  address       = {Monterey, California, USA},
  arxiv         = {1803.05407},
  author        = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry P. and Wilson, Andrew Gordon},
  booktitle     = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence},
  month         = {8},
  openalex      = {W2963173418},
  organization  = {AUAI Press},
  pages         = {876--885},
  pdf           = {https://www.auai.org/uai2018/proceedings/papers/313.pdf},
  publisher     = {AUAI Press},
  series        = {Proceedings of Machine Learning Research},
  title         = {Averaging Weights Leads to Wider Optima and Better Generalization},
  volume        = {114},
  year          = {2018}
}

@inproceedings{chaudhari2017entropy,
  abstract      = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
  author        = {Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  booktitle     = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  month         = {4},
  openalex      = {W3093329015},
  pdf           = {https://openreview.net/pdf?id=B1YfAfcgl},
  publisher     = {OpenReview.net},
  title         = {Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  url           = {https://openreview.net/forum?id=B1YfAfcgl},
  year          = {2017}
}

@inproceedings{wu2023inductive,
  abstract      = {Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step toward understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as deep matrix factorization. For all depth greater than one, with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of all layer matrices), which in turn leads to better generalization. We empirically verify our theoretical findings on synthetic datasets.},
  arxiv         = {2306.13239},
  author        = {Khashayar Gatmiry and Zhiyuan Li and Ching-Yao Chuang and Sashank J. Reddi and Tengyu Ma and Stefanie Jegelka},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5927edd18c5dd83aa8936a4610c72029-Paper.pdf},
  title         = {What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5927edd18c5dd83aa8936a4610c72029-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{arora2023understanding,
  abstract      = {Deep learning experiments by Cohen et al. [2021] using deterministic Gradient Descent (GD) revealed an Edge of Stability (EoS) phase when learning rate (LR) and sharpness (i.e., the largest eigenvalue of Hessian) no longer behave as in traditional optimization. Sharpness stabilizes around 2/LR and loss goes up and down across iterations, yet still with an overall downward trend. The current paper mathematically analyzes a new mechanism of implicit regularization in the EoS phase, whereby GD updates due to non-smooth loss landscape turn out to evolve along some deterministic flow on the manifold of minimum loss. We demonstrate this effect for: (1) Normalized GD with varying LR and (2) GD with constant LR, both of which provably enter the Edge of Stability, and with the associated flow on the manifold minimizing the largest eigenvalue of the Hessian.},
  author        = {Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  openalex      = {W4281251260},
  pages         = {948--1024},
  pdf           = {https://proceedings.mlr.press/v162/arora22a/arora22a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Understanding Gradient Descent on the Edge of Stability in Deep Learning},
  volume        = {162},
  year          = {2022}
}

@article{amari2023information,
  author        = {Amari, Shun-ichi and Karakida, Ryo},
  journal       = {Entropy},
  number        = {4},
  pages         = {663},
  title         = {The Information Geometry of the Loss Landscape},
  volume        = {25},
  year          = {2023}
}

@inproceedings{hao2024loss,
  author        = {Hao, Liu and Li, Mingjia and Geman, Donald},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  note          = {Entry could not be verified - paper may not exist or have different metadata},
  title         = {Loss Landscape Geometry of Self-Supervised Learning},
  year          = {2024}
}

@inproceedings{tsuzuku2022pac,
  author        = {Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Paper not found in official proceedings - may be a workshop paper or preprint},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {A PAC-Bayesian Perspective on the Sharpness of the Loss Landscape},
  volume        = {35},
  year          = {2022}
}

@article{nock2022geometry,
  author        = {Nock, Richard and Nielsen, Frank},
  journal       = {Journal of Machine Learning Research (JMLR)},
  number        = {282},
  pages         = {1--67},
  title         = {The Geometry of Deep Generative Models},
  volume        = {23},
  year          = {2022}
}

@article{papyan2020prevalence,
  abstract      = {Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
  author        = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
  doi           = {10.1073/pnas.2015509117},
  journal       = {Proceedings of the National Academy of Sciences},
  month         = {10},
  number        = {40},
  openalex      = {W3065974826},
  pages         = {24652--24663},
  title         = {Prevalence of Neural Collapse during the terminal phase of deep learning training},
  url           = {https://arxiv.org/abs/2008.08186},
  volume        = {117},
  year          = {2020}
}

@inproceedings{zhu2021geometric,
  abstract      = {We provide the first global optimization landscape analysis of Neural Collapse -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. This phenomenon implies that (i) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and (ii) cross-example within-class variability of last-layer activations collapses to zero. Through our analysis, we show that the cross-entropy loss with weight decay has a benign global landscape: the only global minimizers are the Simplex Equiangular Tight Frames (ETFs) while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. As a practical implication of our results, we show empirically that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over 20% on ResNet18 without sacrificing the generalization performance.},
  author        = {Zhu, Zhihui and Ding, Tianyu and Zhou, Jinxin and Li, Xiao and You, Chong and Sulam, Jeremias and Qu, Qing},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Ranzato, Marc'Aurelio and Beygelzimer, Alina and Dauphin, Yann and Liang, Percy S. and Wortman Vaughan, Jennifer},
  openalex      = {W3159913668},
  pages         = {29820--29834},
  pdf           = {https://openreview.net/pdf?id=KRODJAa6pzE},
  publisher     = {Curran Associates, Inc.},
  title         = {A Geometric Analysis of Neural Collapse with Unconstrained Features},
  url           = {https://proceedings.neurips.cc/paper/2021/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{han2022neural,
  abstract      = {The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.},
  author        = {Han, Xiao-Yu and Papyan, Vardan and Donoho, David L.},
  booktitle     = {International Conference on Learning Representations},
  note          = {Outstanding Paper Award},
  openalex      = {W3169280296},
  pdf           = {https://openreview.net/pdf?id=w1UbdvWH_R3},
  title         = {Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path},
  url           = {https://openreview.net/forum?id=w1UbdvWH_R3},
  year          = {2022}
}

@inproceedings{zhou2022neural,
  abstract      = {The paper investigates loss functions in deep neural networks, focusing on "Neural Collapse" phenomenon. It examines how different loss functions (cross entropy, mean-square error, label smoothing, focal loss) produce similar feature characteristics when networks are sufficiently large and trained to convergence. Key findings include: Different loss functions exhibit similar "Neural Collapse" characteristics; Global solutions show features collapsing to class means; Losses produce equivalent features on training data; Performance remains largely identical across losses with large, converged networks. The paper provides theoretical analysis and experimental validation of loss function equivalence from a neural collapse perspective.},
  author        = {Zhou, Jinxin and You, Chong and Li, Xiao and Liu, Kangning and Liu, Sheng and Qu, Qing and Zhu, Zhihui},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4303439907},
  pages         = {11547--11560},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/cdce17de141c9fba3bdf175a0b721941-Paper-Conference.pdf},
  title         = {Are All Losses Created Equal: A Neural Collapse Perspective},
  volume        = {35},
  year          = {2022}
}

@inproceedings{tirer2023deep,
  abstract      = {Neural collapse (NC) refers to the surprising structure of the last layer of deep neural networks in the terminal phase of gradient descent training. Recently, an increasing amount of experimental evidence has pointed to the propagation of NC to earlier layers of neural networks. However, while the NC in the last layer is well studied theoretically, much less is known about its multi-layered counterpart -- deep neural collapse (DNC). In this work, we fill this gap by generalizing the established analytical framework for NC -- the unconstrained features model -- to multiple non-linear layers. Our key technical contribution is to show that, in a deep unconstrained features model, the unique global optimum for binary classification exhibits all the properties typical of DNC. This explains the existing experimental evidence of DNC. Furthermore, we empirically show that (i) by optimizing deep unconstrained features models via gradient descent, the resulting solution agrees well with our theory, and (ii) trained networks recover the unconstrained features suitable for the occurrence of DNC, thus supporting the validity of this modeling principle.},
  address       = {Red Hook, NY, USA},
  author        = {Tirer, Tom and Bronstein, Michael M.},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  openalex      = {W4377865185},
  pages         = {74535--74563},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a60c43ba078b723d3d517d28c50ded4c-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/a60c43ba078b723d3d517d28c50ded4c-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{jiang2024generalized,
  abstract      = {Neural collapse provides an elegant mathematical characterization of learned last layer representations (a.k.a. features) and classifier weights in deep classification models. Such results not only provide insights but also motivate new techniques for improving practical deep models. However, most of the existing empirical and theoretical studies in neural collapse focus on the case that the number of classes is small relative to the dimension of the feature space. This paper extends neural collapse to cases where the number of classes are much larger than the dimension of feature space, which broadly occur for language models, retrieval systems, and face recognition applications. We show that the features and classifier exhibit a generalized neural collapse phenomenon, where the minimum one-vs-rest margins is maximized. We provide empirical study to verify the occurrence of generalized neural collapse in practical deep neural networks. Moreover, we provide theoretical study to show that the generalized neural collapse provably occurs under unconstrained feature model with spherical constraint, under certain technical conditions on feature dimension and number of classes.},
  author        = {Jiachen Jiang and Jinxin Deng and Zihao Wang and Amartya Sanyal and Yi Ma},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4387559956},
  pages         = {22010--22041},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jiang24i/jiang24i.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Generalized Neural Collapse for a Large Number of Classes},
  volume        = {235},
  year          = {2024}
}

@inproceedings{benshaul2024linguistic,
  abstract      = {Neural collapse (𝒩𝒞) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored 𝒩𝒞 in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as training by token prediction constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards 𝒩𝒞. We find that 𝒩𝒞 properties that develop with scale (and regularization) are linked to generalization. Moreover, there is evidence of some relationship between 𝒩𝒞 and generalization independent of scale. Our work thereby underscores the generality of 𝒩𝒞 as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on 𝒩𝒞-related properties.},
  address       = {Red Hook, NY, USA},
  author        = {Robert Wu and Vardan Papyan},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399151537},
  pages         = {95936},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f88cc8930b47a45ec4733123bf3039b9-Paper-Conference.pdf},
  publisher     = {Curran Associates Inc.},
  series        = {NeurIPS '24},
  title         = {Linguistic Collapse: Neural Collapse in (Large) Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/f88cc8930b47a45ec4733123bf3039b9-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{wu2025neural,
  abstract      = {Neural Collapse is a phenomenon where the last-layer representations of a well-trained neural network converge to a highly structured geometry. In this paper, we focus on its first (and most basic) property, known as NC1: the within-class variability vanishes. While prior theoretical studies establish the occurrence of NC1 via the data-agnostic unconstrained features model, our work adopts a data-specific perspective, analyzing NC1 in a three-layer neural network, with the first two layers operating in the mean-field regime and followed by a linear layer. In particular, we establish a fundamental connection between NC1 and the loss landscape: we prove that points with small empirical loss and gradient norm (thus, close to being stationary) approximately satisfy NC1, and the closeness to NC1 is controlled by the residual loss and gradient norm. We then show that (i) gradient flow on the mean squared error converges to NC1 solutions with small empirical loss, and (ii) for well-separated data distributions, both NC1 and vanishing test loss are achieved simultaneously. This aligns with the empirical observation that NC1 emerges during training while models attain near-zero test error. Overall, our results demonstrate that NC1 arises from gradient training due to the properties of the loss landscape, and they show the co-occurrence of NC1 and small test error for certain data distributions.},
  author        = {Wu, Diyuan and Mondelli, Marco},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  pdf           = {http://arxiv.org/pdf/2501.19104},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Neural Collapse Beyond the Unconstrained Features Model: Landscape, Dynamics, and Generalization in the Mean-Field Regime},
  volume        = {235},
  year          = {2025}
}

@inproceedings{nam2025position,
  abstract      = {In physics, complex systems are often simplified into minimal, solvable models that retain only the core principles. In machine learning, layerwise linear models (e.g., linear neural networks) act as simplified representations of neural network dynamics. These models follow the dynamical feedback principle, which describes how layers mutually govern and amplify each other's evolution. This principle extends beyond the simplified models, successfully explaining a wide range of dynamical phenomena in deep neural networks, including neural collapse, emergence, lazy and rich regimes, and grokking. While these behaviors are often attributed to complex interactions between architecture, data, and non-linear activations, we propose a unifying explanation based on gradient dynamics in layerwise models. Notably, these models lack non-linear activations, highlighting that the layerwise structure alone is a powerful yet underappreciated characteristic of deep neural networks. We argue that focusing on analytically tractable, layerwise models can not only explain existing phenomena but also uncover new insights, accelerating the scientific understanding of deep learning.},
  archiveprefix = {arXiv},
  author        = {Yoonsoo Nam and Seok Hyeong Lee and Clementine C J Domine and Yeachan Park and Charles London and Wonyl Choi and Niclas Goring and Seungjai Lee},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  doi           = {10.48550/arXiv.2502.21009},
  eprint        = {2502.21009},
  organization  = {PMLR},
  pages         = {40104},
  pdf           = {https://arxiv.org/pdf/2502.21009.pdf},
  primaryclass  = {stat.ML},
  title         = {Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)},
  url           = {https://icml.cc/virtual/2025/poster/40104},
  year          = {2025}
}

@article{galanti2025neural,
  abstract      = {The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent.},
  archiveprefix = {arXiv},
  author        = {Súkeník, Peter and Lampert, Christoph H. and Mondelli, Marco},
  doi           = {10.48550/arXiv.2505.15239},
  eprint        = {2505.15239},
  journal       = {arXiv preprint arXiv:2505.15239},
  month         = {5},
  primaryclass  = {cs.LG},
  title         = {Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers},
  url           = {https://arxiv.org/abs/2505.15239},
  year          = {2025}
}

@inproceedings{thrampoulidis2024neural,
  address       = {Athens, Greece},
  author        = {Thrampoulidis, Christos and Hassibi, Babak},
  booktitle     = {2024 IEEE International Symposium on Information Theory (ISIT)},
  month         = {7},
  publisher     = {IEEE},
  title         = {Neural Collapse in the Presence of Label Noise},
  year          = {2024}
}

@article{kothapalli2024can,
  abstract      = {A vast amount of literature has recently focused on the "Neural Collapse" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU). The code is available at: https://github.com/kvignesh1420/shallow_nc1},
  archiveprefix = {arXiv},
  author        = {Kothapalli, Vignesh and Tirer, Tom},
  eprint        = {2406.02105},
  journal       = {Transactions on Machine Learning Research},
  note          = {Accepted to TMLR},
  pdf           = {https://arxiv.org/pdf/2406.02105.pdf},
  primaryclass  = {cs.LG},
  title         = {Can Kernel Methods Explain How the Data Affects Neural Collapse?},
  url           = {https://arxiv.org/abs/2406.02105},
  year          = {2024}
}

@inproceedings{zhu2024neuralcollapse,
  address       = {Red Hook, NY, USA},
  author        = {Zhu, Zhihui and Tian, Jin and Yu, Minyue and Zhao, Han},
  booktitle     = {Advances in Neural Information Processing Systems},
  month         = {12},
  publisher     = {Curran Associates Inc.},
  series        = {NeurIPS},
  title         = {Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization},
  year          = {2024}
}

@article{le2025hierarchical,
  abstract      = {Recently, object detection models have witnessed notable performance improvements, particularly with transformer-based models. However, new objects frequently appear in the real world, requiring detection models to continually learn without suffering from catastrophic forgetting. This paper introduces Hier-DETR (Hierarchical Neural Collapse Detection Transformer), a novel framework for Incremental Object Detection (IOD) that leverages Neural Collapse for imbalanced datasets and hierarchical relationships between class labels. The method introduces Hierarchical Neural Collapse (HNC) which uses neural collapse to pre-assign fixed prototypes as simplex Equiangular Tight Frames (ETFs) for each class, preserving them as consistent priors throughout incremental learning.},
  archiveprefix = {arXiv},
  author        = {Pham, Duc Thanh and Nguyen, Hong Dang and Quoc, Nhat Minh Nguyen and Van, Linh Ngo and Viet, Sang Dinh and Nguyen, Duc Anh},
  eprint        = {2506.08562},
  journal       = {arXiv preprint arXiv:2506.08562},
  month         = {6},
  pdf           = {https://arxiv.org/pdf/2506.08562.pdf},
  primaryclass  = {cs.CV},
  title         = {Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection},
  url           = {https://arxiv.org/abs/2506.08562},
  year          = {2025}
}

@inproceedings{wang2024neuralcollapse,
  abstract      = {A recent study by De et al. (2022) has reported that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks, despite the high dimensionality of the feature space. To theoretically explain this phenomenon, we consider the setting of a layer-peeled model in representation learning, which results in interesting phenomena related to learned features in deep learning and transfer learning, known as Neural Collapse (NC). Within the framework of NC, we establish an error bound indicating that the misclassification error is independent of dimension when the distance between actual features and the ideal ones is smaller than a threshold. Additionally, the quality of the features in the last layer is empirically evaluated under different pre-trained models within the framework of NC, showing that a more powerful transformer leads to a better feature representation. Furthermore, we reveal that DP fine-tuning is less robust compared to fine-tuning without DP, particularly in the presence of perturbations. These observations are supported by both theoretical analyses and experimental evaluation. Moreover, to enhance the robustness of DP fine-tuning, we suggest several strategies, such as feature normalization or employing dimension reduction methods like Principal Component Analysis (PCA). Empirically, we demonstrate a significant improvement in testing accuracy by conducting PCA on the last-layer features.},
  address       = {Vienna, Austria},
  author        = {Wang, Chendi and Zhu, Yuqing and Su, Weijie J. and Wang, Yu-Xiang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4397028934},
  pages         = {50799--50834},
  pdf           = {https://proceedings.mlr.press/v235/wang24ax/wang24ax.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning},
  url           = {https://proceedings.mlr.press/v235/wang24ax.html},
  volume        = {235},
  year          = {2024}
}

@article{fang2021exploring,
  abstract      = {Deep learning's remarkable development over the past decade relies heavily on sophisticated heuristics and tricks. To better exploit its potential, a rigorous framework for reasoning about deep neural networks is needed. We introduce the Layer-Peeled Model, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. This model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts. We show that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse in deep learning training. When class imbalance is introduced, the model predicts a hitherto unknown phenomenon that we term Minority Collapse, which fundamentally limits the performance of deep-learning models on minority classes. The computational experiments confirm the validity of our analysis.},
  author        = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
  doi           = {10.1073/pnas.2103091118},
  journal       = {Proceedings of the National Academy of Sciences},
  number        = {43},
  openalex      = {W3205278400},
  pages         = {e2103091118},
  pdf           = {https://arxiv.org/pdf/2101.12699.pdf},
  publisher     = {National Academy of Sciences},
  title         = {Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training},
  url           = {https://www.pnas.org/doi/10.1073/pnas.2103091118},
  volume        = {118},
  year          = {2021}
}

@article{harun2025controlling,
  abstract      = {Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures.},
  author        = {Harun, Md Yousuf and Gallardo, Jhair and Kanan, Christopher},
  doi           = {10.48550/arxiv.2502.10691},
  journal       = {arXiv preprint arXiv:2502.10691},
  month         = {2},
  openalex      = {W4407683991},
  pdf           = {http://arxiv.org/pdf/2502.10691},
  title         = {Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning},
  url           = {https://arxiv.org/abs/2502.10691},
  year          = {2025}
}

@article{bottou2018optimization,
  abstract      = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  author        = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  doi           = {10.1137/16M1080173},
  journal       = {SIAM Review},
  number        = {2},
  openalex      = {W2963433607},
  pages         = {223--311},
  pdf           = {https://coral.ise.lehigh.edu/frankecurtis/files/papers/BottCurtNoce18.pdf},
  publisher     = {Society for Industrial and Applied Mathematics},
  title         = {Optimization Methods for Large-Scale Machine Learning},
  volume        = {60},
  year          = {2018}
}

@inproceedings{reddi2018convergence,
  abstract      = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSprop, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  author        = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  booktitle     = {International Conference on Learning Representations},
  month         = {2},
  note          = {Best Paper Award},
  openalex      = {W2785523195},
  pdf           = {https://openreview.net/pdf?id=ryQu7f-RZ},
  publisher     = {OpenReview.net},
  title         = {On the Convergence of Adam and Beyond},
  url           = {https://openreview.net/forum?id=ryQu7f-RZ},
  year          = {2018}
}

@inproceedings{luo2019adaptive,
  abstract      = {Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/AdaBound.},
  author        = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W2949935872},
  pdf           = {https://openreview.net/pdf?id=Bkg3g2R9FX},
  title         = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  url           = {https://openreview.net/forum?id=Bkg3g2R9FX},
  year          = {2019}
}

@inproceedings{dziugaite2017computing,
  abstract      = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this ``deep learning'' regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
  address       = {Sydney, Australia},
  author        = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
  booktitle     = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},
  month         = {8},
  openalex      = {W2604117713},
  pdf           = {http://auai.org/uai2017/proceedings/papers/173.pdf},
  publisher     = {AUAI Press},
  series        = {UAI '17},
  title         = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
  url           = {http://auai.org/uai2017/proceedings/papers/173.pdf},
  year          = {2017}
}

@inproceedings{neyshabur2018pac,
  abstract      = {We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.},
  author        = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  booktitle     = {International Conference on Learning Representations},
  openalex      = {W2963285844},
  pdf           = {https://openreview.net/pdf?id=Skz_WfbCZ},
  title         = {A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
  url           = {https://openreview.net/forum?id=Skz_WfbCZ},
  year          = {2018}
}

@article{power2022grokking,
  abstract      = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In some situations we show that neural networks learn through a process of `grokking' a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting.},
  archiveprefix = {arXiv},
  author        = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  doi           = {10.48550/arxiv.2201.02177},
  eprint        = {2201.02177},
  journal       = {arXiv preprint arXiv:2201.02177},
  month         = {1},
  note          = {Presented at ICLR 2021 Workshop on Mathematical Reasoning},
  openalex      = {W4226434736},
  pdf           = {https://arxiv.org/pdf/2201.02177.pdf},
  primaryclass  = {cs.LG},
  title         = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  url           = {https://arxiv.org/abs/2201.02177},
  year          = {2022}
}

@inproceedings{chizat2019lazy,
  abstract      = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this lazy training phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that lazy training is behind the many successes of neural networks in difficult high dimensional tasks.},
  author        = {Chizat, Lénaı̈c and Oyallon, Edouard and Bach, Francis},
  booktitle     = {Advances in Neural Information Processing Systems 32},
  openalex      = {W2952204734},
  pages         = {2937--2947},
  pdf           = {https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {On Lazy Training in Differentiable Programming},
  url           = {https://proceedings.neurips.cc/paper/2019/hash/ae614c557843b1df326cb29c57225459-Abstract.html},
  year          = {2019}
}

@inproceedings{lyle2023surprising,
  abstract      = {Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it often occurs in the absence of saturated units. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings on larger-scale RL benchmarks in the Arcade Learning Environment.},
  author        = {Lyle, Clare and Zheng, Zeyu and Nikishin, Evgenii and Pires, Bernardo Avila and Pascanu, Razvan and Dabney, Will},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4323076644},
  pages         = {23190--23211},
  pdf           = {https://proceedings.mlr.press/v202/lyle23b/lyle23b.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Understanding Plasticity in Neural Networks},
  volume        = {202},
  year          = {2023}
}

@article{lecun2015deep,
  abstract      = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  author        = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  doi           = {10.1038/nature14539},
  journal       = {Nature},
  month         = {5},
  number        = {7553},
  openalex      = {W2919115771},
  pages         = {436--444},
  pdf           = {https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf},
  pmid          = {26017442},
  publisher     = {Nature Publishing Group},
  title         = {Deep Learning},
  volume        = {521},
  year          = {2015}
}

@article{duchi2011adaptive,
  abstract      = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. The adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  author        = {Duchi, John C. and Hazan, Elad and Singer, Yoram},
  journal       = {Journal of Machine Learning Research},
  month         = {2},
  number        = {61},
  openalex      = {W2146502635},
  pages         = {2121--2159},
  pdf           = {https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
  title         = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  url           = {https://jmlr.org/papers/v12/duchi11a.html},
  volume        = {12},
  year          = {2011}
}

@inproceedings{pascanu2013difficulty,
  abstract      = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  address       = {Atlanta, Georgia, USA},
  author        = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle     = {Proceedings of the 30th International Conference on Machine Learning},
  month         = {6},
  number        = {3},
  openalex      = {W1815076433},
  pages         = {1310--1318},
  pdf           = {https://proceedings.mlr.press/v28/pascanu13.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On the difficulty of training recurrent neural networks},
  volume        = {28},
  year          = {2013}
}

@article{srivastava2014dropout,
  abstract      = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  author        = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal       = {Journal of Machine Learning Research},
  number        = {1},
  openalex      = {W2095705004},
  pages         = {1929--1958},
  pdf           = {https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf},
  publisher     = {JMLR.org},
  title         = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  url           = {https://jmlr.org/papers/v15/srivastava14a.html},
  volume        = {15},
  year          = {2014}
}

@inproceedings{ioffe2015batch,
  abstract      = {Training Deep Neural Networks is complicated by the fact that distribution of each layer's inputs changes during training, as parameters of previous layers change. This slows down training, requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. The authors refer to this phenomenon as ``internal covariate shift'' and address the problem by normalizing layer inputs. Their method draws strength from making normalization a part of the model architecture, performing for mini-batch. Batch Normalization allows higher learning rates and less careful initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, it achieves the same accuracy 14 times faster. Using an ensemble of batch-normalized networks, they improve upon the best published result on ImageNet classification, reaching 4.9% top-5 validation error (and 4.8% test error), exceeding human raters.},
  address       = {Lille, France},
  author        = {Ioffe, Sergey and Szegedy, Christian},
  booktitle     = {Proceedings of the 32nd International Conference on Machine Learning},
  openalex      = {W1836465849},
  pages         = {448--456},
  pdf           = {https://proceedings.mlr.press/v37/ioffe15.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  volume        = {37},
  year          = {2015}
}

@inproceedings{cattaneo2024implicit,
  abstract      = {In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSprop and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We provide numerical evidence by training vision models on CIFAR-10 using full-batch Adam, finding that stronger implicit anti-regularization effects correlate with worse generalization. This leads us to propose a novel explanation for the often-reported poor generalization of adaptive gradient algorithms.},
  author        = {Matias D. Cattaneo and Jason Matthew Klusowski and Boris Shigida},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4386435777},
  pages         = {5862--5906},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/cattaneo24a/cattaneo24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {On the Implicit Bias of Adam},
  url           = {https://proceedings.mlr.press/v235/cattaneo24a.html},
  volume        = {235},
  year          = {2024}
}

@article{dipilato2025loss,
  abstract      = {In this paper, we propose a method to perform empirical analysis of the loss landscape of machine learning (ML) models. The method is applied to two ML models for scientific sensing, which necessitates quantization to be deployed and are subject to noise and perturbations due to experimental conditions. Our method allows assessing the robustness of ML models to such effects as a function of quantization precision and under different regularization techniques -- two crucial concerns that remained underexplored so far. By investigating the interplay between performance, efficiency, and robustness by means of loss landscape analysis, we both established a strong correlation between gently-shaped landscapes and robustness to input and weight perturbations and observed other intriguing and non-obvious phenomena. Our method allows a systematic exploration of such trade-offs a priori, i.e., without training and testing multiple models, leading to more efficient development workflows. This work also highlights the importance of incorporating robustness into the Pareto optimization of ML models, enabling more reliable and adaptive scientific sensing systems.},
  author        = {Baldi, Tommaso Lisini and Campos, Javier and Weng, Olivia and Geniesse, Caleb and Tran, Nhan and Kastner, Ryan and Biondi, Alessandro},
  journal       = {arXiv preprint arXiv:2502.08355},
  month         = {2},
  note          = {FERMILAB-CONF-25-0045-CSAID},
  openalex      = {W4407570353},
  pdf           = {https://arxiv.org/pdf/2502.08355.pdf},
  title         = {Loss Landscape Analysis for Reliable Quantized ML Models for Scientific Sensing},
  url           = {https://arxiv.org/abs/2502.08355},
  year          = {2025}
}

@inproceedings{fisher2024pushing,
  abstract      = {Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame (ETF), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this work, we investigate the last-layer activations of mixup-trained neural networks across a variety of architectures and datasets. We discover that mixup induces a distinctive configuration where the activations from mixed-up examples of identical classes align with the classifier, while those from mixed-up examples of different classes delineate channels along the decision boundary. These findings are unexpected, as mixed-up features are not simple convex combinations of feature class means (as one might get, for example, by training mixup with the mean squared error loss). By analyzing this distinctive geometric configuration, we elucidate the mechanisms by which mixup enhances model calibration. To further validate our empirical observations, we conduct a theoretical analysis under the assumption of an unconstrained features model, utilizing the mixup loss. Through this, we characterize and derive the optimal last-layer features under the assumption that the classifier forms a simplex ETF. Our theoretical findings corroborate our empirical observations, confirming that mixup indeed induces a rigid geometric configuration in last-layer features richer than that of Neural Collapse.},
  author        = {Fisher, Quinn and Meng, Haoming and Papyan, Vardan},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  pdf           = {https://arxiv.org/pdf/2402.06171.pdf},
  title         = {Pushing Boundaries: Mixup's Influence on Neural Collapse},
  url           = {https://openreview.net/forum?id=jTSKkcbEsj},
  year          = {2024}
}

@inproceedings{mixon2025can,
  address       = {Lyon, France},
  author        = {Mixon, Dustin G. and Papyan, Vardan and Donoho, David L.},
  booktitle     = {Proceedings of the 38th Annual Conference on Learning Theory},
  month         = {6},
  note          = {Paper appears to be forthcoming - complete publication details pending},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Can Neural Collapse be Avoided? The Role of Regularization},
  year          = {2025}
}
