@article{alonso2024control,
  abstract      = {In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. We provide a systematic review of the most successful SSM proposals and highlight their main features from a control theoretic perspective.},
  author        = {Carmen Amo Alonso and Jérôme Sieber and Melanie N. Zeilinger},
  doi           = {10.48550/arXiv.2403.16899},
  eprint        = {2403.16899},
  eprinttype    = {arXiv},
  journal       = {arXiv preprint arXiv:2403.16899},
  month         = {3},
  note          = {Systematic analysis from control theory perspective highlighting connections to classical systems theory, providing crucial insights for better SSM design and understanding},
  openalex      = {W4393213722},
  primaryclass  = {eess.SY},
  title         = {State Space Models as Foundation Models: A Control Theoretic Overview},
  url           = {https://arxiv.org/abs/2403.16899},
  year          = {2024}
}

@inproceedings{cirone2024theoretical,
  abstract      = {Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.},
  author        = {Cirone, Nicola Muca and Orvieto, Antonio and Walker, Benjamin and Salvi, Cristopher and Lyons, Terry},
  booktitle     = {Advances in Neural Information Processing Systems 37},
  doi           = {10.48550/arxiv.2402.19047},
  openalex      = {W4392492373},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e6231c5f46598cfd09ff1970524e0436-Paper-Conference.pdf},
  series        = {NIPS},
  title         = {Theoretical Foundations of Deep Selective State-Space Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/e6231c5f46598cfd09ff1970524e0436-Abstract-Conference.html},
  year          = {2024}
}

@inproceedings{dao2023h3,
  abstract      = {State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$ imes$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$ imes$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.},
  address       = {Kigali, Rwanda},
  author        = {Dao, Tri and Fu, Daniel Y. and Saab, Khaled K. and Thomas, Armin W. and Rudra, Atri and Ré, Christopher},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  month         = {5},
  note          = {First SSM architecture competitive with Transformers for language modeling, introducing FlashConv for hardware-efficient computation and establishing the template for modern SSM architectures. (Oral presentation)},
  openalex      = {W4313442864},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=COZDy0WYGg},
  publisher     = {ICLR},
  title         = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  url           = {https://arxiv.org/abs/2212.14052},
  year          = {2023}
}

@inproceedings{dao2024ssd,
  abstract      = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
  author        = {Tri Dao and Albert Gu},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  note          = {Landmark theoretical work establishing the Structured State Space Duality (SSD) framework, unifying Transformers and SSMs through structured matrices and enabling Mamba-2 with 2-8× speedup over Mamba.},
  openalex      = {W4399317989},
  pages         = {10041--10071},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/dao24a/dao24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  url           = {https://proceedings.mlr.press/v235/dao24a.html},
  volume        = {235},
  year          = {2024}
}

@article{de2024griffin,
  abstract      = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. All models contain three components: (i) a residual block, (ii) an MLP block, and (iii) a temporal-mixing block. The recurrent block benefits from Real-Gated Linear Recurrent Unit (RG-LRU) -- a novel recurrent layer inspired by the Linear Recurrent Unit. The models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. Griffin is scaled to 14 billion parameters.},
  archiveprefix = {arXiv},
  author        = {De, Soham and Smith, Samuel L. and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and Desjardins, Guillaume and Doucet, Arnaud and Budden, David and Teh, Yee Whye and Pascanu, Razvan and De Freitas, Nando and Gulcehre, Caglar},
  doi           = {10.48550/arXiv.2402.19427},
  eprint        = {2402.19427},
  institution   = {Google DeepMind},
  journal       = {arXiv preprint},
  month         = {2},
  note          = {Hybrid architecture combining gated linear recurrences with local attention. Griffin matches Llama-2 performance using 6× fewer training tokens and scales to 14B parameters.},
  pdf           = {https://arxiv.org/pdf/2402.19427},
  primaryclass  = {cs.LG},
  title         = {Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  url           = {https://arxiv.org/abs/2402.19427},
  volume        = {arXiv:2402.19427},
  year          = {2024}
}

@inproceedings{gu2020hippo,
  abstract      = {A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25--40% accuracy.},
  author        = {Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Ré},
  booktitle     = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  note          = {Established the theoretical foundation for all subsequent SSM work by introducing optimal polynomial projections for online compression of continuous signals, solving the fundamental problem of how to efficiently memorize long sequences.},
  openalex      = {W3064840847},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
  title         = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  url           = {https://arxiv.org/abs/2008.07669},
  year          = {2020}
}

@inproceedings{gu2021lssl,
  abstract      = {Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. Our Linear State-Space Layer (LSSL) maps a sequence through a linear continuous-time state-space representation. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. We incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech.},
  address       = {Red Hook, NY, USA},
  author        = {Albert Gu and Isys Johnson and Karan Goel and Khaled Kamal Saab and Tri Dao and Atri Rudra and Christopher Ré},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {First work to demonstrate that SSMs could be integrated into deep learning architectures, introducing the dual recurrent/convolutional paradigm that enables both efficient training and inference.},
  openalex      = {W3212492847},
  pages         = {572--585},
  pdf           = {https://proceedings.neurips.cc/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf},
  publisher     = {Curran Associates Inc.},
  title         = {Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers},
  url           = {https://papers.neurips.cc/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html},
  volume        = {34},
  year          = {2021}
}

@inproceedings{gu2022s4,
  abstract      = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), and showed that for appropriate choices of the state matrix A, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.},
  author        = {Gu, Albert and Goel, Karan and Ré, Christopher},
  booktitle     = {The Tenth International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2111.00396},
  note          = {Outstanding Paper Honorable Mention. Oral presentation. The breakthrough that made SSMs practical for deep learning through DPLR parameterization and efficient Cauchy kernel computation, achieving state-of-the-art on Long Range Arena and solving the Path-X task.},
  openalex      = {W3209374680},
  pdf           = {https://openreview.net/pdf?id=uYLFoz1vlAC},
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  url           = {https://openreview.net/forum?id=uYLFoz1vlAC},
  year          = {2022}
}

@inproceedings{gu2022s4d,
  abstract      = {State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85% on the Long Range Arena benchmark.},
  address       = {New Orleans, LA, USA},
  author        = {Gu, Albert and Gupta, Ankit and Goel, Karan and Ré, Christopher},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  note          = {Provided theoretical understanding of why diagonal SSMs work and introduced S4D, which requires only 2 lines of code for kernel computation while maintaining S4's performance.},
  openalex      = {W4283461773},
  pages         = {35971--35983},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e9a32fade47b906de908431991440f7c-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {On the Parameterization and Initialization of Diagonal State Space Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e9a32fade47b906de908431991440f7c-Paper-Conference.pdf},
  volume        = {35},
  year          = {2022}
}

@inproceedings{gu2023hippo,
  abstract      = {Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular *time-varying* dynamical system, and the use of this matrix as a *time-invariant* SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.},
  author        = {Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and Ré, Christopher},
  booktitle     = {International Conference on Learning Representations},
  note          = {Unified and generalized the HiPPO framework, explaining S4's success through exponentially-warped Legendre polynomials and introducing new SSM variants for different basis functions.},
  openalex      = {W4283658602},
  pdf           = {https://openreview.net/pdf?id=klK17OQ3KB},
  title         = {How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections},
  url           = {https://openreview.net/forum?id=klK17OQ3KB},
  year          = {2023}
}

@inproceedings{gu2024mamba,
  abstract      = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  author        = {Gu, Albert and Dao, Tri},
  booktitle     = {First Conference on Language Modeling},
  doi           = {10.48550/arxiv.2312.00752},
  month         = {7},
  note          = {Revolutionary introduction of selective SSMs (informally "S6") with input-dependent parameters, finally enabling content-based reasoning and matching Transformer performance on language modeling with linear complexity.},
  openalex      = {W4389326242},
  pdf           = {https://openreview.net/pdf?id=tEYskw1VY2},
  series        = {COLM},
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  url           = {https://openreview.net/forum?id=tEYskw1VY2},
  year          = {2024}
}

@inproceedings{gupta2022dss,
  abstract      = {Modeling long range dependencies in sequential data is a fundamental step towards attaining human-level performance in many modalities such as text, vision, audio and video. While attention-based models are a popular and effective choice in modeling short-range interactions, their performance on tasks requiring long range reasoning has been largely inadequate. In an exciting result, Gu et al. (ICLR 2022) proposed the Structured State Space (S4) architecture delivering large gains over state-of-the-art models on several long-range tasks across various modalities. The core proposition of S4 is the parameterization of state matrices via a diagonal plus low rank structure, allowing efficient computation. In this work, we show that one can match the performance of S4 even without the low rank correction and thus assuming the state matrices to be diagonal. Our Diagonal State Space (DSS) model matches the performance of S4 on Long Range Arena tasks, speech classification on Speech Commands dataset, while being conceptually simpler and straightforward to implement.},
  author        = {Ankit Gupta and Albert Gu and Jonathan Berant},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  note          = {Demonstrated that diagonal matrices could match S4's performance with much simpler implementation, leading to widespread adoption of diagonal SSMs as the preferred approach.},
  openalex      = {W4221145950},
  pages         = {22982--22994},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9156b0f6dfa9bbd18c79cc459ef5d61c-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Diagonal State Spaces are as Effective as Structured State Spaces},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{hasani2022liquid,
  abstract      = {A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on an extensive series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structured SSM, such as S4, is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structured state-space model, dubbed Liquid-S4, improves generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset, Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.},
  author        = {Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  note          = {Combined S4 with liquid time-constant networks for adaptive, input-dependent dynamics, achieving state-of-the-art on Long Range Arena and effectiveness on medical time series.},
  openalex      = {W4297941019},
  pdf           = {https://openreview.net/pdf?id=g4OTKRKfS7R},
  title         = {Liquid Structural State-Space Models},
  url           = {https://openreview.net/forum?id=g4OTKRKfS7R},
  year          = {2023}
}

@inproceedings{li2024graphssm,
  abstract      = {Over the past few years, research on deep graph learning has shifted from static graphs to temporal graphs in response to real-world complex systems that exhibit dynamic behaviors. In practice, temporal graphs are formalized as an ordered sequence of static graph snapshots observed at discrete time points. Sequence models such as RNNs or Transformers have long been the predominant backbone networks for modeling such temporal graphs. Yet, despite the promising results, RNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Recently, state space models (SSMs), which are framed as discretized representations of an underlying continuous-time linear dynamical system, have garnered substantial attention and achieved breakthrough advancements in independent sequence modeling. In this work, we undertake a principled investigation that extends SSM theory to temporal graphs by integrating structural information into the online approximation objective via the adoption of a Laplacian regularization term. The emergent continuous-time system introduces novel algorithmic challenges, thereby necessitating our development of GraphSSM, a graph state space model for modeling the dynamics of temporal graphs. Extensive experimental results demonstrate the effectiveness of our GraphSSM framework across various temporal graph benchmarks.},
  author        = {Li, Jintang and Wu, Ruofan and Jin, Xinzhou and Ma, Boqun and Chen, Liang and Zheng, Zibin},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Extended SSM theory to temporal graphs by integrating structural information, opening up SSMs to graph neural network applications with GraphSSM.},
  openalex      = {W4399399879},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/e5ba3d6d93213db6b1d1931c6517fe1a-Paper-Conference.pdf},
  title         = {State Space Models on Temporal Graphs: A First-Principles Study},
  volume        = {37},
  year          = {2024}
}

@article{lieber2024jamba,
  abstract      = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.},
  author        = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and Abend, Omri and Alon, Raz and Asida, Tomer and Bergman, Amir and Glozman, Roman and Gokhman, Michael and Manevich, Avashalom and Ratner, Nir and Rozen, Noam and Shwartz, Erez and Zusman, Mor and Shoham, Yoav},
  doi           = {10.48550/arxiv.2403.19887},
  journal       = {arXiv preprint arXiv:2403.19887},
  month         = {3},
  note          = {First production-grade hybrid SSM-Transformer architecture combining Mamba with attention and mixture-of-experts, achieving 3$ imes$ throughput on long contexts with 256K context window},
  openalex      = {W4393399080},
  pdf           = {https://arxiv.org/pdf/2403.19887.pdf},
  title         = {Jamba: A Hybrid Transformer-Mamba Language Model},
  url           = {https://arxiv.org/abs/2403.19887},
  year          = {2024}
}

@inproceedings{liu2024vmamba,
  abstract      = {Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Extensive experiments demonstrate VMamba's promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models.},
  author        = {Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Jiao, Jianbin and Liu, Yunfan},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Introduced 2D Selective Scan (SS2D) module bridging 1D scans and 2D image structure, achieving linear complexity for vision tasks while maintaining competitive performance with transformers.},
  openalex      = {W4391047432},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/baa2da9ae4bfed26520bb61d259a3653-Paper-Conference.pdf},
  title         = {VMamba: Visual State Space Model},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/baa2da9ae4bfed26520bb61d259a3653-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{lv2024decisionmamba,
  abstract      = {While the conditional sequence modeling with the transformer architecture has demonstrated its effectiveness in dealing with offline reinforcement learning (RL) tasks, it is struggle to handle out-of-distribution states and actions. Existing work attempts to address this issue by data augmentation with the learned policy or adding extra constraints with the value-based RL algorithm. However, these studies still fail to overcome the following challenges: (1) insufficiently utilizing the historical temporal information among inter-steps, (2) overlooking the local intra-step relationships among states, actions and return-to-gos (RTGs), (3) overfitting suboptimal trajectories with noisy labels. To address these challenges, we propose Decision Mamba (DM), a novel multi-grained state space model (SSM) with a self-evolving policy learning strategy. DM explicitly models the historical hidden state to extract the temporal information by using the mamba architecture. To capture the relationship among state-action-RTG triplets, a fine-grained SSM module is designed and integrated into the original coarse-grained SSM in mamba, resulting in a novel mamba architecture tailored for offline RL. Finally, to mitigate the overfitting issue on noisy trajectories, a self-evolving policy is proposed by using progressive regularization. The policy evolves by using its own past knowledge to refine the suboptimal actions, thus enhancing its robustness on noisy demonstrations. Extensive experiments on various tasks show that DM outperforms other baselines substantially. Our code is available at r̆lhttps://github.com/aopolin-lv/DecisionMamba.},
  author        = {Lv, Qi and Deng, Xiang and Chen, Gongwei and Wang, Michael Yu and Nie, Liqiang},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Successfully adapted SSMs to reinforcement learning with multi-grained modules and self-evolving policy learning, showing superior performance on D4RL benchmarks.},
  openalex      = {W4399554814},
  pages         = {22827--22849},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/288b63aa98084366c4536ba0574a0f22-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL},
  url           = {https://arxiv.org/abs/2406.05427},
  volume        = {37},
  year          = {2024}
}

@inproceedings{malach2024repeat,
  abstract      = {Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on length, which we refer to as 'generalized state space models' (GSSMs). In this paper we show while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformers for tasks that require copying from input context. We start with a theoretical analysis of a simple string copying task and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic copying tasks. Finally, we evaluate pretrained large language models and find that transformers dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.},
  author        = {Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  note          = {An essential critical analysis showing that Transformers significantly outperform SSMs on tasks requiring precise copying or retrieval from context, highlighting a fundamental trade-off between the architectures.},
  openalex      = {W4391555700},
  pages         = {21502--21521},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jelassi24a/jelassi24a.pdf},
  series        = {Proceedings of Machine Learning Research},
  title         = {Repeat After Me: Transformers are Better than State Space Models at Copying},
  volume        = {235},
  year          = {2024}
}

@inproceedings{mambalrp2024,
  abstract      = {Recent sequence modeling approaches using selective state space sequence models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.},
  author        = {Jafari, Farnoush Rezaei and Montavon, Grégoire and Müller, Klaus-Robert and Eberle, Oliver},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4399694442},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d6d0e41e0b1ed38c76d13c9e417a8f1f-Paper-Conference.pdf},
  series        = {NeurIPS '24},
  title         = {MambaLRP: Explaining Selective State Space Sequence Models},
  url           = {https://papers.nips.cc/paper_files/paper/2024/hash/d6d0e41e0b1ed38c76d13c9e417a8f1f-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@inproceedings{mambarecon2024,
  abstract      = {Magnetic Resonance Imaging (MRI) is one of the most important medical imaging modalities as it provides superior resolution of soft tissues, albeit with a notable limitation in scanning speed. The advent of deep learning has catalyzed the development of cutting-edge methods for the expedited reconstruction of MRI scans, utilizing convolutional neural networks and, more recently, vision transformers. Recently proposed structured state space models (e.g., Mamba) have gained some traction due to their efficiency and low computational requirements compared to transformer models. We propose an innovative MRI reconstruction framework that employs structured state space models at its core, aimed at amplifying both long-range contextual sensitivity and reconstruction efficacy. Comprehensive experiments on public brain MRI datasets show that our model sets new benchmarks beating state-of-the-art reconstruction baselines. Code will be available (https://github.com/yilmazkorkmaz1/MambaRecon).},
  address       = {Tucson, AZ, USA},
  author        = {Korkmaz, Yilmaz and Patel, Vishal M.},
  booktitle     = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  month         = {2},
  note          = {Demonstrates a high-impact application in medical imaging, using Mamba to achieve state-of-the-art MRI reconstruction quality by capturing long-range contextual information more efficiently than Transformers.},
  openalex      = {W4403747244},
  organization  = {IEEE},
  pdf           = {https://openaccess.thecvf.com/content/WACV2025/papers/Korkmaz_MambaRecon_MRI_Reconstruction_with_Structured_State_Space_Models_WACV_2025_paper.pdf},
  publisher     = {IEEE},
  title         = {MambaRecon: MRI Reconstruction with Structured State Space Models},
  url           = {https://github.com/yilmazkorkmaz1/MambaRecon},
  year          = {2025}
}

@inproceedings{merrill2024illusion,
  abstract      = {State-space models (SSMs) have emerged as a potential alternative to transformer architectures for large language models. The paper argues that SSMs, despite claims of improved state tracking, actually have very limited expressive power. The research reveals that SSMs cannot express computation outside the complexity class $\mathsfTC^0$, which means they have similar limitations to transformers in state tracking. SSMs struggle to solve simple state-tracking problems like permutation composition and cannot accurately track chess moves, evaluate code, or track entities in a long narrative. Experiments show that Mamba-style models indeed struggle with state tracking tasks. The authors conclude that the state in state-space models is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers.},
  author        = {William Merrill and Jackson Petty and Ashish Sabharwal},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  note          = {A theoretical paper arguing that despite their recurrent formulation, SSMs are confined to the same computational complexity class (TC⁰) as Transformers and thus may not have a fundamental advantage in complex state-tracking tasks.},
  openalex      = {W4394867439},
  pages         = {35492--35506},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/merrill24a/merrill24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {The Illusion of State in State-Space Models},
  url           = {https://proceedings.mlr.press/v235/merrill24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{nguyen2022s4nd,
  abstract      = {Visual data such as images and videos are typically modeled as discretizations of inherently continuous, multidimensional signals. Existing continuous-signal models attempt to exploit this fact by modeling the underlying signals of visual (e.g., image) data directly. However, these models have not yet been able to achieve competitive performance on practical vision tasks such as large-scale image and video classification. Building on a recent line of work on deep state space models (SSMs), we propose S4ND, a new multidimensional SSM layer that extends the continuous-signal modeling ability of SSMs to multidimensional data including images and videos. We show that S4ND can model large-scale visual data in 1D, 2D, and 3D as continuous multidimensional signals and demonstrates strong performance by simply swapping Conv2D and self-attention layers with S4ND layers in existing state-of-the-art models. On ImageNet-1k, S4ND exceeds the performance of a Vision Transformer baseline by 1.5% when training with a 1D sequence of patches, and matches ConvNeXt when modeling images in 2D. For videos, S4ND improves on an inflated 3D ConvNeXt in activity classification on HMDB-51 by 4%. S4ND implicitly learns global, continuous convolutional kernels that are resolution invariant by construction, providing an inductive bias that enables generalization across multiple resolutions. By developing a simple bandlimiting modification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen at training time) resolution performance, outperforming a baseline Conv2D by 40% on CIFAR-10 when trained on 8 × 8 and tested on 32 × 32 images. When trained with progressive resizing, S4ND comes within ∼ 1% of a high-resolution model while training 22% faster.},
  author        = {Nguyen, Eric and Goel, Karan and Gu, Albert and Downs, Gordon W. and Shah, Preey and Dao, Tri and Baccus, Stephen A. and Ré, Christopher},
  booktitle     = {Advances in Neural Information Processing Systems 35 (NeurIPS)},
  note          = {Extended S4 to N-dimensional signals by creating N-D convolution kernels as outer products of 1-D S4 kernels, enabling efficient processing of images and videos.},
  openalex      = {W4306295032},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/13388efc819c09564c66ab2dc8463809-Paper-Conference.pdf},
  title         = {S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces},
  url           = {https://arxiv.org/abs/2210.06583},
  year          = {2022}
}

@inproceedings{peng2023rwkv,
  abstract      = {Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.},
  address       = {Singapore},
  author        = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Lin, Jiaju and Kazienko, Przemyslaw and Kocon, Jan and Kong, Jiaming and Koptyra, Bartlomiej and Lau, Hayden and Mantri, Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and Song, Guangyu and Tang, Xiangru and Wang, Bolun and Wind, Johan S. and Wozniak, Stanislaw and Zhang, Ruichong and Zhang, Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhou, Qinghua and Zhu, Jian and Zhu, Rui-Jie},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  doi           = {10.18653/v1/2023.findings-emnlp.936},
  note          = {A highly popular and performant alternative architecture that combines the parallelizable training of Transformers with the O(1) inference efficiency of RNNs, representing a parallel path in the quest for efficient sequence models.},
  openalex      = {W4389524555},
  pages         = {14048--14077},
  pdf           = {https://aclanthology.org/2023.findings-emnlp.936.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  url           = {https://aclanthology.org/2023.findings-emnlp.936/},
  year          = {2023}
}

@inproceedings{poli2023hyena,
  abstract      = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In challenging reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-space models, transfer functions, and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets WikiText103 and The Pile, reaching Transformer quality with a 20% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, with speedups of 100x at 64k.},
  author        = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  doi           = {10.48550/arxiv.2302.10866},
  note          = {First subquadratic attention replacement approaching Transformer quality through interleaved implicit parametrized convolutions, achieving 100× speedup at 64K sequence length.},
  openalex      = {W4321593177},
  pages         = {28043--28078},
  pdf           = {https://proceedings.mlr.press/v202/poli23a/poli23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  volume        = {202},
  year          = {2023}
}

@software{poli2023stripedhyena,
  abstract      = {StripedHyena is a hybrid neural network architecture composed of rotary attention and gated convolutions arranged in Hyena blocks, designed to scale more efficiently than decoder-only Transformers. The architecture leverages the specialization of each layer class, with Hyena layers implementing bulk sequence processing and attention layers supplementing targeted pattern recall. StripedHyena achieves over 30%, 50%, and 100% faster training on sequences of length 32k, 64k, and 128k respectively, compared to optimized Transformer baselines. The model enables efficient autoregressive generation with caches over 50% smaller than equivalent Transformers and supports generation of over 500k tokens on a single 80GB GPU.},
  author        = {Poli, Michael and Wang, Jue and Massaroli, Stefano and Quesnelle, Jeffrey and Carlow, Ryan and Nguyen, Eric and Thomas, Armin W.},
  doi           = {10.57967/hf/1595},
  howpublished  = {GitHub repository},
  license       = {Apache-2.0},
  month         = {12},
  note          = {Open-source implementation with StripedHyena-Hessian-7B (base model) and StripedHyena-Nous-7B (chat model). Competitive performance with Llama-2 and Mistral 7B using hybrid architectures.},
  publisher     = {Together Computer},
  title         = {StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models},
  url           = {https://github.com/togethercomputer/stripedhyena},
  year          = {2023}
}

@inproceedings{rusch2024oscillatory,
  abstract      = {We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba and LRU by nearly 2x on a sequence modeling task with sequences of length 50k.},
  author        = {Rusch, T. Konstantin and Rus, Daniela},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2410.03943},
  month         = {5},
  note          = {Oral presentation. Introduced Linear Oscillatory State-Space models based on forced harmonic oscillators with biological inspiration, proving universality properties and stable dynamics.},
  openalex      = {W4403929263},
  pdf           = {https://openreview.net/pdf?id=GRMfXcAAFh},
  series        = {ICLR},
  title         = {Oscillatory State-Space Models},
  url           = {https://openreview.net/forum?id=GRMfXcAAFh},
  year          = {2025}
}

@inproceedings{samba2024,
  abstract      = {Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, Samba shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, Samba achieves a 3.73x higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64x speedup when generating 64K tokens with unlimited streaming. Our code for training on open source data is publicly available at https://github.com/microsoft/Samba.},
  archiveprefix = {arXiv},
  author        = {Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
  booktitle     = {International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2406.07522},
  eprint        = {2406.07522},
  month         = {5},
  openalex      = {W4399655853},
  pdf           = {https://arxiv.org/pdf/2406.07522.pdf},
  primaryclass  = {cs.CL},
  title         = {SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
  url           = {https://arxiv.org/abs/2406.07522},
  year          = {2025}
}

@article{senocak2024audiomamba,
  abstract      = {Transformers have rapidly become the preferred choice for audio classification, surpassing methods based on CNNs. However, Audio Spectrogram Transformers (ASTs) exhibit quadratic scaling due to self-attention. The removal of this quadratic self-attention cost presents an appealing direction. Recently, state space models (SSMs), such as Mamba, have demonstrated potential in language and vision tasks in this regard. In this study, we explore whether reliance on self-attention is necessary for audio classification tasks. By introducing Audio Mamba (AuM), the first self-attention-free, purely SSM-based model for audio classification, we aim to address this question. We evaluate AuM on various audio datasets -- comprising six different benchmarks -- where it achieves comparable or better performance compared to well-established AST model.},
  author        = {Erol, Mehmet Hamza and Senocak, Arda and Feng, Jiu and Chung, Joon Son},
  doi           = {10.1109/lsp.2024.3483009},
  journal       = {IEEE Signal Processing Letters},
  openalex      = {W4403511013},
  pages         = {2975--2979},
  title         = {Audio Mamba: Bidirectional State Space Model for Audio Representation Learning},
  url           = {https://arxiv.org/abs/2406.03344},
  volume        = {31},
  year          = {2024}
}

@article{serpent2025,
  abstract      = {State Space Models (SSMs) have recently enjoyed a rise to prominence in the field of deep learning for sequence modeling, especially as an alternative to Transformers. Their success stems from avoiding two well-known drawbacks of attention-based models: quadratic complexity with respect to the sequence length and inability to model long-range dependencies. The SSM variant Mamba has demonstrated performance comparable to Transformers without any form of attention, thanks to the use of a selective mechanism for the state parameters. Selectivity, however, is only evaluated empirically and the reasons of its effectiveness remain unclear. In this work, we show how selectivity is related to the sequence processing. Our analysis shows that selective time intervals in Mamba act as linear approximators of information. Then, we propose our SeRpEnt architecture, a SSM that further exploits selectivity to compress sequences in an information-aware fashion. It employs a resampling mechanism that aggregates elements based on their information content. Our empirical results in the Long Range Arena benchmark and other language modeling tasks show benefits of the SeRpEnt's resampling mechanism.},
  author        = {Stefano Rando and Lucia Romani and Matteo Migliarini and Luca Franco and Denis Gudovskiy and Fabio Galasso},
  journal       = {arXiv preprint arXiv:2501.11729},
  month         = {1},
  note          = {A new architecture building on Mamba's selectivity. It proposes an information-aware method to compress sequences, further improving the efficiency of global processing.},
  openalex      = {W4406733257},
  pdf           = {https://arxiv.org/pdf/2501.11729},
  title         = {SeRpEnt: Selective Resampling for Expressive State Space Models},
  url           = {https://arxiv.org/abs/2501.11729},
  year          = {2025}
}

@article{slutzky2024poison,
  abstract      = {Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs), regarded as an efficient alternative to transformers. Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e. having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, particularly in large language models, we believe significant efforts should be invested in further delineating their susceptibility to clean-label poisoning, and in developing methods for overcoming this susceptibility.},
  author        = {Yonatan Slutzky and Yotam Alexander and Noam Razin and Nadav Cohen},
  doi           = {10.48550/arxiv.2410.10473},
  journal       = {arXiv preprint arXiv:2410.10473},
  month         = {10},
  note          = {A very recent and high-impact paper uncovering a potential security vulnerability, proving that SSMs can be ``poisoned'' with clean-label data to disrupt their generalization capabilities.},
  openalex      = {W4403570877},
  pdf           = {http://arxiv.org/pdf/2410.10473},
  title         = {The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels},
  url           = {https://arxiv.org/abs/2410.10473},
  year          = {2024}
}

@inproceedings{smith2023s5,
  abstract      = {Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We show that the S5 layer can leverage efficient and widely implemented parallel scans, allowing it to match the computational efficiency of S4 while achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.},
  author        = {Smith, Jimmy T. H. and Warrington, Andrew and Linderman, Scott W.},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2208.04933},
  note          = {Notable-top-5% (Oral). Introduced a simplified architecture using multi-input, multi-output SSMs with parallel scans, removing complex convolutional representations while achieving state-of-the-art results.},
  openalex      = {W4290802752},
  pdf           = {https://openreview.net/pdf?id=Ai8Hw3AXqks},
  title         = {Simplified State Space Layers for Sequence Modeling},
  url           = {https://openreview.net/forum?id=Ai8Hw3AXqks},
  year          = {2023}
}

@article{somvanshi2025survey,
  abstract      = {Recent advancements in sequence modeling have led to the emergence of Structured State Space Models (SSMs) as an efficient alternative to Recurrent Neural Networks (RNNs) and Transformers, addressing challenges in long-range dependency modeling and computational efficiency. While RNNs suffer from vanishing gradients and sequential inefficiencies, and Transformers face quadratic complexity, SSMs leverage structured recurrence and state-space representations to achieve superior long-sequence processing with linear or near-linear complexity. This survey provides a comprehensive review of SSMs, tracing their evolution from the foundational S4 model to its successors like Mamba, Simplified Structured State Space Sequence Model (S5), and Jamba, highlighting their improvements in computational efficiency, memory optimization, and inference speed. By comparing SSMs with traditional sequence models across domains such as natural language processing (NLP), speech recognition, vision, and time-series forecasting, we demonstrate their advantages in handling long-range dependencies while reducing computational overhead. Despite their potential, challenges remain in areas such as training optimization, hybrid modeling, and interpretability. This survey serves as a structured guide for researchers and practitioners, detailing the advancements, trade-offs, and future directions of SSM-based architectures in AI and deep learning.},
  author        = {Somvanshi, Shriyank and Islam, Md Monzurul and Mimi, Mahmuda Sultana and Polock, Sazzad Bin Bashar and Chhetri, Gaurab and Das, Subasish},
  doi           = {10.48550/arXiv.2503.18970},
  journal       = {arXiv preprint arXiv:2503.18970},
  month         = {3},
  note          = {An extensive and up-to-date survey that provides a structured overview of the entire field, making it an excellent starting point for new researchers.},
  pdf           = {https://arxiv.org/pdf/2503.18970.pdf},
  title         = {From S4 to Mamba: A Comprehensive Survey on Structured State Space Models},
  url           = {https://arxiv.org/abs/2503.18970},
  year          = {2025}
}

@article{sun2023retnet,
  abstract      = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  author        = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  eprint        = {2307.08621},
  journal       = {arXiv preprint arXiv:2307.08621},
  month         = {7},
  note          = {Introduced retention mechanism supporting parallel training, recurrent inference, and chunkwise processing with O(1) inference complexity, demonstrating favorable scaling laws compared to Transformers.},
  openalex      = {W4384648484},
  pdf           = {https://arxiv.org/pdf/2307.08621.pdf},
  primaryclass  = {cs.CL},
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  url           = {https://arxiv.org/abs/2307.08621},
  year          = {2023}
}

@article{sushma2024incontext,
  abstract      = {Deep state-space models (Deep SSMs) are becoming popular for modeling sequence data due to their favorable computational properties for long sequences and promising empirical performance. Like transformers, Deep SSMs have shown to be capable of in-context learning, i.e., the ability to perform new tasks when prompted with input-output examples, without parameter updates. However, a complete mechanistic understanding of how Deep SSMs perform in-context learning has been missing. In this paper, we provide a direct and explicit construction to show that state-space models can perform gradient-based learning and use it for in-context learning. Specifically, we prove that a single structured state-space model layer, augmented with local self-attention, can reproduce the outputs of an implicit linear model with least squares loss after one step of gradient descent. Our key insight is that the diagonal linear recurrent layer can act as a gradient accumulator, which can be 'applied' to the parameters of the implicit regression model. We extend our approach to multi-layer models and show how in-context learning can be performed on linear and non-linear regression tasks. Finally, we show that when considering input and output gating, which are essential architectural components of modern Deep SSMs, in-context learning can be performed even more expressively.},
  author        = {Sushma, Neeraj Mohan and Tian, Yudou and Mestha, Harshvardhan and Colombo, Nicolo and Kappel, David and Subramoney, Anand},
  doi           = {10.48550/arXiv.2410.11687},
  journal       = {arXiv preprint arXiv:2410.11687},
  month         = {10},
  note          = {Provides a theoretical construction showing how SSMs, particularly with gating, can perform in-context learning via gradient-based methods, helping to explain their emergent capabilities},
  openalex      = {W4403575037},
  pages         = {1--20},
  pdf           = {https://arxiv.org/pdf/2410.11687.pdf},
  title         = {State-space models can learn in-context by gradient descent},
  url           = {https://arxiv.org/abs/2410.11687},
  year          = {2024}
}

@inproceedings{voelker2019legendre,
  abstract      = {We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit~(LMU) is mathematically derived to orthogonalize its continuous-time history -- doing so by solving $d$ coupled ordinary differential equations~(ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree $d - 1$. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning $100 ext,000$ time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time -- exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using $m$ recurrently-connected Poisson spiking neurons, $\mathcalO( m )$ time and memory, with error scaling as $\mathcalO( d / \sqrtm )$. We discuss implementations of LMUs on analog and digital neuromorphic hardware.},
  author        = {Voelker, Aaron R. and Kajić, Ivana and Eliasmith, Chris},
  booktitle     = {Advances in Neural Information Processing Systems},
  note          = {Pioneered the use of orthogonal polynomials (Legendre polynomials) for continuous-time memory in neural networks, establishing the mathematical foundation that would inspire the HiPPO framework and modern SSMs.},
  openalex      = {W2970783931},
  pages         = {15544--15553},
  pdf           = {https://proceedings.neurips.cc/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf},
  series        = {NIPS},
  title         = {Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks},
  volume        = {32},
  year          = {2019}
}

@inproceedings{wang2025bottlenecks,
  abstract      = {Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.},
  author        = {Wang, Peihao and Cai, Ruisi and Wang, Yuehao and Zhu, Jiajun and Srivastava, Pragya and Wang, Zhangyang and Li, Pan},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  doi           = {10.48550/arxiv.2501.00658},
  note          = {Accepted at ICLR 2025. Addresses fundamental SSM limitations by identifying recency bias and over-smoothing issues, proposing polarization techniques that solve the recency-over-smoothing dilemma for deeper models.},
  openalex      = {W4406032139},
  pdf           = {https://arxiv.org/pdf/2501.00658.pdf},
  title         = {Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing},
  url           = {https://openreview.net/forum?id=pymXpl4qvi},
  year          = {2025}
}

@article{xie2025mmunet,
  abstract      = {State Space Models (SSMs) have recently demonstrated outstanding performance in long-sequence modeling, particularly in natural language processing. However, their direct application to medical image segmentation poses several challenges. SSMs, originally designed for 1D sequences, struggle with 3D spatial structures in medical images due to discontinuities introduced by flattening. Additionally, SSMs have difficulty fitting high-variance data, which is common in medical imaging. In this paper, we analyze the intrinsic limitations of SSMs in medical image segmentation and propose a unified U-shaped encoder-decoder architecture, Meta Mamba UNet (MM-UNet), designed to leverage the advantages of SSMs while mitigating their drawbacks. MM-UNet incorporates hybrid modules that integrate SSMs within residual connections, reducing variance and improving performance. Furthermore, we introduce a novel bi-directional scan order strategy to alleviate discontinuities when processing medical images. Extensive experiments on the AMOS2022 and Synapse datasets demonstrate the superiority of MM-UNet over state-of-the-art methods. MM-UNet achieves a Dice score of 91.0% on AMOS2022, surpassing nnUNet by 3.2%, and a Dice score of 87.1% on Synapse. These results confirm the effectiveness of integrating SSMs in medical image segmentation through architectural design optimizations.},
  author        = {Xie, Bin and Yan, Yan and Agam, Gady},
  journal       = {arXiv preprint arXiv:2503.17540},
  month         = {3},
  note          = {Integrates Mamba blocks into the popular U-Net architecture, showing superior performance for 3D medical image segmentation by better modeling the global spatial structure of volumetric data.},
  pdf           = {https://arxiv.org/pdf/2503.17540.pdf},
  title         = {MM-UNet: Meta Mamba UNet for Medical Image Segmentation},
  url           = {https://arxiv.org/abs/2503.17540},
  year          = {2025}
}

@article{zamba2024,
  abstract      = {In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.},
  archiveprefix = {arXiv},
  author        = {Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  doi           = {10.48550/arXiv.2405.16712},
  eprint        = {2405.16712},
  journal       = {arXiv preprint arXiv:2405.16712},
  month         = {5},
  note          = {A novel and efficient hybrid architecture that combines a Mamba backbone with a single shared attention module, providing the benefits of attention at a minimal parameter and memory cost},
  pdf           = {https://arxiv.org/pdf/2405.16712.pdf},
  primaryclass  = {cs.LG},
  title         = {Zamba: A Compact 7B SSM Hybrid Model},
  url           = {https://arxiv.org/abs/2405.16712},
  year          = {2024}
}

@inproceedings{zhu2024visionmamba,
  abstract      = {Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8x faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248x1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models.},
  author        = {Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  month         = {7},
  note          = {Pioneering adaptation of SSMs to computer vision with bidirectional scanning and position embeddings, demonstrating that self-attention is not necessary for visual representation learning.},
  openalex      = {W4391013663},
  pages         = {62429--62442},
  pdf           = {https://proceedings.mlr.press/v235/zhu24f.html},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  volume        = {235},
  year          = {2024}
}

@inproceedings{zubic2024eventssm,
  abstract      = {Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minimal performance degradation when tested at higher frequencies than the training input. Traditional RNN and Transformer models exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.76 mAP, highlighting the effectiveness of SSMs in event-based vision tasks.},
  author        = {Zubić, Nikola and Gehrig, Mathias and Scaramuzza, Davide},
  booktitle     = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  note          = {Highlights a unique strength of SSMs' continuous-time formulation, showing they are exceptionally effective for processing asynchronous data from event cameras and generalizing across different sampling frequencies. CVPR 2024 Spotlight.},
  openalex      = {W4402660164},
  pages         = {5819--5828},
  pdf           = {https://openaccess.thecvf.com/content/CVPR2024/papers/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.pdf},
  publisher     = {IEEE},
  title         = {State Space Models for Event Cameras},
  url           = {https://arxiv.org/abs/2402.15584},
  year          = {2024}
}

@inproceedings{lieber2024jambaiclr,
  title = {Jamba: Hybrid {T}ransformer-{M}amba Language Models},
  author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and Abend, Omri and Alon, Raz and Asida, Tomer and Bergman, Amir and Glozman, Roman and Gokhman, Michael and Manevich, Avashalom and Ratner, Nir and Rozen, Noam and Shwartz, Erez and Zusman, Mor and Shoham, Yoav},
  booktitle = {Proceedings of the Thirteenth International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=JFPaD7lpBD},
  pdf = {https://openreview.net/pdf?id=JFPaD7lpBD},
  openalex = {W4393399080},
  abstract = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.}
}