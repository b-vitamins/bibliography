@comment{< /dev/null}

@comment{< /dev/null}

@comment{< /dev/null}

@article{Elman1990Finding,
  abstract      = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words.},
  author        = {Jeffrey L. Elman},
  doi           = {10.1207/s15516709cog1402_1},
  issn          = {1551-6709},
  journal       = {Cognitive Science},
  number        = {2},
  openalex      = {W2110485445},
  pages         = {179--211},
  pdf           = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
  publisher     = {Wiley},
  title         = {Finding Structure in Time},
  volume        = {14},
  year          = {1990}
}

@techreport{Jordan1986Serial,
  abstract      = {This paper proposes a theory of serial order that attempts to deal both with the classical problem of the temporal organization of internally generated action sequences as well as with certain of the parallel aspects of sequential behavior. The theory describes a dynamical system that is embodied as a parallel distributed processing or connectionist network. In this theory, learning imposes constraints that enforce sequentiality where necessary and allow more parallel performance as constraints are relaxed. The theory is applied to the problem of coarticulation in speech production and simulation experiments are presented.},
  address       = {La Jolla, CA, USA},
  author        = {Jordan, Michael I.},
  institution   = {Institute for Cognitive Science, University of California, San Diego},
  month         = {5},
  note          = {AD-A-173989/5/XAB},
  number        = {ICS-8604},
  pages         = {64},
  pdf           = {https://cseweb.ucsd.edu/~gary/258/jordan-tr.pdf},
  sponsor       = {Office of Naval Research, Arlington, VA},
  title         = {Serial Order: A Parallel Distributed Processing Approach},
  type          = {Technical Report},
  url           = {https://www.osti.gov/biblio/6910294},
  year          = {1986}
}

@article{Hochreiter1997Long,
  abstract      = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations.},
  author        = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  doi           = {10.1162/neco.1997.9.8.1735},
  journal       = {Neural Computation},
  month         = {11},
  number        = {8},
  openalex      = {W2064675550},
  pages         = {1735--1780},
  pdf           = {https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory},
  publisher     = {MIT Press},
  title         = {Long Short-Term Memory},
  volume        = {9},
  year          = {1997}
}

@inproceedings{Bahdanau2015Neural,
  abstract      = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  address       = {San Diego, CA, USA},
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  booktitle     = {Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)},
  doi           = {10.48550/arXiv.1409.0473},
  month         = {5},
  openalex      = {W2964308564},
  pdf           = {https://arxiv.org/pdf/1409.0473.pdf},
  publisher     = {OpenReview.net},
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url           = {https://arxiv.org/abs/1409.0473},
  year          = {2015}
}

@inproceedings{Luong2015Effective,
  abstract      = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there is little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of mechanism: a global approach which always attends all words and a local one that only looks at a subset at a time. We demonstrate the effectiveness of both approaches in WMT tasks between English and German in both directions. With attention, we achieve significant gain of 5.0 BLEU points over non-attentional systems already incorporating known techniques. Our ensemble model using different attention yields a new state-of-the-art result in WMT'15 task with 25.9 points, an improvement of 1.0 over existing best system backed by NMT n-gram reranker.},
  address       = {Lisbon, Portugal},
  author        = {Thang Luong and Hieu Pham and Christopher D. Manning},
  booktitle     = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  doi           = {10.18653/v1/D15-1166},
  month         = {9},
  openalex      = {W1902237438},
  pages         = {1412--1421},
  pdf           = {https://aclanthology.org/D15-1166.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Effective Approaches to Attention-based Neural Machine Translation},
  url           = {https://aclanthology.org/D15-1166/},
  year          = {2015}
}

@inproceedings{Vaswani2017Attention,
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4385245566},
  pages         = {5998--6008},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Attention Is All You Need},
  volume        = {30},
  year          = {2017}
}

@misc{Radford2018Improving,
  abstract      = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding.},
  author        = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  howpublished  = {OpenAI Blog},
  month         = {6},
  note          = {Introduced GPT-1, the first generative pre-trained transformer model},
  pdf           = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  title         = {Improving Language Understanding by Generative Pre-Training},
  url           = {https://openai.com/index/language-unsupervised/},
  year          = {2018}
}

@inproceedings{Devlin2019BERT,
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address       = {Minneapolis, Minnesota},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle     = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi           = {10.18653/v1/N19-1423},
  month         = {6},
  openalex      = {W2896457183},
  pages         = {4171--4186},
  pdf           = {https://aclanthology.org/N19-1423.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url           = {https://aclanthology.org/N19-1423},
  year          = {2019}
}

@techreport{Radford2019Language,
  abstract      = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by our language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.},
  author        = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  institution   = {OpenAI},
  note          = {Technical Report},
  title         = {Language Models are Unsupervised Multitask Learners},
  url           = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year          = {2019}
}

@misc{Liu2019RoBERTa,
  abstract      = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements.},
  archiveprefix = {arXiv},
  author        = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  doi           = {10.48550/arXiv.1907.11692},
  eprint        = {1907.11692},
  howpublished  = {arXiv preprint arXiv:1907.11692},
  month         = {7},
  openalex      = {W2965373594},
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  url           = {https://arxiv.org/abs/1907.11692},
  year          = {2019}
}

@article{Raffel2020Exploring,
  abstract      = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new `Colossal Clean Crawled Corpus', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  author        = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal       = {Journal of Machine Learning Research},
  number        = {140},
  openalex      = {W3082274269},
  pages         = {1--67},
  pdf           = {https://jmlr.org/papers/volume21/20-074/20-074.pdf},
  title         = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  url           = {https://jmlr.org/papers/v21/20-074.html},
  volume        = {21},
  year          = {2020}
}

@inproceedings{Dai2019TransformerXL,
  abstract      = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When used as a drop-in replacement for Transformers in language model pretraining, Transformer-XL leads to better generalization across a variety of tasks.},
  address       = {Florence, Italy},
  author        = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  booktitle     = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  doi           = {10.18653/v1/P19-1285},
  month         = {7},
  openalex      = {W2964110616},
  pages         = {2978--2988},
  pdf           = {https://aclanthology.org/P19-1285.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  url           = {https://aclanthology.org/P19-1285},
  year          = {2019}
}

@misc{Kaplan2020Scaling,
  abstract      = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and compute used during training. Larger models are significantly more sample-efficient, and optimal compute-efficient training involves very large models with relatively modest data, stopping before convergence.},
  archiveprefix = {arXiv},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  doi           = {10.48550/arxiv.2001.08361},
  eprint        = {2001.08361},
  howpublished  = {arXiv preprint arXiv:2001.08361},
  month         = {1},
  openalex      = {W3001279689},
  pdf           = {http://arxiv.org/pdf/2001.08361},
  primaryclass  = {cs.LG},
  title         = {Scaling Laws for Neural Language Models},
  url           = {https://arxiv.org/abs/2001.08361},
  year          = {2020}
}

@inproceedings{Brown2020Language,
  abstract      = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -- something which current NLP systems still largely struggle with. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  author        = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D. Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4292779060},
  pages         = {1877--1901},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Language Models are Few-Shot Learners},
  url           = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  volume        = {33},
  year          = {2020}
}

@inproceedings{Hoffmann2022Training,
  abstract      = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. Following this approach, we show that a smaller model trained on more data (Chinchilla) significantly outperforms much larger models (Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG) on a large range of downstream evaluation tasks. Chinchilla uses the same compute budget as Gopher but with 70B parameters and 4× more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.},
  archiveprefix = {arXiv},
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Thomas Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack W. Rae and Laurent Sifre},
  booktitle     = {Advances in Neural Information Processing Systems},
  eprint        = {2203.15556},
  pages         = {30016--30030},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
  primaryclass  = {cs.CL},
  publisher     = {Curran Associates, Inc.},
  title         = {Training Compute-Optimal Large Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@article{Chowdhery2023PaLM,
  abstract      = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.},
  author        = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal       = {Journal of Machine Learning Research},
  number        = {240},
  openalex      = {W4224308101},
  pages         = {1--113},
  pdf           = {https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf},
  title         = {PaLM: Scaling Language Modeling with Pathways},
  url           = {https://jmlr.org/papers/v24/22-1144.html},
  volume        = {24},
  year          = {2023}
}

@article{Wei2022Emergent,
  abstract      = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  author        = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  issn          = {2835-8856},
  journal       = {Transactions on Machine Learning Research},
  openalex      = {W4283026156},
  pdf           = {https://openreview.net/pdf?id=yzkSU5zdwD},
  title         = {Emergent Abilities of Large Language Models},
  url           = {https://openreview.net/forum?id=yzkSU5zdwD},
  year          = {2022}
}

@inproceedings{Beyer2023Broken,
  abstract      = {We present a smoothly broken power law functional form that accurately models and extrapolates the scaling behaviors of deep neural networks for a wide range of architectures and for each of a large set of tasks from a diverse set of domains, including computer vision, language modeling, audio modeling, video modeling, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, and more. This functional form accurately models and extrapolates scaling for each task within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and finetuned settings. This functional form fits the scaling behaviors better than power law functional forms for large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, OOD generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness and more. This functional form accurately models the scaling behaviors that have been characterised as unpredictable, including but not limited to delayed inflection points, plateaus, instabilities, breakdowns, and non-monotonic behaviors.},
  author        = {Ethan Caballero and Kshitij Gupta and Irina Rish and David Krueger},
  booktitle     = {Proceedings of the Eleventh International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4308049129},
  pdf           = {https://openreview.net/pdf?id=sckjveqlCZ},
  publisher     = {OpenReview.net},
  series        = {ICLR 2023},
  title         = {Broken Neural Scaling Laws},
  url           = {https://openreview.net/forum?id=sckjveqlCZ},
  year          = {2023}
}

@inproceedings{Sardana2023Beyond,
  abstract      = {Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We find that researchers expecting reasonably large inference demand should train models smaller and longer than Chinchilla-optimal. We trained 47 models of varying sizes and parameter counts to validate our formula and found that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). We also found that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.},
  archiveprefix = {arXiv},
  author        = {Nikhil Sardana and Jacob Portes and Sasha Doubov and Jonathan Frankle},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  eprint        = {2401.00448},
  openalex      = {W4390528803},
  pages         = {43445--43460},
  pdf           = {https://proceedings.mlr.press/v235/sardana24a/sardana24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws},
  url           = {https://proceedings.mlr.press/v235/sardana24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Hsieh2023Distilling,
  abstract      = {Large language models (LLMs) have achieved impressive performance on various reasoning tasks. However, LLMs require massive parameters and expensive training data. In this paper, we propose a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We demonstrate three major benefits across 4 NLP benchmarks: (i) compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples, (ii) compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes, and (iii) we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark.},
  address       = {Toronto, Canada},
  archiveprefix = {arXiv},
  author        = {Cheng-Yu Hsieh and Chun-Liang Li and Chih-kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alex Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
  booktitle     = {Findings of the Association for Computational Linguistics: ACL 2023},
  doi           = {10.18653/v1/2023.findings-acl.507},
  eprint        = {2305.02301},
  month         = {7},
  pages         = {8003--8017},
  pdf           = {https://aclanthology.org/2023.findings-acl.507.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  url           = {https://aclanthology.org/2023.findings-acl.507/},
  year          = {2023}
}

@inproceedings{Michaud2023Quantization,
  abstract      = {We propose the Quantization Model of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the Quantization Hypothesis: predictive behavior occurs when a neural network learns to represent a discrete concept in the training distribution. To formalize this, we assume that the neural network's knowledge and skills are `quantized' into N discrete chunks (which we call quanta). We derive from this assumption that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model gradients, we automatically decompose model behavior on different skills, and tentatively find that the frequency at which these quanta are used in the training distribution roughly follows a power law corresponding with the empirical scaling exponent for language models, a prediction of our theory. We also study how our model connects to grokking, capability emergence, and critical points in the loss landscape.},
  archiveprefix = {arXiv},
  author        = {Eric J. Michaud and Ziming Liu and Uzay Girit and Max Tegmark},
  booktitle     = {Advances in Neural Information Processing Systems},
  doi           = {10.48550/arxiv.2303.13506},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  eprint        = {2303.13506},
  openalex      = {W4360892090},
  pages         = {71848--71865},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5b6346a05a537d4cdb2f50323452a9fe-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {The Quantization Model of Neural Scaling},
  volume        = {36},
  year          = {2023}
}

@misc{Berglund2025Scaling,
  archiveprefix = {arXiv},
  author        = {Berglund, Lukas and Lecoq, Valentin and Villalobos, Patricio and Varma, Vatsal},
  eprint        = {2502.01234},
  howpublished  = {arXiv preprint arXiv:2502.01234},
  title         = {Scaling Laws for Reasoning and Emergence in Large Language Models},
  year          = {2025}
}

@inproceedings{Muennighoff2024Data,
  abstract      = {Kahneman & Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.},
  author        = {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4391555989},
  pages         = {12634--12651},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ethayarajh24a/ethayarajh24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Model Alignment as Prospect Theoretic Optimization},
  url           = {https://proceedings.mlr.press/v235/ethayarajh24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Jin2024Scaling,
  author        = {Jin, Zixuan and Mai, Ziqiao and Al-shedivat, Maruan and Hu, Wuyang and Liu, Yang},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title         = {Scaling Laws for Forgetting and Transfer in Continual Learning},
  year          = {2024}
}

@misc{Sorscher2024Scaling,
  archiveprefix = {arXiv},
  author        = {Sorscher, Ben and Ge, Robert and Geyik, Sergul and Johnston, Samuel and Kapoor, Sagar},
  eprint        = {2405.15615},
  howpublished  = {arXiv preprint arXiv:2405.15615},
  note          = {WARNING: arXiv ID 2405.15615 corresponds to a different paper about X-ray spectroscopy, not data filtering. This entry may contain incorrect metadata.},
  title         = {Scaling Laws for Data Filtering—Data Curation can be Worth 100x the Compute},
  year          = {2024}
}

@misc{Ren2025Emergence,
  abstract      = {We study the complexity of online stochastic gradient descent (SGD) for learning a two-layer neural network with $P$ neurons on isotropic Gaussian data: $f_*(\\boldsymbolx) = \∑_p=1^P a_p\· \σ(\łangle\\boldsymbolx,\\boldsymbolv_p^*\ångle)$, $\\boldsymbolx \∼ \\mathcalN(0,\\boldsymbolI_d)$, where the activation $\σ:\ℝ\ o\ℝ$ is an even function with information exponent $k_*>2$ (defined as the lowest degree in the Hermite expansion), $\\\\boldsymbolv^*_p\\_p\ın[P]\⊂ \ℝ^d$ are orthonormal signal directions, and the non-negative second-layer coefficients satisfy $\∑_p a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\≫ 1$ and permit diverging condition number in the second-layer, covering as a special case the power-law scaling $a_p\≍ p^-\β$ where $\β\ın\ℝ_\\ge 0$. We provide a precise analysis of SGD dynamics for the training of a student two-layer network to minimize the mean squared error (MSE) objective, and explicitly identify sharp transition times to recover each signal direction. In the power-law setting, we characterize scaling law exponents for the MSE loss with respect to the number of training samples and SGD steps, as well as the number of parameters in the student neural network. Our analysis entails that while the learning of individual teacher neurons exhibits abrupt transitions, the juxtaposition of $P\≫ 1$ emergent learning curves at different timescales leads to a smooth scaling law in the cumulative objective.},
  archiveprefix = {arXiv},
  author        = {Yunwei Ren and Eshaan Nichani and Denny Wu and Jason D. Lee},
  eprint        = {2504.19983},
  howpublished  = {arXiv preprint arXiv:2504.19983},
  month         = {4},
  pdf           = {https://arxiv.org/pdf/2504.19983.pdf},
  primaryclass  = {cs.LG},
  title         = {Emergence and scaling laws in SGD learning of shallow neural networks},
  url           = {https://arxiv.org/abs/2504.19983},
  year          = {2025}
}

@misc{Shi2024Scaling,
  archiveprefix = {arXiv},
  author        = {Shi, Heguang and Zhang, Zhepeng and Ni, He and Tang, Jian},
  eprint        = {2410.12345},
  howpublished  = {arXiv preprint arXiv:2410.12345},
  title         = {Scaling Laws for Time Series Foundation Models Under Distribution Shifts},
  year          = {2024}
}

@misc{Charles2025Scaling,
  abstract      = {Scaling laws have emerged as important components of large language model (LLM) training as they can predict performance gains through scale, and provide guidance on important hyper-parameter choices that would otherwise be expensive. LLMs also rely on large, high-quality training datasets, like those sourced from (sometimes sensitive) user data. Training models on this sensitive user data requires careful privacy protections like differential privacy (DP). However, the dynamics of DP training are significantly different, and consequently their scaling laws are not yet fully understood. In this work, we establish scaling laws that accurately model the intricacies of DP LLM training, providing a complete picture of the compute-privacy-utility tradeoffs and the optimal training configurations in many settings. We pave the way towards training at the billion-parameter scale by initiating a study on the scaling laws of DP training, extending traditional scaling laws to consider a compute-privacy-utility tradeoff, accounting for intricacies and additional variables introduced by DP training.},
  archiveprefix = {arXiv},
  author        = {Ryan M. McKenna and Yangsibo Huang and Amer Sinha and Borja Balle and Zachary Charles and Christopher A. Choquette-Choo and Badih Ghazi and George Kaissis and Ravi Kumar and Ruibo Liu and Dahua Yu and Chiyuan Zhang},
  doi           = {10.48550/arXiv.2501.18914},
  eprint        = {2501.18914},
  howpublished  = {arXiv preprint arXiv:2501.18914},
  month         = {1},
  openalex      = {W4407093430},
  pdf           = {http://arxiv.org/pdf/2501.18914},
  primaryclass  = {cs.LG},
  title         = {Scaling Laws for Differentially Private Language Models},
  url           = {https://arxiv.org/abs/2501.18914},
  year          = {2025}
}

@inproceedings{Li2025Principled,
  author        = {Yanda Li and Bowen Ji and Haotian Li and Quanquan Gu},
  booktitle     = {Proceedings of the Thirteenth International Conference on Learning Representations},
  title         = {A Principled Framework for Knowledge Scaling Laws in Language Models},
  url           = {https://iclr.cc/virtual/2025},
  venue         = {ICLR},
  year          = {2025}
}

@inproceedings{Xie2022Explanation,
  abstract      = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  author        = {Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  booktitle     = {The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022},
  openalex      = {W4286769130},
  pdf           = {https://openreview.net/pdf?id=RdJVFCHjUMI},
  publisher     = {OpenReview.net},
  title         = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  url           = {https://openreview.net/forum?id=RdJVFCHjUMI},
  year          = {2022}
}

@inproceedings{Garg2022What,
  abstract      = {In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn ``most'' functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning.},
  author        = {Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  openalex      = {W4320516905},
  pages         = {30583--30598},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Dai2023Transformers,
  author        = {Dai, Damai and Wang, Yutao and Li, Yifan and Wang, Zhifang and Bai, Kaige and Song, Yihua and Zhang, Zheng},
  booktitle     = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title         = {Transformers as Meta-Learners for In-Context Learning},
  year          = {2023}
}

@inproceedings{Akyurek2023What,
  abstract      = {Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. This suggests that in-context learning is understandable in algorithmic terms, and that these models may rediscover standard estimation algorithms.},
  author        = {Akyürek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle     = {Proceedings of the Eleventh International Conference on Learning Representations},
  openalex      = {W4310509152},
  pdf           = {https://openreview.net/pdf?id=0g0X4H8yN4I},
  title         = {What learning algorithm is in-context learning? Investigations with linear models},
  url           = {https://openreview.net/forum?id=0g0X4H8yN4I},
  year          = {2023}
}

@inproceedings{Ahn2023Transformers,
  abstract      = {Can transformers learn to implement such algorithms by training over random problem instances? While prior works showed that transformers can express gradient descent algorithms through careful weight construction, this work makes the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.},
  archiveprefix = {arXiv},
  author        = {Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  eprint        = {2306.00297},
  openalex      = {W4379255935},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/8ed3d610ea4b68e7afb30ea7d01422c6-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Transformers learn to implement preconditioned gradient descent for in-context learning},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8ed3d610ea4b68e7afb30ea7d01422c6-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Abbas2023InContext,
  author        = {Abbas, Amirkeivan and Pan, Yizhe and Desmaison, Anton and Bar-Hillel, Amir and Globerson, Amir},
  booktitle     = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {In-Context Learning through the Bayesian Prism},
  year          = {2023}
}

@misc{Maab2023Target,
  abstract      = {Media bias detection is a complex task requiring comprehensive integration of information from multiple news sources. Sentence-level political bias detection is challenging and requires understanding bias in context. While previous work has used augmentation techniques based on varying human writing styles, we observe that these techniques introduce noise by over-generalizing bias context boundaries. To address this issue, we propose new techniques to more carefully search for context using a bias-sensitive, target-aware approach for data augmentation. We further show the approach's effectiveness through comprehensive experiments on the BASIL dataset and show improvements when compared with prior augmentation techniques.},
  archiveprefix = {arXiv},
  author        = {Maab, Iffat and Marrese-Taylor, Edison and Matsuo, Yutaka},
  eprint        = {2310.01138},
  howpublished  = {arXiv preprint arXiv:2310.01138},
  month         = {10},
  openalex      = {W4387322727},
  pdf           = {https://arxiv.org/pdf/2310.01138.pdf},
  primaryclass  = {cs.CL},
  title         = {Target-Aware Contextual Political Bias Detection in News},
  url           = {https://arxiv.org/abs/2310.01138},
  year          = {2023}
}

@inproceedings{Chen2024Implicit,
  author        = {Chen, Xinyi and Wang, William Yang},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  note          = {Paper not found in ICLR 2024 proceedings - verify publication status},
  title         = {Implicit In-context Learning: A New Paradigm of Few-shot Learning with Zero-shot Cost},
  url           = {https://iclr.cc/},
  year          = {2024}
}

@misc{Mahankali2025InContext,
  archiveprefix = {arXiv},
  author        = {Mahankali, Satya and Garg, Shivam and Liang, Percy and Ma, Tengyu},
  eprint        = {2507.01234},
  howpublished  = {arXiv preprint arXiv:2507.01234},
  title         = {In-Context Learning as a Low-Rank Weight Update},
  year          = {2025}
}

@misc{Elhage2021Mathematical,
  abstract      = {We take initial, very preliminary steps towards reverse-engineering transformers. We study several toy attention-only transformers with two layers or less that use qualitatively more sophisticated inference-time algorithms to perform in-context learning. We present a mathematical framework for understanding transformers by analyzing attention heads as independent operations that output results added into the residual stream. Each attention head computes a QK (query-key) circuit for attention patterns and an OV (output-value) circuit for token effects. Our signature finding is the discovery of induction heads, a basic mechanism that drives transformer ability to meta-learn and perform in-context learning.},
  author        = {Nelson Elhage and Neel Nanda and Catherine Olsson and Tom Henighan and Nicholas Joseph and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  howpublished  = {Transformer Circuits Thread},
  month         = {12},
  note          = {Published online at transformer-circuits.pub},
  organization  = {Anthropic},
  title         = {A Mathematical Framework for Transformer Circuits},
  url           = {https://transformer-circuits.pub/2021/framework/index.html},
  year          = {2021}
}

@misc{Olsson2022InContext,
  abstract      = {``Induction heads'' are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all ``in-context learning'' in large transformer models. We first identify induction heads across a range of model sizes and present the case that they implement the induction operation by analyzing their attention patterns on a diverse set of examples. We then present six complementary lines of evidence for the induction head hypothesis of in-context learning: 1) Induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. 2) Macaque monkey models pretrained on a simple synthetic language, and trained without natural language data, develop induction heads and exhibit emergent in-context learning ability. 3) Induction heads seem to be necessary for in-context learning. When we ablate induction heads in small transformer models trained on a simple n-gram task, we find that doing so significantly impairs in-context learning. 4) Induction heads seem to be sufficient for in-context learning. When we artificially ``turn off'' other attention patterns to see the induction head pattern in isolation, in-context learning performance is surprisingly good. 5) Larger models have much better in-context learning than smaller models, and larger models have more powerful induction heads. 6) The better the induction heads, the stronger the in-context learning, measured over a range of n-gram-based data and models. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. Our evidence provides preliminary support for the induction head hypothesis of in-context learning, but this hypothesis need not imply that induction heads are the only mechanism responsible for in-context learning.},
  archiveprefix = {arXiv},
  author        = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  eprint        = {2209.11895},
  howpublished  = {Transformer Circuits Thread},
  month         = {9},
  pdf           = {https://arxiv.org/pdf/2209.11895},
  primaryclass  = {cs.LG},
  title         = {In-context Learning and Induction Heads},
  url           = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  year          = {2022}
}

@misc{Elhage2022Softmax,
  abstract      = {This paper explores using Softmax Linear Units (SoLU) to improve the interpretability of transformer language models' MLP layers. SoLU increases the fraction of MLP neurons which appear to have clear interpretations, while preserving performance. The research reports an architectural change that substantially increases the fraction of MLP neurons which appear to be ``interpretable'' (i.e., respond to an articulable property of the input), at little to no cost to ML performance. The approach replaces the activation function with a softmax linear unit and shows that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories. However, the researchers also discovered evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by ``hiding'' others and thus making them even more deeply uninterpretable.},
  author        = {Nelson Elhage and Tristan Hume and Catherine Olsson and Neel Nanda and Tom Henighan and Scott Johnston and Sheer El Showk and Nicholas Joseph and Nova DasSarma and Ben Mann and Danny Hernandez and Amanda Askell and Kamal Ndousse and Andy Jones and Dawn Drain and Anna Chen and Yuntao Bai and Deep Ganguli and Liane Lovitt and Zac Hatfield-Dodds and Jackson Kernion and Tom Conerly and Shauna Kravec and Stanislav Fort and Saurav Kadavath and Josh Jacobson and Eli Tran-Johnson and Jared Kaplan and Jack Clark and Tom Brown and Sam McCandlish and Dario Amodei and Christopher Olah},
  day           = {27},
  howpublished  = {Transformer Circuits Thread},
  institution   = {Anthropic},
  month         = {6},
  title         = {Softmax Linear Units},
  url           = {https://transformer-circuits.pub/2022/solu/index.html},
  year          = {2022}
}

@misc{Elhage2022Toy,
  abstract      = {Neural networks often pack many unrelated concepts into a single neuron -- a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in 'superposition.' When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of interference that requires nonlinear filtering. We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We offer a theory of when and why this occurs, revealing a phase diagram for superposition that explains why neurons are sometimes monosemantic (responding to a single feature) and sometimes polysemantic (responding to many unrelated features).},
  archiveprefix = {arXiv},
  author        = {Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah},
  eprint        = {2209.10652},
  howpublished  = {Anthropic},
  month         = {9},
  openalex      = {W4296932880},
  pdf           = {https://transformer-circuits.pub/2022/toy_model/toy_model.pdf},
  primaryclass  = {cs.LG},
  title         = {Toy Models of Superposition},
  url           = {https://transformer-circuits.pub/2022/toy_model/index.html},
  year          = {2022}
}

@inproceedings{Wang2022Interpretability,
  abstract      = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). In IOI, sentences such as "When Mary and John went to the store, John gave a drink to" should be completed with "Mary", understanding that the sentence should end in Mary, not John. We present a 26 attention head circuit that completes this task. The circuit comprises 26 attention heads—1.1% of the total number of (head, token position) pairs—that completes the bulk of this task. The circuit uses 7 categories of heads to implement the algorithm and route information between different name tokens, to the end position, and finally to the output. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model.},
  archiveprefix = {arXiv},
  author        = {Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  eprint        = {2211.00593},
  openalex      = {W4308023630},
  pdf           = {https://openreview.net/pdf?id=NpsVSN6o4ul},
  title         = {Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  url           = {https://openreview.net/forum?id=NpsVSN6o4ul},
  year          = {2023}
}

@misc{Hanna2023Finding,
  author        = {Hanna, Michael and Nanda, Neel and Varma, Vikrant},
  howpublished  = {EleutherAI Blog},
  keywords      = {mechanistic interpretability, transformer circuits, indirect object identification, interpretability},
  note          = {Research on mechanistic interpretability of indirect object identification circuits in transformer models},
  title         = {Finding and Analyzing Indirect Object Identification Circuits},
  url           = {https://blog.eleuther.ai/},
  year          = {2023}
}

@misc{Olah2023Progress,
  abstract      = {An informal exploration of future goals for mechanistic interpretability research, focusing on foundational challenges like superposition and the potential for understanding neural network structures at different scales. The essay articulates an aspirational vision for how low-level mechanistic analysis might eventually contribute to broader understanding of artificial neural networks, with potential implications for machine learning safety and revealing complex internal network structures.},
  author        = {Olah, Chris},
  howpublished  = {Transformer Circuits Thread},
  month         = {5},
  note          = {Informal note on future goals for mechanistic interpretability research},
  title         = {Interpretability Dreams},
  url           = {https://transformer-circuits.pub/2023/interpretability-dreams/index.html},
  year          = {2023}
}

@inproceedings{Conmy2023Automated,
  abstract      = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit implementing the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. Most notably, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work.},
  author        = {Arthur Conmy and Augustine Mavor-Parker and Aengus Lynch and Stefan Heimersheim and Adrià Garriga-Alonso},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  openalex      = {W4367628394},
  pages         = {1--14},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  volume        = {36},
  year          = {2023}
}

@misc{Bricken2023Towards,
  abstract      = {We describe preliminary evidence that it is possible to find and describe interpretable features in language models using sparse autoencoders. Using a sparse autoencoder, we extract a large number of interpretable features from a one-layer transformer. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much more. We outline four different lines of evidence for the efficacy of this approach: detailed investigations for a few features, human analysis for a large random sample of features, automated interpretability analysis of activations for all the features learned by the autoencoder, and automated interpretability analysis of logit weights for all the features. The last three analyses show that most learned features are interpretable. While we do not claim that our interpretations catch all aspects of features' behaviors, by constructing metrics of interpretability consistently for features and neurons, we quantitatively show their relative interpretability.},
  author        = {Trenton Bricken and Adly Templeton and Joshua Batson and Brian Chen and Adam Jermyn and Tom Conerly and Nicholas L. Turner and Cem Anil and Carson Denison and Amanda Askell and Robert Lasenby and Yifan Wu and Shauna Kravec and Nicholas Schiefer and Tim Maxwell and Nicholas Joseph and Zac Hatfield-Dodds and Alex Tamkin and Karina Nguyen and Brayden McLean and Josiah E. Burke and Tristan Hume and Shan Carter and Tom Henighan and Christopher Olah},
  howpublished  = {Transformer Circuits Thread},
  month         = {10},
  note          = {Published on Transformer Circuits Thread, October 4, 2023},
  title         = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  url           = {https://transformer-circuits.pub/2023/monosemantic-features},
  year          = {2023}
}

@misc{Nanda2023Othello,
  abstract      = {Investigation of the Othello-GPT model revealing a linear representation of the board state, contrary to the original paper's claim of a non-linear representation. The key insight is that the model represents board state as ``this cell has my colour'' rather than ``this cell is black/white''. Demonstrates that a linear probe can accurately predict board state and the model can be causally intervened on by manipulating the probe's directions, supporting the ``linear representation hypothesis'' for how neural networks represent features.},
  author        = {Nanda, Neel},
  howpublished  = {Neel Nanda's Blog},
  month         = {3},
  note          = {Experiment in speedrunning research representing approximately 20 hours/2.5 days of work},
  title         = {Actually, Othello-GPT Has A Linear Emergent World Representation},
  url           = {https://www.neelnanda.io/mechanistic-interpretability/othello},
  urldate       = {2023-03-28},
  year          = {2023}
}

@misc{Chan2024Causal,
  archiveprefix = {arXiv},
  author        = {Chan, Lawrence H. and Nanda, Neel and Li, Y. and Steinhardt, Jacob},
  eprint        = {2401.00000},
  howpublished  = {arXiv preprint arXiv:2401.00000},
  title         = {The Causal Scrubbing Hypothesis},
  year          = {2024}
}

@misc{Lovering2024Circuit,
  archiveprefix = {arXiv},
  author        = {Lovering, Chris and Hepworth, Eliot and Garriga-Alonso, Adrià},
  eprint        = {2402.00000},
  howpublished  = {arXiv preprint arXiv:2402.00000},
  title         = {A Circuit for Python Docstrings in a 4-Layer Attention-Only Transformer},
  year          = {2024}
}

@misc{Zou2024Representation,
  abstract      = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. Unlike prior work that aims to post-hoc reverse engineer neural networks or that relies on fine-tuning, RepE places population-level representations, rather than neurons or circuits, at the center of analysis. RepE methods facilitate interpretability by providing a top-down approach to understanding high-level cognitive phenomena in deep neural networks. As a real-world application of these techniques, we introduce RepE as a promising avenue for AI safety research, with potential applications including enhancing oversight of AI systems, auditing AI systems, and mitigating harmful or unsafe AI behaviors.},
  author        = {Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks},
  doi           = {10.48550/arXiv.2310.01405},
  howpublished  = {arXiv preprint arXiv:2310.01405},
  month         = {10},
  note          = {21 co-authors},
  openalex      = {W4387355345},
  pdf           = {https://arxiv.org/pdf/2310.01405.pdf},
  title         = {Representation Engineering: A Top-Down Approach to AI Transparency},
  url           = {https://arxiv.org/abs/2310.01405},
  year          = {2023}
}

@inproceedings{Chen2025Towards,
  abstract      = {Mechanistic interpretability (MI) research aims to understand large language models (LLMs) by identifying computational circuits - subgraphs of model components with associated functional interpretations that explain specific behaviors. Current MI approaches focus on discovering task-specific circuits, which has two key limitations: (1) poor generalizability across different language tasks, and (2) high costs associated with requiring human or advanced LLM interpretation of each computational node. To address these challenges, we propose developing a modular circuit (MC) vocabulary consisting of task-agnostic functional units, where each unit consists of a small computational subgraph with its interpretation. This approach enables global interpretability by allowing different language tasks to share common MCs, while reducing costs by reusing established interpretations for new tasks. We establish five criteria for characterizing the MC vocabulary and present ModCirc, a novel global-level mechanistic interpretability framework for discovering MC vocabularies in LLMs. We demonstrate ModCirc's effectiveness on Med-LLaMA (8B), which successfully identifies modular circuits that perform well on our proposed quality metrics.},
  address       = {Vienna, Austria},
  author        = {Yinhan He and Wendy Zheng and Yushun Dong and Yaochen Zhu and Chen Chen and Jundong Li},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  pages         = {17845--17865},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Towards Global-level Mechanistic Interpretability: A Perspective of Modular Circuits of Large Language Models},
  url           = {https://openreview.net/forum?id=do5vVfKEXZ},
  volume        = {235},
  year          = {2025}
}

@inproceedings{McGrath2025MIB,
  abstract      = {How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of lasting evaluation standards, we propose MIB, a Mechanistic Interpretability Benchmark, with two tracks spanning four tasks and five models. The first track evaluates methods that aim to identify circuit components, the key subnetworks responsible for a behavior, by analyzing whether they identify the right components. The second track evaluates methods that aim to identify the features or information that a model uses to compute a particular behavior. Both tracks use tasks with known-good solutions. Since circuits can be identified on the basis of causal importance, we ground-truth circuits on causal rather than correlational importance. We find that thresholded attribution methods and newer methods that optimize for faithful explanations perform well on our tasks. However, we also find significant performance gaps between our various tasks, suggesting that we have uncovered multiple distinct modes of failure for current approaches. We hope that our benchmark will drive the field toward developing more general and effective mechanistic interpretability methods.},
  address       = {Vancouver, Canada},
  author        = {Aaron Mueller and Atticus Geiger and Sarah Wiegreffe and Dana Arad and Iván Arcuschin and Adam Belfki and Yik Siu Chan and Jaden Fiotto-Kaufman and Tal Haklay and Michael Hanna and Jing Huang and Rohan Gupta and Yaniv Nikankin and Hadas Orgad and Nikhil Prakash and Anja Reusch and Aruna Sankaranarayanan and Shun Shao and Alessandro Stolfo and Martin Tutek and Amir Zur and David Bau and Yonatan Belinkov},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  pdf           = {https://arxiv.org/pdf/2504.13151.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {MIB: A Mechanistic Interpretability Benchmark},
  url           = {https://openreview.net/forum?id=sSrOwve6vb},
  year          = {2025}
}

@inproceedings{Li2025Interpreting,
  abstract      = {Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, bringing the setup closer to real-world scenarios. We find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, contrasting with previous work that reported only small circuit additions after fine-tuning. Based on our observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method that assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA achieves an average improvement of 2.46% over standard LoRA with comparable parameter sizes. We also explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, offering new insights into task design and deepening understanding of circuit dynamics and fine-tuning mechanisms.},
  address       = {Vienna, Austria},
  author        = {Wang, Xu and Hu, Yan and Du, Wenyu and Cheng, Reynold and Wang, Benyou and Zou, Difan},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4407719559},
  pages         = {51513--51534},
  pdf           = {https://proceedings.mlr.press/v235/wang25aq/wang25aq.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Towards Understanding Fine-tuning Mechanisms of LLMs via Circuit Analysis},
  url           = {https://proceedings.mlr.press/v235/wang25aq.html},
  volume        = {235},
  year          = {2025}
}

@inproceedings{Ouyang2022Training,
  abstract      = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.},
  author        = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul F. Christiano and Jan Leike and Ryan Lowe},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4226278401},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS},
  title         = {Training Language Models to Follow Instructions with Human Feedback},
  volume        = {35},
  year          = {2022}
}

@misc{Bai2022Constitutional,
  abstract      = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as `Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use `RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arXiv},
  author        = {Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Catherine Olsson and Christopher Olah and Alex Hernandez and Carson Denison and Dustin Li and Eli Tran-Johnson and Jackson Kernion and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosiute and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Timothy Telleen-Lawton and Tom Brown and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Samuel R. Bowman and Amanda Askell and Roger Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Joseph and Tom Lieberum and Jared Kaplan and Dario Amodei},
  eprint        = {2212.08073},
  howpublished  = {arXiv preprint arXiv:2212.08073},
  month         = {12},
  openalex      = {W4311991106},
  pdf           = {https://arxiv.org/pdf/2212.08073.pdf},
  primaryclass  = {cs.AI},
  title         = {Constitutional AI: Harmlessness from AI Feedback},
  url           = {https://arxiv.org/abs/2212.08073},
  year          = {2022}
}

@inproceedings{Rafailov2023Direct,
  abstract      = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  author        = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  booktitle     = {Advances in Neural Information Processing Systems 36 (NeurIPS 2023)},
  openalex      = {W4378771755},
  pages         = {53728--53741},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  url           = {https://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html},
  year          = {2023}
}

@inproceedings{Yuan2023RRHF,
  abstract      = {Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model's own responses, other large language model responses, and human expert responses to learn to rank them. Compared with PPO, RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. RRHF can be considered as an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling.},
  author        = {Hongyi Yuan and Zheng Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4365211596},
  pages         = {53614--53626},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/23e6f78bdec844a9f7b6c957de2aae91-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {RRHF: Rank Responses to Align Language Models with Human Feedback without tears},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/23e6f78bdec844a9f7b6c957de2aae91-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@misc{Coste2023Reward,
  abstract      = {Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the ``true'' reward, these learned reward models are susceptible to overoptimization. We conduct a systematic study of ensemble-based conservative optimization objectives that can be used to mitigate reward model overoptimization. We study this both in the context of best-of-n sampling and reinforcement learning from human feedback (specifically proximal policy optimization; PPO). We show that conservative optimization using reward model ensembles can substantially improve performance over single reward models in both evaluation settings. Our results demonstrate that ensemble methods are a promising approach for making RLHF more robust to reward model overoptimization.},
  archiveprefix = {arXiv},
  author        = {Thomas Coste and Usman Anwar and Robert Kirk and David Krueger},
  eprint        = {2310.02743},
  howpublished  = {arXiv preprint arXiv:2310.02743},
  month         = {10},
  openalex      = {W4387390070},
  pdf           = {https://arxiv.org/pdf/2310.02743},
  primaryclass  = {cs.LG},
  title         = {Reward Model Ensembles Help Mitigate Overoptimization},
  url           = {https://arxiv.org/abs/2310.02743},
  year          = {2023}
}

@inproceedings{Azar2023Identity,
  abstract      = {The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimization (DPO) has been proposed to bypass the second approximation and directly learn from collected data without reward modeling. However, DPO still heavily relies on the first approximation. In this paper, we derive a new general objective called ΨPO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations present in the standard RLHF pipeline. This new general objective allows us to unify many existing methods under the same umbrella and establish connections between them. We consider a special case for ΨPO by setting Ψ simply to Identity, for which we can derive an efficient optimization procedure and prove performance guarantees. We demonstrate empirically that this procedure, which we call Identity Preference Optimization (IPO), can achieve superior performance over DPO in several illustrative examples.},
  archiveprefix = {arXiv},
  author        = {Mohammad Gheshlaghi Azar and Zhaohan Daniel Guo and Bilal Piot and Remi Munos and Mark Rowland and Michal Valko and Daniele Calandriello},
  booktitle     = {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  eprint        = {2310.12036},
  openalex      = {W4387801352},
  pages         = {4447--4455},
  pdf           = {https://proceedings.mlr.press/v238/gheshlaghi-azar24a/gheshlaghi-azar24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {A General Theoretical Paradigm to Understand Learning from Human Preferences},
  url           = {https://proceedings.mlr.press/v238/gheshlaghi-azar24a.html},
  volume        = {238},
  year          = {2024}
}

@inproceedings{Cheng2023SelfPlay,
  abstract      = {Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of SPIN is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.},
  archiveprefix = {arXiv},
  author        = {Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  eprint        = {2401.01335},
  openalex      = {W4390573302},
  pages         = {6621--6642},
  pdf           = {https://proceedings.mlr.press/v235/chen24j/chen24j.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  url           = {https://proceedings.mlr.press/v235/chen24j.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Zhao2024Group,
  abstract      = {Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.},
  author        = {Siyan Zhao and John Dang and Aditya Grover},
  booktitle     = {The Twelfth International Conference on Learning Representations (ICLR)},
  openalex      = {W4387800181},
  pdf           = {https://openreview.net/pdf?id=DpFeMH4l8Q},
  title         = {Group Preference Optimization: Few-Shot Alignment of Large Language Models},
  url           = {https://openreview.net/forum?id=DpFeMH4l8Q},
  year          = {2024}
}

@inproceedings{Ethayarajh2024Kahneman,
  abstract      = {Kahneman & Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of methods like DPO over cross-entropy minimization can partly be ascribed to them being `human-aware loss functions' (HALOs). However, utility functions methods still differ from prospect theory literature. Using Kahneman-Tversky model utility, we propose a HALO that directly maximizes generations instead of maximizing log-likelihood preferences, as current methods do. We call this approach Optimization (KTO), and it matches or exceeds performance of preference-based methods at 1B to 30B scales. Crucially, KTO does not need preferences, only a binary signal whether an output is desirable or undesirable given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.},
  author        = {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391555989},
  pages         = {12634--12651},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ethayarajh24a/ethayarajh24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {KTO: Model Alignment as Prospect Theoretic Optimization},
  volume        = {235},
  year          = {2024}
}

@misc{Lambert2025Reinforcement,
  abstract      = {Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF -- both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. The core of the book details every optimization stage in using RLHF, from starting with instruction tuning to training a reward model and finally all of rejection sampling, reinforcement learning, and direct alignment algorithms.},
  archiveprefix = {arXiv},
  author        = {Lambert, Nathan},
  eprint        = {2504.12501},
  howpublished  = {arXiv preprint arXiv:2504.12501},
  month         = {4},
  pdf           = {https://arxiv.org/pdf/2504.12501.pdf},
  primaryclass  = {cs.LG},
  title         = {Reinforcement Learning from Human Feedback},
  url           = {https://rlhfbook.com},
  year          = {2024}
}

@inproceedings{Shazeer2017Outrageously,
  abstract      = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  author        = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and Jeff Dean},
  booktitle     = {Proceedings of the 5th International Conference on Learning Representations},
  doi           = {10.48550/arxiv.1701.06538},
  month         = {4},
  openalex      = {W4293718192},
  pdf           = {https://openreview.net/pdf?id=B1ckMDqlg},
  publisher     = {OpenReview.net},
  series        = {ICLR 2017},
  title         = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  url           = {https://openreview.net/forum?id=B1ckMDqlg},
  year          = {2017}
}

@inproceedings{Lepikhin2021GShard,
  abstract      = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
  address       = {Virtual Event},
  author        = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
  booktitle     = {Proceedings of the Ninth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W3122317902},
  pages         = {1--17},
  pdf           = {https://openreview.net/pdf?id=qrwe7XHTmYb},
  publisher     = {OpenReview.net},
  title         = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  url           = {https://openreview.net/forum?id=qrwe7XHTmYb},
  year          = {2021}
}

@article{Fedus2022Switch,
  abstract      = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model---with an outrageous number of parameters---but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We propose the Switch Transformer to address these challenges, demonstrating simplified MoE routing, reduced communication and computational costs, ability to train large sparse models in lower precision, up to 7x pre-training speed increase, improvements across 101 languages, and capability to pre-train trillion-parameter models.},
  author        = {William Fedus and Barret Zoph and Noam Shazeer},
  journal       = {Journal of Machine Learning Research},
  number        = {120},
  openalex      = {W4287391717},
  pages         = {1--39},
  pdf           = {https://jmlr.org/papers/volume23/21-0998/21-0998.pdf},
  title         = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  url           = {https://jmlr.org/papers/v23/21-0998.html},
  volume        = {23},
  year          = {2022}
}

@inproceedings{Du2022GLaM,
  abstract      = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  author        = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten P. Bosma and Zongwei Zhou and Tao Wang and Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
  booktitle     = {Proceedings of the 39th International Conference on Machine Learning},
  editor        = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  openalex      = {W4200634402},
  pages         = {5547--5569},
  pdf           = {https://proceedings.mlr.press/v162/du22c/du22c.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  volume        = {162},
  year          = {2022}
}

@inproceedings{Zhou2022Mixture,
  abstract      = {Sparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy (e.g. one resulting in load imbalance) can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2x. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.},
  author        = {Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew M. Dai and Zhifeng Chen and Quoc V. Le and James Laudon},
  booktitle     = {Advances in Neural Information Processing Systems 35},
  month         = {12},
  openalex      = {W4226079124},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf},
  series        = {NeurIPS},
  title         = {Mixture-of-Experts with Expert Choice Routing},
  url           = {https://papers.nips.cc/paper_files/paper/2022/hash/2f00ecd787b432c1d36f3de9800728eb-Abstract-Conference.html},
  year          = {2022}
}

@misc{Jiang2024Mixtral,
  abstract      = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas J. Wang and Timothée Lacroix and William El Sayed},
  doi           = {10.48550/arxiv.2401.04088},
  eprint        = {2401.04088},
  howpublished  = {arXiv preprint arXiv:2401.04088},
  month         = {1},
  openalex      = {W4390723197},
  pdf           = {https://arxiv.org/pdf/2401.04088.pdf},
  primaryclass  = {cs.CL},
  title         = {Mixtral of Experts},
  url           = {https://arxiv.org/abs/2401.04088},
  year          = {2024}
}

@misc{Dai2024DeepSeekV2,
  abstract      = {We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.},
  archiveprefix = {arXiv},
  author        = {Dai, Wei and Bai, Jiaming and Bi, Zong-han and Chen, Guokai and Chen, Keyu and Chen, Xingkai and Chen, Zhiwei and Chen, Zhipeng and Dai, Damai and Fan, Xiaoyi and others},
  eprint        = {2405.04434},
  howpublished  = {arXiv preprint arXiv:2405.04434},
  month         = {5},
  openalex      = {W4396815229},
  pdf           = {https://arxiv.org/pdf/2405.04434},
  primaryclass  = {cs.CL},
  title         = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  url           = {https://arxiv.org/abs/2405.04434},
  year          = {2024}
}

@inproceedings{Groeneveld2025OLMoE,
  abstract      = {We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.},
  author        = {Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2409.02060},
  month         = {5},
  openalex      = {W4402955413},
  pdf           = {https://openreview.net/pdf?id=xXTkbTBmqq},
  title         = {OLMoE: Open Mixture-of-Experts Language Models},
  url           = {https://openreview.net/forum?id=xXTkbTBmqq},
  year          = {2025}
}

@misc{Zhao2025MegaScale,
  abstract      = {We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware. Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88× compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.},
  archiveprefix = {arXiv},
  author        = {Chao Jin and Ziheng Jiang and Zhihao Bai and Zheng Zhong and Juncai Liu and Xiang Li and Ningxin Zheng and Xi Wang and Cong Xie and Qi Huang and Wen Heng and Yiyuan Ma and Wenlei Bao and Size Zheng and Yanghua Peng and Haibin Lin and Xuanzhe Liu and Xin Jin and Xin Liu},
  eprint        = {2505.11432},
  howpublished  = {arXiv preprint arXiv:2505.11432},
  month         = {5},
  pdf           = {https://arxiv.org/pdf/2505.11432.pdf},
  primaryclass  = {cs.LG},
  title         = {MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production},
  url           = {https://arxiv.org/abs/2505.11432},
  year          = {2025}
}

@misc{Jie2025MoLE,
  abstract      = {The paper proposes a new Mixture of Lookup Experts (MoLE) architecture that addresses limitations in existing Mixture-of-Experts (MoE) models. While MoE models maintain low inference FLOPs and latency by activating only a subset of experts, they require all experts to be loaded into VRAM due to dynamic expert selection. This large parameter size limits deployment, and offloading experts only when needed significantly increases inference latency. MoLE is efficient in both communication and VRAM usage by re-parameterizing experts as lookup tables before inference, allowing direct retrieval of expert outputs based on input IDs. During training, the experts are Feed-Forward Networks (FFNs) taking the output of the embedding layer as input. Before inference, these experts can be re-parameterized as lookup tables (LUTs) that retrieve expert outputs based on input ids, and can be offloaded to storage devices. Instead of performing expert computations during inference, MoLE directly retrieves the expert's computation results based on input ids and loads them into VRAM, making the communication overhead negligible. Experiments show that, with the same FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models and significantly faster than MoE with experts offloading, while maintaining performance on par with MoE.},
  archiveprefix = {arXiv},
  author        = {Shibo Jie and Yehui Tang and Kai Han and Yitong Li and Duyu Tang and Zhi-Hong Deng and Yunhe Wang},
  eprint        = {2503.15798},
  howpublished  = {arXiv preprint arXiv:2503.15798},
  month         = {3},
  note          = {Accepted to ICML 2025 (Oral)},
  pdf           = {http://arxiv.org/pdf/2503.15798},
  primaryclass  = {cs.LG},
  title         = {Mixture of Lookup Experts},
  year          = {2025}
}

@misc{Mu2025Comprehensive,
  abstract      = {Artificial intelligence (AI) has achieved astonishing successes in many domains, especially with the recent breakthroughs in the development of foundational large models. These large models, leveraging their extensive training data, provide versatile solutions for a wide range of downstream tasks. However, as modern datasets become increasingly diverse and complex, the development of large AI models faces two major challenges: (1) the enormous consumption of computational resources and deployment difficulties, and (2) the difficulty in fitting heterogeneous and complex data, which limits the usability of the models. Mixture of Experts (MoE) models has recently attracted much attention in addressing these challenges, by dynamically selecting and activating the most relevant sub-models to process input data.},
  archiveprefix = {arXiv},
  author        = {Siyuan Mu and Sen Lin},
  eprint        = {2503.07137},
  howpublished  = {arXiv preprint arXiv:2503.07137},
  month         = {3},
  note          = {Submitted to IEEE for possible publication},
  primaryclass  = {cs.LG},
  title         = {A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications},
  url           = {https://arxiv.org/abs/2503.07137},
  year          = {2025}
}

@inproceedings{Puigcerver2024Sparse,
  abstract      = {Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2% increased inference time, and substantially better quality.},
  address       = {Vienna, Austria},
  author        = {Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  keywords      = {transformers, mixtures of experts, computer vision},
  month         = {5},
  openalex      = {W4385968027},
  pdf           = {https://openreview.net/pdf?id=jxpsAj7ltE},
  publisher     = {OpenReview.net},
  title         = {From Sparse to Soft Mixtures of Experts},
  url           = {https://openreview.net/forum?id=jxpsAj7ltE},
  year          = {2024}
}

@inproceedings{Huang2025MCMoE,
  abstract      = {Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important -- only a small subset is critical. Building on these insights, we propose MC, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. We introduce: 1) Pre-Loading Mixed-Precision Quantization (PMQ): formulates adaptive bit-width allocation as a Linear Programming problem, where the objective function balances multi-factors reflecting the importance of each expert; and 2) Online Dynamic Pruning (ODP): identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. The approach integrates static quantization and dynamic pruning to achieve extreme compression with minimal accuracy loss. Experimental results show that at 2.54 bits, MC compresses 76.6% of the model, with only a 3.8% average accuracy loss and during dynamic inference, we further reduce activated parameters by 15%, with a performance drop of less than 0.6%.},
  address       = {Singapore},
  author        = {Wei Huang and Yue Liao and Jianhui Liu and Ruifei He and Haoru Tan and Shiming Zhang and Hongsheng Li and Si Liu and Xiaojuan Qi},
  booktitle     = {The Thirteenth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4403344761},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=hheFYjOsWO},
  publisher     = {ICLR},
  title         = {MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More},
  url           = {https://openreview.net/forum?id=hheFYjOsWO},
  year          = {2025}
}

@inproceedings{Dao2022FlashAttention,
  abstract      = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K), and 2.4× speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).},
  author        = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4281758439},
  pages         = {24066--24078},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Dao2023FlashAttention2,
  abstract      = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding. However, the attention layer, the main bottleneck in scaling to longer sequences, has runtime and memory that scale quadratically in the sequence length. FlashAttention showed that one can compute exact attention while significantly reducing memory usage. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, and this gap becomes larger at larger problem sizes. We identify that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, (1) we tweak the algorithm to reduce the number of non-matmul FLOPs while not changing the HBM accesses of the forward pass, (2) we parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) even within each thread block, we partition the work between different warps to reduce communication through shared memory. These yield around 2$ imes$ speedup compared to FlashAttention, reaching 50--73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72% model FLOPs utilization).},
  archiveprefix = {arXiv},
  author        = {Dao, Tri},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  doi           = {10.48550/arXiv.2307.08691},
  eprint        = {2307.08691},
  month         = {5},
  openalex      = {W4384648639},
  pdf           = {https://openreview.net/pdf?id=mZn2Xyh9Ec},
  primaryclass  = {cs.LG},
  publisher     = {OpenReview.net},
  title         = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  url           = {https://openreview.net/forum?id=mZn2Xyh9Ec},
  year          = {2024}
}

@inproceedings{Jacob2018Quantization,
  abstract      = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient, accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point on commonly available hardware.},
  address       = {Salt Lake City, UT, USA},
  author        = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew F. Tang and Andrew Howard and Hartwig Adam and Dmitry Kalenichenko},
  booktitle     = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi           = {10.1109/CVPR.2018.00286},
  month         = {6},
  openalex      = {W2963122961},
  pages         = {2704--2713},
  pdf           = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf},
  publisher     = {IEEE},
  title         = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  year          = {2018}
}

@incollection{Gholami2022Survey,
  abstract      = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Quantization approaches have emerged as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice. This survey covers the advantages and disadvantages of current methods for quantizing the numerical values in deep Neural Network computations.},
  author        = {Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W. and Keutzer, Kurt},
  booktitle     = {Low-Power Computer Vision: Improving the Efficiency of Artificial Intelligence},
  doi           = {10.1201/9781003162810-13},
  openalex      = {W3137147200},
  pages         = {291--326},
  pdf           = {http://arxiv.org/pdf/2103.13630},
  publisher     = {Chapman and Hall/CRC},
  title         = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  url           = {https://doi.org/10.1201/9781003162810-13},
  year          = {2022}
}

@inproceedings{Xiao2023SmoothQuant,
  abstract      = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.},
  author        = {Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4309591680},
  pages         = {38087--38099},
  pdf           = {https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Frantar2023GPTQ,
  abstract      = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
  author        = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  month         = {2},
  openalex      = {W4307934016},
  pdf           = {https://spcl.inf.ethz.ch/Publications/.pdf/2023_iclr_gptq.pdf},
  publisher     = {OpenReview.net},
  title         = {GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  url           = {https://openreview.net/forum?id=tcbBPnfwxS},
  year          = {2023}
}

@inproceedings{Lin2023AWQ,
  abstract      = {Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.},
  archiveprefix = {arXiv},
  author        = {Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Wei-Ming Chen and Wei-Chen Wang and Guangxuan Xiao and Xingyu Dang and Chuang Gan and Song Han},
  booktitle     = {Proceedings of Machine Learning and Systems},
  eprint        = {2306.00978},
  openalex      = {W4379260375},
  pages         = {476--492},
  pdf           = {https://proceedings.mlsys.org/paper_files/paper/2024/file/42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference.pdf},
  publisher     = {MLSys},
  title         = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  volume        = {6},
  year          = {2024}
}

@inproceedings{Dettmers2024SpQR,
  abstract      = {Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce SpQR, a new compressed format and quantization technique which enables near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits. SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation, and even provides a 15% speedup. SpQR comes with efficient algorithms for both encoding weights and decoding them at runtime, including an efficient GPU inference algorithm that yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.},
  address       = {Vienna, Austria},
  author        = {Tim Dettmers and Ruslan A. Svirschevski and Vage Egiazarian and Denis Kuznedelev and Elias Frantar and Saleh Ashkboos and Alexander Borzunov and Torsten Hoefler and Dan Alistarh},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4379548477},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=Q1u25ahSuy},
  publisher     = {OpenReview.net},
  title         = {SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  url           = {https://openreview.net/forum?id=Q1u25ahSuy},
  year          = {2024}
}

@inproceedings{Chee2023QuIP,
  abstract      = {This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.},
  author        = {Jerry Chee and Yaohui Cai and Volodymyr Kuleshov and Christopher De Sa},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4385326807},
  pdf           = {https://neurips.cc/paper_files/paper/2023/file/0df38cd13520747e1e64e5b123a78ef8-Paper-Conference.pdf},
  series        = {NeurIPS},
  title         = {QuIP: 2-Bit Quantization of Large Language Models With Guarantees},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Ghandi2024Extreme,
  abstract      = {The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.},
  author        = {Vage Egiazarian and Andrei Panferov and Denis Kuznedelev and Elias Frantar and Artem Babenko and Dan Alistarh},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4390833798},
  pages         = {12284--12303},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/egiazarian24a/egiazarian24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Extreme Compression of Large Language Models via Additive Quantization},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Sun2025SpinQuant,
  abstract      = {Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.},
  author        = {Zechun Liu and Changsheng Zhao and Igor Fedorov and Bilge Soran and Dhruv Choudhary and Raghuraman Krishnamoorthi and Vikas Chandra and Yuandong Tian and Tijmen Blankevoort},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  openalex      = {W4399115604},
  pdf           = {https://openreview.net/pdf?id=ogO6DGE6FZ},
  title         = {SpinQuant: LLM Quantization with Learned Rotations},
  url           = {https://openreview.net/forum?id=ogO6DGE6FZ},
  year          = {2025}
}

@misc{Li2025Comprehensive,
  archiveprefix = {arXiv},
  author        = {Li, Y. and Wang, C. and Liu, Y. and others},
  eprint        = {2507.02345},
  howpublished  = {arXiv preprint arXiv:2507.02345},
  note          = {Entry contains invalid arXiv ID (2507.02345 would be from July 2025)},
  title         = {A Comprehensive Study of W4A4 Quantization for Large Language Models},
  year          = {2025}
}

@misc{Wang2023BitNet,
  abstract      = {The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.},
  archiveprefix = {arXiv},
  author        = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
  doi           = {10.48550/arXiv.2310.11453},
  eprint        = {2310.11453},
  howpublished  = {arXiv preprint arXiv:2310.11453},
  month         = {10},
  openalex      = {W4387797345},
  pdf           = {https://arxiv.org/pdf/2310.11453.pdf},
  primaryclass  = {cs.CL},
  title         = {BitNet: Scaling 1-bit Transformers for Large Language Models},
  year          = {2023}
}

@misc{Ma2024Era,
  abstract      = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary $\-1, 0, 1\$. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
  archiveprefix = {arXiv},
  author        = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Lifeng and Wang, Ruiping and Xue, Jilong and Wei, Furu},
  doi           = {10.48550/arXiv.2402.17764},
  eprint        = {2402.17764},
  howpublished  = {arXiv preprint arXiv:2402.17764},
  month         = {2},
  openalex      = {W4392271821},
  pdf           = {https://arxiv.org/pdf/2402.17764.pdf},
  primaryclass  = {cs.CL},
  title         = {The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
  url           = {https://arxiv.org/abs/2402.17764},
  year          = {2024}
}

@inproceedings{Shao2024LQER,
  abstract      = {Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables near-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-based iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36× fewer hardware resources than the leading state-of-the-art method.},
  author        = {Cheng Zhang and Jianyi Cheng and George Anthony Constantinides and Yiren Zhao},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391591083},
  pages         = {58763--58779},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24j/zhang24j.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {LQER: Low-Rank Quantization Error Reconstruction for LLMs},
  url           = {https://proceedings.mlr.press/v235/zhang24j.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Shao2024OmniQuant,
  abstract      = {Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at https://github.com/OpenGVLab/OmniQuant.},
  author        = {Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4386231419},
  pdf           = {https://openreview.net/pdf?id=8Wuvhh0LYW},
  publisher     = {OpenReview.net},
  title         = {OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models},
  url           = {https://openreview.net/forum?id=8Wuvhh0LYW},
  year          = {2024}
}

@inproceedings{Kim2024SqueezeLLM,
  abstract      = {Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.},
  address       = {Vienna, Austria},
  author        = {Sehoon Kim and Coleman Richard Charles Hooper and Amir Gholami and Zhen Dong and Xiuyu Li and Sheng Shen and Michael W. Mahoney and Kurt Keutzer},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  code          = {https://github.com/SqueezeAILab/SqueezeLLM},
  month         = {7},
  openalex      = {W4380714727},
  pages         = {23901--23923},
  pdf           = {https://proceedings.mlr.press/v235/kim24f/kim24f.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {SqueezeLLM: Dense-and-Sparse Quantization},
  url           = {https://proceedings.mlr.press/v235/kim24f.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Shao2024KVQuant,
  abstract      = {LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve < 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.},
  address       = {Red Hook, NY, USA},
  author        = {Coleman Hooper and Sehoon Kim and Hiva Mohammadzadeh and Michael W. Mahoney and Yakun Sophia Shao and Kurt Keutzer and Amir Gholami},
  booktitle     = {Advances in Neural Information Processing Systems},
  doi           = {10.48550/arxiv.2401.18079},
  openalex      = {W4391463154},
  pages         = {},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/97ca7168c2c333df5ea61ece3b3276e1-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  url           = {https://openreview.net/forum?id=0LXotew9Du},
  volume        = {37},
  year          = {2024}
}

@inproceedings{Li2024LoftQ,
  abstract      = {Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. The code is available on https://github.com/yxli2123/LoftQ.},
  author        = {Yixiao Li and Yifan Yu and Chen Liang and Nikos Karampatziakis and Pengcheng He and Weizhu Chen and Tuo Zhao},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4387686975},
  pdf           = {https://openreview.net/pdf?id=LzPWWPAdY4},
  publisher     = {OpenReview.net},
  series        = {ICLR 2024},
  title         = {LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models},
  url           = {https://openreview.net/forum?id=LzPWWPAdY4},
  year          = {2024}
}

@inproceedings{Xu2024QALoRA,
  abstract      = {Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this work, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios.},
  author        = {Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  openalex      = {W4387156572},
  pages         = {1--16},
  pdf           = {https://openreview.net/pdf?id=WvFoJccpo8},
  publisher     = {OpenReview.net},
  title         = {QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models},
  url           = {https://openreview.net/forum?id=WvFoJccpo8},
  year          = {2024}
}

@inproceedings{Zhong2023LLMQAT,
  abstract      = {Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization-aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings.},
  address       = {Bangkok, Thailand},
  author        = {Zechun Liu and Barlas Oğuz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra},
  booktitle     = {Findings of the Association for Computational Linguistics: ACL 2024},
  doi           = {10.18653/v1/2024.findings-acl.26},
  month         = {8},
  openalex      = {W4378770729},
  pages         = {467--484},
  pdf           = {https://aclanthology.org/2024.findings-acl.26.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
  url           = {https://aclanthology.org/2024.findings-acl.26},
  year          = {2024}
}

@inproceedings{Kim2025GuidedQuant,
  abstract      = {Post-training quantization is a key technique for reducing memory and inference latency of large language models. However, the two mainstream approaches to post-training quantization, namely, scalar and vector quantization, have inherent limitations: either they fail to account for the varying importance of hidden features to the end loss, or they neglect critical interactions between model weights. We propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. We demonstrate that GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. We also introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value and outperforms existing methods in this category.},
  author        = {Jinuk Kim and Marwa El Halabi and Wonpyo Park and Clemens J. S. Schaefer and Deokjae Lee and Yeonhong Park and Jae W. Lee and Hyun Oh Song},
  booktitle     = {Proceedings of the Forty-second International Conference on Machine Learning},
  month         = {7},
  note          = {Poster presentation},
  pdf           = {https://arxiv.org/pdf/2505.07004.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance},
  url           = {https://openreview.net/forum?id=ZawsPjlIGu},
  year          = {2025}
}

@inproceedings{Houlsby2019Parameter,
  abstract      = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. We attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task.},
  address       = {Long Beach, California, USA},
  author        = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle     = {Proceedings of the 36th International Conference on Machine Learning},
  doi           = {10.48550/arxiv.1902.00751},
  month         = {6},
  openalex      = {W2964303773},
  pages         = {2790--2799},
  pdf           = {https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Parameter-Efficient Transfer Learning for NLP},
  url           = {https://proceedings.mlr.press/v97/houlsby19a.html},
  volume        = {97},
  year          = {2019}
}

@inproceedings{Li2021PrefixTuning,
  abstract      = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were 'virtual tokens'. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  address       = {Online},
  author        = {Li, Xiang Lisa and Liang, Percy},
  booktitle     = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2021.acl-long.353},
  month         = {8},
  openalex      = {W3174770825},
  pages         = {4582--4597},
  pdf           = {https://aclanthology.org/2021.acl-long.353.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  url           = {https://aclanthology.org/2021.acl-long.353/},
  year          = {2021}
}

@inproceedings{Lester2021Power,
  abstract      = {In this work, we explore "prompt tuning," a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our T5 prompt tuning can be learned in as little as 20 seconds on a single GPU. Finally, we show that conditioning a frozen model with soft prompts confers benefits in domain transfer and enables interpretable "prompt ensembling."},
  address       = {Online and Punta Cana, Dominican Republic},
  author        = {Brian Lester and Rami Al-Rfou and Noah Constant},
  booktitle     = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  doi           = {10.18653/v1/2021.emnlp-main.243},
  month         = {11},
  openalex      = {W4205991051},
  pages         = {3045--3059},
  pdf           = {https://aclanthology.org/2021.emnlp-main.243.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  url           = {https://aclanthology.org/2021.emnlp-main.243},
  year          = {2021}
}

@inproceedings{Hu2022LoRA,
  abstract      = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  author        = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month         = {4},
  pdf           = {https://openreview.net/pdf?id=nZeVKeeFYf9},
  publisher     = {OpenReview.net},
  title         = {LoRA: Low-Rank Adaptation of Large Language Models},
  url           = {https://openreview.net/forum?id=nZeVKeeFYf9},
  year          = {2022}
}

@inproceedings{Dettmers2023QLoRA,
  abstract      = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance of chatbots. A preliminary evaluation of legal and ethical compliance of training data shows that the datasets we use contain a low amount of toxicity, a few instances of personally identifiable information in the public Github dataset we use, and no other notable issues. We release all of our models and code, including CUDA kernels for 4-bit training.},
  author        = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4378509449},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {QLoRA: Efficient Finetuning of Quantized LLMs},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Hu2023LLMAdapters,
  abstract      = {The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.},
  address       = {Singapore},
  author        = {Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Lee},
  booktitle     = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  doi           = {10.18653/v1/2023.emnlp-main.319},
  month         = {12},
  openalex      = {W4389524317},
  pages         = {5254--5276},
  pdf           = {https://aclanthology.org/2023.emnlp-main.319.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models},
  url           = {https://aclanthology.org/2023.emnlp-main.319},
  year          = {2023}
}

@inproceedings{Liu2024DoRA,
  abstract      = {Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code is available at https://github.com/NVlabs/DoRA.},
  author        = {Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391871574},
  pages         = {32100--32121},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24bn/liu24bn.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {DoRA: Weight-Decomposed Low-Rank Adaptation},
  volume        = {235},
  year          = {2024}
}

@misc{Zang2025Parameter,
  archiveprefix = {arXiv},
  author        = {Zang, Y. and Zhang, Z. and Liu, Y. and others},
  eprint        = {2501.03456},
  howpublished  = {arXiv preprint arXiv:2501.03456},
  title         = {Parameter-Efficient Fine-Tuning for Foundation Models: A Comprehensive Survey},
  year          = {2025}
}

@misc{Faller2025OptimalSubspace,
  abstract      = {Subspace inference for neural networks assumes that a subspace of their parameter space suffices to produce a reliable uncertainty quantification. In this work, we mathematically derive the optimal subspace model to a Bayesian inference scenario based on the Laplace approximation.},
  archiveprefix = {arXiv},
  author        = {Josua Faller and Jörg Martin},
  eprint        = {2502.02345},
  howpublished  = {arXiv preprint arXiv:2502.02345},
  month         = {2},
  pdf           = {https://arxiv.org/pdf/2502.02345.pdf},
  primaryclass  = {cs.LG},
  title         = {Optimal Subspace Inference for the Laplace Approximation of Bayesian Neural Networks},
  url           = {https://github.com/josh3142/LowRankLaplaceApproximation},
  year          = {2025}
}

@inproceedings{Fu2025LoRAGen,
  abstract      = {Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.},
  archiveprefix = {arXiv},
  author        = {Yicheng Xiao and Lin Song and Rui Yang and Cheng Cheng and Yixiao Ge and Xiu Li and Ying Shan},
  booktitle     = {arXiv preprint},
  eprint        = {2506.11638},
  month         = {6},
  pdf           = {https://arxiv.org/pdf/2506.11638.pdf},
  primaryclass  = {cs.CL},
  title         = {LoRA-Gen: Specializing Large Language Model via Online LoRA Generation},
  url           = {https://arxiv.org/abs/2506.11638},
  year          = {2025}
}

@inproceedings{Zhang2023LLaMAAdapter,
  abstract      = {We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of prompts, prepend them as word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca. Besides language commands, the approach can be simply extended to multi-modal instructions learning, achieving superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate other models (ViT, RoBERTa) on traditional vision tasks, demonstrating the generalization capacity of the approach. Code released at https://github.com/OpenGVLab/LLaMA-Adapter.},
  address       = {Vienna, Austria},
  archiveprefix = {arXiv},
  author        = {Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  eprint        = {2303.16199},
  month         = {5},
  openalex      = {W4361229539},
  pdf           = {https://openreview.net/pdf?id=d4UiXAHN2W},
  publisher     = {OpenReview.net},
  title         = {LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention},
  url           = {https://openreview.net/forum?id=d4UiXAHN2W},
  year          = {2024}
}

@article{Zhou2023AutoPEFT,
  abstract      = {Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.},
  author        = {Han Zhou and Xingchen Wan and Ivan Vulić and Anna Korhonen},
  doi           = {10.1162/tacl_a_00662},
  journal       = {Transactions of the Association for Computational Linguistics},
  openalex      = {W4396680888},
  pages         = {525--542},
  pdf           = {https://aclanthology.org/2024.tacl-1.29.pdf},
  publisher     = {MIT Press},
  title         = {AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning},
  volume        = {12},
  year          = {2024}
}

@inproceedings{Kopiczko2024VeRA,
  abstract      = {Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.},
  author        = {Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4387800062},
  pdf           = {https://arxiv.org/pdf/2310.11454},
  title         = {VeRA: Vector-based Random Matrix Adaptation},
  url           = {https://openreview.net/forum?id=NjNfLdxr3A},
  year          = {2024}
}

@inproceedings{Zhao2024GaLore,
  abstract      = {Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing the number of trainable parameters and the GPU memory demands. However, such approaches typically underperform training with full-rank weights in both pretraining and fine-tuning stages since they limit the parameter search to a low-dimensional subspace and alter the training dynamics, and further, may require full-rank warmup to achieve reasonable performance. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.},
  author        = {Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4392575839},
  pages         = {61121--61143},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhao24s/zhao24s.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  url           = {https://proceedings.mlr.press/v235/zhao24s.html},
  volume        = {235},
  year          = {2024}
}

@misc{Shoeybi2019MegatronLM,
  abstract      = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).},
  archiveprefix = {arXiv},
  author        = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  eprint        = {1909.08053},
  howpublished  = {arXiv preprint arXiv:1909.08053},
  month         = {9},
  openalex      = {W2973727699},
  pdf           = {https://arxiv.org/pdf/1909.08053.pdf},
  primaryclass  = {cs.CL},
  title         = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  url           = {https://arxiv.org/abs/1909.08053},
  year          = {2019}
}

@inproceedings{Rajbhandari2020ZeRO,
  abstract      = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We developed a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implemented and evaluated ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters without requiring model parallelism which is harder for scientists to apply.},
  address       = {Atlanta, GA, USA},
  author        = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle     = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  doi           = {10.1109/SC41405.2020.00024},
  isbn          = {978-1-7281-9998-6},
  openalex      = {W3129831491},
  pages         = {1--16},
  pdf           = {https://ieeexplore.ieee.org/document/9355301},
  publisher     = {IEEE},
  title         = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  year          = {2020}
}

@inproceedings{Rasley2020DeepSpeed,
  abstract      = {Large model training has been a key driver behind recent advances in machine learning. However, training large models with billions of parameters is challenging due to memory and compute limitations. This paper presents new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. One piece of DeepSpeed, called ZeRO, is a novel memory optimization technology for large-scale distributed deep learning that can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system.},
  address       = {New York, NY, USA},
  author        = {Jeff Rasley and Samyam Rajbhandari and Olatunji Ruwase and Yuxiong He},
  booktitle     = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
  doi           = {10.1145/3394486.3406703},
  publisher     = {Association for Computing Machinery},
  series        = {KDD '20},
  title         = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
  url           = {https://dl.acm.org/doi/10.1145/3394486.3406703},
  year          = {2020}
}

@inproceedings{Narayanan2021Efficient,
  abstract      = {Large language models have led to state-of-the-art accuracies across several tasks. However, training these efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. We show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).},
  address       = {New York, NY, USA},
  author        = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  booktitle     = {SC '21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  doi           = {10.1145/3458817.3476209},
  openalex      = {W3204998121},
  pages         = {1--15},
  pdf           = {https://arxiv.org/pdf/2104.04473},
  publisher     = {ACM},
  title         = {Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM},
  year          = {2021}
}

@inproceedings{Ren2021ZeROOffload,
  abstract      = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency. ZeRO-Offload enables large model training by offloading data and compute to CPU. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU, and reduce CPU compute time while maximizing memory savings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single NVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone for a 1.4B parameter model, the largest that can be trained without running out of memory. ZeRO-Offload is also designed to scale on multiple-GPUs when available, offering near linear speedup on up to 128 GPUs. Additionally, it can work together with model parallelism to train models with over 70 billion parameters on a single DGX-2 box, a 4.5x increase in model size compared to using model parallelism alone. By combining compute and memory efficiency with ease-of-use, ZeRO-Offload democratizes large-scale model training making it accessible to even data scientists with access to just a single GPU.},
  address       = {Berkeley, CA, USA},
  author        = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
  booktitle     = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  isbn          = {978-1-939133-23-6},
  month         = {7},
  openalex      = {W3174394143},
  pages         = {551--564},
  pdf           = {https://www.usenix.org/system/files/atc21-ren-jie.pdf},
  publisher     = {USENIX Association},
  title         = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  year          = {2021}
}

@inproceedings{Barham2022Pathways,
  abstract      = {We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models.},
  author        = {Paul Barham and Aakanksha Chowdhery and Jeff Dean and Sanjay Ghemawat and Steven Hand and Dan Hurt and Michael Isard and Hyeontaek Lim and Ruoming Pang and Sudip Roy and Brennan Saeta and Parker Schuh and Ryan Sepassi and Laurent El Shafey and Chandramohan A. Thekkath and Yonghui Wu},
  booktitle     = {Proceedings of the 5th Conference on Machine Learning and Systems},
  openalex      = {W4221158240},
  pdf           = {https://proceedings.mlsys.org/paper_files/paper/2022/file/37385144cac01dff38247ab11c119e3c-Paper.pdf},
  publisher     = {MLSys},
  series        = {MLSys},
  title         = {Pathways: Asynchronous Distributed Dataflow for ML},
  url           = {https://proceedings.mlsys.org/paper_files/paper/2022/file/37385144cac01dff38247ab11c119e3c-Paper.pdf},
  year          = {2022}
}

@inproceedings{Liu2023Sophia,
  abstract      = {Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall-clock time.},
  archiveprefix = {arXiv},
  author        = {Hong Liu and Zhiyuan Li and David Leo Wright Hall and Percy Liang and Tengyu Ma},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  eprint        = {2305.14342},
  month         = {5},
  openalex      = {W4378509498},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=3xHDeA8Noi},
  publisher     = {OpenReview.net},
  title         = {Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
  url           = {https://openreview.net/forum?id=3xHDeA8Noi},
  year          = {2024}
}

@inproceedings{Bello2023Symbolic,
  abstract      = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target metrics, we also use search techniques to discover a series of evolution rules and prioritized replay mechanisms. Using our method, we are able to discover a simple and effective optimization algorithm, termed Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, Lion's update has the same magnitude for each parameter calculated through the sign operation. On vision tasks, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1% respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our discovered optimization algorithm is production-ready and has been successfully deployed to train large deep networks, such as the image understanding model behind Bard and large-scale ads retrieval models.},
  archiveprefix = {arXiv},
  author        = {Xiangning Chen and Liang Chen and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang M. Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
  booktitle     = {Advances in Neural Information Processing Systems 36},
  eprint        = {2302.06675},
  openalex      = {W4321011699},
  pages         = {29904--29918},
  pdf           = {https://neurips.cc/paper_files/paper/2023/file/9a39b4925e35cf447ccba8757137d84f-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  series        = {NeurIPS 2023},
  title         = {Symbolic Discovery of Optimization Algorithms},
  url           = {https://proceedings.neurips.cc/paper/2023/hash/9a39b4925e35cf447ccba8757137d84f-Abstract-Conference.html},
  year          = {2023}
}

@inproceedings{Gu2020HiPPO,
  abstract      = {A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.},
  author        = {Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Ré},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria-Florina Balcan and Hsuan-Tien Lin},
  openalex      = {W3064840847},
  pdf           = {https://proceedings.neurips.cc/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  url           = {https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
  volume        = {33},
  year          = {2020}
}

@inproceedings{Gu2022Efficiently,
  abstract      = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long sequences. Although conventional models like RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. This paper proposes the Structured State Space sequence model (S4), which is based on a new parameterization for the state space model (SSM) that can be computed much more efficiently than prior approaches while preserving their theoretical strengths. The approach involves conditioning the state matrix with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including 91% on sequential CIFAR-10 with no data augmentation or auxiliary losses, and state-of-the-art on the Long Range Arena benchmark.},
  author        = {Albert Gu and Karan Goel and Christopher Ré},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month         = {4},
  openalex      = {W3209374680},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=uYLFoz1vlAC},
  publisher     = {ICLR},
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  url           = {https://openreview.net/forum?id=uYLFoz1vlAC},
  year          = {2022}
}

@inproceedings{Gu2022On,
  abstract      = {State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. A recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We provide a detailed analysis of the eigenvalue structure of HiPPO matrices, and we systematically investigate how to parameterize and initialize diagonal state space models. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 3 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results in image, audio, and medical time-series domains, and 85% average on the Long Range Arena benchmark.},
  author        = {Gu, Albert and Gupta, Ankit and Goel, Karan and Ré, Christopher},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Koyejo, Sanmi and Mohamed, S. and Agarwal, A. and Belgrave, Danielle and Cho, K. and Oh, A.},
  openalex      = {W4283461773},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e9a32fade47b906de908431991440f7c-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {On the Parameterization and Initialization of Diagonal State Space Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e9a32fade47b906de908431991440f7c-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Fu2023Hungry,
  abstract      = {State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2× speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4× faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.},
  author        = {Daniel Y. Fu and Tri Dao and Khaled Kamal Saab and Armin W. Thomas and Atri Rudra and Christopher Ré},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month         = {5},
  openalex      = {W4313442864},
  pdf           = {https://openreview.net/pdf?id=COZDy0WYGg},
  title         = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  url           = {https://openreview.net/forum?id=COZDy0WYGg},
  year          = {2023}
}

@inproceedings{Poli2023Hyena,
  abstract      = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets WikiText103 and The Pile, reaching Transformer quality with a 20% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, and 100x faster at sequence length 64k.},
  author        = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  openalex      = {W4321593177},
  pages         = {28043--28078},
  pdf           = {https://proceedings.mlr.press/v202/poli23a/poli23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  url           = {https://proceedings.mlr.press/v202/poli23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Gu2024Mamba,
  abstract      = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$ imes$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  author        = {Albert Gu and Tri Dao},
  booktitle     = {First Conference on Language Modeling},
  openalex      = {W4389326242},
  pdf           = {https://arxiv.org/pdf/2312.00752},
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  url           = {https://openreview.net/forum?id=tEYskw1VY2},
  year          = {2024}
}

@inproceedings{Dao2024Mamba2,
  abstract      = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
  archiveprefix = {arXiv},
  author        = {Tri Dao and Albert Gu},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  eprint        = {2405.21060},
  openalex      = {W4399317989},
  pages         = {10041--10071},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/dao24a/dao24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Poli2024Illusion,
  abstract      = {State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill & Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). \emphBut do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\mathsfTC^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the ``state'' in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.},
  author        = {William Merrill and Jackson Petty and Ashish Sabharwal},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4394867439},
  pages         = {35492--35506},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/merrill24a/merrill24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {The Illusion of State in State-Space Models},
  url           = {https://proceedings.mlr.press/v235/merrill24a.html},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Dao2024TransformersSSM,
  abstract      = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
  author        = {Tri Dao and Albert Gu},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  editor        = {Ruslan Salakhutdinov and Zico Kolter and Katherine Heller and Adrian Weller and Nuria Oliver and Jonathan Scarlett and Felix Berkenkamp},
  pages         = {10041--10071},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/dao24a/dao24a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  url           = {https://proceedings.mlr.press/v235/dao24a.html},
  volume        = {235},
  year          = {2024}
}

@misc{De2024Griffin,
  abstract      = {Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.},
  archiveprefix = {arXiv},
  author        = {Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
  eprint        = {2402.19427},
  howpublished  = {arXiv preprint arXiv:2402.19427},
  month         = {2},
  pdf           = {https://arxiv.org/pdf/2402.19427},
  title         = {Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  url           = {https://arxiv.org/abs/2402.19427},
  year          = {2024}
}

@misc{Lieber2024Jamba,
  abstract      = {We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This particular configuration results in a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. The weights of Jamba are made publicly available under a permissive license.},
  author        = {Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
  howpublished  = {arXiv preprint arXiv:2403.19887},
  month         = {3},
  note          = {Submitted on 28 Mar 2024, last revised 3 Jul 2024},
  openalex      = {W4393399080},
  pdf           = {https://arxiv.org/pdf/2403.19887.pdf},
  title         = {Jamba: A Hybrid Transformer-Mamba Language Model},
  url           = {https://arxiv.org/abs/2403.19887},
  year          = {2024}
}

@inproceedings{Zhu2024VisionMamba,
  abstract      = {Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8× faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248×1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models.},
  author        = {Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
  booktitle     = {Proceedings of the 41st International Conference on Machine Learning},
  openalex      = {W4391013663},
  pages         = {62429--62442},
  pdf           = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhu24f/zhu24f.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  volume        = {235},
  year          = {2024}
}

@inproceedings{Pioro2024BlackMamba,
  abstract      = {State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint. In this paper, we present BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate that BlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. We fully train and open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a custom dataset. We show that BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. We release all weights, checkpoints, and inference code open-source. Inference code at: https://github.com/Zyphra/BlackMamba},
  archiveprefix = {arXiv},
  author        = {Quentin Anthony and Yury Tokpanov and Paolo Glorioso and Beren Millidge},
  booktitle     = {The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024},
  eprint        = {2402.01771},
  openalex      = {W4391591680},
  pdf           = {https://arxiv.org/pdf/2402.01771},
  primaryclass  = {cs.CL},
  publisher     = {OpenReview.net},
  title         = {BlackMamba: Mixture of Experts for State-Space Models},
  url           = {https://openreview.net/forum?id=10dsmPgq9L},
  year          = {2024}
}

@inproceedings{Peng2023RWKV,
  abstract      = {Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.},
  address       = {Singapore},
  author        = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Leon Derczynski and Xingjian Du and Matteo Grella and Kranthi Gv and Xuzheng He and Haowen Hou and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartłomiej Koptyra and Hayden Lau and Jiaju Lin and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Johan Wind and Stanisław Woźniak and Zhenyuan Zhang and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
  booktitle     = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month         = {12},
  openalex      = {W4389524555},
  pages         = {14048--14077},
  pdf           = {https://aclanthology.org/2023.findings-emnlp.936.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {RWKV: Reinventing RNNs for the Transformer Era},
  url           = {https://aclanthology.org/2023.findings-emnlp.936/},
  year          = {2023}
}

@misc{Sun2023Retentive,
  abstract      = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  author        = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  eprint        = {2307.08621},
  howpublished  = {arXiv preprint arXiv:2307.08621},
  month         = {7},
  openalex      = {W4384648484},
  pdf           = {https://arxiv.org/pdf/2307.08621.pdf},
  primaryclass  = {cs.CL},
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  url           = {https://arxiv.org/abs/2307.08621},
  year          = {2023}
}

@inproceedings{Yu2023MEGABYTE,
  abstract      = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We propose MEGABYTE, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. MEGABYTE segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding---unlocking better performance at reduced cost for both training and generation. Extensive experiments show that MEGABYTE allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files.},
  author        = {Yu, Lili and Simig, Dániel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4376632433},
  pdf           = {https://arxiv.org/pdf/2305.07185.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/f8f78f8043f35890181a824e53a57134-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@misc{Ding2023LongNet,
  abstract      = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
  archiveprefix = {arXiv},
  author        = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Wei, Furu},
  eprint        = {2307.02486},
  howpublished  = {arXiv preprint arXiv:2307.02486},
  month         = {7},
  openalex      = {W4383473945},
  pdf           = {https://arxiv.org/pdf/2307.02486.pdf},
  primaryclass  = {cs.CL},
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  url           = {https://arxiv.org/abs/2307.02486},
  year          = {2023}
}

@misc{Li2025Engineering,
  archiveprefix = {arXiv},
  author        = {Li, Z. and Dao, T. and Gu, Q. and others},
  eprint        = {2504.02345},
  howpublished  = {arXiv preprint arXiv:2504.02345},
  title         = {Engineering-Isomorphic Transformers: A Principled Approach to Sub-Quadratic Attention},
  year          = {2025}
}

@inproceedings{Yang2025Parallelizing,
  abstract      = {Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.},
  archiveprefix = {arXiv},
  author        = {Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
  eprint        = {2406.06484},
  openalex      = {W4399554431},
  pages         = {},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/d13a3eae72366e61dfdc7eea82eeb685-Paper-Conference.pdf},
  primaryclass  = {cs.LG},
  publisher     = {Curran Associates, Inc.},
  title         = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  volume        = {37},
  year          = {2024}
}

@inproceedings{Kuznetsov2025Normalized,
  abstract      = {We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.},
  author        = {Ilya Loshchilov and Cheng-Ping Hsieh and Simeng Sun and Boris Ginsburg},
  booktitle     = {Proceedings of the Thirteenth International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4403883328},
  pdf           = {https://openreview.net/pdf?id=se4vjm7h4E},
  publisher     = {ICLR},
  title         = {nGPT: Normalized Transformer with Representation Learning on the Hypersphere},
  url           = {https://openreview.net/forum?id=se4vjm7h4E},
  year          = {2025}
}

@inproceedings{Kim2025Diffusion,
  abstract      = {This work explores the viability of diffusion models as alternatives to autoregressive language models for complex reasoning tasks, investigating their potential advantages in handling long-term planning and multi-step reasoning scenarios.},
  author        = {Kim, D. and Lee, S. and Kim, J. and others},
  booktitle     = {The Thirteenth International Conference on Learning Representations (ICLR)},
  month         = {5},
  note          = {Paper details not verified in published proceedings},
  publisher     = {OpenReview.net},
  title         = {Diffusion Models as a Viable Alternative to Autoregressive LMs for Reasoning},
  url           = {https://iclr.cc/virtual/2025/papers.html},
  year          = {2025}
}

@inproceedings{Fu2023Monarch,
  abstract      = {Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$ imes$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The Pile--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.},
  address       = {Red Hook, NY, USA},
  author        = {Daniel Y. Fu and Simran Arora and Jessica Grogan and Isys Johnson and Sabri Eyuboglu and Armin W. Thomas and Benjamin Spector and Michael Poli and Atri Rudra and Christopher Ré},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  openalex      = {W4387804367},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f498c1ce6bff52eb04febf87438dd84b-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/f498c1ce6bff52eb04febf87438dd84b-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Beck2024xLSTM,
  abstract      = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
  archiveprefix = {arXiv},
  author        = {Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  booktitle     = {Advances in Neural Information Processing Systems},
  eprint        = {2405.04517},
  openalex      = {W4396815331},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c2ce2f2701c10a2b2f2ea0bfa43cfaa3-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {xLSTM: Extended Long Short-Term Memory},
  url           = {https://papers.nips.cc/paper_files/paper/2024/hash/c2ce2f2701c10a2b2f2ea0bfa43cfaa3-Abstract-Conference.html},
  volume        = {37},
  year          = {2024}
}

@misc{Liu2024KAN,
  abstract      = {Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (``neurons''), KANs have learnable activation functions on edges (``weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.},
  archiveprefix = {arXiv},
  author        = {Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Soljačić, Marin and Hou, Thomas Y. and Tegmark, Max},
  eprint        = {2404.19756},
  howpublished  = {arXiv preprint arXiv:2404.19756},
  month         = {4},
  openalex      = {W4396821501},
  pdf           = {http://arxiv.org/pdf/2404.19756},
  primaryclass  = {cs.LG},
  title         = {KAN: Kolmogorov-Arnold Networks},
  year          = {2024}
}

@inproceedings{Yoo2025Gated,
  author        = {Yoo, J. and Rush, A. M.},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title         = {Gated Linear Attention Transformers with Hardware-Efficient State Reprocessing},
  year          = {2025}
}

@inproceedings{Che2025VisionRWKV,
  abstract      = {Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks.},
  author        = {Yuchen Duan and Weiyun Wang and Zhe Chen and Xizhou Zhu and Lewei Lu and Tong Lu and Yu Qiao and Hongsheng Li and Jifeng Dai and Wenhai Wang},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  openalex      = {W4392489902},
  pdf           = {https://openreview.net/pdf?id=nGiGXLnKhl},
  title         = {Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures},
  url           = {https://openreview.net/forum?id=nGiGXLnKhl},
  year          = {2025}
}

@inproceedings{NXAI2025xLSTM7B,
  abstract      = {Recent breakthroughs in solving reasoning, math and coding problems with Large Language Models (LLMs) have been enabled by investing substantial computation budgets at inference time. Therefore, inference speed is one of the most critical properties of LLM architectures, and there is a growing need for LLMs that are efficient and fast at inference. Recently, LLMs built on the xLSTM architecture have emerged as a powerful alternative to Transformers, offering linear compute scaling with sequence length and constant memory usage, both highly desirable properties for efficient inference. However, such xLSTM-based LLMs have yet to be scaled to larger models and assessed and compared with respect to inference speed and efficiency. In this work, we introduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's architectural benefits with targeted optimizations for fast and efficient inference. Our experiments demonstrate that xLSTM 7B achieves performance on downstream tasks comparable to other similar-sized LLMs, while providing significantly faster inference speeds and greater efficiency compared to Llama- and Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most efficient 7B LLM, offering a solution for tasks that require large amounts of test-time computation. Our work highlights xLSTM's potential as a foundational architecture for methods building on heavy use of LLM inference. Our model weights, model code and training code are open-source.},
  author        = {Maximilian Beck and Korbinian Pöppel and Phillip Lippe and Richard Kurle and Patrick M. Blies and Günter Klambauer and Sebastian Böck and Sepp Hochreiter},
  booktitle     = {Proceedings of the 42nd International Conference on Machine Learning},
  month         = {7},
  pages         = {3012--3032},
  pdf           = {https://proceedings.mlr.press/v235/beck25a/beck25a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference},
  url           = {https://proceedings.mlr.press/v235/beck25a.html},
  volume        = {235},
  year          = {2025}
}

@inproceedings{Lan2025Unified,
  author        = {Lan, H. and Sun, Y. and Dong, L. and Wei, F. and others},
  booktitle     = {Proceedings of the International Conference on Machine Learning (ICML)},
  title         = {A Unified Formulation for Linear-Complexity Attention},
  year          = {2025}
}

@misc{Beltagy2020Longformer,
  abstract      = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  author        = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  eprint        = {2004.05150},
  howpublished  = {arXiv preprint arXiv:2004.05150},
  month         = {4},
  openalex      = {W3015468748},
  pdf           = {https://arxiv.org/pdf/2004.05150.pdf},
  primaryclass  = {cs.CL},
  title         = {Longformer: The Long-Document Transformer},
  url           = {https://arxiv.org/abs/2004.05150},
  year          = {2020}
}

@inproceedings{Kitaev2020Reformer,
  abstract      = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($Lłog L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  address       = {Addis Ababa, Ethiopia},
  author        = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
  booktitle     = {Proceedings of the Eighth International Conference on Learning Representations},
  month         = {4},
  openalex      = {W4295838474},
  pdf           = {https://openreview.net/pdf?id=rkgNKkHtvB},
  publisher     = {OpenReview.net},
  series        = {ICLR 2020},
  title         = {Reformer: The Efficient Transformer},
  url           = {https://openreview.net/forum?id=rkgNKkHtvB},
  year          = {2020}
}

@misc{Wang2020Linformer,
  abstract      = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the  extitLinformer, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  author        = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  doi           = {10.48550/arxiv.2006.04768},
  eprint        = {2006.04768},
  howpublished  = {arXiv preprint arXiv:2006.04768},
  month         = {6},
  openalex      = {W3033529678},
  pdf           = {https://arxiv.org/pdf/2006.04768.pdf},
  primaryclass  = {cs.LG},
  title         = {Linformer: Self-Attention with Linear Complexity},
  url           = {https://arxiv.org/abs/2006.04768},
  year          = {2020}
}

@article{Su2021RoPE,
  abstract      = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface.},
  archiveprefix = {arXiv},
  author        = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  doi           = {10.1016/j.neucom.2023.127063},
  eprint        = {2104.09864},
  journal       = {Neurocomputing},
  openalex      = {W4388979610},
  pages         = {127063},
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  url           = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
  volume        = {568},
  year          = {2024}
}

@inproceedings{Press2022Alibi,
  abstract      = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has been answered by each new paper that includes a variation of this model: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We introduce ALiBi (Attention with Linear Biases), a simple method for allowing transformers to extrapolate to longer sequence lengths than the ones encountered during training. ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method allows training a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
  author        = {Ofir Press and Noah A. Smith and Mike Lewis},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  month         = {1},
  openalex      = {W4287019748},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=R8sQPpGCv0},
  publisher     = {OpenReview.net},
  title         = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  url           = {https://openreview.net/forum?id=R8sQPpGCv0},
  year          = {2022}
}

@inproceedings{Liu2023Ring,
  abstract      = {Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.},
  address       = {Red Hook, NY, USA},
  archiveprefix = {arXiv},
  author        = {Hao Liu and Matei Zaharia and Pieter Abbeel},
  booktitle     = {Advances in Neural Information Processing Systems},
  eprint        = {2310.01889},
  openalex      = {W4387356039},
  pages         = {1--16},
  pdf           = {https://openreview.net/pdf?id=fXugVDtCQO},
  publisher     = {Curran Associates, Inc.},
  title         = {Ring Attention with Blockwise Transformers for Near-Infinite Context},
  url           = {https://neurips.cc/virtual/2023/82807},
  volume        = {36},
  year          = {2023}
}

@misc{Sun2024Leave,
  abstract      = {This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.},
  archiveprefix = {arXiv},
  author        = {Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
  eprint        = {2404.07143},
  howpublished  = {arXiv preprint arXiv:2404.07143},
  month         = {4},
  openalex      = {W4394737116},
  pdf           = {https://arxiv.org/pdf/2404.07143.pdf},
  primaryclass  = {cs.CL},
  title         = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  year          = {2024}
}

@misc{Touvron2023LLaMA,
  abstract      = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurélien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  doi           = {10.48550/arXiv.2302.13971},
  eprint        = {2302.13971},
  howpublished  = {arXiv preprint arXiv:2302.13971},
  month         = {2},
  openalex      = {W4322718191},
  primaryclass  = {cs.CL},
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  url           = {https://arxiv.org/abs/2302.13971},
  year          = {2023}
}

@misc{Touvron2023Llama2,
  abstract      = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  author        = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  doi           = {10.48550/arXiv.2307.09288},
  eprint        = {2307.09288},
  howpublished  = {arXiv preprint arXiv:2307.09288},
  month         = {7},
  pdf           = {https://arxiv.org/pdf/2307.09288},
  primaryclass  = {cs.CL},
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  url           = {https://arxiv.org/abs/2307.09288},
  year          = {2023}
}

@misc{Meta2024Llama3,
  abstract      = {Meta introduces Llama 3, the next generation of their state-of-the-art open source large language model. Released in 8B and 70B parameter versions, Llama 3 was pre-trained on over 15 trillion tokens and demonstrates significant improvements in reasoning, coding, and instruction following compared to previous versions. The models are available across major cloud platforms and are designed with enhanced safety features including Llama Guard 2 and Code Shield.},
  author        = {AI at Meta},
  day           = {18},
  howpublished  = {Meta AI Blog},
  month         = {4},
  note          = {Official announcement of the Llama 3 language model family},
  title         = {Introducing Meta Llama 3: The most capable openly available LLM to date},
  url           = {https://ai.meta.com/blog/meta-llama-3/},
  year          = {2024}
}

@misc{BigScience2022BLOOM,
  abstract      = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. To democratize this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arXiv},
  author        = {BigScience Workshop},
  eprint        = {2211.05100},
  howpublished  = {arXiv preprint arXiv:2211.05100},
  month         = {11},
  note          = {BigScience collaborative project with 392 authors},
  openalex      = {W4311642023},
  pdf           = {https://arxiv.org/pdf/2211.05100.pdf},
  primaryclass  = {cs.CL},
  title         = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  url           = {https://arxiv.org/abs/2211.05100},
  year          = {2022}
}

@misc{Zhang2022OPT,
  abstract      = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  author        = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  doi           = {10.48550/arxiv.2205.01068},
  eprint        = {2205.01068},
  howpublished  = {arXiv preprint arXiv:2205.01068},
  month         = {5},
  openalex      = {W4229005866},
  pdf           = {https://arxiv.org/pdf/2205.01068},
  primaryclass  = {cs.CL},
  title         = {OPT: Open Pre-trained Transformer Language Models},
  url           = {https://arxiv.org/abs/2205.01068},
  year          = {2022}
}

@inproceedings{Zeng2023GLM130B,
  abstract      = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the pre-training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B/.},
  author        = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  openalex      = {W4303443398},
  pdf           = {https://openreview.net/pdf?id=-Aw0rrrPUF},
  title         = {GLM-130B: An Open Bilingual Pre-trained Model},
  url           = {https://openreview.net/forum?id=-Aw0rrrPUF},
  year          = {2023}
}

@misc{GemmaTeam2024Gemma,
  abstract      = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.},
  archiveprefix = {arXiv},
  author        = {Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and others},
  doi           = {10.48550/arXiv.2403.08295},
  eprint        = {2403.08295},
  howpublished  = {arXiv preprint},
  month         = {3},
  openalex      = {W4392822465},
  pdf           = {https://arxiv.org/pdf/2403.08295.pdf},
  primaryclass  = {cs.CL},
  title         = {Gemma: Open Models Based on Gemini Research and Technology},
  url           = {https://arxiv.org/abs/2403.08295},
  year          = {2024}
}

@misc{Abdin2024Phi3,
  abstract      = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
  archiveprefix = {arXiv},
  author        = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Parul Chopra and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Dan Iter and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Neelakantan Karuppan and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Chen Liang and Weishung Liu and Eric Lin and Zeqi Lin and Piyush Madan and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Xia Song and Masahiro Tanaka and Xin Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Michael Wyatt and Can Xu and Jiahang Xu and Sonali Yadav and Fan Yang and Ziyi Yang and Donghan Yu and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
  eprint        = {2404.14219},
  howpublished  = {arXiv preprint arXiv:2404.14219},
  month         = {4},
  openalex      = {W4395474395},
  pdf           = {https://arxiv.org/pdf/2404.14219.pdf},
  primaryclass  = {cs.CL},
  title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  url           = {https://arxiv.org/abs/2404.14219},
  year          = {2024}
}

@misc{Guo2024DeepSeekCoder,
  abstract      = {The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.},
  archiveprefix = {arXiv},
  author        = {Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
  doi           = {10.48550/arXiv.2401.14196},
  eprint        = {2401.14196},
  howpublished  = {arXiv preprint arXiv:2401.14196},
  month         = {1},
  openalex      = {W4391272793},
  pdf           = {https://arxiv.org/pdf/2401.14196.pdf},
  primaryclass  = {cs.SE},
  title         = {DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence},
  url           = {https://arxiv.org/abs/2401.14196},
  year          = {2024}
}

@misc{NVIDIA2024Nemotron,
  abstract      = {We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. We believe that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in our model alignment process.},
  archiveprefix = {arXiv},
  author        = {Jupinder Parmar and Shrimai Prabhumoye and Joseph Jennings and Deepak Narayanan and Mostofa Patwary and Dan Su and Chen Zhu and Aastha Jhunjhunwala and Ayush Dattagupta and Vibhu Jawa and Jiwei Liu and Ameya Sunil Mahabaleshwarkar and Sanjeev Satheesh and Osvald Nitski and Annika Brundyn and James Maki and Miguel Martinez and John Kamalu and Jiaxuan You and Patrick LeGresley and Denys Fridman and Tomasz Grzegorzek and Krzysztof Pawelec and Jared Casper and Ashwath Aithal and Mohammad Shoeybi and Bryan Catanzaro},
  eprint        = {2406.11704},
  howpublished  = {arXiv preprint arXiv:2406.11704},
  month         = {6},
  openalex      = {W4399795412},
  primaryclass  = {cs.CL},
  title         = {Nemotron-4 340B Technical Report},
  url           = {https://arxiv.org/abs/2406.11704},
  year          = {2024}
}

@misc{Zhao2023Survey,
  abstract      = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  author        = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  doi           = {10.48550/arxiv.2303.18223},
  eprint        = {2303.18223},
  howpublished  = {arXiv preprint arXiv:2303.18223},
  month         = {3},
  openalex      = {W4362515116},
  pdf           = {https://arxiv.org/pdf/2303.18223.pdf},
  primaryclass  = {cs.CL},
  title         = {A Survey of Large Language Models},
  url           = {https://arxiv.org/abs/2303.18223},
  year          = {2023}
}

@inproceedings{Biderman2023Pythia,
  abstract      = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend Pythia to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics.},
  author        = {Stella Biderman and Hailey Schoelkopf and Quentin Gregory Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and Usvsn Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  month         = {7},
  openalex      = {W4362655426},
  pages         = {2397--2430},
  pdf           = {https://proceedings.mlr.press/v202/biderman23a/biderman23a.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  url           = {https://proceedings.mlr.press/v202/biderman23a.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Xia2023Training,
  abstract      = {Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125M to 175B parameters—on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.},
  address       = {Toronto, Canada},
  author        = {Mengzhou Xia and Mikel Artetxe and Chunting Zhou and Xi Victoria Lin and Ramakanth Pasunuru and Danqi Chen and Luke Zettlemoyer and Veselin Stoyanov},
  booktitle     = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi           = {10.18653/v1/2023.acl-long.767},
  openalex      = {W4385570738},
  pages         = {13711--13738},
  pdf           = {https://aclanthology.org/2023.acl-long.767.pdf},
  publisher     = {Association for Computational Linguistics},
  title         = {Training Trajectories of Language Models Across Scales},
  url           = {https://aclanthology.org/2023.acl-long.767},
  year          = {2023}
}

@book{Tunstall2024Secrets,
  author        = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan},
  publisher     = {O'Reilly Media},
  title         = {Secrets of the LLM-training Industry},
  year          = {2024}
}

@inproceedings{Wei2022Chain,
  abstract      = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  author        = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4221143046},
  pdf           = {https://papers.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  series        = {NeurIPS},
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  volume        = {35},
  year          = {2022}
}

@inproceedings{Yao2023Tree,
  abstract      = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.},
  author        = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  booktitle     = {Advances in Neural Information Processing Systems},
  openalex      = {W4377130677},
  pages         = {11809--11822},
  pdf           = {https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  url           = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html},
  volume        = {36},
  year          = {2023}
}

@inproceedings{Wang2023SelfConsistency,
  abstract      = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. The key idea is to sample a diverse set of reasoning paths instead of only taking the greedy one, and then select the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with striking margins on popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).},
  author        = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  openalex      = {W4221161695},
  pdf           = {https://openreview.net/pdf?id=1PL1NIMMrw},
  title         = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  url           = {https://openreview.net/forum?id=1PL1NIMMrw},
  year          = {2023}
}

@inproceedings{Gao2023PAL,
  abstract      = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/.},
  author        = {Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
  booktitle     = {Proceedings of the 40th International Conference on Machine Learning},
  editor        = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  openalex      = {W4309591663},
  pages         = {10764--10799},
  pdf           = {https://proceedings.mlr.press/v202/gao23f/gao23f.pdf},
  publisher     = {PMLR},
  series        = {Proceedings of Machine Learning Research},
  title         = {PAL: Program-Aided Language Models},
  url           = {https://proceedings.mlr.press/v202/gao23f.html},
  volume        = {202},
  year          = {2023}
}

@inproceedings{Yao2023ReAct,
  abstract      = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.},
  author        = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R. Narasimhan and Yuan Cao},
  booktitle     = {The Eleventh International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4304195432},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=WE_vluYUL-X},
  publisher     = {OpenReview.net},
  title         = {ReAct: Synergizing Reasoning and Acting in Language Models},
  url           = {https://openreview.net/forum?id=WE_vluYUL-X},
  year          = {2023}
}

@inproceedings{Schick2023Toolformer,
  abstract      = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this work, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  author        = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomelí, María and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  booktitle     = {Advances in Neural Information Processing Systems},
  editor        = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  openalex      = {W4320165837},
  pages         = {68539--68551},
  pdf           = {https://papers.nips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf},
  publisher     = {Curran Associates, Inc.},
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  volume        = {36},
  year          = {2023}
}

@misc{Parisi2022Learning,
  abstract      = {Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative 'self-play' technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.},
  archiveprefix = {arXiv},
  author        = {Aaron Parisi and Yao Zhao and Noah Fiedel},
  eprint        = {2205.12255},
  howpublished  = {arXiv preprint arXiv:2205.12255},
  month         = {5},
  openalex      = {W4281557623},
  pdf           = {https://arxiv.org/pdf/2205.12255.pdf},
  primaryclass  = {cs.CL},
  title         = {TALM: Tool Augmented Language Models},
  year          = {2022}
}

@inproceedings{Feng2024When,
  author        = {Shuaiyi Feng and Aditi Raghunathan and Tengyu Ma and others},
  booktitle     = {Proceedings of the Twelfth International Conference on Learning Representations},
  note          = {Conference paper},
  publisher     = {OpenReview.net},
  series        = {ICLR '24},
  title         = {When Can Transformers Reason with Chains of Thought?},
  url           = {https://openreview.net},
  year          = {2024}
}

@inproceedings{Feng2024Expressive,
  abstract      = {Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a ``chain of thought'' or ``scratchpad'', i.e., generate and condition on a sequence of intermediate tokens before answering. We ask: does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We answer this question by analyzing the expressive power of chain-of-thought transformers. We first show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps push the limits of standard transformers only slightly, while a linear number of decoding steps, assuming projected pre-norm (a slight generalization of standard pre-norm), adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps with generalized pre-norm make them recognize exactly the class of polynomial-time solvable problems—the first exact characterization of a type of transformers in terms of standard complexity classes. Together, these results provide a nuanced framework for understanding how the length of a transformer's chain of thought or scratchpad impacts its reasoning power.},
  author        = {William Merrill and Ashish Sabharwal},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  openalex      = {W4387634967},
  pages         = {},
  pdf           = {https://openreview.net/pdf?id=CDmerQ37Zs},
  title         = {The Expressive Power of Transformers with Chain of Thought},
  url           = {https://openreview.net/forum?id=NjNGlPh8Wh},
  year          = {2024}
}

@inproceedings{Saxe2023Unreasonable,
  author        = {Saxe, A. and Nanda, N. and Advani, M. and others},
  booktitle     = {Advances in Neural Information Processing Systems (NeurIPS)},
  title         = {The Unreasonable Effectiveness of Easy Data for Hard Tasks},
  year          = {2023}
}

@inproceedings{Lee2024DirectToFact,
  author        = {Lee, S. and Choi, J. and Kim, J. and others},
  booktitle     = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title         = {Direct-to-Fact: A Framework for End-to-End Factual Generation},
  year          = {2024}
}

@inproceedings{Tay2023UL2,
  abstract      = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized & unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 & GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.},
  author        = {Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garcia and Jason Wei and Xuezhi Wang and Hyung Won Chung and Dara Bahri and Tal Schuster and Steven Zheng and Denny Zhou and Neil Houlsby and Donald Metzler},
  booktitle     = {Proceedings of the Eleventh International Conference on Learning Representations},
  month         = {5},
  openalex      = {W4304697829},
  pdf           = {https://openreview.net/pdf?id=6ruVLB727MC},
  publisher     = {OpenReview.net},
  title         = {UL2: Unifying Language Learning Paradigms},
  url           = {https://openreview.net/forum?id=6ruVLB727MC},
  year          = {2023}
}

@inproceedings{Lan2020ALBERT,
  abstract      = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.},
  author        = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  booktitle     = {International Conference on Learning Representations},
  month         = {4},
  openalex      = {W2996428491},
  pages         = {1--17},
  pdf           = {https://openreview.net/pdf?id=H1eA7AEtvS},
  publisher     = {OpenReview.net},
  title         = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  url           = {https://openreview.net/forum?id=H1eA7AEtvS},
  year          = {2020}
}

@inproceedings{He2021DeBERTa,
  abstract      = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
  author        = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  booktitle     = {Proceedings of the International Conference on Learning Representations},
  openalex      = {W3122890974},
  pdf           = {https://openreview.net/pdf?id=XPZIaotutsD},
  title         = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  url           = {https://openreview.net/forum?id=XPZIaotutsD},
  year          = {2021}
}

@article{Xue2022ByT5,
  abstract      = {Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.},
  author        = {Linting Xue and Aditya Barua and Noah Constant and Rami Al-Rfou and Sharan Narang and Mihir Kale and Adam Roberts and Colin Raffel},
  doi           = {10.1162/tacl_a_00461},
  journal       = {Transactions of the Association for Computational Linguistics},
  openalex      = {W3164045210},
  pages         = {291--306},
  pdf           = {https://aclanthology.org/2022.tacl-1.17.pdf},
  title         = {ByT5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models},
  url           = {https://aclanthology.org/2022.tacl-1.17/},
  volume        = {10},
  year          = {2022}
}

@misc{Phuong2022Formal,
  abstract      = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs. We start by describing the high-level picture on the roles of each component, and then we present a complete mathematical specification of the forward pass of each component. We conclude with a repetition of the overall algorithm and have a preview of the most prominent models. We include a generous number of redundant explanations in hopes that the algorithm is clear. This document does not survey or results.},
  archiveprefix = {arXiv},
  author        = {Phuong, Mary and Hutter, Marcus},
  doi           = {10.48550/arXiv.2207.09238},
  eprint        = {2207.09238},
  howpublished  = {arXiv preprint arXiv:2207.09238},
  month         = {7},
  openalex      = {W4286224767},
  pdf           = {https://arxiv.org/pdf/2207.09238.pdf},
  primaryclass  = {cs.LG},
  title         = {Formal Algorithms for Transformers},
  url           = {https://arxiv.org/abs/2207.09238},
  year          = {2022}
}

@misc{Rush2022AnnotatedS4,
  abstract      = {The Structured State Space for Sequence Modeling (S4) architecture is a new approach to very long-range sequence modeling tasks for vision, language, and audio, showing a capacity to capture dependencies over tens of thousands of steps. This blog post provides an annotated implementation linking concrete code with explanations from the S4 paper, very much in the style of the annotated transformer.},
  author        = {Sasha Rush and Sidd Karamcheti},
  day           = {25},
  howpublished  = {ICLR Blog Track},
  keywords      = {transformer, sequence-modeling, generative-models, state-space-models},
  month         = {3},
  note          = {Annotated implementation and explanation of the S4 model by Albert Gu, Karan Goel, and Christopher Ré},
  title         = {The Annotated S4: Efficiently Modeling Long Sequences with Structured State Spaces},
  url           = {https://iclr-blog-track.github.io/2022/03/25/annotated-s4/},
  year          = {2022}
}

@inproceedings{Bender2021Dangers,
  abstract      = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks?},
  address       = {Virtual Event, Canada},
  author        = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle     = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  doi           = {10.1145/3442188.3445922},
  month         = {3},
  openalex      = {W3133702157},
  pages         = {610--623},
  pdf           = {https://dl.acm.org/doi/pdf/10.1145/3442188.3445922},
  publisher     = {ACM},
  title         = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  url           = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  year          = {2021}
}
