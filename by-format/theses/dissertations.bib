@phdthesis{feynman1942principle,
  author = {Richard Phillips Feynman},
  title = {The Principle of Least Action in Quantum Mechanics},
  school = {Princeton University},
  year = {1942},
  type = {PhD thesis},
  advisor = {John Archibald Wheeler},
  month = {May},
  address = {Princeton, NJ, USA},
  note = {Foundational work on path integral formulation of quantum mechanics. Later republished by World Scientific (2005) as ``Feynman's Thesis: A New Approach to Quantum Theory''},
  keywords = {quantum mechanics, path integrals, least action principle, Wheeler-Feynman absorber theory},
  file = {:/home/b/documents/phdthesis/thesis-1942-feynman-least-action.pdf:pdf}
}

@phdthesis{ariosto2025,
  author = {Sebastiano Ariosto},
  title = {Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning},
  school = {Università degli Studi dell'Insubria},
  year = {2025},
  month = {January},
  address = {Varese, Italy},
  type = {{PhD} thesis},
  department = {Dipartimento di Scienza e Alta Tecnologia},
  url = {https://arxiv.org/abs/2501.19281},
  pdf = {https://arxiv.org/pdf/2501.19281.pdf},
  openalex = {W4407091726},
  file = {:/home/b/documents/phdthesis/thesis-2025-ariosto-statistical-physics-deep-neural-networks.pdf:pdf},
  note = {Available as arXiv preprint arXiv:2501.19281}
}

@phdthesis{mignacco2022,
  author = {Francesca Mignacco},
  title = {Statistical physics insights on the dynamics and generalisation of artificial neural networks},
  school = {Université Paris-Saclay},
  year = {2022},
  type = {PhD thesis},
  address = {Paris, France},
  advisor = {Lenka Zdeborová and Pierfrancesco Urbani},
  note = {NNT: 2022UPASP074},
  url = {https://theses.hal.science/tel-03848811v1},
  pdf = {https://theses.hal.science/tel-03848811v1/file/115507_MIGNACCO_2022_archivage.pdf},
  file = {:/home/b/documents/phdthesis/thesis-2022-mignacco-statistical-physics-artificial-neural-networks.pdf:pdf},
  hal = {tel-03848811},
  halversion = {v1},
  keywords = {Artificial neural networks, Disordered systems, Dynamics of learning, Stochastic gradient descent},
  abstract = {This thesis explores the mechanisms underlying learning in artificial neural networks through the prism of statistical physics. It addresses both static properties of learning problems and the dynamics of learning algorithms, examining prototype classification, multi-class classification, stochastic gradient descent dynamics, and the role of stochasticity in non-convex optimization.}
}

@phdthesis{cui2024,
  author = {Hugo Chao Cui},
  title = {Topics in statistical physics of high-dimensional machine learning},
  school = {École Polytechnique Fédérale de Lausanne},
  year = {2024},
  advisor = {Lenka Zdeborová},
  doi = {10.5075/epfl-thesis-10948},
  url = {https://infoscience.epfl.ch/handle/20.500.14299/208803},
  pdf = {https://infoscience.epfl.ch/record/311765/files/EPFL_TH10948.pdf},
  file = {:/home/b/documents/phdthesis/thesis-2024-cui-statistical-physics-high-dimensional-ml.pdf:pdf},
  note = {EPFL Physics doctoral thesis award 1st prize; EPFL best 8\% PhD distinction in Physics},
  address = {Lausanne, Switzerland},
  pages = {xvi + 190},
  type = {PhD thesis},
  isbn = {9782832213609},
  keywords = {statistical physics, machine learning, high-dimensional learning, feature extraction, neural networks, kernel methods, Bayesian learning}
}

@phdthesis{marion2023,
  author = {Pierre Marion},
  title = {Mathematics of deep learning: generalization, optimization, continuous-time models},
  school = {Sorbonne Université},
  year = {2023},
  type = {PhD thesis},
  address = {Paris, France},
  month = {November},
  note = {NNT: 2023SORUS517},
  url = {https://theses.hal.science/tel-04453458},
  pdf = {https://theses.hal.science/tel-04453458v1/file/MARION_Pierre_these_2023.pdf},
  file = {:/home/b/documents/phdthesis/thesis-2023-marion-mathematics-deep-learning.pdf:pdf},
  openalex = {W4392673599},
  hal = {tel-04453458},
  keywords = {deep learning, neural networks, generalization bounds, optimization, continuous-time models, residual networks, transformers}
}

@phdthesis{baskerville2023,
  author = {Nicholas P. Baskerville},
  title = {Random matrix theory and the loss surfaces of neural networks},
  school = {University of Bristol},
  year = {2023},
  month = {June},
  type = {PhD thesis},
  pages = {320},
  supervisor = {J. Najnudel and F. Mezzadri and J. P. Keating},
  url = {https://arxiv.org/abs/2306.02108},
  doi = {10.48550/arXiv.2306.02108},
  eprint = {2306.02108},
  archivePrefix = {arXiv},
  primaryClass = {math-ph},
  file = {:/home/b/documents/phdthesis/thesis-2023-baskerville-random-matrix-theory-neural-networks.pdf:pdf},
  note = {arXiv:2306.02108},
  keywords = {random matrix theory, neural networks, loss surfaces, mathematical physics, machine learning},
  abstract = {Neural network models are one of the most successful approaches to machine learning, yet the theoretical understanding of neural networks trails significantly behind their practical success. Random matrix theory provides a rich framework of tools with which aspects of neural network phenomenology can be explored theoretically. This thesis establishes significant extensions of prior work using random matrix theory to understand and describe the loss surfaces of large neural networks, particularly generalising to different architectures.}
}

@phdthesis{canatar2022,
  author = {Abdulkadir Canatar},
  title = {Statistical Mechanics of Generalization in Kernel Regression and Wide Neural Networks},
  school = {Harvard University},
  year = {2022},
  month = {September},
  type = {Ph.D. dissertation},
  address = {Cambridge, MA, USA},
  advisor = {Cengiz Pehlevan},
  url = {https://dash.harvard.edu/handle/1/37373673},
  file = {:/home/b/documents/phdthesis/thesis-2022-canatar-statistical-mechanics-generalization.pdf:pdf},
  keywords = {Generalization Error, Kernel Regression, Replica Theory, Artificial intelligence, Statistical physics},
  abstract = {A theoretical exploration of generalization in machine learning models, focusing on kernel regression and wide neural networks using statistical mechanics' replica theory. The work develops an analytical theory applicable to any kernel and data distribution, identifying inductive bias towards simple functions and phase transitions in generalization performance.}
}

@phdthesis{zhong2023,
  author = {Weishun Zhong},
  title = {Non-equilibrium physics: from spin glasses to machine and neural learning},
  school = {Massachusetts Institute of Technology},
  year = {2023},
  month = {June},
  address = {Cambridge, MA},
  type = {Ph.D. thesis},
  department = {Department of Physics},
  advisor = {Haim Sompolinsky and Mehran Kardar},
  url = {https://hdl.handle.net/1721.1/152568},
  pdf = {https://arxiv.org/pdf/2308.01538.pdf},
  file = {:/home/b/documents/phdthesis/thesis-2023-zhong-nonequilibrium-physics-spin-glasses-ml.pdf:pdf},
  openalex = {W4385964745},
  note = {Also available as arXiv:2308.01538}
}

@phdthesis{chung2017,
  author = {SueYeon Chung},
  title = {Statistical Mechanics of Neural Processing of Object Manifolds},
  school = {Harvard University},
  year = {2017},
  month = {May},
  type = {Ph.D. dissertation},
  url = {http://nrs.harvard.edu/urn-3:HUL.InstRepos:41141361},
  pdf = {https://dash.harvard.edu/bitstream/handle/1/41141361/CHUNG-DISSERTATION-2017.pdf?sequence=1&isAllowed=y},
  file = {:/home/b/documents/phdthesis/thesis-2017-chung-statistical-mechanics-neural-processing.pdf:pdf},
  note = {Graduate School of Arts and Sciences},
  advisor = {Haim Sompolinsky and Ryan P. Adams},
  address = {Cambridge, MA},
  keywords = {Replica Theory, Perceptron, Support Vector Machines, Object Manifolds, Perceptual Invariance, Object Recognition, Statistical Mechanics}
}

@phdthesis{zavatoneveth2023,
  author = {Jacob Andreas Zavatone-Veth},
  title = {Statistical mechanics of {Bayesian} inference and learning in neural networks},
  school = {Harvard University Graduate School of Arts and Sciences},
  year = {2024},
  month = {April},
  address = {Cambridge, MA, USA},
  url = {https://dash.harvard.edu/entities/publication/081c6cc0-6ae2-4066-8618-bd19ebc24293},
  pdf = {https://dash.harvard.edu/bitstreams/3c4d5429-39e9-4bb7-90ee-17dd110a3499/download},
  keywords = {Deep learning, Random matrices, Theoretical neuroscience, Theoretical physics, Neurosciences, Statistical physics},
  abstract = {This thesis collects a few of my essays towards understanding representation learning and generalization in neural networks. I focus on the model setting of Bayesian learning and inference, where the problem of deep learning is naturally viewed through the lens of statistical mechanics.},
  note = {PhD thesis defended April 4, 2024. Recipient of 2024 American Physical Society Dissertation Award in Statistical and Nonlinear Physics. Permanent URI: https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37378715},
    file = {:/home/b/documents/phdthesis/thesis-2024-zavatoneveth-statistical-mechanics-bayesian-inference.pdf:pdf},
}

@phdthesis{saxe2015,
  author = {Andrew Michael Saxe},
  title = {Deep Linear Neural Networks: A Theory of Learning in the Brain and Mind},
  school = {Stanford University},
  year = {2015},
  type = {Ph.D. thesis},
  address = {Stanford, CA, USA},
  advisor = {James L. McClelland and Andrew Y. Ng and Surya Ganguli and Christoph E. Schreiner},
  department = {Department of Electrical Engineering},
  pdf = {https://stacks.stanford.edu/file/druid:nv482qj2831/Thesis-augmented.pdf},
  file = {:/home/b/documents/phdthesis/thesis-2015-saxe-deep-linear-neural-networks.pdf:pdf},
  note = {Robert J. Glushko Dissertation Prize recipient},
  month = {June},
  url = {https://purl.stanford.edu/nv482qj2831}
}

@phdthesis{pezzicoli2024,
  author = {Francesco Saverio Pezzicoli},
  title = {Statistical Physics - Machine Learning Interplay: From Addressing Class Imbalance with Replica Theory to Predicting Dynamical Heterogeneities with {SE}(3)-Equivariant Graph Neural Networks},
  school = {Université Paris-Saclay},
  year = {2024},
  type = {PhD thesis},
  address = {Paris, France},
  month = {January},
  note = {Supervised by Guillaume Charpiat and François Pascal Landes},
  url = {https://theses.hal.science/tel-04910839v1},
  pdf = {https://theses.hal.science/tel-04910839v1/file/142018_PEZZICOLI_2024_archivage.pdf},
  nnt = {2024UPASG115},
  hal = {tel-04910839},
  halversion = {v1},
  openalex = {W4406880525},
  subject = {Machine Learning [cs.LG]},
  keywords = {Statistical Physics, Machine Learning, Replica Theory, Class Imbalance, Graph Neural Networks, SE(3)-Equivariant, Dynamical Heterogeneities, Glassy Liquids},
    file = {:/home/b/documents/phdthesis/thesis-2024-pezzicoli-statistical-physics-ml-interplay.pdf:pdf},
}

@phdthesis{zdeborova2008,
  author = {Lenka Zdeborová},
  title = {Statistical Physics of Hard Optimization Problems},
  school = {Université Paris-Sud and Univerzita Karlova v Praze},
  year = {2008},
  month = {May},
  address = {Paris, France and Prague, Czech Republic},
  type = {PhD thesis},
  note = {Joint PhD degree (cotutelle)},
  supervisor = {Marc Mézard and Václav Janiš},
  nnt = {2008PA112080},
  hal_id = {tel-00294232},
  url = {https://theses.hal.science/tel-00294232},
  pdf = {https://theses.hal.science/tel-00294232/file/lenicka.pdf},
  arxiv = {0806.4112},
  keywords = {optimization problems, computational complexity, cavity method, spin glasses, phase transitions},
  abstract = {This thesis addresses the question: How to recognize if an NP-complete constraint satisfaction problem is typically hard and what are the main reasons for this? The research adopts approaches from the statistical physics of disordered systems, in particular the cavity method developed originally to describe glassy systems.},
  file = {:/home/b/documents/phdthesis/thesis-2008-zdeborova-statistical-physics-hard-optimization.pdf:pdf}
}

@phdthesis{loureiro2018,
  author = {Bruno Loureiro},
  title = {Disorder in holographic field theories: inhomogeneous geometries, momentum relaxation and {SYK} models},
  school = {University of Cambridge},
  year = {2019},
  month = {April},
  address = {Cambridge, UK},
  type = {PhD thesis},
  department = {Department of Applied Mathematics and Theoretical Physics},
  advisor = {Antonio M. Garcia-Garcia},
  url = {https://www.repository.cam.ac.uk/handle/1810/277911},
  doi = {10.17863/CAM.25246},
  openalex = {W2887505930},
  abstract = {This thesis investigates holographic dualities as tools for studying universal properties of strongly coupled field theories, with focus on theories without translational symmetry through three new approaches. Covers phenomenological holographic models achieving momentum relaxation, holographic theories that explicitly break translational symmetry, and studies of spatially varying random Maxwell potentials driving dual field theory to non-trivial infra-red fixed points with emerging scale invariance.},
  keywords = {holographic duality, AdS/CFT correspondence, disorder, momentum relaxation, SYK model, condensed matter theory},
  pages = {1--180},
  file = {:/home/b/documents/phdthesis/thesis-2018-loureiro-disorder-holographic-field-theories.pdf:pdf},
  note = {Thesis focuses on applications of holographic principle to strongly coupled condensed matter systems with particular focus on disordered systems}
}

@phdthesis{dunmur1994,
  author = {Alan P. Dunmur},
  title = {Statistical mechanics, generalisation and regularisation of neural network models},
  school = {University of Edinburgh},
  year = {1994},
  type = {PhD thesis},
  url = {https://era.ed.ac.uk/handle/1842/13743},
  file = {:/home/b/documents/phdthesis/thesis-1994-dunmur-statistical-mechanics-neural-networks.pdf:pdf},
  note = {Available from Edinburgh Research Archive; PDF download possible.},
  openalex = {W2302528735},
  abstract = {Examines the performance of a simple neural network model learning a rule from noisy examples using methods of statistical mechanics. The free energy for the model is defined and order parameters that capture the statistical behaviour of the system are evaluated analytically. A weight decay term is used to regularise the effect of the noise added to the examples.},
  keywords = {statistical mechanics, neural networks, regularisation, generalisation, weight decay, noisy data}
}

@phdthesis{whyte1995,
  author = {William John Whyte},
  title = {Statistical mechanics of neural networks},
  school = {University of Oxford},
  year = {1995},
  type = {DPhil thesis},
  url = {https://ora.ox.ac.uk/objects/uuid:e17f9b27-58ac-41ad-8722-cfab75139d9a},
  note = {EThOS thesis ID: uk.bl.ethos.297261},
  abstract = {This thesis investigates five problems in the statistical mechanics of neural networks. The first three problems involve attractor neural networks that optimize particular cost functions for storage of static memories as attractors of the neural dynamics. Research areas include effects of replica symmetry breaking, noise-optimal networks, perceptron algorithm performance, sequence storage and processing, and impact of correlations on network behavior.},
  keywords = {Neural networks, Statistical mechanics, Attractor networks, Replica symmetry breaking},
    file = {:/home/b/documents/phdthesis/thesis-1995-whyte-statistical-mechanics-neural-networks.pdf:pdf},
}