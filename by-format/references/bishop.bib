@techreport{Bubeck2023,
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  institution = {Microsoft Research},
  month = {3},
  number = {2303.12712},
  openalex = {W4360836968},
  pdf = {https://arxiv.org/pdf/2303.12712.pdf},
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  url = {https://arxiv.org/abs/2303.12712},
  year = {2023}
}

@techreport{Hu2023,
  abstract = {Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves. To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.},
  author = {Anthony Hu and Lloyd Russell and Hudson Yeo and Zak Murez and George Fedoseev and Alex Kendall and Jamie Shotton and Gianluca Corrado},
  eprint = {2309.17080},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {9},
  number = {2309.17080},
  openalex = {W4387294122},
  pdf = {https://arxiv.org/pdf/2309.17080.pdf},
  title = {GAIA-1: A Generative World Model for Autonomous Driving},
  url = {https://arxiv.org/abs/2309.17080},
  year = {2023}
}

@inproceedings{Lipman2022,
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matthew},
  booktitle = {The Eleventh International Conference on Learning Representations},
  eprint = {2210.02747},
  eprinttype = {arXiv},
  pdf = {https://openreview.net/pdf?id=PqvMRDCJT9t},
  title = {Flow Matching for Generative Modeling},
  url = {https://openreview.net/forum?id=PqvMRDCJT9t},
  year = {2023}
}

@book{Murphy2023,
  abstract = {An advanced counterpart to Probabilistic Machine Learning: An Introduction, providing researchers and graduate students detailed coverage of cutting-edge topics in machine learning, including deep generative modeling, graphical models, Bayesian inference, reinforcement learning, and causality. The volume puts deep learning into a larger statistical context and unifies approaches based on deep learning with ones based on probabilistic modeling and inference. With contributions from top scientists and domain experts, this rigorous book covers generation of high dimensional outputs, methods for discovering insights about data based on latent variable models, training and testing under different distributions, and how to use probabilistic models and inference for causal inference and decision making.},
  author = {Murphy, Kevin P.},
  edition = {First},
  isbn = {978-0-262-04843-9},
  month = {8},
  openalex = {W4313400118},
  pages = {1360},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Probabilistic Machine Learning: Advanced Topics},
  url = {https://probml.github.io/book2},
  year = {2023}
}

@techreport{OpenAI2023,
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  author = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and others},
  eprint = {2303.08774},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {3},
  number = {2303.08774},
  pdf = {https://arxiv.org/pdf/2303.08774},
  title = {GPT-4 Technical Report},
  url = {https://arxiv.org/abs/2303.08774},
  year = {2023}
}

@book{Prince2023,
  abstract = {An authoritative, accessible, and up-to-date treatment of deep learning that strikes a pragmatic middle ground between theory and practice. Covers cutting-edge topics like transformers and diffusion models with minimal mathematical prerequisites and includes programming exercises in Python notebooks.},
  address = {Cambridge, MA},
  author = {Prince, Simon J.D.},
  edition = {First},
  isbn = {978-0-262-04864-4},
  month = {12},
  note = {http://udlbook.com},
  pages = {544},
  pdf = {https://udlbook.github.io/udlbook/},
  publisher = {MIT Press},
  title = {Understanding Deep Learning},
  year = {2023}
}

@article{Saharia2021b,
  abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
  author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  doi = {10.1109/TPAMI.2022.3204461},
  eprint = {2104.07636},
  eprinttype = {arxiv},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {4},
  number = {4},
  pages = {4713--4726},
  title = {Image Super-Resolution via Iterative Refinement},
  url = {https://ieeexplore.ieee.org/document/9887996/},
  volume = {45},
  year = {2023}
}

@article{Velickovic2023,
  abstract = {Graphs are the main modality of data we receive from nature: most of the patterns we see, both in natural and artificial systems, are elegantly representable using the language of graph structures. Prominent examples include molecules (represented as graphs of atoms and bonds), social networks and transportation networks. This potential has already been recognised by key scientific and industrial groups, with already-impacted application areas including traffic forecasting, drug discovery, social network analysis and recommender systems. On the flip side, some of the most successful domains of application for machine learning in previous years -- images, text and speech processing -- can be seen as special cases of graph representation learning. The main aim of this short survey is to enable the reader to assimilate the key concepts in the area, and position graph representation learning in a proper context with related fields.},
  author = {Veli0̆10dkovi\1̆07, Petar},
  doi = {10.1016/j.sbi.2023.102538},
  eprint = {2301.08210},
  eprinttype = {arxiv},
  journal = {Current Opinion in Structural Biology},
  month = {4},
  pages = {102538},
  pdf = {https://arxiv.org/pdf/2301.08210.pdf},
  title = {Everything is Connected: Graph Neural Networks},
  volume = {79},
  year = {2023}
}

@article{Wang2023,
  abstract = {We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scaled up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find VALL-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.},
  author = {Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and He, Lei and Zhao, Sheng and Wei, Furu},
  doi = {10.1109/TASLP.2023.3334194},
  eprint = {2301.02111},
  eprinttype = {arxiv},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  pages = {705--718},
  pdf = {https://arxiv.org/pdf/2301.02111},
  publisher = {IEEE},
  title = {Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers},
  url = {https://ieeexplore.ieee.org/document/10834251},
  volume = {33},
  year = {2023}
}

@book{Winn2023,
  author = {Winn, John and Bishop, Christopher M. and Diethe, Tom and Guiver, John and Zaykov, Yordan},
  note = {Www.mbmlbook.com},
  publisher = {Chapman / Hall},
  title = {Model-Based Machine Learning},
  year = {2023}
}

@article{Yang2022,
  abstract = {Denoising diffusion probabilistic models are a promising new class of generative models that mark a milestone in high-quality image generation. This paper showcases their ability to sequentially generate video, surpassing prior methods in perceptual and probabilistic forecasting metrics. We propose an autoregressive, end-to-end optimized video diffusion model inspired by recent advances in neural video compression. The model successively generates future frames by correcting a deterministic next-frame prediction using a stochastic residual generated by an inverse diffusion process. We compare our approach against five baselines on four datasets involving natural and simulation-based videos, finding significant improvements in terms of perceptual quality for all datasets. By introducing a scalable version of the Continuous Ranked Probability Score (CRPS) applicable to video, we additionally show that our model outperforms existing approaches in their probabilistic frame forecasting.},
  author = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  doi = {10.3390/e25101469},
  eprint = {2203.09481},
  eprinttype = {arxiv},
  issn = {1099-4300},
  journal = {Entropy},
  keywords = {video generation; diffusion models; probabilistic modeling; neural compression; autoregressive models},
  note = {Open Access},
  number = {10},
  pages = {1469},
  pdf = {https://www.mdpi.com/1099-4300/25/10/1469/pdf},
  publisher = {MDPI},
  title = {Diffusion Probabilistic Modeling for Video Generation},
  url = {https://www.mdpi.com/1099-4300/25/10/1469},
  volume = {25},
  year = {2023}
}

@techreport{Yu2023,
  abstract = {We present CM3Leon (pronounced ``Chameleon''), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective: CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.},
  author = {Yu, Lili and Shi, Bowen and Pasunuru, Ramakanth and Muller, Benjamin and Golovneva, Olga and Wang, Tianlu and Babu, Arun and Tang, Binh and Karrer, Brian and Sheynin, Shelly and Ross, Candace and Polyak, Adam and Howes, Russell and Sharma, Vasu and Xu, Puxin and Tamoyan, Hovhannes and Ashual, Oron and Singer, Uriel and Li, Shang-Wen and Zhang, Susan and James, Richard and Ghosh, Gargi and Taigman, Yaniv and Fazel-Zarandi, Maryam and Celikyilmaz, Asli and Zettlemoyer, Luke and Aghajanyan, Armen},
  doi = {10.48550/arXiv.2309.02591},
  eprint = {2309.02591},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {9},
  note = {Presents CM3Leon, a multi-modal language model achieving state-of-the-art text-to-image generation performance},
  number = {2309.02591},
  pdf = {https://arxiv.org/pdf/2309.02591.pdf},
  title = {Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning},
  url = {https://arxiv.org/abs/2309.02591},
  year = {2023}
}

@techreport{Zhao2023,
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLMs) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been very rapid, and in the pursuit of the most recent progress, many important findings and techniques are recorded in grey literature, like arXiv papers. The goal of this survey is to review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training (how to pre-train a capable LLM), adaptation tuning (how to effectively adapt pre-trained LLMs for better usability), utilization (how to use LLMs for solving various downstream tasks), and capacity evaluation (how to evaluate the abilities of LLMs and existing empirical findings). Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  author = {Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
  doi = {10.48550/arXiv.2303.18223},
  eprint = {2303.18223},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {3},
  note = {Comprehensive survey on large language models covering pre-training, adaptation, utilization, and evaluation},
  number = {2303.18223},
  pdf = {https://arxiv.org/pdf/2303.18223.pdf},
  title = {A Survey of Large Language Models},
  url = {https://arxiv.org/abs/2303.18223},
  year = {2023}
}

@techreport{Aghajanyan2022,
  abstract = {We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked language-image models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multi-modal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM. We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model.},
  author = {Armen Aghajanyan and Bernie Huang and Candace Ross and Vladimir Karpukhin and Hu Xu and Naman Goyal and Dmytro Okhonko and Mandar Joshi and Gargi Ghosh and Michael Lewis and Luke Zettlemoyer},
  doi = {10.48550/arxiv.2201.07520},
  eprint = {2201.07520},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {1},
  number = {2201.07520},
  openalex = {W4226279206},
  pdf = {http://arxiv.org/pdf/2201.07520},
  title = {CM3: A Causal Masked Multimodal Model of the Internet},
  url = {https://arxiv.org/abs/2201.07520},
  year = {2022}
}

@inproceedings{Alayrac2022,
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation, exploring and measuring desirable properties such as ability to rapidly adapt to new tasks and compute efficiency. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\ŉkowski, Mikołaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karén},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {2204.14198},
  eprinttype = {arxiv},
  openalex = {W4225323055},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  url = {https://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@article{Dufter2021,
  abstract = {Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reordering of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this article, we provide an overview and theoretical comparison of existing methods to incorporate position information into Transformer models. We (1) showcase that position information in Transformer is a vibrant and extensive research area; (2) enable the reader to compare existing methods by providing a unified notation and systematization of different approaches along important model dimensions; (3) indicate what characteristics of an application should be taken into account when selecting a position encoding; (4) provide stimuli for future research.},
  address = {Cambridge, MA},
  author = {Dufter, Philipp and Schmitt, Martin and Schütze, Hinrich},
  doi = {10.1162/coli_a_00445},
  eprint = {2102.11090},
  eprinttype = {arxiv},
  journal = {Computational Linguistics},
  number = {3},
  openalex = {W3132607382},
  pages = {733--763},
  pdf = {https://arxiv.org/pdf/2102.11090},
  publisher = {MIT Press},
  title = {Position Information in Transformers: An Overview},
  url = {https://aclanthology.org/2022.cl-3.7/},
  volume = {48},
  year = {2022}
}

@inproceedings{He2021,
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  address = {New Orleans, LA, USA},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross B.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR52688.2022.01553},
  eprint = {2111.06377},
  eprinttype = {arxiv},
  month = {6},
  openalex = {W4313156423},
  pages = {16000--16009},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  title = {Masked Autoencoders Are Scalable Vision Learners},
  year = {2022}
}

@article{Ho2021,
  abstract = {We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Conditioning augmentation prevents compounding error during sampling in a cascaded model, helping our trained cascading pipelines achieve FID scores of 1.48 at 64×64, 3.52 at 128×128 and 4.88 at 256×256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256×256, outperforming VQ-VAE-2.},
  author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J. and Norouzi, Mohammad and Salimans, Tim},
  eprint = {2106.15282},
  eprinttype = {arxiv},
  journal = {Journal of Machine Learning Research},
  number = {47},
  pages = {1--33},
  pdf = {https://www.jmlr.org/papers/volume23/21-0635/21-0635.pdf},
  title = {Cascaded Diffusion Models for High Fidelity Image Generation},
  url = {http://jmlr.org/papers/v23/21-0635.html},
  volume = {23},
  year = {2022}
}

@article{Hospedales2021,
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  author = {Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
  doi = {10.1109/TPAMI.2021.3079209},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9},
  openalex = {W3163842339},
  pages = {5149--5169},
  pdf = {http://arxiv.org/pdf/2004.05439},
  title = {Meta-Learning in Neural Networks: A Survey},
  volume = {44},
  year = {2022}
}

@inproceedings{Hu2021,
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle = {The Tenth International Conference on Learning Representations},
  doi = {10.48550/arxiv.2106.09685},
  eprint = {2106.09685},
  openalex = {W3168867926},
  pdf = {https://openreview.net/pdf?id=nZeVKeeFYf9},
  primaryclass = {cs.CL},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  url = {https://openreview.net/forum?id=nZeVKeeFYf9},
  year = {2022}
}

@inproceedings{Li2022,
  abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1 on COCO), image captioning (+2.8% in CIDEr on COCO), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.},
  author = {Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  month = {7},
  openalex = {W4226182655},
  pages = {12888--12900},
  pdf = {https://proceedings.mlr.press/v162/li22n/li22n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  volume = {162},
  year = {2022}
}

@article{Lin2021,
  abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, lots of X-formers (i.e. Transformer variants) have been proposed, yet a systematic and comprehensive literature review on the topic is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  doi = {10.1016/j.aiopen.2022.10.001},
  eprint = {2106.04554},
  eprinttype = {arxiv},
  issn = {2666-6510},
  journal = {AI Open},
  keywords = {Transformer, Self-attention, Pre-trained models, Deep learning},
  openalex = {W4306955484},
  pages = {111--132},
  pdf = {https://arxiv.org/pdf/2106.04554.pdf},
  publisher = {KeAi Communications Co. Ltd.},
  title = {A Survey of Transformers},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651022000146},
  volume = {3},
  year = {2022}
}

@techreport{Luo2022,
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work, we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn conditional distributions using diffusion models, which unlocks powerful capabilities such as text-conditioned image generation, image inpainting, and more.},
  author = {Calvin Luo},
  eprint = {2208.11970},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {8},
  number = {2208.11970},
  pdf = {https://arxiv.org/pdf/2208.11970.pdf},
  title = {Understanding Diffusion Models: A Unified Perspective},
  url = {https://arxiv.org/abs/2208.11970},
  year = {2022}
}

@book{Murphy2022,
  abstract = {A detailed and up-to-date introduction to machine learning (including deep learning) through the unifying lens of probabilistic modeling and Bayesian decision theory. The book covers mathematical background (including linear algebra and optimization), basic supervised learning (including linear and logistic regression and deep neural networks), as well as more advanced topics (including transfer learning and unsupervised learning). This book grew out of the author's 2012 book Machine Learning: A Probabilistic Perspective but is a completely new book that reflects the dramatic developments in the field since 2012, most notably deep learning.},
  author = {Murphy, Kevin P.},
  isbn = {9780262046824},
  note = {probml.ai},
  pages = {864},
  pdf = {https://github.com/probml/pml-book/releases/latest/download/book1.pdf},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Probabilistic Machine Learning: An Introduction},
  url = {http://probml.github.io/book1},
  year = {2022}
}

@inproceedings{Nichol2021b,
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  author = {Alexander Quinn Nichol and Prafulla Dhariwal and Aditya Ramesh and Pranav Shyam and Pamela Mishkin and Bob McGrew and Ilya Sutskever and Mark Chen},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  doi = {10.48550/arXiv.2112.10741},
  editor = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  openalex = {W4291263442},
  pages = {16784--16804},
  pdf = {https://proceedings.mlr.press/v162/nichol22a/nichol22a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  url = {https://proceedings.mlr.press/v162/nichol22a.html},
  volume = {162},
  year = {2022}
}

@techreport{Phuong2022,
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  author = {Phuong, Mary and Hutter, Marcus},
  doi = {10.48550/arxiv.2207.09238},
  eprint = {2207.09238},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {7},
  number = {2207.09238},
  openalex = {W4286224767},
  pdf = {https://arxiv.org/pdf/2207.09238},
  title = {Formal Algorithms for Transformers},
  url = {https://arxiv.org/abs/2207.09238},
  year = {2022}
}

@inproceedings{Rombach2021,
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  month = {6},
  pages = {10684--10695},
  pdf = {https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf},
  publisher = {IEEE},
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
  year = {2022}
}

@inproceedings{Saharia2021a,
  abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  address = {New York, NY, USA},
  author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  booktitle = {Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings (SIGGRAPH '22)},
  doi = {10.1145/3528233.3530757},
  isbn = {978-1-4503-9337-9},
  month = {8},
  openalex = {W3212516020},
  pages = {1--10},
  pdf = {https://arxiv.org/pdf/2111.05826.pdf},
  publisher = {ACM},
  title = {Palette: Image-to-Image Diffusion Models},
  url = {https://dl.acm.org/doi/10.1145/3528233.3530757},
  year = {2022}
}

@inproceedings{Saharia2022,
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.},
  author = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and S. Sara Mahdavi and Rapha Gontijo Lopes and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {2205.11487},
  eprinttype = {arxiv},
  pages = {36684--36695},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html},
  volume = {35},
  year = {2022}
}

@book{Szeliski2022,
  abstract = {This textbook explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both in specialized applications such as medical imaging and autonomous navigation, as well as for fun, consumer-level tasks that students can apply to their own personal photos and videos. More than just a source of ``recipes,'' this exceptionally authoritative and comprehensive textbook/reference takes a scientific approach to the formulation of computer vision problems. These problems are then analyzed using the latest classical and deep learning models and solved using rigorous engineering principles. The book includes exercises at the end of each chapter with emphasis on testing algorithms and containing numerous suggestions for small mid-term projects, provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory, and suggests additional reading at the end of each chapter, including the latest research in each sub-field.},
  address = {Cham},
  author = {Richard Szeliski},
  doi = {10.1007/978-3-030-34372-9},
  edition = {Second},
  isbn = {978-3-030-34371-2},
  pages = {xxiv + 925},
  publisher = {Springer},
  series = {Texts in Computer Science},
  title = {Computer Vision: Algorithms and Applications},
  url = {https://szeliski.org/Book/},
  year = {2022}
}

@article{Tay2020,
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of providing an organized and comprehensive overview, we characterize a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains. To supplement this, we propose a simple taxonomy categorized along three axes: where the efficiency is obtained (attention-related, input-related, or other), how the efficiency is obtained (sparsity, decomposition, or other), and what type of efficiency is obtained (time or space). We conclude with synthesized lessons and comments.},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  doi = {10.1145/3530811},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  journal = {ACM Computing Surveys},
  month = {12},
  number = {6},
  pages = {109:1--109:28},
  pdf = {https://dl.acm.org/doi/pdf/10.1145/3530811},
  title = {Efficient Transformers: A Survey},
  url = {https://dl.acm.org/doi/10.1145/3530811},
  volume = {55},
  year = {2022}
}

@inproceedings{Yilmaz2022,
  abstract = {The risk of overparameterized models, in particular deep neural networks, is often double-descent shaped as a function of the model size. Recently, it was shown that the risk as a function of the early-stopping time can also be double-descent shaped, and this behavior can be explained as a superposition of bias-variance tradeoffs. In this paper, we show that the risk of explicit L2-regularized models can exhibit double descent behavior as a function of the regularization strength, both in theory and practice. For linear regression, a double descent shaped risk is caused by a superposition of bias-variance tradeoffs corresponding to different parts of the model and can be mitigated by scaling the regularization strength of each part appropriately. Motivated by this result, we study a two-layer neural network and show that double descent can be eliminated by adjusting the regularization strengths for the first and second layer.},
  address = {Espoo, Finland},
  author = {Fatih Furkan Yılmaz and Reinhard Heckel},
  booktitle = {2022 IEEE International Symposium on Information Theory (ISIT)},
  doi = {10.1109/ISIT50566.2022.9834569},
  eprint = {2206.01378},
  eprinttype = {arXiv},
  month = {6},
  openalex = {W4289655249},
  pages = {426--431},
  pdf = {https://ieeexplore.ieee.org/document/9834569},
  publisher = {IEEE},
  title = {Regularization-wise double descent: Why it occurs and how to eliminate it},
  url = {https://doi.org/10.1109/ISIT50566.2022.9834569},
  year = {2022}
}

@inproceedings{Yu2021,
  abstract = {Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at 256×256 resolution, we achieve Inception Score (IS) of 175.1 and Fréchet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.},
  author = {Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
  booktitle = {International Conference on Learning Representations (ICLR)},
  doi = {10.48550/arXiv.2110.04627},
  eprint = {2110.04627},
  eprinttype = {arxiv},
  openalex = {W3202761878},
  pdf = {https://arxiv.org/pdf/2110.04627.pdf},
  title = {Vector-quantized Image Modeling with Improved VQGAN},
  url = {https://openreview.net/forum?id=pfNyExj7z2},
  year = {2022}
}

@article{Yu2022,
  abstract = {Pathways Autoregressive Text-to-Image (Parti) model generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation -- but instead of transforming a sequence of source language tokens to target language tokens, it transforms sequences of image tokens to sequences of text tokens, or vice versa. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives shows the effectiveness of joint optimization of text and image representations.},
  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  doi = {10.48550/arXiv.2206.10789},
  eprint = {2206.10789},
  eprinttype = {arxiv},
  journal = {Transactions on Machine Learning Research},
  keywords = {text-to-image generation, autoregressive models, transformer, pathways, parti},
  month = {6},
  title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  url = {https://arxiv.org/abs/2206.10789},
  year = {2022}
}

@inproceedings{Aghajanyan2020,
  abstract = {Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.},
  address = {Online},
  author = {Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  doi = {10.18653/v1/2021.acl-long.568},
  month = {8},
  openalex = {W3177323791},
  pages = {7319--7328},
  pdf = {https://aclanthology.org/2021.acl-long.568.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  url = {https://aclanthology.org/2021.acl-long.568},
  year = {2021}
}

@inproceedings{Austin2021,
  abstract = {Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  address = {Red Hook, NY, USA},
  author = {Jacob Austin and Daniel D. Johnson and Jonathan Ho and Daniel Tarlow and Rianne van den Berg},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  openalex = {W4287083626},
  pages = {17981--17993},
  pdf = {https://papers.neurips.cc/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
  url = {https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html},
  volume = {34},
  year = {2021}
}

@techreport{Bommasani2021,
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Oakden-Rayner, Luke and Papadimitriou, Odette and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  doi = {10.48550/arXiv.2108.07258},
  eprint = {2108.07258},
  eprinttype = {arxiv},
  institution = {Stanford University, Center for Research on Foundation Models},
  month = {8},
  number = {2108.07258},
  openalex = {W3195577433},
  pdf = {https://arxiv.org/pdf/2108.07258.pdf},
  title = {On the Opportunities and Risks of Foundation Models},
  type = {Technical Report},
  url = {https://crfm.stanford.edu/report.html},
  year = {2021}
}

@techreport{Bronstein2021,
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  doi = {10.48550/arxiv.2104.13478},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  institution = {arXiv},
  number = {2104.13478},
  openalex = {W3157286395},
  pdf = {http://arxiv.org/pdf/2104.13478},
  title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  year = {2021}
}

@inproceedings{Dhariwal2021,
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512. We release our code at https://github.com/openai/guided-diffusion},
  author = {Prafulla Dhariwal and Alexander Nichol},
  booktitle = {Advances in Neural Information Processing Systems 34},
  editor = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
  openalex = {W3162926177},
  pdf = {https://proceedings.neurips.cc/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf},
  publisher = {NeurIPS Foundation},
  title = {Diffusion Models Beat GANs on Image Synthesis},
  url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  year = {2021}
}

@inproceedings{Dosovitskiy2020,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021},
  openalex = {W3119786062},
  pdf = {https://openreview.net/pdf?id=YicbFdNTTy},
  publisher = {OpenReview.net},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  url = {https://openreview.net/forum?id=YicbFdNTTy},
  year = {2021}
}

@inproceedings{Esser2020,
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image.},
  author = {Patrick Esser and Robin Rombach and Björn Ommer},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr46437.2021.01268},
  openalex = {W3180355996},
  pages = {12868--12878},
  pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf},
  publisher = {IEEE},
  title = {Taming Transformers for High-Resolution Image Synthesis},
  year = {2021}
}

@inproceedings{Gong2021,
  abstract = {In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.},
  author = {Gong, Yuan and Chung, Yu-An and Glass, James R.},
  booktitle = {Interspeech 2021},
  doi = {10.21437/Interspeech.2021-698},
  openalex = {W3196974791},
  pages = {571--575},
  pdf = {https://www.isca-archive.org/interspeech_2021/gong21b_interspeech.pdf},
  publisher = {ISCA},
  title = {AST: Audio Spectrogram Transformer},
  year = {2021}
}

@article{Jumper2021,
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the 'protein folding problem'---has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated this method in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the method is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  doi = {10.1038/s41586-021-03819-2},
  journal = {Nature},
  month = {8},
  number = {7873},
  pages = {583--589},
  pdf = {https://www.nature.com/articles/s41586-021-03819-2.pdf},
  pmid = {34265844},
  publisher = {Springer Nature},
  title = {Highly accurate protein structure prediction with AlphaFold},
  volume = {596},
  year = {2021}
}

@article{Kobyzev2019,
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  arxiv = {1908.09257},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  doi = {10.1109/TPAMI.2020.2992934},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {11},
  pages = {3964--3979},
  pdf = {https://arxiv.org/pdf/1908.09257.pdf},
  pmid = {32396070},
  title = {Normalizing Flows: An Introduction and Review of Current Methods},
  volume = {43},
  year = {2021}
}

@techreport{Liu2021,
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub 'prompt-based learning'. Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g. the choice of pre-trained models, prompts, and tuning strategies.},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  eprint = {2107.13586},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {7},
  number = {2107.13586},
  openalex = {W4297801719},
  pdf = {https://arxiv.org/pdf/2107.13586.pdf},
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  url = {https://arxiv.org/abs/2107.13586},
  year = {2021}
}

@article{Nakkiran2019,
  abstract = {We show that a variety of modern deep learning tasks exhibit a 'double-descent' phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  doi = {10.1088/1742-5468/ac3a74},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  month = {12},
  number = {12},
  pages = {124003},
  pdf = {https://arxiv.org/pdf/1912.02292.pdf},
  publisher = {IOP Publishing},
  title = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  volume = {2021},
  year = {2021}
}

@inproceedings{Nichol2021a,
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality.},
  author = {Alexander Quinn Nichol and Prafulla Dhariwal},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  eprint = {2102.09672},
  eprinttype = {arxiv},
  month = {7},
  pages = {8162--8171},
  pdf = {https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Denoising Diffusion Probabilistic Models},
  volume = {139},
  year = {2021}
}

@article{Papamakarios2019,
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  author = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  journal = {Journal of Machine Learning Research},
  month = {1},
  openalex = {W3150807214},
  pages = {1--64},
  pdf = {https://jmlr.org/papers/volume22/19-1028/19-1028.pdf},
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  url = {https://jmlr.org/papers/v22/19-1028.html},
  volume = {22},
  year = {2021}
}

@inproceedings{Radford2021,
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  editor = {Marina Meila and Tong Zhang},
  month = {7},
  pages = {8748--8763},
  pdf = {https://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  url = {https://proceedings.mlr.press/v139/radford21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{Rakhimov2020,
  abstract = {The video generation task can be formulated as a prediction of future video frames given some past frames. Recent generative models for videos face the problem of high computational requirements. Some models require up to 512 Tensor Processing Units for parallel training. In this work, we address this problem via modeling the dynamics in a latent space. After the transformation of frames into the latent space, our model predicts latent representation for the next frames in an autoregressive manner. We demonstrate the performance of our approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach tends to reduce requirements to 8 Graphical Processing Units for training the models while maintaining comparable generation quality.},
  address = {Setúbal, Portugal},
  author = {Rakhimov, Ruslan and Volkhonskiy, Denis and Artemov, Alexey and Zorin, Denis and Burnaev, Evgeny},
  booktitle = {Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -- Volume 5: VISAPP},
  doi = {10.5220/0010241801010112},
  eprint = {2006.10704},
  eprinttype = {arxiv},
  isbn = {978-989-758-488-6},
  pages = {101--112},
  pdf = {https://www.scitepress.org/Papers/2021/102418/102418.pdf},
  publisher = {SCITEPRESS -- Science and Technology Publications},
  title = {Latent Video Transformer},
  year = {2021}
}

@article{Ruthotto2021,
  abstract = {Deep generative models (DGM) are neural networks with many hidden layers trained to approximate complicated, high-dimensional probability distributions using a large number of samples. When trained successfully, we can use the DGMs to estimate the likelihood of each observation and to create new samples from the underlying distribution. Developing DGMs has become one of the most hotly researched fields in artificial intelligence in recent years. The literature on DGMs has become vast and is growing rapidly. Some advances have even reached the public sphere, for example, the recent successes in generating realistic-looking images, voices, or movies; so-called deep fakes. Despite these successes, several mathematical and practical issues limit the broader use of DGMs: given a specific dataset, it remains challenging to design and train a DGM and even more challenging to find out why a particular model is or is not effective. To help advance the theoretical understanding of DGMs, we provide an introduction to DGMs and provide a concise mathematical framework for modeling the three most popular approaches: normalizing flows (NF), variational autoencoders (VAE), and generative adversarial networks (GAN). We illustrate the advantages and disadvantages of these basic approaches using numerical experiments. Our goal is to enable and motivate the reader to contribute to this proliferating research area. Our presentation also emphasizes relations between generative modeling and optimal transport.},
  author = {Ruthotto, Lars and Haber, Eldad},
  doi = {10.1002/gamm.202100008},
  eprint = {2103.05180},
  eprinttype = {arXiv},
  journal = {GAMM-Mitteilungen},
  month = {5},
  number = {2},
  pages = {e202100008},
  pdf = {https://arxiv.org/pdf/2103.05180.pdf},
  publisher = {Wiley},
  title = {An introduction to deep generative modeling},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/gamm.202100008},
  volume = {44},
  year = {2021}
}

@inproceedings{Satorras2021,
  abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
  author = {Satorras, Víctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  eprint = {2102.09844},
  eprinttype = {arxiv},
  pages = {9323--9332},
  pdf = {http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {E(n) Equivariant Graph Neural Networks},
  url = {https://proceedings.mlr.press/v139/satorras21a.html},
  volume = {139},
  year = {2021}
}

@inproceedings{Schuhmann2021,
  abstract = {Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.},
  arxiv = {2111.02114},
  author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  booktitle = {Proceedings of the NeurIPS 2021 Workshop on Data-Centric AI},
  month = {12},
  pages = {1--6},
  pdf = {https://arxiv.org/pdf/2111.02114.pdf},
  publisher = {NeurIPS},
  title = {LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
  url = {https://nips.cc/virtual/2021/38263},
  year = {2021}
}

@inproceedings{Song2020a,
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and perform semantically meaningful image interpolation directly in the latent space.},
  author = {Jiaming Song and Chenlin Meng and Stefano Ermon},
  booktitle = {International Conference on Learning Representations},
  eprint = {2010.02502},
  eprinttype = {arxiv},
  pdf = {https://arxiv.org/pdf/2010.02502.pdf},
  publisher = {OpenReview.net},
  title = {Denoising Diffusion Implicit Models},
  url = {https://openreview.net/forum?id=St1giarCHLP},
  year = {2021}
}

@techreport{Song2021,
  abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Contrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
  author = {Song, Yang and Kingma, Diederik P.},
  eprint = {2101.03288},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {1},
  number = {2101.03288},
  pdf = {https://arxiv.org/pdf/2101.03288.pdf},
  title = {How to Train Your Energy-Based Models},
  url = {https://arxiv.org/abs/2101.03288},
  year = {2021}
}

@inproceedings{Song2020b,
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ( extita.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 $ imes$ 1024 images for the first time from a score-based generative model.},
  author = {Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
  booktitle = {International Conference on Learning Representations},
  eprint = {2011.13456},
  eprinttype = {arXiv},
  note = {Outstanding Paper Award},
  openalex = {W3109824205},
  pdf = {https://openreview.net/pdf?id=PxTIG12RRHS},
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  year = {2021}
}

@article{Wu2019,
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  doi = {10.1109/TNNLS.2020.2978386},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  month = {1},
  number = {1},
  openalex = {W4210257598},
  pages = {4--24},
  pdf = {https://arxiv.org/pdf/1901.00596.pdf},
  title = {A Comprehensive Survey on Graph Neural Networks},
  url = {https://ieeexplore.ieee.org/document/9046288},
  volume = {32},
  year = {2021}
}

@techreport{Yan2021,
  abstract = {We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at the project website.},
  author = {Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  eprint = {2104.10157},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {4},
  number = {2104.10157},
  pdf = {https://arxiv.org/pdf/2104.10157.pdf},
  title = {VideoGPT: Video Generation using VQ-VAE and Transformers},
  url = {https://arxiv.org/abs/2104.10157},
  urldate = {2021-04-20},
  year = {2021}
}

@inproceedings{Brown2020,
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems 33},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  openalex = {W4292779060},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  year = {2020}
}

@inproceedings{Chen2020a,
  abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models.},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor = {Daumé III, Hal and Singh, Aarti},
  month = {7},
  openalex = {W3034445277},
  pages = {1691--1703},
  pdf = {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Pretraining From Pixels},
  url = {https://proceedings.mlr.press/v119/chen20s.html},
  volume = {119},
  year = {2020}
}

@inproceedings{Chen2020b,
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  author = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  editor = {Hal Daumé III and Aarti Singh},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  openalex = {W2963793847},
  pages = {1597--1607},
  pdf = {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  volume = {119},
  year = {2020}
}

@book{Deisenroth2020,
  abstract = {The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts.},
  author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
  doi = {10.1017/9781108679930},
  isbn = {9781108679930},
  month = {2},
  openalex = {W2974922583},
  pdf = {https://mml-book.github.io/book/mml-book.pdf},
  publisher = {Cambridge University Press},
  title = {Mathematics for Machine Learning},
  url = {https://doi.org/10.1017/9781108679930},
  year = {2020}
}

@book{Hamilton2020,
  abstract = {The field of graph representation learning has grown at an incredible (and sometimes unwieldy) pace over the past seven years, transforming from a small subset of researchers working on a relatively niche topic to one of the fastest growing sub-areas of deep learning. This book provides a brief but comprehensive introduction to graph representation learning, including methods for embedding graph data, graph neural networks, and deep generative models of graphs.},
  author = {Hamilton, William L.},
  doi = {10.2200/S01045ED1V01Y202009AIM046},
  isbn = {9781681739656},
  month = {9},
  number = {3},
  pages = {1--159},
  pdf = {https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf},
  publisher = {Morgan & Claypool},
  series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  title = {Graph Representation Learning},
  volume = {14},
  year = {2020}
}

@inproceedings{He2019,
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  address = {Seattle, WA, USA},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  month = {6},
  openalex = {W3035524453},
  pages = {9729--9738},
  pdf = {https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf},
  publisher = {IEEE},
  title = {Momentum Contrast for Unsupervised Visual Representation Learning},
  year = {2020}
}

@inproceedings{Ho2020,
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  author = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
  booktitle = {Advances in Neural Information Processing Systems 33},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  pages = {6840--6851},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Denoising Diffusion Probabilistic Models},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  year = {2020}
}

@inproceedings{Holtzman2019,
  abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration --- output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality --- as measured by human evaluation --- and as diverse as human-written text.},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  eprint = {1904.09751},
  eprinttype = {arxiv},
  openalex = {W2938704169},
  pdf = {https://openreview.net/pdf?id=rygGQyrFvH},
  publisher = {OpenReview.net},
  title = {The Curious Case of Neural Text Degeneration},
  url = {https://openreview.net/forum?id=rygGQyrFvH},
  year = {2020}
}

@misc{Kaplan2020,
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  eprint = {2001.08361},
  eprinttype = {arxiv},
  month = {1},
  note = {arXiv preprint},
  openalex = {W3003314179},
  pdf = {https://arxiv.org/pdf/2001.08361.pdf},
  title = {Scaling Laws for Neural Language Models},
  url = {https://arxiv.org/abs/2001.08361},
  year = {2020}
}

@inproceedings{Khosla2020,
  abstract = {Supervised Contrastive Learning extends the self-supervised batch contrastive approach to the fully-supervised setting, allowing effective leverage of label information. The method pulls clusters of points belonging to the same class together in embedding space, while simultaneously pushing apart clusters of samples from different classes. On ResNet-200, the method achieves top-1 accuracy of 81.4% on the ImageNet dataset, which is 0.8% above the best number reported for this architecture.},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  note = {Originally appeared as arXiv:2004.11362},
  pages = {18661--18673},
  pdf = {https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Supervised Contrastive Learning},
  url = {https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
  volume = {33},
  year = {2020}
}

@misc{Prince2020,
  abstract = {The goal of the variational autoencoder (VAE) is to learn a probability distribution over a multi-dimensional variable. VAEs can generate samples and interpolate between them by manipulating the latent variable, but cannot easily compute likelihood for new examples. This tutorial covers latent variable models, non-linear latent variable models, evidence lower bound (ELBO), variational autoencoder architecture, reparameterization trick, and extensions and problems with VAEs.},
  author = {Prince, Simon J.D.},
  howpublished = {Technical Tutorial, Borealis AI},
  month = {1},
  note = {Research Area: Unsupervised Learning},
  title = {Tutorial #5: variational autoencoders},
  url = {https://rbcborealis.com/research-blogs/tutorial-5-variational-auto-encoders/},
  year = {2020}
}

@inproceedings{Shen2019,
  abstract = {Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use a Hessian based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13$ imes$ compression of the model parameters, and up to 4$ imes$ compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.},
  author = {Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v34i05.6409},
  eprint = {1909.05840},
  eprinttype = {arxiv},
  number = {05},
  openalex = {W2970164186},
  pages = {8815--8821},
  pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/6409/6265},
  title = {Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6409},
  volume = {34},
  year = {2020}
}

@techreport{Vig2020,
  author = {Vig, Jesse and Madani, Ali and Varshney, Lav R. and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
  eprint = {2006.15222},
  eprinttype = {arxiv},
  institution = {arXiv},
  number = {2006.15222},
  title = {BERTology Meets Biology: Interpreting Attention in Protein Language Models},
  year = {2020}
}

@article{Zhou2018,
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we provide a comprehensive overview of graph neural networks which includes the methods and their applications. We first introduce the common notations and define the problems that GNNs are trying to address. We then describe different variants of GNNs and categorize them based on types of graph convolutions. We further discuss the applications of GNNs across various domains and summarize the open-source codes and benchmark datasets.},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  doi = {10.1016/j.aiopen.2021.01.001},
  eprint = {1812.08434},
  eprinttype = {arxiv},
  journal = {AI Open},
  month = {1},
  pages = {57--81},
  pdf = {https://arxiv.org/pdf/1812.08434.pdf},
  title = {Graph Neural Networks: A Review of Methods and Applications},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
  volume = {1},
  year = {2020}
}

@article{Belkin2019,
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias-variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This 'double-descent' curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  doi = {10.1073/pnas.1903070116},
  journal = {Proceedings of the National Academy of Sciences},
  month = {8},
  number = {32},
  openalex = {W2963518130},
  pages = {15849--15854},
  title = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
  volume = {116},
  year = {2019}
}

@article{Brinker2019,
  abstract = {Melanoma is the most dangerous type of skin cancer but curable if detected early. Recent publications demonstrated that artificial intelligence is capable in classifying images of benign nevi and melanoma with dermatologist-level precision. However, a statistically significant improvement compared dermatologist classification has not been reported to date. For this comparative study, 4204 biopsy-proven images of melanoma and nevi (1:1) were used for training convolutional neural network (CNN). New techniques deep learning integrated. For experiment, an additional 804 dermoscopic images randomly presented dermatologists nine German university hospitals, who evaluated quality each image stated their recommended treatment (19,296 recommendations total). Three McNemar's tests comparing results CNN's test runs terms sensitivity, specificity overall correctness predefined as main outcomes. The respective sensitivity lesion by 67.2% (95% confidence interval [CI]: 62.6%--71.7%) 62.2% (CI: 57.6%--66.9%). In comparison, trained CNN achieved higher 82.3% (78.3%--85.7%) 77.9% (73.8%--81.8%). The three 2 $ imes$ tables all reached significance level $p < 0.001$. This was sustained both subgroups. For first time, automated shown be significantly superior junior board-certified ($p < 0.001$).},
  author = {Titus J. Brinker and Achim Hekler and Alexander H. Enk and Carola Berking and Sebastian Haferkamp and Axel Hauschild and Michael Weichenthal and Joachim Klode and Dirk Schadendorf and Tim Holland-Letz and Christof von Kalle and Stefan Fröhling and Bastian Schilling and Jochen S. Utikal},
  doi = {10.1016/j.ejca.2019.05.023},
  journal = {European Journal of Cancer},
  keywords = {melanoma, dermatology, convolutional neural network, deep learning, artificial intelligence, image classification},
  month = {9},
  openalex = {W2967473922},
  pages = {11--17},
  pmid = {31401469},
  publisher = {Elsevier},
  title = {Deep Neural Networks Are Superior to Dermatologists in Melanoma Image Classification},
  url = {https://www.sciencedirect.com/science/article/pii/S0959804919303491},
  volume = {119},
  year = {2019}
}

@inproceedings{Brock2018,
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ``truncation trick,'' allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fréchet Inception Distance (FID) of 7.4, improving over the previous best results.},
  address = {New Orleans, LA, USA},
  archiveprefix = {arXiv},
  author = {Andrew Brock and Jeff Donahue and Karen Simonyan},
  booktitle = {Proceedings of the 7th International Conference on Learning Representations},
  eprint = {1809.11096},
  month = {5},
  openalex = {W2893749619},
  pdf = {https://openreview.net/pdf?id=B1xsqj09Fm},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  url = {https://openreview.net/forum?id=B1xsqj09Fm},
  year = {2019}
}

@inproceedings{Devlin2018,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT advances the state of the art for eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address = {Minneapolis, Minnesota},
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi = {10.18653/v1/N19-1423},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  month = {6},
  openalex = {W2963093368},
  pages = {4171--4186},
  pdf = {https://aclanthology.org/N19-1423.pdf},
  publisher = {Association for Computational Linguistics},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url = {https://aclanthology.org/N19-1423/},
  year = {2019}
}

@inproceedings{Grathwohl2018,
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  author = {Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and Ilya Sutskever and David Duvenaud},
  booktitle = {International Conference on Learning Representations},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  pdf = {https://openreview.net/pdf?id=rJxgknCcK7},
  title = {FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
  url = {https://openreview.net/forum?id=rJxgknCcK7},
  year = {2019}
}

@article{Kingma2019,
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  author = {Diederik P. Kingma and Max Welling},
  doi = {10.1561/2200000056},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  journal = {Foundations and Trends in Machine Learning},
  number = {4},
  openalex = {W2948978827},
  pages = {307--392},
  title = {An Introduction to Variational Autoencoders},
  url = {https://arxiv.org/abs/1906.02691},
  volume = {12},
  year = {2019}
}

@techreport{Radford2019,
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  institution = {OpenAI},
  month = {2},
  pdf = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year = {2019}
}

@inproceedings{Song2019a,
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores for all noise levels. For sampling, we propose an annealed Langevin dynamics where gradients corresponding to gradually decreasing noise levels are used as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or adversarial methods, and provides a learning objective that can be used for principled model comparison. We produce samples with quality comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art FID score of 25.32 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  author = {Yang Song and Stefano Ermon},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alché-Buc and E. Fox and R. Garnett},
  note = {arXiv:1907.05600},
  pages = {11895--11907},
  pdf = {https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  series = {NIPS},
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
  volume = {32},
  year = {2019}
}

@inproceedings{Song2019b,
  abstract = {Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple, shallow models or low-dimensional data, due to the difficulty of computing the Hessian of log-density functions. We show this difficulty can be mitigated by projecting the scores onto random vectors before comparing them. This objective, called sliced score matching, only involves Hessian-vector products, which can be easily implemented using reverse-mode automatic differentiation. Therefore, sliced score matching is amenable to more complex models and higher dimensional data compared to score matching. Theoretically, we prove the consistency and asymptotic normality of sliced score matching estimators. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show sliced score matching can learn deep energy-based models effectively, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.},
  address = {Tel Aviv, Israel},
  author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence},
  editor = {Globerson, Amir and Silva, Ricardo},
  month = {July},
  note = {arXiv:1905.07088},
  openalex = {W2950164865},
  pages = {204--213},
  pdf = {https://proceedings.mlr.press/v115/song20a/song20a.pdf},
  publisher = {AUAI Press},
  series = {Proceedings of Machine Learning Research},
  title = {Sliced score matching: A scalable approach to density and score estimation},
  url = {https://proceedings.mlr.press/v115/song20a.html},
  volume = {115},
  year = {2019}
}

@misc{Sutton2019,
  abstract = {The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore's law, or rather its generalization of continued exponentially falling cost per unit of computation. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.},
  author = {Sutton, Richard Stuart},
  howpublished = {URL: r̆lhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  month = {3},
  note = {Published March 13, 2019},
  title = {The Bitter Lesson},
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  year = {2019}
}

@techreport{Battaglia2018,
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ``hand-engineering'' and ``end-to-end'' learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  author = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Álvaro Sánchez-González and Vinícius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Çağlar Gülçehre and H. Francis Song and Andrew J. Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey R. Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matthew Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  doi = {10.48550/arxiv.1806.01261},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {6},
  number = {1806.01261},
  openalex = {W2805516822},
  pdf = {https://arxiv.org/pdf/1806.01261.pdf},
  title = {Relational inductive biases, deep learning, and graph networks},
  url = {https://arxiv.org/abs/1806.01261},
  year = {2018}
}

@article{Baydin2018,
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply 'autodiff', is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names 'dynamic computational graphs' and 'differentiable programming'. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms 'autodiff', 'automatic differentiation', and 'symbolic differentiation' as these are encountered more and more in machine learning settings.},
  author = {Baydin, Atılım Güneş and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  journal = {Journal of Machine Learning Research},
  openalex = {W2962727772},
  pages = {1--43},
  pdf = {https://www.jmlr.org/papers/volume18/17-468/17-468.pdf},
  title = {Automatic Differentiation in Machine Learning: A Survey},
  url = {https://jmlr.csail.mit.edu/papers/v18/17-468.html},
  volume = {18},
  year = {2018}
}

@inproceedings{Chen2018,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K.},
  booktitle = {Advances in Neural Information Processing Systems 31},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  openalex = {W2963755523},
  pages = {6572--6583},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Neural Ordinary Differential Equations},
  url = {https://proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html},
  year = {2018}
}

@inproceedings{Eykholt2018,
  abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.},
  author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/cvpr.2018.00175},
  openalex = {W2798302089},
  pages = {1625--1634},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/papers/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.pdf},
  title = {Robust Physical-World Attacks on Deep Learning Visual Classification},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.html},
  year = {2018}
}

@misc{Grosse2018,
  abstract = {This lecture covers how to build an automatic differentiation (autodiff) library based on a simplified version of Autograd. Topics include reverse mode autodiff, computation graph construction, vector-Jacobian products, and the implementation of backpropagation. The lecture explains what autodiff is and is not, distinguishing it from finite differences and symbolic differentiation, and provides both theoretical foundations and practical implementation details for modern machine learning frameworks.},
  author = {Roger Grosse},
  howpublished = {CSC321 Lecture 10, University of Toronto},
  month = {Winter},
  note = {Course: Introduction to Neural Networks and Machine Learning},
  title = {Automatic Differentiation},
  url = {https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf},
  year = {2018}
}

@inproceedings{Karras2017,
  abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024². We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  eprint = {1710.10196},
  eprinttype = {arxiv},
  openalex = {W2963587220},
  pdf = {https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf},
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  url = {https://openreview.net/forum?id=Hk99zCeAb},
  year = {2018}
}

@inproceedings{Li2017,
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  address = {Red Hook, NY, USA},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle = {Advances in Neural Information Processing Systems 31},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  pages = {6389--6399},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Visualizing the Loss Landscape of Neural Nets},
  url = {https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
  year = {2018}
}

@inproceedings{Mescheder2018,
  abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the contrary, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. Finally, we propose a novel regularization scheme with finite sample guarantees. Together, these results help to explain the success of recently proposed GAN variants.},
  author = {Lars Mescheder and Andreas Geiger and Sebastian Nowozin},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  eprint = {1801.04406},
  eprinttype = {arxiv},
  month = {7},
  pages = {3481--3490},
  pdf = {https://proceedings.mlr.press/v80/mescheder18a/mescheder18a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which Training Methods for GANs do actually Converge?},
  url = {https://proceedings.mlr.press/v80/mescheder18a.html},
  volume = {80},
  year = {2018}
}

@techreport{vandenOord2018,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  author = {van den Oord, Aäron and Li, Yazhe and Vinyals, Oriol},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {7},
  number = {1807.03748},
  pdf = {https://arxiv.org/pdf/1807.03748.pdf},
  title = {Representation Learning with Contrastive Predictive Coding},
  url = {https://arxiv.org/abs/1807.03748},
  year = {2018}
}

@inproceedings{Ramachandran2017,
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, $f(x) = x ·  extsigmoid(β x)$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  booktitle = {International Conference on Learning Representations},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  month = {4},
  openalex = {W4320930577},
  pdf = {https://openreview.net/pdf?id=SkBYYyZRZ},
  publisher = {OpenReview.net},
  title = {Searching for Activation Functions},
  url = {https://openreview.net/forum?id=SkBYYyZRZ},
  year = {2018}
}

@inproceedings{Santurkar2018,
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called ``internal covariate shift''. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  address = {Red Hook, NY, USA},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {1805.11604},
  eprinttype = {arxiv},
  pages = {2488--2498},
  pdf = {https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf},
  publisher = {Curran Associates Inc.},
  series = {NeurIPS},
  title = {How Does Batch Normalization Help Optimization?},
  url = {https://proceedings.neurips.cc/paper/2018/hash/905056c1ac1dad141560467e0a99e1cf-Abstract.html},
  volume = {31},
  year = {2018}
}

@inproceedings{Wu2018,
  abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable in that it learns a compact feature representation. In our experiments, 128 features per image are sufficient, which is an impressive degree of compactness. Our code is available at https://github.com/zhirongw/lemniscate.pytorch},
  address = {Salt Lake City, UT},
  arxiv = {1805.01978},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2018.00393},
  month = {6},
  openalex = {W2951292523},
  pages = {3733--3742},
  pdf = {https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf},
  publisher = {IEEE},
  title = {Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf},
  year = {2018}
}

@inproceedings{Arjovsky2017,
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches.},
  author = {Martin Arjovsky and Soumith Chintala and Léon Bottou},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Doina Precup and Yee Whye Teh},
  openalex = {W2739748921},
  pages = {214--223},
  pdf = {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Wasserstein Generative Adversarial Networks},
  url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
  volume = {70},
  year = {2017}
}

@article{Badrinarayanan2015,
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  doi = {10.1109/TPAMI.2016.2644615},
  eprint = {1511.00561},
  eprinttype = {arXiv},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {12},
  number = {12},
  openalex = {W2963881378},
  pages = {2481--2495},
  pdf = {https://ieeexplore.ieee.org/document/7803544/},
  title = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  volume = {39},
  year = {2017}
}

@inproceedings{Balduzzi2017,
  abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although largely overcome via carefully constructed initializations, batch normalization, and architectures incorporating skip-connections like resnets, these networks perform much better than standard feedforward networks despite well-chosen initialization and normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that correlation between network gradients decays exponentially with depth, resulting in gradients that resemble white noise. In contrast, resnets are far more resistant to shattering, with gradients decaying sublinearly. Detailed empirical evidence is presented to support this analysis, on both fully-connected and convnet architectures. Finally, we present a new looks linear (LL) initialization that prevents gradient shattering and allows training very deep networks without skip-connections.},
  author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  openalex = {W2591954064},
  pages = {342--350},
  pdf = {http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Shattered Gradients Problem: If Resnets Are the Answer, Then What Is the Question?},
  url = {http://proceedings.mlr.press/v70/balduzzi17b.html},
  volume = {70},
  year = {2017}
}

@article{Bronstein2017,
  abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  doi = {10.1109/MSP.2017.2693418},
  journal = {IEEE Signal Processing Magazine},
  month = {7},
  number = {4},
  openalex = {W2558748708},
  pages = {18--42},
  pdf = {http://arxiv.org/pdf/1611.08097},
  title = {Geometric Deep Learning: Going beyond Euclidean Data},
  volume = {34},
  year = {2017}
}

@inproceedings{Christiano2017,
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {1706.03741},
  eprinttype = {arxiv},
  openalex = {W4321392130},
  pages = {4299--4307},
  pdf = {https://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Deep reinforcement learning from human preferences},
  volume = {30},
  year = {2017}
}

@inproceedings{Dinh2016,
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  author = {Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  openalex = {W4297798428},
  pdf = {https://openreview.net/pdf?id=HkpbnH9lx},
  publisher = {OpenReview.net},
  title = {Density estimation using Real NVP},
  url = {https://openreview.net/forum?id=HkpbnH9lx},
  year = {2017}
}

@inproceedings{Dodge2017,
  abstract = {Deep neural networks (DNNs) achieve excellent performance on standard classification tasks. However, under image quality distortions such as blur and noise, classification accuracy becomes poor. In this work, we compare the performance of DNNs with human subjects on distorted images. We show that, although DNNs perform better than or on par with humans on good quality images, DNN performance is still much lower than human performance on distorted images. We additionally find that there is little correlation in errors between DNNs and human subjects. This could be an indication that the internal representation of images are different between DNNs and the human visual system. These comparisons with human performance could be used to guide future development of more robust DNNs.},
  address = {Vancouver, BC, Canada},
  author = {Samuel F. Dodge and Lina J. Karam},
  booktitle = {2017 26th International Conference on Computer Communication and Networks (ICCCN)},
  doi = {10.1109/ICCCN.2017.8038465},
  eprint = {1705.02498},
  eprinttype = {arxiv},
  isbn = {978-1-5090-2991-4},
  month = {7},
  openalex = {W2612573399},
  pages = {1--7},
  pdf = {http://arxiv.org/pdf/1705.02498},
  publisher = {IEEE},
  title = {A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions},
  year = {2017}
}

@article{Esteva2017,
  abstract = {Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening followed by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Here we demonstrate classification of skin lesions using a single convolutional neural network (CNN), trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic.},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  doi = {10.1038/nature21056},
  journal = {Nature},
  month = {2},
  openalex = {W2581082771},
  pages = {115--118},
  pdf = {http://rdcu.be/oNac},
  pmid = {28117445},
  title = {Dermatologist-level classification of skin cancer with deep neural networks},
  volume = {542},
  year = {2017}
}

@inproceedings{Gemmeke2017,
  abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
  address = {New Orleans, LA, USA},
  author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/ICASSP.2017.7952261},
  month = {3},
  openalex = {W2194775991},
  pages = {776--780},
  pdf = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45857.pdf},
  publisher = {IEEE},
  title = {Audio Set: An ontology and human-labeled dataset for audio events},
  year = {2017}
}

@inproceedings{Gilmer2017,
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  editor = {Precup, Doina and Teh, Yee Whye},
  eprint = {1704.01212},
  eprinttype = {arxiv},
  month = {8},
  pages = {1263--1272},
  pdf = {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Message Passing for Quantum Chemistry},
  volume = {70},
  year = {2017}
}

@inproceedings{Gulrajani2017,
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C.},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W4295521014},
  pages = {5767--5777},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Improved Training of Wasserstein GANs},
  volume = {30},
  year = {2017}
}

@inproceedings{Higgins2017,
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce $β$-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework.},
  author = {Higgins, Irina and Matthey, Loïc and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  booktitle = {International Conference on Learning Representations},
  openalex = {W2753738274},
  pdf = {https://openreview.net/pdf?id=Sy2fzU9gl},
  title = {$β$-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  year = {2017}
}

@inproceedings{Kipf2016,
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  address = {Toulon, France},
  author = {Kipf, Thomas N. and Welling, Max},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  month = {4},
  openalex = {W2964015378},
  pdf = {https://openreview.net/pdf?id=SJU4ayYgl},
  publisher = {OpenReview.net},
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  year = {2017}
}

@inproceedings{Mao2016,
  abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $χ^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. The experimental results on several datasets demonstrate that LSGANs can generate higher quality images than regular GANs.},
  address = {Los Alamitos, CA, USA},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  eprint = {1611.04076},
  eprinttype = {arxiv},
  month = {10},
  openalex = {W2963141555},
  pages = {2794--2802},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {Least Squares Generative Adversarial Networks},
  year = {2017}
}

@inproceedings{vandenOord2017,
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. Using the VQ method allows the model to circumvent issues of ``posterior collapse'' -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  openalex = {W2963613802},
  pages = {6306--6315},
  pdf = {https://proceedings.neurips.cc/paper/7210-neural-discrete-representation-learning.pdf},
  title = {Neural Discrete Representation Learning},
  volume = {30},
  year = {2017}
}

@inproceedings{Papamakarios2017,
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W2963047245},
  pages = {2338--2347},
  pdf = {https://papers.nips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
  title = {Masked Autoregressive Flow for Density Estimation},
  volume = {30},
  year = {2017}
}

@inproceedings{Selvaraju2016,
  abstract = {We propose a technique for producing ``visual explanations'' for decisions from a large class of CNN-based models, making them more transparent. Our approach -- Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, we show both theoretically and empirically that Grad-CAM localizations are class-discriminative and are localized even when the CNN is trained on image-level labels only. We make the important distinction between explanations which are ``faithful'' to the model versus those which are just human-interpretable. We design and conduct human studies which measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ``stronger'' deep network from a ``weaker'' one even when both make identical predictions. We show that Grad-CAM enables previously impossible tasks like (1) explaining errors in predictions, (2) diagnosing deep networks in medical imaging, and (3) identifying bias in data sets.},
  address = {Venice, Italy},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV.2017.74},
  month = {10},
  openalex = {W3102564565},
  pages = {618--626},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf},
  publisher = {IEEE},
  title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
  url = {https://ieeexplore.ieee.org/document/8237336/},
  year = {2017}
}

@inproceedings{Sonderby2016,
  abstract = {Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. The paper introduces a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. Using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. The authors propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. The experiments show that the GAN based approach performs best on real image data.},
  author = {Casper Kaae Sønderby and Jose Caballero and Lucas Theis and Wenzhe Shi and Ferenc Huszár},
  booktitle = {International Conference on Learning Representations},
  eprint = {1610.04490},
  eprinttype = {arxiv},
  pdf = {https://openreview.net/pdf?id=S1RP6GLle},
  title = {Amortised MAP Inference for Image Super-resolution},
  year = {2017}
}

@inproceedings{Vaswani2017,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  address = {Red Hook, NY, USA},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  openalex = {W2626778328},
  pages = {5998--6008},
  pdf = {https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Attention Is All You Need},
  url = {https://papers.nips.cc/paper/7181-attention-is-all-you-need},
  volume = {30},
  year = {2017}
}

@techreport{Velickovic2017,
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  institution = {arXiv},
  number = {1710.10903},
  title = {Graph Attention Networks},
  year = {2017}
}

@inproceedings{Zaheer2017,
  abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approaches of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Póczos, Barnabás and Salakhutdinov, Ruslan and Smola, Alexander J.},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  openalex = {W2963316506},
  pages = {3391--3401},
  pdf = {https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Deep Sets},
  url = {http://papers.nips.cc/paper/6931-deep-sets.pdf},
  year = {2017}
}

@inproceedings{Zhang2016,
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle = {International Conference on Learning Representations},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  note = {Best Paper Award at ICLR 2017},
  pdf = {https://arxiv.org/pdf/1611.03530.pdf},
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  url = {https://openreview.net/forum?id=Sy8gdB9xx},
  year = {2017}
}

@inproceedings{Zhu2017,
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G: X → Y, such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F: Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc.},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV.2017.244},
  openalex = {W2964034354},
  pages = {2242--2251},
  pdf = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf},
  publisher = {IEEE},
  title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  url = {https://ieeexplore.ieee.org/document/8237506/},
  year = {2017}
}

@techreport{Ba2016,
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {7},
  number = {1607.06450},
  pdf = {https://arxiv.org/pdf/1607.06450.pdf},
  title = {Layer Normalization},
  url = {https://arxiv.org/abs/1607.06450},
  year = {2016}
}

@techreport{Doersch2016,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  author = {Carl Doersch},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {6},
  number = {1606.05908},
  pdf = {https://arxiv.org/pdf/1606.05908.pdf},
  title = {Tutorial on Variational Autoencoders},
  url = {https://arxiv.org/abs/1606.05908},
  year = {2016}
}

@techreport{Dumoulin2016,
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers},
  author = {Vincent Dumoulin and Francesco Visin},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {3},
  number = {1603.07285},
  openalex = {W2304648132},
  pdf = {https://arxiv.org/pdf/1603.07285.pdf},
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  url = {https://arxiv.org/abs/1603.07285},
  year = {2016}
}

@inproceedings{Gatys2015,
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  address = {Las Vegas, NV, USA},
  author = {Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2016.265},
  month = {6},
  pages = {2414--2423},
  pdf = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf},
  publisher = {IEEE},
  title = {Image Style Transfer Using Convolutional Neural Networks},
  year = {2016}
}

@book{Goodfellow2016,
  author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  edition = {First},
  isbn = {978-0262035613},
  month = {11},
  note = {Available online for free at r̆lhttps://www.deeplearningbook.org/},
  pages = {800},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Deep Learning},
  url = {https://www.deeplearningbook.org/},
  year = {2016}
}

@inproceedings{He2015a,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset, we evaluate residual nets with a depth of up to 152 layers---deeper than VGG nets (40 layers) but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2016.90},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  pages = {770--778},
  pdf = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf},
  title = {Deep Residual Learning for Image Recognition},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
  year = {2016}
}

@inproceedings{Kingma2016,
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle = {Advances in Neural Information Processing Systems 29},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike von and Guyon, Isabelle and Garnett, Roman},
  pages = {4743--4751},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
  publisher = {Curran Associates},
  series = {NIPS Proceedings},
  title = {Improved Variational Inference with Inverse Autoregressive Flow},
  url = {https://proceedings.neurips.cc/paper/2016/hash/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Abstract.html},
  year = {2016}
}

@inproceedings{vandenOord2016a,
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  author = {van den Oord, Aäron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
  doi = {10.5555/3045390.3045575},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  eprint = {1601.06759},
  eprinttype = {arxiv},
  month = {6},
  pages = {1747--1756},
  pdf = {https://proceedings.mlr.press/v48/oord16.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pixel Recurrent Neural Networks},
  url = {https://proceedings.mlr.press/v48/oord16.html},
  volume = {48},
  year = {2016}
}

@inproceedings{vandenOord2016b,
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  booktitle = {Advances in Neural Information Processing Systems 29},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  eprint = {1606.05328},
  eprinttype = {arxiv},
  pages = {4790--4798},
  pdf = {https://proceedings.neurips.cc/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Conditional Image Generation with PixelCNN Decoders},
  url = {https://proceedings.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html},
  year = {2016}
}

@inproceedings{Radford2015,
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  booktitle = {4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  editor = {Yoshua Bengio and Yann LeCun},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  openalex = {W1899688384},
  pdf = {https://arxiv.org/pdf/1511.06434.pdf},
  publisher = {OpenReview.net},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  url = {https://openreview.net/forum?id=Hk99zCeAb},
  year = {2016}
}

@inproceedings{Redmon2015,
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  address = {Las Vegas, NV, USA},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2016.91},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  month = {6},
  openalex = {W2963037989},
  pages = {779--788},
  pdf = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf},
  publisher = {IEEE},
  title = {You Only Look Once: Unified, Real-Time Object Detection},
  year = {2016}
}

@inproceedings{Sennrich2015,
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  address = {Berlin, Germany},
  author = {Rico Sennrich and Barry Haddow and Alexandra Birch},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  openalex = {W2962784628},
  pages = {1715--1725},
  pdf = {https://aclanthology.org/P16-1162.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Neural Machine Translation of Rare Words with Subword Units},
  year = {2016}
}

@inproceedings{Bahdanau2014,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoder models and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  openalex = {W2133564696},
  pdf = {https://arxiv.org/pdf/1409.0473.pdf},
  publisher = {OpenReview.net},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url = {http://arxiv.org/abs/1409.0473},
  year = {2015}
}

@inproceedings{Dinh2014,
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy.},
  author = {Laurent Dinh and David Krueger and Yoshua Bengio},
  booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings},
  eprint = {1410.8516},
  eprinttype = {arXiv},
  openalex = {W1583912456},
  pdf = {https://arxiv.org/pdf/1410.8516.pdf},
  title = {NICE: Non-linear Independent Components Estimation},
  year = {2015}
}

@inproceedings{Germain2015,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  address = {Lille, France},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  editor = {Bach, Francis and Blei, David},
  month = {7},
  openalex = {W2952838738},
  pages = {881--889},
  pdf = {http://proceedings.mlr.press/v37/germain15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MADE: Masked Autoencoder for Distribution Estimation},
  url = {https://proceedings.mlr.press/v37/germain15.html},
  volume = {37},
  year = {2015}
}

@inproceedings{Girshick2015,
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate.},
  author = {Girshick, Ross},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  doi = {10.1109/ICCV.2015.169},
  note = {Originally appeared as arXiv:1504.08083},
  pages = {1440--1448},
  pdf = {https://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {Fast R-CNN},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html},
  year = {2015}
}

@inproceedings{He2015b,
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.},
  address = {Santiago, Chile},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV.2015.123},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  isbn = {978-1-4673-8391-2},
  month = {12},
  pages = {1026--1034},
  pdf = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf},
  publisher = {IEEE Computer Society},
  title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  url = {https://arxiv.org/abs/1502.01852},
  year = {2015}
}

@techreport{Hinton2015,
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  author = {Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  doi = {10.48550/arxiv.1503.02531},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {3},
  number = {1503.02531},
  openalex = {W1821462560},
  pdf = {https://arxiv.org/pdf/1503.02531},
  title = {Distilling the Knowledge in a Neural Network},
  url = {https://arxiv.org/abs/1503.02531},
  year = {2015}
}

@inproceedings{Ioffe2015,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  author = {Sergey Ioffe and Christian Szegedy},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  openalex = {W1836465849},
  pages = {448--456},
  pdf = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  volume = {37},
  year = {2015}
}

@inproceedings{Kingma2014,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. We analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  doi = {10.48550/arXiv.1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  openalex = {W2964138229},
  pdf = {https://arxiv.org/pdf/1412.6980.pdf},
  publisher = {ICLR},
  title = {Adam: A Method for Stochastic Optimization},
  url = {https://arxiv.org/abs/1412.6980},
  year = {2015}
}

@article{LeCun2015,
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  doi = {10.1038/nature14539},
  journal = {Nature},
  month = {5},
  number = {7553},
  openalex = {W2741809807},
  pages = {436--444},
  title = {Deep Learning},
  url = {https://www.nature.com/articles/nature14539},
  volume = {521},
  year = {2015}
}

@inproceedings{Long2014,
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  address = {Boston, MA, USA},
  author = {Jonathan Long and Evan Shelhamer and Trevor Darrell},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  doi = {10.1109/CVPR.2015.7298965},
  month = {6},
  openalex = {W1903029394},
  pages = {3431--3440},
  pdf = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf},
  publisher = {IEEE},
  title = {Fully Convolutional Networks for Semantic Segmentation},
  year = {2015}
}

@misc{Mordvintsev2015,
  abstract = {Artificial neural networks have spurred remarkable recent progress in image classification and speech recognition. But even though these are very useful tools based on well-known mathematical methods, we actually understand surprisingly little of why certain models work and others don't. This blog post explores techniques for visualizing and understanding what neural networks learn by turning the network upside down and generating images that reveal how neural networks interpret and classify visual information. The authors demonstrate methods of extracting and enhancing features from different network layers, creating surprising and artistic visualizations of what neural networks see in images. The technique, called Inceptionism in reference to the neural net architecture used, gives a qualitative sense of the level of abstraction that a particular layer has achieved in its understanding of images.},
  author = {Mordvintsev, Alexander and Olah, Christopher and Tyka, Mike},
  howpublished = {Google Research Blog},
  month = {6},
  note = {Published June 17, 2015},
  title = {Inceptionism: Going Deeper into Neural Networks},
  url = {https://research.google/blog/inceptionism-going-deeper-into-neural-networks/},
  year = {2015}
}

@inproceedings{Noh2015,
  abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
  address = {Santiago, Chile},
  author = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  doi = {10.1109/ICCV.2015.178},
  eprint = {1505.04366},
  eprinttype = {arxiv},
  month = {12},
  pages = {1520--1528},
  pdf = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf},
  publisher = {IEEE},
  title = {Learning Deconvolution Network for Semantic Segmentation},
  url = {https://openaccess.thecvf.com/content_iccv_2015/html/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.html},
  year = {2015}
}

@inproceedings{Ren2015,
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image.},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B. and Sun, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  openalex = {W639708223},
  pages = {91--99},
  pdf = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  url = {https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks},
  volume = {28},
  year = {2015}
}

@incollection{Ronneberger2015,
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  address = {Cham},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
  doi = {10.1007/978-3-319-24574-4_28},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  isbn = {978-3-319-24574-4},
  month = {10},
  openalex = {W2113618406},
  pages = {234--241},
  pdf = {https://arxiv.org/pdf/1505.04597.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  url = {https://doi.org/10.1007/978-3-319-24574-4_28},
  volume = {9351},
  year = {2015}
}

@inproceedings{Simonyan2014,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  author = {Karen Simonyan and Andrew Zisserman},
  booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  doi = {10.48550/arXiv.1409.1556},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  openalex = {W2962977596},
  pdf = {https://arxiv.org/pdf/1409.1556.pdf},
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  url = {http://arxiv.org/abs/1409.1556},
  year = {2015}
}

@inproceedings{SohlDickstein2015,
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model.},
  address = {Lille, France},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  editor = {Bach, Francis and Blei, David},
  eprint = {1503.03585},
  eprinttype = {arxiv},
  pages = {2256--2265},
  pdf = {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  volume = {37},
  year = {2015}
}

@techreport{Yosinski2015,
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea have produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh Mai and Fuchs, Thomas J. and Lipson, Hod},
  doi = {10.48550/arxiv.1506.06579},
  eprint = {1506.06579},
  eprinttype = {arxiv},
  institution = {arXiv},
  keywords = {deep learning, neural networks, visualization, interpretability, convolutional neural networks},
  month = {6},
  note = {To appear at ICML Deep Learning Workshop 2015},
  number = {1506.06579},
  openalex = {W1825675169},
  title = {Understanding Neural Networks Through Deep Visualization},
  url = {https://arxiv.org/abs/1506.06579},
  year = {2015}
}

@inproceedings{Cho2014,
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  address = {Doha, Qatar},
  author = {Kyunghyun Cho and Bart van Merriënboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  doi = {10.3115/v1/D14-1179},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  openalex = {W2157331557},
  pages = {1724--1734},
  pdf = {https://aclanthology.org/D14-1179.pdf},
  publisher = {Association for Computational Linguistics},
  title = {Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  url = {https://aclanthology.org/D14-1179/},
  year = {2014}
}

@inproceedings{Goodfellow2014a,
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  eprint = {1406.2661},
  eprinttype = {arXiv},
  pages = {2672--2680},
  pdf = {https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
  publisher = {Curran Associates, Inc.},
  title = {Generative Adversarial Nets},
  url = {https://papers.nips.cc/paper/5423-generative-adversarial-nets},
  volume = {27},
  year = {2014}
}

@techreport{Goodfellow2014b,
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {12},
  note = {Presented at ICLR 2015},
  number = {1412.6572},
  openalex = {W1945616565},
  pdf = {https://arxiv.org/pdf/1412.6572.pdf},
  title = {Explaining and Harnessing Adversarial Examples},
  url = {https://arxiv.org/abs/1412.6572},
  year = {2014}
}

@inproceedings{Kingma2013,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. We show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  author = {Kingma, Diederik P. and Welling, Max},
  booktitle = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  note = {arXiv:1312.6114},
  openalex = {W2963143768},
  pdf = {https://arxiv.org/pdf/1312.6114.pdf},
  publisher = {OpenReview.net},
  title = {Auto-Encoding Variational Bayes},
  url = {https://openreview.net/forum?id=33X9fd2-9FyZd},
  year = {2014}
}

@inproceedings{Lin2013,
  abstract = {We propose a novel deep network structure called ``Network In Network'' (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  booktitle = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {1312.4400},
  eprinttype = {arXiv},
  openalex = {W2124486237},
  pdf = {https://arxiv.org/pdf/1312.4400.pdf},
  title = {Network in Network},
  url = {http://arxiv.org/abs/1312.4400},
  year = {2014}
}

@misc{Mirza2014,
  abstract = {Generative Adversarial Nets were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. The conditional model can generate MNIST digits conditioned on class labels. We also show that this model could be extended to learn a multi-modal model, and we provide preliminary examples of an application to image tagging in which we demonstrate the ability of the model to generate descriptive tags which are not part of training labels.},
  author = {Mehdi Mirza and Simon Osindero},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  month = {11},
  openalex = {W2125389028},
  pdf = {https://arxiv.org/pdf/1411.1784.pdf},
  title = {Conditional Generative Adversarial Nets},
  url = {https://arxiv.org/abs/1411.1784},
  year = {2014}
}

@inproceedings{Montufar2014,
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  author = {Montúfar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle = {Advances in Neural Information Processing Systems},
  note = {arXiv:1402.1869},
  openalex = {W2951603627},
  pages = {2924--2932},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2014/file/fa6f2a469cc4d61a92d96e74617c3d2a-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {On the Number of Linear Regions of Deep Neural Networks},
  volume = {27},
  year = {2014}
}

@inproceedings{Rezende2014,
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. The algorithm introduces a recognition model to represent approximate posterior distributions, and a novel stochastic backpropagation algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  address = {Beijing, China},
  author = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  number = {2},
  openalex = {W1909320841},
  pages = {1278--1286},
  pdf = {https://proceedings.mlr.press/v32/rezende14.pdf},
  publisher = {PMLR},
  series = {ICML'14},
  title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  url = {https://proceedings.mlr.press/v32/rezende14.html},
  volume = {32},
  year = {2014}
}

@inproceedings{Sermanet2013,
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  address = {Banff, AB, Canada},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  booktitle = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {1312.6229},
  eprinttype = {arXiv},
  month = {4},
  pdf = {https://arxiv.org/pdf/1312.6229.pdf},
  publisher = {International Conference on Learning Representations},
  title = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
  url = {http://arxiv.org/abs/1312.6229},
  year = {2014}
}

@inproceedings{Simonyan2013,
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first technique generates an image which maximises the class score, thus visualising the notion of the class captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets.},
  archiveprefix = {arXiv},
  author = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  booktitle = {Workshop at International Conference on Learning Representations},
  eprint = {1312.6034},
  pdf = {http://arxiv.org/pdf/1312.6034.pdf},
  primaryclass = {cs.CV},
  title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  url = {http://arxiv.org/abs/1312.6034},
  year = {2014}
}

@article{Srivastava2014,
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  issn = {1532-4435},
  journal = {Journal of Machine Learning Research},
  number = {1},
  openalex = {W2095705004},
  pages = {1929--1958},
  pdf = {https://jmlr.csail.mit.edu/papers/volume15/srivastava14a/srivastava14a.pdf},
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  volume = {15},
  year = {2014}
}

@inproceedings{Szegedy2013,
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error.},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
  booktitle = {2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {1312.6199},
  eprinttype = {arXiv},
  month = {4},
  note = {Test of Time Award recipient at ICLR 2024},
  pdf = {https://arxiv.org/pdf/1312.6199.pdf},
  title = {Intriguing Properties of Neural Networks},
  url = {https://openreview.net/forum?id=kklr_MTHMRQjG},
  year = {2014}
}

@inproceedings{Zeiler2013,
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = {Computer Vision -- ECCV 2014: 13th European Conference on Computer Vision, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I},
  doi = {10.1007/978-3-319-10590-1_53},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  isbn = {978-3-319-10590-1},
  note = {Originally published as arXiv:1311.2901. Honourable Mention for Best Paper Award at ECCV 2014},
  pages = {818--833},
  pdf = {https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Visualizing and Understanding Convolutional Networks},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53},
  volume = {8689},
  year = {2014}
}

@article{Bengio2012,
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  doi = {10.1109/TPAMI.2013.50},
  eprint = {1206.5538},
  eprinttype = {arxiv},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8},
  openalex = {W2163922914},
  pages = {1798--1828},
  pdf = {https://ieeexplore.ieee.org/iel7/34/6541932/06472238.pdf},
  title = {Representation Learning: A Review and New Perspectives},
  volume = {35},
  year = {2013}
}

@techreport{Bengio2013,
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  institution = {arXiv},
  month = {8},
  number = {1308.3432},
  openalex = {W2242818861},
  pdf = {https://arxiv.org/pdf/1308.3432.pdf},
  title = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  year = {2013}
}

@book{Kloeden2013,
  abstract = {The aim of this book is to provide an accessible introduction to stochastic differential equations and their applications together with a systematic presentation of methods available for their numerical solution. This book provides an introduction to stochastic calculus and stochastic differential equations, in both theory and applications, emphasising the numerical methods needed to solve such equations. It assumes of the reader an undergraduate background in mathematical methods typical of engineers and physicists, though many chapters begin with a descriptive summary. The stochastic Taylor expansion provides the basis for the discrete time numerical methods for stochastic differential equations. The book presents many new results on high-order methods for strong sample path approximations and for weak functional approximations, including implicit, predictor-corrector, extrapolation and variance-reduction methods.},
  address = {Berlin, Heidelberg},
  author = {Kloeden, Peter E. and Platen, Eckhard},
  doi = {10.1007/978-3-662-12616-5},
  edition = {Corrected Softcover Reprint of the Original 1st Edition 1995},
  isbn = {9783540540625},
  openalex = {W1575104486},
  pages = {xxxv, 632},
  publisher = {Springer},
  series = {Stochastic Modelling and Applied Probability},
  title = {Numerical Solution of Stochastic Differential Equations},
  volume = {23},
  year = {2013}
}

@inproceedings{Mikolov2013,
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  author = {Mikolov, Tomás and Chen, Kai and Corrado, Greg S. and Dean, Jeffrey},
  booktitle = {1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  editor = {Bengio, Yoshua and LeCun, Yann},
  month = {5},
  openalex = {W2950577311},
  pages = {1--12},
  pdf = {https://arxiv.org/pdf/1301.3781.pdf},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://arxiv.org/abs/1301.3781},
  year = {2013}
}

@inproceedings{Sutskever2013,
  abstract = {Deep and recurrent neural networks (DNNs and RNNs, respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
  address = {Atlanta, Georgia, USA},
  author = {Sutskever, Ilya and Martens, James and Dahl, George E. and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  month = {6},
  pages = {1139--1147},
  pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the importance of initialization and momentum in deep learning},
  url = {https://proceedings.mlr.press/v28/sutskever13.html},
  volume = {28},
  year = {2013}
}

@misc{Hinton2012,
  abstract = {A comprehensive course on neural networks that use learning algorithms inspired by understanding of how the brain learns. Covers topics including network architectures, perceptrons, backpropagation, optimization techniques, sequence modeling, object recognition, Hopfield networks, Boltzmann machines, belief networks, deep belief networks, and autoencoders. Consists of 16 lecture series with approximately 70 video lectures total.},
  author = {Hinton, Geoffrey E.},
  howpublished = {Online course},
  institution = {University of Toronto},
  note = {Originally offered on Coursera, now discontinued but materials available via University of Toronto},
  publisher = {Coursera},
  title = {Neural Networks for Machine Learning},
  url = {https://www.cs.toronto.edu/~hinton/coursera_lectures.html},
  year = {2012}
}

@inproceedings{Krizhevsky2012,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7% and 18.9% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, the authors used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers they employed a recently-developed regularization method called dropout that proved to be very effective.},
  address = {Red Hook, NY},
  author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  booktitle = {Advances in Neural Information Processing Systems 25},
  openalex = {W2163605009},
  pages = {1097--1105},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012}
}

@inproceedings{Martens2012,
  abstract = {We develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show experimentally that CP works well in practice, giving very accurate estimates of the Hessian of neural networks with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.},
  address = {Edinburgh, Scotland, UK},
  author = {James Martens and Ilya Sutskever and Kevin Swersky},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning},
  month = {6},
  note = {arXiv:1206.6464},
  openalex = {W2951252698},
  pdf = {https://icml.cc/2012/papers/866.pdf},
  publisher = {icml.cc},
  series = {ICML '12},
  title = {Estimating the Hessian by Back-propagating Curvature},
  url = {https://www.cs.toronto.edu/~jmartens/docs/Curvature_Propagation.pdf},
  year = {2012}
}

@inproceedings{Schuster2012,
  abstract = {This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. The techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how dictionaries, language and acoustic models are built in this framework. Additionally, it shows how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in parts became the default process for internationalization of voice search.},
  address = {Kyoto, Japan},
  author = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  doi = {10.1109/ICASSP.2012.6289079},
  month = {3},
  pages = {5149--5152},
  pdf = {https://research.google.com/pubs/archive/37842.pdf},
  publisher = {IEEE},
  title = {Japanese and Korean voice search},
  year = {2012}
}

@article{Duchi2011,
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  author = {John Duchi and Elad Hazan and Yoram Singer},
  journal = {Journal of Machine Learning Research},
  openalex = {W2146502635},
  pages = {2121--2159},
  pdf = {https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  url = {https://jmlr.org/papers/v12/duchi11a.html},
  volume = {12},
  year = {2011}
}

@article{Vincent2011,
  abstract = {We show that the training criterion of denoising autoencoders, previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture, is equivalent to matching the score (i.e., the gradient of the log-density) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that does not require computing second derivatives and is related to learning to denoise. It justifies the use of tied weights between encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
  author = {Vincent, Pascal},
  doi = {10.1162/NECO_a_00142},
  journal = {Neural Computation},
  month = {7},
  number = {7},
  pages = {1661--1674},
  pdf = {https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf},
  publisher = {MIT Press},
  title = {A Connection Between Score Matching and Denoising Autoencoders},
  volume = {23},
  year = {2011}
}

@inproceedings{Welling2011,
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a sampling threshold and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  address = {Bellevue, Washington, USA},
  author = {Welling, Max and Teh, Yee Whye},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning},
  editor = {Getoor, Lise and Scheffer, Tobias},
  isbn = {978-1-4503-0619-5},
  month = {6},
  openalex = {W2100495488},
  pages = {681--688},
  pdf = {https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf},
  publisher = {Omnipress},
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
  url = {https://dl.acm.org/doi/10.5555/3104482.3104568},
  year = {2011}
}

@inproceedings{Bottou2010,
  abstract = {During the last decade, data sizes have grown faster than processor speeds. In this context, statistical machine learning methods are limited by computing time rather than sample size. The analysis reveals qualitatively different tradeoffs for small-scale and large-scale problems, involving computational complexity of optimization algorithms in non-trivial ways.},
  address = {Paris, France},
  author = {Bottou, Léon},
  booktitle = {Proceedings of COMPSTAT'2010: 19th International Conference on Computational Statistics},
  doi = {10.1007/978-3-7908-2604-3_16},
  editor = {Lechevallier, Yves and Saporta, Gilbert},
  isbn = {978-3-7908-2603-6},
  month = {8},
  openalex = {W114517082},
  pages = {177--186},
  pdf = {https://leon.bottou.org/publications/pdf/compstat-2010.pdf},
  publisher = {Physica-Verlag HD},
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  year = {2010}
}

@inproceedings{Gutmann2010,
  abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: we show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
  author = {Michael U. Gutmann and Aapo Hyvärinen},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  editor = {Yee Whye Teh and Mike Titterington},
  openalex = {W2152790380},
  pages = {297--304},
  pdf = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  url = {https://proceedings.mlr.press/v9/gutmann10a.html},
  volume = {9},
  year = {2010}
}

@inproceedings{Deng2009,
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical question. We introduce ImageNet, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images.},
  address = {Miami, FL, USA},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2009.5206848},
  isbn = {978-1-4244-3992-8},
  month = {6},
  openalex = {W2108598243},
  pages = {248--255},
  pdf = {https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf},
  publisher = {IEEE},
  title = {ImageNet: A Large-Scale Hierarchical Image Database},
  year = {2009}
}

@book{Hastie2009,
  address = {New York, NY},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  doi = {10.1007/978-0-387-84858-7},
  edition = {Second},
  isbn = {978-0-387-84858-7},
  openalex = {W2073404525},
  pages = {xxii + 745},
  pdf = {http://web.stanford.edu/~hastie/Papers/ESLII_print5.pdf},
  publisher = {Springer},
  series = {Springer Series in Statistics},
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  year = {2009}
}

@book{Hyvarinen2009,
  abstract = {This book is both an introductory textbook and a research monograph on modeling the statistical structure of natural images. In very simple terms, natural images are photographs of the typical environment where we live. In this book, their statistical structure is described using a number of statistical models whose parameters are estimated from image samples. This book is the first comprehensive introduction to the multidisciplinary field of natural image statistics. The main motivation for exploring natural image statistics is computational modeling of biological visual systems. A theoretical framework which is gaining more and more support considers the properties of the visual system to be reflections of the statistical structure of natural images because of evolutionary adaptation processes. The book is targeted for advanced undergraduate students, graduate students and researchers in vision science, computational neuroscience, computer vision and image processing.},
  author = {Hyvärinen, Aapo and Hurri, Jarmo and Hoyer, Patrick O.},
  doi = {10.1007/978-1-84882-491-1},
  isbn = {978-1-84882-490-4},
  isbn-13 = {978-1-84882-491-1},
  month = {4},
  openalex = {W1533072162},
  pages = {XIX, 448},
  publisher = {Springer-Verlag London},
  series = {Computational Imaging and Vision},
  title = {Natural Image Statistics: A Probabilistic Approach to Early Computational Vision},
  volume = {39},
  year = {2009}
}

@book{Griewank2008,
  abstract = {Algorithmic, or automatic, differentiation (AD) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. AD has been applied in particular to optimization, parameter identification, nonlinear equation solving, the numerical integration of differential equations, and combinations of these. This second edition has been updated and expanded to cover recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity, a generalization of sparsity. There is also added material on checkpointing and iterative differentiation.},
  address = {Philadelphia, PA},
  author = {Griewank, Andreas and Walther, Andrea},
  doi = {10.1137/1.9780898717761},
  edition = {Second},
  eisbn = {978-0-89871-776-1},
  isbn = {978-0-89871-659-7},
  number = {105},
  pages = {xxi + 426},
  publisher = {Society for Industrial and Applied Mathematics (SIAM)},
  series = {Other Titles in Applied Mathematics},
  title = {Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9780898717761},
  year = {2008}
}

@inproceedings{Vincent2008,
  abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
  address = {Helsinki, Finland},
  author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  doi = {10.1145/1390156.1390294},
  isbn = {978-1-60558-205-4},
  month = {7},
  openalex = {W2124273309},
  pages = {1096--1103},
  pdf = {https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf},
  publisher = {ACM},
  series = {ICML '08},
  title = {Extracting and Composing Robust Features with Denoising Autoencoders},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390294},
  year = {2008}
}

@incollection{LeCun2006,
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu-Jie},
  booktitle = {Predicting Structured Data},
  editor = {Bakir, Gökhan H. and Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J. and Taskar, Ben},
  isbn = {978-0-262-02617-8},
  openalex = {W2161914416},
  pages = {1--21},
  publisher = {MIT Press},
  title = {A Tutorial on Energy-Based Learning},
  url = {https://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf},
  year = {2007}
}

@book{Bishop2006,
  abstract = {This leading textbook provides a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions and covers topics including probability distributions, linear models for regression and classification, neural networks, kernel methods, sparse kernel machines, graphical models, mixture models and EM, approximate inference, sampling methods, continuous latent variables, sequential data, and combining models.},
  address = {New York},
  author = {Bishop, Christopher M.},
  isbn = {978-0-387-31073-2},
  openalex = {W2318752494},
  pages = {xx + 738},
  publisher = {Springer},
  series = {Information Science and Statistics},
  title = {Pattern Recognition and Machine Learning},
  year = {2006}
}

@article{Fawcett2006,
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  author = {Tom Fawcett},
  doi = {10.1016/j.patrec.2005.10.010},
  journal = {Pattern Recognition Letters},
  number = {8},
  openalex = {W2158698691},
  pages = {861--874},
  publisher = {Elsevier BV},
  title = {An Introduction to ROC Analysis},
  volume = {27},
  year = {2006}
}

@inproceedings{Lasserre2006,
  abstract = {When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by 'training them discriminatively', they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedures have been proposed which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a 'discriminatively trained' generative model is fundamentally a new model. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of 'discriminative training', this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition.},
  address = {New York, NY, USA},
  author = {Lasserre, Julia A. and Bishop, Christopher M. and Minka, Thomas P.},
  booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)},
  doi = {10.1109/CVPR.2006.227},
  isbn = {0-7695-2597-0},
  month = {6},
  openalex = {W2153939756},
  pages = {87--94},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LasserreBishopMinka06.pdf},
  publisher = {IEEE Computer Society},
  title = {Principled Hybrids of Generative and Discriminative Models},
  volume = {1},
  year = {2006}
}

@article{Hyvarinen2005,
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and Gaussian mixture models, and on a difficult problem of estimating natural image statistics.},
  author = {Hyvärinen, Aapo},
  issn = {1533-7928},
  journal = {Journal of Machine Learning Research},
  month = {4},
  number = {24},
  openalex = {W1505878979},
  pages = {695--709},
  pdf = {https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf},
  publisher = {JMLR.org},
  title = {Estimation of Non-Normalized Statistical Models by Score Matching},
  url = {https://jmlr.org/papers/v6/hyvarinen05a.html},
  volume = {6},
  year = {2005}
}

@book{Zarchan2005,
  abstract = {This text is a practical guide to building Kalman filters and shows how the filtering equations can be applied to real-life problems. Numerous examples are presented in detail, showing the many ways in which Kalman filters can be designed. Computer code written in FORTRAN, MATLAB, and True BASIC accompanies all of the examples so that the interested reader can verify concepts and explore issues beyond the scope of the text. The second edition includes two new chapters: a recursive digital filter known as the fading memory filter is introduced and it is shown that for some radar tracking applications the fading memory filter can yield similar performance to a Kalman filter at far less computational cost; and techniques for improving Kalman filter performance are presented. A new appendix serves as a central location and summary for the text's most important concepts and formulas.},
  author = {Zarchan, Paul and Musoff, Howard},
  doi = {10.2514/4.866777},
  edition = {Second},
  isbn = {978-1-56347-694-5},
  publisher = {American Institute of Aeronautics and Astronautics},
  series = {Progress in Astronautics and Aeronautics},
  title = {Fundamentals of Kalman Filtering: A Practical Approach},
  url = {https://arc.aiaa.org/doi/book/10.2514/4.866777},
  volume = {208},
  year = {2005}
}

@phdthesis{Collobert2004,
  author = {Collobert, Ronan},
  month = {6},
  note = {Supervisor: Patrick Gallinari (UPMC), Co-supervisor: Samy Bengio (IDIAP)},
  pdf = {https://ronan.collobert.com/pub/2004_phdthesis_lip6.pdf},
  school = {Université Paris VI},
  title = {Large Scale Machine Learning},
  year = {2004}
}

@book{Hartley2004,
  abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features expanded coverage of the latest developments, over 1000 new citations to the literature, and a new chapter on multi-view reconstruction. An algorithmic approach is taken throughout, with theory supported by practical examples and working code. The book is intended for students and researchers in computer vision, and also for practitioners in the entertainment industry.},
  author = {Hartley, Richard I. and Zisserman, Andrew},
  edition = {Second},
  isbn = {0521540518},
  month = {3},
  openalex = {W2209124607},
  pages = {672},
  pdf = {https://www.robots.ox.ac.uk/~vgg/hzbook/},
  publisher = {Cambridge University Press},
  title = {Multiple View Geometry in Computer Vision},
  url = {https://www.cambridge.org/core/books/multiple-view-geometry-in-computer-vision/0B6F289C78B2B23F596CAA76D3D43F7A},
  year = {2004}
}

@book{Jebara2004,
  abstract = {Machine Learning: Discriminative and Generative covers the main contemporary themes and tools in machine learning ranging from Bayesian probabilistic models to discriminative support-vector machines. Unlike previous books that only discuss these rather different approaches in isolation, it bridges the two schools of thought together within a common framework, elegantly connecting their various theories and making one common big-picture. This bridge brings forth new hybrid discriminative-generative tools that combine the strengths of both camps. The book is designed for researchers and practitioners in industry and academia, and is also suitable as a secondary text for graduate-level students in computer science and engineering.},
  address = {Boston, MA},
  author = {Jebara, Tony},
  isbn = {978-1-4020-7647-3},
  pages = {217},
  publisher = {Kluwer Academic Publishers},
  series = {The Springer International Series in Engineering and Computer Science},
  title = {Machine Learning: Discriminative and Generative},
  volume = {755},
  year = {2004}
}

@book{Nesterov2004,
  abstract = {This is the first elementary exposition of the main ideas of complexity theory for convex optimization. Most of the material was previously available only in special journals and research monographs. The book covers optimal methods and lower complexity bounds for smooth and non-smooth convex optimization, with a separate chapter devoted to polynomial-time interior-point methods. The monograph emerged from the seminal developments in the mid-1980s when Karmarkar's breakthrough paper opened a new epoch in nonlinear optimization, introducing polynomial-time algorithms for linear optimization that were both theoretically efficient and computationally excellent.},
  address = {New York},
  author = {Nesterov, Yurii},
  doi = {10.1007/978-1-4419-8853-9},
  isbn = {978-1-4020-7553-7},
  pages = {xviii + 236},
  publisher = {Springer},
  series = {Applied Optimization},
  title = {Introductory Lectures on Convex Optimization: A Basic Course},
  url = {https://link.springer.com/book/10.1007/978-1-4419-8853-9},
  volume = {87},
  year = {2004}
}

@book{Stone2004,
  abstract = {This is a tutorial-style introduction to a class of methods for extracting independent signals from a mixture of signals originating from different physical sources, and includes MATLAB computer code examples. Independent component analysis (ICA) is becoming an increasingly important tool for analyzing large data sets. In essence, ICA separates an observed set of signal mixtures into a set of statistically independent component signals, or source signals. The applications for ICA range from speech processing, brain imaging, and electrical brain signals to telecommunications and stock predictions. Topics covered include the geometry of mixing and unmixing; methods for blind source separation; and applications of ICA, including voice mixtures, EEG, fMRI, and fetal heart monitoring. The appendixes provide a vector matrix tutorial, plus basic demonstration computer code that allows the reader to see how each mathematical method described in the text translates into working MATLAB computer code.},
  address = {Cambridge, MA},
  author = {James V. Stone},
  isbn = {9780262693158},
  openalex = {W2099917186},
  pages = {193},
  publisher = {MIT Press},
  series = {Bradford Books},
  title = {Independent Component Analysis: A Tutorial Introduction},
  year = {2004}
}

@article{Ahn2003,
  abstract = {We propose a constrained EM algorithm for principal component analysis (PCA) using a coupled probability model derived from single-standard factor analysis models with isotropic noise structure. The single probabilistic PCA, especially for the case where there is no noise, can find only a vector set that is a linear superposition of principal components and requires postprocessing, such as diagonalization of symmetric matrices. By contrast, the proposed algorithm finds the actual principal components, which are sorted in descending order of eigenvalue size and require no additional calculation or postprocessing. The method is easily applied to kernel PCA. It is also shown that the new EM algorithm is derived from a generalized least-squares formulation.},
  author = {Jong-Hoon Ahn and Jong-Hoon Oh},
  doi = {10.1162/089976603321043694},
  journal = {Neural Computation},
  month = {1},
  number = {1},
  openalex = {W2168321118},
  pages = {57--65},
  publisher = {MIT Press},
  title = {A constrained EM algorithm for principal component analysis},
  volume = {15},
  year = {2003}
}

@article{Chan2003,
  abstract = {Missing data are common in real-world data sets and are a problem for many estimation techniques. We have developed a variational Bayesian method to perform independent component analysis (ICA) on high-dimensional data containing missing entries. Missing data are handled naturally in the Bayesian framework by integrating the generative density model. Modeling the distributions of the independent sources with mixture of gaussians allows sources to be estimated with different kurtosis and skewness. Unlike the maximum likelihood approach, the variational Bayesian method automatically determines the dimensionality of the data and yields an accurate density model for the observed data without overfitting problems.},
  author = {Chan, Kwokleung and Lee, Te-Won and Sejnowski, Terrence J.},
  doi = {10.1162/08997660360675116},
  journal = {Neural Computation},
  month = {8},
  number = {8},
  openalex = {W2139111279},
  pages = {1991--2011},
  pdf = {https://direct.mit.edu/neco/article-pdf/15/8/1991/815682/08997660360675116.pdf},
  title = {Variational Bayesian Learning of ICA with Missing Data},
  volume = {15},
  year = {2003}
}

@article{Choudrey2003,
  abstract = {There has been growing interest in subspace data modeling over the past few years. Methods such as principal component analysis, factor analysis, and independent component analysis have gained in popularity and have found many applications in image modeling, signal processing, and data compression. As applications and computing power grow, more and more sophisticated analyses and meaningful representations are sought. Mixture modeling methods have been proposed for principal and factor analyzers that exploit local gaussian features in the subspace manifolds. Meaningful representations may be lost, however, if these local features are nongaussian or discontinuous. In this article, we propose extending the gaussian analyzers mixture model to an independent component analyzers mixture model. We employ recent developments in variational Bayesian inference and structure determination to construct a novel approach for modeling nongaussian, discontinuous manifolds. Our method automatically determines the local dimensionality of each manifold and uses variational inference to calculate the optimum number of ICA components needed in the mixture model. We demonstrate our framework on complex synthetic data and illustrate its application to real data by decomposing functional magnetic resonance images into meaningful and medically useful features.},
  author = {Choudrey, Rizwan A. and Roberts, Stephen J.},
  doi = {10.1162/089976603321043766},
  journal = {Neural Computation},
  month = {1},
  number = {1},
  openalex = {W2131067207},
  pages = {213--252},
  pmid = {12590826},
  title = {Variational mixture of Bayesian independent component analyzers},
  volume = {15},
  year = {2003}
}

@book{Forsyth2003,
  address = {Upper Saddle River, NJ},
  author = {Forsyth, David A. and Ponce, Jean},
  edition = {First},
  isbn = {9780130851987},
  pages = {693},
  publisher = {Prentice Hall},
  title = {Computer Vision: A Modern Approach},
  year = {2003}
}

@book{MacKay2003,
  abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The book uniquely covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes.},
  address = {Cambridge, UK},
  author = {MacKay, David J. C.},
  isbn = {9780521642989},
  month = {October},
  openalex = {W1483310858},
  pages = {640},
  pdf = {https://www.inference.org.uk/itprnn/book.pdf},
  publisher = {Cambridge University Press},
  title = {Information Theory, Inference and Learning Algorithms},
  year = {2003}
}

@inproceedings{Simard2003,
  abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple ``do-it-yourself'' implementation of convolution with a flexible architecture is suitable for many visual document problems.},
  address = {Edinburgh, Scotland},
  author = {Patrice Y. Simard and Dave Steinkraus and John C. Platt},
  booktitle = {Proceedings of the Seventh International Conference on Document Analysis and Recognition},
  doi = {10.1109/ICDAR.2003.1227801},
  month = {August},
  openalex = {W2156163116},
  pages = {958--962},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2003/08/icdar03.pdf},
  publisher = {IEEE Computer Society},
  title = {Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis},
  year = {2003}
}

@article{Bach2002,
  abstract = {We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. We show that these contrast functions are related to mutual information, and have desirable theoretical properties as measures of statistical dependence. Building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. We illustrate with simulations involving a wide variety of source distributions that our algorithms outperform many of the presently known algorithms. We also demonstrate that the performance of our algorithms is not very sensitive to the choice of kernel.},
  author = {Bach, Francis R. and Jordan, Michael I.},
  doi = {10.1162/153244303768966085},
  journal = {Journal of Machine Learning Research},
  month = {7},
  pages = {1--48},
  pdf = {https://jmlr.org/papers/volume3/bach02a/bach02a.pdf},
  title = {Kernel Independent Component Analysis},
  volume = {3},
  year = {2002}
}

@book{Casella2002,
  abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. The book covers all topics from a standard inference course including distributions, random variables, data reduction, point estimation, hypothesis testing, and interval estimation. Primarily aimed at graduate students of statistics, but can be used by advanced undergraduate students majoring in statistics who have a solid mathematics background.},
  address = {Pacific Grove, CA},
  author = {George Casella and Roger L. Berger},
  edition = {Second},
  isbn = {9780534243128},
  pages = {xviii + 650},
  publisher = {Duxbury Press},
  title = {Statistical Inference},
  year = {2002}
}

@article{Hinton2002,
  abstract = {It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual ``expert'' models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called ``contrastive divergence'' whose derivatives with regard to the parameters can be approximated accurately and efficiently.},
  author = {Hinton, Geoffrey E.},
  doi = {10.1162/089976602760128018},
  journal = {Neural Computation},
  month = {8},
  number = {8},
  openalex = {W2116064496},
  pages = {1771--1800},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf},
  title = {Training Products of Experts by Minimizing Contrastive Divergence},
  url = {https://doi.org/10.1162/089976602760128018},
  volume = {14},
  year = {2002}
}

@article{HojenSorensen2002,
  abstract = {We develop mean-field approaches for probabilistic independent component analysis (ICA). The sources are estimated from the mean of their posterior distribution and the mixing matrix (and noise level) is estimated by maximum a posteriori (MAP). The latter requires the computation of (a good approximation to) the correlations between sources. For this purpose, we investigate three increasingly advanced mean-field methods: the variational (also known as naive mean field) approach, linear response corrections, and an adaptive version of the Thouless, Anderson and Palmer (1977) (TAP) mean-field approach, which is due to Opper and Winther (2001). The resulting algorithms are tested on a number of problems. On synthetic data, the advanced mean-field approaches are able to recover the correct mixing matrix in cases where the variational mean-field theory fails. For handwritten digits, sparse encoding is achieved using nonnegative source and mixing priors. For speech, the mean-field method is able to separate in the underdetermined (overcomplete) case of two sensors and three sources. One major advantage of the proposed method is its generality and algorithmic simplicity. Finally, we point out several possible extensions of the approaches developed here.},
  author = {Højen-Sørensen, Pedro A. d. F. R. and Winther, Ole and Hansen, Lars Kai},
  doi = {10.1162/089976602317319009},
  journal = {Neural Computation},
  month = {4},
  number = {4},
  pages = {889--918},
  pdf = {https://direct.mit.edu/neco/article-pdf/14/4/889/815139/089976602317319009.pdf},
  pmid = {11936966},
  publisher = {MIT Press},
  title = {Mean-Field Approaches to Independent Component Analysis},
  volume = {14},
  year = {2002}
}

@book{Jolliffe2002,
  address = {New York},
  author = {Jolliffe, I. T.},
  doi = {10.1007/b98835},
  edition = {Second},
  isbn = {9780387954424},
  pages = {518},
  publisher = {Springer-Verlag},
  series = {Springer Series in Statistics},
  title = {Principal Component Analysis},
  year = {2002}
}

@book{Scholkopf2002,
  abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs---kernels---for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. This book provides a comprehensive introduction to SVMs for pattern recognition, regression, and novelty detection, including recent advances and extensions.},
  author = {Schölkopf, Bernhard and Smola, Alexander J.},
  isbn = {9780262194754},
  month = {12},
  openalex = {W1510322235},
  pages = {626},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  year = {2002}
}

@inproceedings{Hinton2001,
  abstract = {This paper presents a novel perspective on Independent Component Analysis (ICA) by interpreting it as a causal generative model and an energy-based density model. The authors argue that ICA differs from traditional factor analysis through its use of non-Gaussian priors over hidden activities, restoring tractability by eliminating sensor noise. The energy-based framework is explored to suggest potential generalizations of the ICA algorithm that maintain computational efficiency.},
  address = {San Diego, California, USA},
  author = {Geoffrey E. Hinton and Max Welling and Yee Whye Teh and Simon Osindero},
  booktitle = {Proceedings of the Third International Conference on Independent Component Analysis and Signal Separation},
  editor = {Te-Won Lee and Scott Makeig and Terrence J. Sejnowski},
  month = {12},
  openalex = {W1654789990},
  pages = {746--751},
  pdf = {https://www.academia.edu/326401/A_New_View_of_ICA},
  series = {ICA 2001},
  title = {A New View of ICA},
  year = {2001}
}

@incollection{Miskin2001,
  abstract = {This chapter applies ensemble learning to the problem of blind source separation and deconvolution of images. The approach assumes that the observed images were constructed by mixing a set of images consisting of independent, identically distributed pixels, convolving the mixtures with unknown blurring filters and then adding Gaussian noise. The ensemble learning methodology provides a principled Bayesian approach to handle the uncertainty in both the source signals and the mixing process, making it particularly suitable for blind separation tasks where traditional approaches may fail to preserve important constraints such as pixel intensity positivity.},
  address = {Cambridge},
  author = {Miskin, James W. and MacKay, David J. C.},
  booktitle = {Independent Component Analysis: Principles and Practice},
  editor = {Roberts, Stephen J. and Everson, Richard M.},
  isbn = {0-521-79298-3},
  pages = {209--233},
  publisher = {Cambridge University Press},
  title = {Ensemble Learning for Blind Source Separation},
  year = {2001}
}

@book{Bather2000,
  address = {Chichester},
  author = {John Bather},
  edition = {First},
  isbn = {978-0-471-97649-3},
  month = {7},
  pages = {x + 191},
  publisher = {John Wiley & Sons},
  title = {Decision Theory: An Introduction to Dynamic Programming and Sequential Decisions},
  year = {2000}
}

@book{Mardia2000,
  abstract = {Presents new and up-to-date material on both the underlying theory and the practical methodology of directional statistics. The book is divided into three parts: the first concentrates on statistics on the circle, including tests of uniformity, tests of goodness-of-fit, inference on von Mises distributions and non-parametric methods; the second considers statistics on spheres of arbitrary dimension with detailed coverage of inference on the main distributions on spheres; and the third considers statistics on more general sample spaces, particularly rotation groups, Stiefel manifolds, Grassmann manifolds and complex projective spaces. Recent material on correlation, regression time series, robust techniques, bootstrap methods, density estimation and curve fitting is presented.},
  address = {Chichester},
  author = {Mardia, K. V. and Jupp, P. E.},
  doi = {10.1002/9780470316979},
  edition = {Second},
  isbn = {978-0-471-95333-3},
  pages = {xii + 429},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Directional Statistics},
  year = {2000}
}

@book{McLachlan2000,
  address = {New York},
  author = {McLachlan, Geoffrey J. and Peel, David},
  doi = {10.1002/0471721182},
  edition = {First},
  isbn = {978-0-471-00626-8},
  note = {More than 800 references and appendix listing available mixture software},
  pages = {464},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Finite Mixture Models},
  year = {2000}
}

@incollection{Opper2000,
  abstract = {We elaborate on the well-known relationship between Gaussian processes (GP) and Support Vector Machines (SVM), and present approximate solutions for two computational problems. The first is the calculation of the posterior mean for GP classifiers using a naive mean field approach based on the TAP approach originally proposed in statistical physics of disordered systems. The second is a leave-one-out estimator for the generalization error of SVM based on a linear response method. From the TAP approach, it is possible to derive both a simpler naive mean field theory and support vector machines (SVM) as limiting cases. Simulation results on benchmark datasets show similar performances for the GP mean field algorithm and the SVM algorithm, with the approximate leave-one-out estimator being in very good agreement with the exact leave-one-out error.},
  address = {Cambridge, MA},
  author = {Opper, Manfred and Winther, Ole},
  booktitle = {Advances in Large Margin Classifiers},
  editor = {Smola, Alexander J. and Bartlett, Peter L. and Schölkopf, Bernhard and Schuurmans, Dale},
  isbn = {0262194481},
  keywords = {gaussian processes, support vector machines, mean field theory, TAP approach, statistical physics, leave-one-out estimation},
  pages = {43--65},
  publisher = {MIT Press},
  series = {Neural Information Processing},
  title = {Gaussian processes and SVM: Mean field results and leave-one-out},
  year = {2000}
}

@article{Attias1999,
  abstract = {We introduce a method for recovering independent hidden sources from their observed mixtures, called independent factor analysis (IFA). IFA generalizes and unifies ordinary factor analysis (FA), principal component analysis (PCA), and independent component analysis (ICA), and can handle not only square noiseless mixing but also the general case where the number of mixtures differs from the number of sources and the data are noisy. IFA is a two-step procedure. In the first step, the source densities, mixing matrix, and noise covariance are estimated from the observed data by maximum likelihood. We present an expectation-maximization (EM) algorithm, which performs unsupervised learning of an associated probabilistic model of the mixing situation.},
  author = {Attias, Hagai},
  doi = {10.1162/089976699300016458},
  journal = {Neural Computation},
  month = {5},
  number = {4},
  pages = {803--851},
  pmid = {10226184},
  publisher = {MIT Press},
  title = {Independent Factor Analysis},
  volume = {11},
  year = {1999}
}

@book{Magnus1999,
  abstract = {This exhaustive, self-contained book on matrix theory and matrix differential calculus provides a treatment of matrix calculus based on differentials and shows how easy it is to use this theory once you have mastered the technique. Jan Magnus, who, along with the late Heinz Neudecker, pioneered the theory, develops it further in this revised edition and provides many examples along the way to support it. Matrix calculus has become an essential tool for quantitative methods in a large number of applications, ranging from social and behavioral sciences to econometrics. The book is divided into six parts, beginning with a treatment of matrix algebra, discussing the Schur, Jordan, and singular-value decompositions, the Hadamard and Kronecker products, and more. The second section is the theoretical core of the book and presents a thorough development of the theory of differentials.},
  address = {Chichester},
  author = {Magnus, Jan R. and Neudecker, Heinz},
  edition = {Revised},
  isbn = {9780471986331},
  pages = {424},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Matrix Differential Calculus with Applications in Statistics and Econometrics},
  year = {1999}
}

@book{Mallat1999,
  abstract = {This book is intended to serve as an invaluable reference for anyone concerned with the application of wavelets to signal processing. It has evolved from material used to teach wavelet signal processing courses in electrical engineering departments at Massachusetts Institute of Technology and Tel Aviv University, as well as applied mathematics departments at the Courant Institute of New York University and École Polytechnique in Paris. The book provides a broad perspective on the principles and applications of transient signal processing with wavelets, emphasizing intuitive understanding, while providing the mathematical foundations and description of fast algorithms.},
  address = {San Diego, CA},
  author = {Mallat, Stéphane},
  edition = {Second},
  isbn = {9780124666061},
  month = {9},
  openalex = {W2115755118},
  pages = {620},
  publisher = {Academic Press},
  series = {Wavelet Analysis & Its Applications},
  title = {A Wavelet Tour of Signal Processing},
  url = {https://www.oreilly.com/library/view/a-wavelet-tour/9780124666061/},
  year = {1999}
}

@incollection{NealHinton1999,
  abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
  address = {Cambridge, MA, USA},
  author = {Neal, Radford M. and Hinton, Geoffrey E.},
  booktitle = {Learning in Graphical Models},
  doi = {10.1007/978-94-011-5014-9_12},
  editor = {Jordan, Michael I.},
  isbn = {9780262600323},
  pages = {355--368},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/emk.pdf},
  publisher = {MIT Press},
  title = {A View of the EM Algorithm That Justifies Incremental, Sparse, and Other Variants},
  year = {1999}
}

@book{Nocedal1999,
  address = {New York},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  doi = {10.1007/b98874},
  edition = {First},
  isbn = {978-0-387-98793-4},
  isbn10 = {0-387-98793-2},
  pages = {656},
  publisher = {Springer},
  series = {Springer Series in Operations Research and Financial Engineering},
  title = {Numerical Optimization},
  url = {https://doi.org/10.1007/b98874},
  year = {1999}
}

@book{Robert1999,
  abstract = {Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This textbook provides a thorough introduction to Monte Carlo methods in statistics with an emphasis on Markov chain Monte Carlo (MCMC) methods. The book is self-contained and does not assume prior knowledge of simulation or Markov chains. It is intended for a second year graduate course, but will also be useful to practitioners who want to apply simulation techniques for the resolution of practical problems or wish to grasp the fundamental principles behind those methods.},
  address = {New York},
  author = {Robert, Christian P. and Casella, George},
  doi = {10.1007/978-1-4757-3071-5},
  isbn = {978-0-387-98707-1},
  openalex = {W4292403327},
  pages = {xvi + 507},
  publisher = {Springer-Verlag},
  series = {Springer Texts in Statistics},
  title = {Monte Carlo Statistical Methods},
  year = {1999}
}

@article{Roweis1999,
  abstract = {Factor analysis, principal component analysis (PCA), mixtures of Gaussian clusters, vector quantization (VQ), Kalman filter models and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities we show how independent component analysis (ICA) is also a variation of the same basic generative model. We show that factor analysis and mixtures of Gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data known as sensible principal component analysis (SPCA) as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudo-code for inference and learning for all the basic models.},
  author = {Roweis, Sam and Ghahramani, Zoubin},
  doi = {10.1162/089976699300016674},
  journal = {Neural Computation},
  month = {2},
  number = {2},
  pages = {305--345},
  pdf = {https://direct.mit.edu/neco/article-pdf/11/2/305/814052/089976699300016674.pdf},
  publisher = {MIT Press},
  title = {A Unifying Review of Linear Gaussian Models},
  url = {https://direct.mit.edu/neco/article/11/2/305/6249/A-Unifying-Review-of-Linear-Gaussian-Models},
  volume = {11},
  year = {1999}
}

@article{Tipping1999,
  abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
  author = {Tipping, Michael E. and Bishop, Christopher M.},
  doi = {10.1111/1467-9868.00196},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  number = {3},
  pages = {611--622},
  pdf = {http://www.miketipping.com/papers/met-mppca.pdf},
  publisher = {Wiley},
  title = {Probabilistic Principal Component Analysis},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00196},
  volume = {61},
  year = {1999}
}

@book{Vidakovic1999,
  abstract = {A comprehensive, step-by-step introduction to wavelets in statistics. This book provides a much-needed introduction to the latest tools afforded statisticians by wavelet theory, compiling, organizing, and explaining in depth research data previously available only in disparate journal articles. The book covers both introductory and data-oriented modeling topics, including continuous and discrete wavelet transformations, statistical optimality properties of wavelet shrinkage, theoretical aspects of wavelet density estimation, Bayesian modeling in the wavelet domain, properties of wavelet-based random functions and densities, several novel and important wavelet applications in statistics, and wavelet methods in time series.},
  address = {New York, NY, USA},
  author = {Brani Vidakovic},
  isbn = {9780471293651},
  month = {5},
  pages = {408},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Statistical Modeling by Wavelets},
  year = {1999}
}

@book{Weisstein1999,
  abstract = {A compendium of mathematical definitions, formulas, figures, tabulations, and references presented in a dictionary format with an informal style that makes it accessible to readers with diverse mathematical backgrounds. The encyclopedia contains thousands of explicit examples, formulas, and derivations, providing comprehensive coverage through its highly readable text that diverges from specialized jargon and dry formal exposition. Originally developed as a web-based resource before publication, the work connects mathematical concepts to other areas of mathematics and science while giving readers a flavor of subjects without getting lost in minutiae.},
  address = {Boca Raton, FL},
  author = {Weisstein, Eric W.},
  edition = {First},
  isbn = {0-8493-9640-9},
  note = {Originally based on the author's web encyclopedia MathWorld},
  openalex = {W3083083886},
  pages = {1969},
  publisher = {Chapman & Hall/CRC},
  title = {CRC Concise Encyclopedia of Mathematics},
  url = {https://mathworld.wolfram.com/},
  year = {1999}
}

@article{Cardoso1998,
  abstract = {Blind signal separation (BSS) and independent component analysis (ICA) are emerging techniques of array processing and data analysis that aim to recover unobserved signals or sources from observed mixtures (typically, the output of an array of sensors), exploiting only the assumption of mutual independence between the signals.},
  author = {Cardoso, Jean-François},
  doi = {10.1109/5.720250},
  journal = {Proceedings of the IEEE},
  month = {10},
  number = {10},
  pages = {2009--2025},
  pdf = {http://www2.iap.fr/users/cardoso/papers/ProcIEEE.pdf},
  title = {Blind signal separation: statistical principles},
  volume = {86},
  year = {1998}
}

@article{LeCun1998,
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques.},
  author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
  doi = {10.1109/5.726791},
  journal = {Proceedings of the IEEE},
  month = {11},
  number = {11},
  pages = {2278--2324},
  pdf = {http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf},
  publisher = {IEEE},
  title = {Gradient-Based Learning Applied to Document Recognition},
  url = {https://doi.org/10.1109/5.726791},
  volume = {86},
  year = {1998}
}

@incollection{Neal1999,
  abstract = {Markov chain Monte Carlo methods such as Gibbs sampling and simple forms of the Metropolis algorithm typically move about the distribution being sampled via a random walk. For the complex, high-dimensional distributions commonly encountered in Bayesian inference and statistical physics, the distance moved in each iteration of these algorithms will usually be small, because it is difficult or impossible to transform the problem to eliminate dependencies between variables. The inefficiency inherent in taking such small steps is greatly exacerbated when the algorithm operates via a random walk, as in such a case moving to a point n steps away will typically take around n^2 iterations. Such random walks can sometimes be suppressed using ``overrelaxed'' variants of Gibbs sampling (a.k.a. the heatbath algorithm), but such methods have hitherto been largely restricted to problems where all the full conditional distributions are Gaussian. This paper presents an overrelaxed Markov chain Monte Carlo algorithm based on order statistics that is more widely applicable. In particular, the algorithm can be applied whenever the full conditional distributions are such that their cumulative distribution functions and inverse cumulative distribution functions can be efficiently computed. The method is demonstrated on an inference problem for a simple hierarchical Bayesian model.},
  address = {Cambridge, MA},
  author = {Neal, Radford M.},
  booktitle = {Learning in Graphical Models},
  editor = {Jordan, Michael I.},
  isbn = {978-0-262-60032-3},
  note = {Also available as arXiv:bayes-an/9506004},
  pages = {205--225},
  publisher = {MIT Press},
  series = {Adaptive Computation and Machine Learning},
  title = {Suppressing Random Walks in Markov Chain Monte Carlo Using Ordered Over-relaxation},
  url = {https://arxiv.org/abs/bayes-an/9506004},
  year = {1998}
}

@inproceedings{Roweis1998,
  abstract = {We present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data and is computationally very efficient in space and time. It does not require computing or storing the sample covariance matrix. The algorithm naturally accommodates missing information and enjoys all the benefits of other EM algorithms in terms of estimating the maximum likelihood values for missing information. We also introduce a new model called sensible principal component analysis (SPCA) which defines a proper density model in the data space. SPCA can be used to fit data that is intrinsically lower dimensional than the space it is embedded in, but unlike PCA it defines a proper covariance structure in the original data space.},
  author = {Sam T. Roweis},
  booktitle = {Advances in Neural Information Processing Systems 10},
  editor = {Michael I. Jordan and Michael J. Kearns and Sara A. Solla},
  pages = {626--632},
  pdf = {https://papers.nips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},
  publisher = {MIT Press},
  title = {EM Algorithms for PCA and SPCA},
  url = {https://papers.nips.cc/paper/1398-em-algorithms-for-pca-and-spca},
  year = {1998}
}

@article{Caruana1997,
  abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression.},
  author = {Caruana, Rich},
  doi = {10.1023/A:1007379606734},
  journal = {Machine Learning},
  number = {1},
  openalex = {W2913340405},
  pages = {41--75},
  pdf = {https://www.cs.cornell.edu/~caruana/mlj97.pdf},
  publisher = {Springer},
  title = {Multitask Learning},
  volume = {28},
  year = {1997}
}

@article{Hochreiter1997,
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  doi = {10.1162/neco.1997.9.8.1735},
  journal = {Neural Computation},
  month = {11},
  number = {8},
  openalex = {W2064675550},
  pages = {1735--1780},
  pdf = {https://www.bioinf.jku.at/publications/older/2604.pdf},
  publisher = {MIT Press},
  title = {Long Short-Term Memory},
  url = {https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory},
  volume = {9},
  year = {1997}
}

@article{Hyvarinen1997,
  abstract = {We introduce a novel fast algorithm for independent component analysis, which can be used for blind source separation and feature extraction. We show how a neural network learning rule can be transformed into a fixed-point iteration, which provides an algorithm that is very simple, does not depend on any user-defined parameters, and is fast to converge to the most accurate solution allowed by the data. The algorithm finds, one at a time, all nongaussian independent components, regardless of their probability distributions. The computations can be performed in either batch mode or a semiadaptive manner. The convergence of the algorithm is rigorously proved, and the convergence speed is shown to be cubic.},
  author = {Hyvärinen, Aapo and Oja, Erkki},
  doi = {10.1162/neco.1997.9.7.1483},
  journal = {Neural Computation},
  number = {7},
  openalex = {W2019502123},
  pages = {1483--1492},
  pdf = {https://www.cs.helsinki.fi/u/ahyvarin/papers/NC97.pdf},
  title = {A Fast Fixed-Point Algorithm for Independent Component Analysis},
  volume = {9},
  year = {1997}
}

@book{McLachlan1997,
  abstract = {The first unified account of the theory, methodology, and applications of the EM algorithm and its extensions. The book describes the formulation of the EM algorithm, details its methodology, discusses its implementation, and illustrates applications in many statistical contexts. The authors examine applications both in evidently incomplete data situations---where data are missing, distributions are truncated, or observations are censored or grouped---and in a broad variety of situations in which incompleteness is neither natural nor evident. Areas of application include regression, medical imaging, categorical data analysis, finite mixture analysis, robust statistical modeling, variance-components estimation, survival analysis, and repeated-measures designs.},
  address = {New York},
  author = {Geoffrey J. McLachlan and Thriyambakam Krishnan},
  isbn = {0471123587},
  isbn13 = {9780471123583},
  pages = {xvii+274},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {The EM Algorithm and Extensions},
  year = {1997}
}

@book{Ogden1997,
  abstract = {This book provides an accessible introductory survey of new wavelet-analysis tools and the way they can be applied to fundamental data-analysis problems. Written for the non-specialist, with its main thrust toward wavelet applications, it gives enough theory to help the reader gain a basic understanding of how wavelets work in practice, using only a basic level of mathematics. The book concentrates solely on the application of wavelets to statistics and data analysis, covering density estimation, estimation of regression functions, statistical testing, and Bayesian methods.},
  address = {Boston, MA},
  author = {Ogden, R. Todd},
  doi = {10.1007/978-1-4612-0709-2},
  isbn = {978-0-8176-3864-1},
  pages = {xviii + 206},
  publisher = {Birkhäuser},
  title = {Essential Wavelets for Statistical Applications and Data Analysis},
  url = {https://link.springer.com/book/10.1007/978-1-4612-0709-2},
  year = {1997}
}

@techreport{Tipping1997,
  abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
  address = {Birmingham, UK},
  author = {Tipping, Michael E. and Bishop, Christopher M.},
  institution = {Neural Computing Research Group, Aston University},
  month = {9},
  note = {Later published in Journal of the Royal Statistical Society Series B, 61(3):611--622, 1999},
  number = {NCRG/97/010},
  title = {Probabilistic Principal Component Analysis},
  url = {https://publications.aston.ac.uk/id/eprint/38367/1/NCRG_97_010.pdf},
  year = {1997}
}

@article{Breiman1996,
  abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  author = {Breiman, Leo},
  doi = {10.1007/BF00058655},
  journal = {Machine Learning},
  month = {8},
  number = {2},
  openalex = {W4212883601},
  pages = {123--140},
  pdf = {https://www.stat.berkeley.edu/~breiman/bagging.pdf},
  title = {Bagging predictors},
  volume = {24},
  year = {1996}
}

@inproceedings{Freund1996,
  abstract = {In an earlier paper, we introduced a new ``boosting'' algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a ``pseudo-loss'' which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments carried out to assess how well AdaBoost with and without pseudo-loss performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's ``bagging'' method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.},
  address = {San Francisco, CA, USA},
  author = {Freund, Yoav and Schapire, Robert E.},
  booktitle = {Machine Learning: Proceedings of the Thirteenth International Conference},
  editor = {Saitta, Lorenza},
  isbn = {1-55860-419-7},
  month = {7},
  pages = {148--156},
  pdf = {https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf},
  publisher = {Morgan Kaufmann Publishers Inc.},
  title = {Experiments with a New Boosting Algorithm},
  year = {1996}
}

@book{Gilks1996,
  doi = {10.1201/b14835},
  edition = {First},
  editor = {Gilks, W. R. and Richardson, S. and Spiegelhalter, D. J.},
  isbn = {9780412055515},
  openalex = {W4248373945},
  pages = {512},
  pdf = {https://api.taylorfrancis.com/content/books/mono/download?identifierName=doi&identifierValue=10.1201%2Fb14835&type=googlepdf},
  publisher = {Chapman & Hall},
  series = {Chapman & Hall/CRC Interdisciplinary Statistics},
  title = {Markov Chain Monte Carlo in Practice},
  year = {1996}
}

@book{Golub1996,
  address = {Baltimore, MD},
  author = {Gene H. Golub and Charles F. Van Loan},
  edition = {Third},
  isbn = {978-0-8018-5414-9},
  openalex = {W3165900540},
  pages = {694},
  publisher = {Johns Hopkins University Press},
  series = {Johns Hopkins Studies in the Mathematical Sciences},
  title = {Matrix Computations},
  year = {1996}
}

@book{Lauritzen1996,
  abstract = {The idea of modelling systems using graph theory has its origin in several scientific areas: in statistical physics, genetics, and interactions in contingency tables. This book provides the first comprehensive and authoritative account of the theory of graphical models. It covers the fundamental graph theory required and a thorough study of Markov properties associated with various types of graphs, the statistical theory of log-linear and graphical models for contingency tables, covariance selection models, and graphical models with mixed discrete-continuous variables. Written by a leading expert in the field, the book includes applications to probabilistic expert systems and other areas of artificial intelligence.},
  address = {Oxford},
  author = {Steffen L. Lauritzen},
  doi = {10.1093/oso/9780198522195.001.0001},
  isbn = {978-0-19-852219-5},
  month = {5},
  openalex = {W2146585970},
  pages = {308},
  publisher = {Clarendon Press},
  series = {Oxford Statistical Science Series},
  title = {Graphical Models},
  volume = {17},
  year = {1996}
}

@book{Lutkepohl1996,
  abstract = {This book provides a comprehensive collection of matrix results that serves as a handbook and dictionary of terms for matrix theory. It contains definitions and results for matrix operations, trace, determinant, rank, eigenvalues, singular values, decompositions, canonical forms, vectorization operators, and special matrices. The handbook is designed as a quick reference resource for researchers, professionals, and students working with matrices in statistics, econometrics, and related fields.},
  address = {New York},
  author = {Lütkepohl, Helmut},
  isbn = {0-471-96688-6},
  openalex = {W2993845287},
  pages = {xvi + 304},
  publisher = {John Wiley & Sons},
  title = {Handbook of Matrices},
  year = {1996}
}

@inproceedings{Pearlmutter1997,
  abstract = {In the square linear blind source separation problem, one must find a linear unmixing operator which can detangle the result Xi(t) of mixing n unknown independent sources 8i(t) through an unknown n x n mixing matrix A(t) of causal linear filters: Xi = E j aij * 8 j. We cast the problem as one of maximum likelihood density estimation, and in that framework introduce an algorithm that searches for independent components using both temporal and spatial cues. We call the resulting algorithm 'Contextual ICA,' after the (Bell and Sejnowski 1995) Infomax algorithm, which we show to be a special case of cICA. Because cICA can make use of the temporal structure of its input, it is able separate in a number of situations where standard methods cannot, including sources with low kurtosis, colored Gaussian sources, and sources which have Gaussian histograms.},
  author = {Pearlmutter, Barak A. and Parra, Lucas C.},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Mozer, Michael C. and Jordan, Michael I. and Petsche, Thomas},
  pages = {613--619},
  pdf = {https://papers.nips.cc/paper_files/paper/1996/file/dabd8d2ce74e782c65a973ef76fd540b-Paper.pdf},
  publisher = {MIT Press},
  title = {Maximum Likelihood Blind Source Separation: A Context-Sensitive Generalization of ICA},
  url = {https://proceedings.neurips.cc/paper/1996/hash/dabd8d2ce74e782c65a973ef76fd540b-Abstract.html},
  volume = {9},
  year = {1996}
}

@article{Tibshirani1996,
  abstract = {We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. The method can be seen as a convex relaxation of subset selection. The lasso is related to soft-thresholding of wavelet coefficients, forward stagewise regression, and boosting methods.},
  author = {Tibshirani, Robert},
  doi = {10.1111/j.2517-6161.1996.tb02080.x},
  issn = {1369-7412},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  number = {1},
  pages = {267--288},
  pdf = {https://webdoc.agsci.colostate.edu/koontz/arec-econ535/papers/Tibshirani%20(JRSS-B%201996).pdf},
  publisher = {Wiley},
  title = {Regression Shrinkage and Selection via the Lasso},
  volume = {58},
  year = {1996}
}

@article{Williams1996,
  abstract = {Neural network outputs are interpreted as parameters of statistical distributions. This allows us to fit conditional distributions in which the parameters depend on the inputs to the network. The approach is exploited in modeling multivariate data, including the univariate case, in which there may be input-dependent (e.g., time-dependent) correlations between output components. This provides a novel way of modeling conditional correlation that extends existing techniques for determining input-dependent (local) error bars.},
  author = {Peter M. Williams},
  doi = {10.1162/neco.1996.8.4.843},
  issn = {0899-7667},
  journal = {Neural Computation},
  month = {5},
  number = {4},
  pages = {843--854},
  publisher = {MIT Press},
  title = {Using neural networks to model conditional multivariate densities},
  url = {https://direct.mit.edu/neco/article-abstract/8/4/843/5986/Using-Neural-Networks-to-Model-Conditional},
  volume = {8},
  year = {1996}
}

@article{Wolpert1996,
  abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. It is shown, loosely speaking, that for any two algorithms A and B, there are ``as many'' targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is ``anti-cross-validation'' (choose the learning algorithm with largest cross-validation error). The paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small.},
  author = {Wolpert, David H.},
  doi = {10.1162/neco.1996.8.7.1341},
  journal = {Neural Computation},
  month = {10},
  number = {7},
  openalex = {W2100483895},
  pages = {1341--1390},
  publisher = {MIT Press},
  title = {The Lack of A Priori Distinctions Between Learning Algorithms},
  url = {https://direct.mit.edu/neco/article/8/7/1341/6016/The-Lack-of-A-Priori-Distinctions-Between-Learning},
  volume = {8},
  year = {1996}
}

@inproceedings{Amari1996,
  abstract = {A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm are verified by computer simulations.},
  author = {Amari, Shun-ichi and Cichocki, Andrzej and Yang, Howard H.},
  booktitle = {Advances in Neural Information Processing Systems 8},
  editor = {Touretzky, David S. and Mozer, Michael C. and Hasselmo, Michael E.},
  openalex = {W2133069808},
  pages = {757--763},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/1995/file/e19347e1c3ca0c0b97de5fb3b690855a-Paper.pdf},
  publisher = {MIT Press},
  title = {A New Learning Algorithm for Blind Signal Separation},
  url = {https://papers.nips.cc/paper/1115-a-new-learning-algorithm-for-blind-signal-separation},
  year = {1995}
}

@article{Bell1995,
  abstract = {We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case. The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.},
  author = {Bell, Anthony J. and Sejnowski, Terrence J.},
  doi = {10.1162/neco.1995.7.6.1129},
  journal = {Neural Computation},
  month = {11},
  number = {6},
  openalex = {W2108384452},
  pages = {1129--1159},
  pdf = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
  publisher = {MIT Press},
  title = {An Information-Maximization Approach to Blind Separation and Blind Deconvolution},
  volume = {7},
  year = {1995}
}

@inproceedings{Bishop1995a,
  abstract = {In this paper we consider four alternative approaches to complexity control in feed-forward networks based respectively on architecture selection, regularization, early stopping, and training with noise. We show that there are close similarities between these approaches and we argue that, for most practical applications, the technique of regularization should be the method of choice.},
  author = {Bishop, Christopher M.},
  booktitle = {Proceedings International Conference on Artificial Neural Networks ICANN'95},
  editor = {Fougelman-Soulie, Françoise and Gallinari, Patrick},
  month = {1},
  pages = {141--148},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/1995/01/NCRG_95_022.pdf},
  publisher = {EC2 et Cie},
  title = {Regularization and Complexity Control in Feed-forward Networks},
  volume = {1},
  year = {1995}
}

@book{Bishop1995b,
  abstract = {This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. As well as providing a detailed discussion of learning and generalization in neural networks, the book also covers the important topics of data processing, feature extraction, and prior knowledge. The book concludes with an extensive treatment of Bayesian techniques and their applications to neural networks. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
  address = {New York},
  author = {Bishop, Christopher M.},
  isbn = {0-19-853864-2},
  pages = {xvii+482},
  publisher = {Oxford University Press},
  series = {Advanced Texts in Econometrics},
  title = {Neural Networks for Pattern Recognition},
  year = {1995}
}

@article{Bishop1995c,
  abstract = {It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization, in which an extra term is added to the error function. However, the regularization term, which involves the second derivatives of the error function, is not positive definite and can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper it is shown that, for the purposes of network training, the regularization term can be replaced by one which involves only first derivatives of the network mapping and which, for a sum-of-squares error function, is positive definite. This modified regularization term corresponds to a generalized Tikhonov regularizer.},
  author = {Bishop, Christopher M.},
  doi = {10.1162/neco.1995.7.1.108},
  journal = {Neural Computation},
  number = {1},
  openalex = {W2111406701},
  pages = {108--116},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf},
  title = {Training with noise is equivalent to Tikhonov regularization},
  url = {https://doi.org/10.1162/neco.1995.7.1.108},
  volume = {7},
  year = {1995}
}

@book{Elliott1995,
  abstract = {This book aims to present graduate students with a thorough survey of reference probability models and their applications to optimal estimation and control. These new and powerful methods are particularly useful in signal processing applications where signal models are only partially known and are in noisy environments. Well-known results, including Kalman filters and the Wonham filter, emerge as special cases. The authors begin with discrete time and discrete state spaces, then proceed to cover continuous time, and progress from linear models to nonlinear models, and from completely known models to only partially known models.},
  address = {New York, NY},
  author = {Elliott, Robert J. and Aggoun, Lakhdar and Moore, John B.},
  doi = {10.1007/978-0-387-84854-9},
  isbn = {978-0-387-94364-0},
  pages = {xiv+382},
  publisher = {Springer-Verlag},
  series = {Stochastic Modelling and Applied Probability},
  title = {Hidden Markov Models: Estimation and Control},
  volume = {29},
  year = {1995}
}

@article{Gilks1995,
  abstract = {Gibbs sampling is a powerful technique for statistical inference that involves sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild showed that in practice full conditionals are often log-concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log-concave distributions. To deal with non-log-concave full conditional distributions, the authors generalize adaptive rejection sampling to include a Hastings-Metropolis algorithm step. Adaptive rejection Metropolis sampling (ARMS) is a method for efficiently sampling from complicated univariate densities, such as typically occur in applications of Gibbs sampling. The method addresses situations where log-concavity does not obtain, typically in non-linear models, or with non-exponential-family distributions.},
  author = {Gilks, Walter R. and Best, Nicky G. and Tan, K. K. C.},
  doi = {10.2307/2986138},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  month = {12},
  number = {4},
  openalex = {W41222772},
  pages = {455--472},
  pdf = {https://academic.oup.com/jrsssc/article-pdf/44/4/455/48749639/jrsssc_44_4_455.pdf},
  title = {Adaptive Rejection Metropolis Sampling within Gibbs Sampling},
  volume = {44},
  year = {1995}
}

@article{Jensen1995,
  abstract = {This paper presents blocking Gibbs, which combines exact local computations with Gibbs sampling in a way that complements the strengths of both. The methodology is illustrated on a real-world problem involving a heavily inbred pedigree containing 20,000 individuals.},
  author = {Claus Skaanning Jensen and Augustine Kong and Uffe Kjærulff},
  doi = {10.1006/ijhc.1995.1029},
  journal = {International Journal of Human-Computer Studies},
  number = {6},
  openalex = {W2090361527},
  pages = {647--666},
  title = {Blocking Gibbs sampling in very large probabilistic expert systems},
  volume = {42},
  year = {1995}
}

@article{Leen1995,
  abstract = {Ideally pattern recognition machines provide constant output when the inputs are transformed under a group G of desired invariances. These invariances can be achieved by enhancing the training data to include examples of inputs transformed by elements of G, while leaving the corresponding targets unchanged. Alternatively the cost function for training can include a regularization term that penalizes changes in the output when the input is transformed under the group. This paper relates the two approaches, showing precisely the sense in which the regularized cost function approximates the result of adding transform examples to the training data.},
  author = {Todd K. Leen},
  doi = {10.1162/neco.1995.7.5.974},
  journal = {Neural Computation},
  month = {9},
  number = {5},
  pages = {974--981},
  pdf = {http://papers.neurips.cc/paper/925-from-data-distributions-to-regularization-in-invariant-learning.pdf},
  publisher = {MIT Press},
  title = {From Data Distributions to Regularization in Invariant Learning},
  volume = {7},
  year = {1995}
}

@inproceedings{Tarassenko1995,
  address = {Cambridge, UK},
  author = {Tarassenko, L.},
  booktitle = {Proceedings of the Fourth IEE International Conference on Artificial Neural Networks},
  doi = {10.1049/cp:19950597},
  month = {6},
  pages = {442--447},
  publisher = {IEE},
  title = {Novelty detection for the identification of masses in mammograms},
  volume = {4},
  year = {1995}
}

@book{Vapnik1995,
  abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. The book covers the setting of learning problems based on the model of minimizing the risk functional from empirical data, a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency, non-asymptotic bounds for the risk achieved using the empirical risk minimization principle, principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds, and the Support Vector methods that control the generalization ability when estimating function using small sample size.},
  address = {New York},
  author = {Vapnik, Vladimir N.},
  doi = {10.1007/978-1-4757-3264-1},
  isbn = {0-387-94559-8},
  note = {First edition; Second edition published in 2000 with ISBN 0-387-98780-0},
  pages = {203},
  publisher = {Springer-Verlag},
  series = {Information Science and Statistics},
  title = {The Nature of Statistical Learning Theory},
  year = {1995}
}

@book{Basilevsky1994,
  abstract = {This work provides a unified treatment of both the theory and practice of factor analysis and latent variable models. Bridging the gap between mathematical and statistical theory of factor analysis, it represents the first unified treatment of the theory and practice of factor analysis and latent variable models. The book focuses on areas including the classical principal components model and sample-population inference, several extensions and modifications of principal components including Q and three-mode analysis and principal components in the complex domain, and maximum likelihood and weighted factor models.},
  address = {New York},
  author = {Basilevsky, Alexander T.},
  edition = {First},
  isbn = {9780471570820},
  month = {6},
  pages = {768},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Statistical Factor Analysis and Related Methods: Theory and Applications},
  year = {1994}
}

@book{Bernardo1994,
  abstract = {This highly acclaimed text provides a thorough account of key concepts and theoretical results, with particular emphasis on viewing statistical inference as a special case of decision theory. Information-theoretic concepts play a central role in the development of the theory, which provides, in particular, a detailed discussion of the problem of specification of so-called ``prior ignorance.'' The work is written from the authors' committed Bayesian perspective, but an overview of non-Bayesian theories is also provided, and each chapter contains a wide-ranging critical re-examination of controversial issues. The level of mathematics used is such that most material is accessible to readers with knowledge of advanced calculus.},
  address = {Chichester},
  author = {Bernardo, José M. and Smith, Adrian F. M.},
  doi = {10.1002/9780470316870},
  isbn = {978-0-471-92416-6},
  openalex = {W4232632925},
  pages = {608},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Statistics},
  title = {Bayesian Theory},
  year = {1994}
}

@article{Bishop1994,
  abstract = {One of the key factors which limits the use of neural networks in many industrial applications has been the difficulty of demonstrating that a trained network will continue to generate reliable outputs once it is in routine use. Novel input data; that is, input data which differ significantly from the data used to train the network, constitute an important potential source of errors. The paper investigates the relationship between the degree of novelty of input data and the corresponding reliability of the outputs from the network. A quantitative procedure for assessing novelty is described and demonstrated using an application involving the monitoring of oil flow in multi-phase pipelines.},
  author = {Bishop, Christopher M.},
  doi = {10.1049/ip-vis:19941330},
  journal = {IEE Proceedings: Vision, Image and Signal Processing},
  month = {8},
  number = {4},
  pages = {217--222},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/Bishop-Novelty-Detection_IEE-Proceedings-94b.pdf},
  title = {Novelty Detection and Neural Network Validation},
  volume = {141},
  year = {1994}
}

@article{Kurkova1994,
  abstract = {For a feedforward perceptron type architecture with a single hidden layer but with a quite general activation function, we characterize the relation between pairs of weight vectors determining networks with the same input-output function. We prove that given the input-output mapping of a Tanh neural network, its architecture can be determined and weights are identified up to permutations and sign flips.},
  author = {Kurková, Věra and Kainen, Paul C.},
  doi = {10.1162/neco.1994.6.3.543},
  journal = {Neural Computation},
  month = {5},
  number = {3},
  pages = {543--558},
  publisher = {MIT Press},
  title = {Functionally Equivalent Feed-forward Neural Networks},
  url = {https://doi.org/10.1162/neco.1994.6.3.543},
  volume = {6},
  year = {1994}
}

@article{Pearlmutter1994,
  abstract = {Just storing the Hessian H (the matrix of second derivatives $∂^2 E/∂ w_i ∂ w_j$ of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator $R_v\f(w)\ = (∂/∂ r) f(w + rv)|_r=0$, note that $R_v\∇_w\ = Hv$ and $R_v\w\ = v$, and then apply $R_v\·\$ to the equations used to compute $∇_w$. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation.},
  author = {Pearlmutter, Barak A.},
  doi = {10.1162/neco.1994.6.1.147},
  journal = {Neural Computation},
  month = {1},
  number = {1},
  openalex = {W2006903949},
  pages = {147--160},
  pdf = {https://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf},
  publisher = {MIT Press},
  title = {Fast Exact Multiplication by the Hessian},
  volume = {6},
  year = {1994}
}

@article{Chen1993,
  abstract = {Many feedforward neural network architectures have the property that their overall input-output function is unchanged by certain weight permutations and sign flips. The geometric structure of these equioutput weight space transformations is explored for the case of multilayer perceptron networks with tanh activation functions. It is shown that these transformations form an algebraic group isomorphic to a direct product of Weyl groups. Results concerning the root spaces of the Lie algebras associated with these Weyl groups are then used to derive sets of simple equations for minimal sufficient search sets in weight space. These sets, which take the geometric forms of a wedge and a cone, occupy only a minute fraction of the volume of weight space.},
  author = {An Mei Chen and Haw-minn Lu and Robert Hecht-Nielsen},
  doi = {10.1162/neco.1993.5.6.910},
  journal = {Neural Computation},
  month = {11},
  number = {6},
  openalex = {W2139801605},
  pages = {910--927},
  publisher = {MIT Press},
  title = {On the Geometry of Feedforward Neural Network Error Surfaces},
  url = {https://direct.mit.edu/neco/article-abstract/5/6/910/5755},
  volume = {5},
  year = {1993}
}

@article{Leshno1993,
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  doi = {10.1016/S0893-6080(05)80131-5},
  journal = {Neural Networks},
  number = {6},
  openalex = {W3125537303},
  pages = {861--867},
  pdf = {http://archive.nyu.edu/bitstream/2451/14329/1/IS-92-13.pdf},
  title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  volume = {6},
  year = {1993}
}

@article{Meng1993,
  author = {Xiao-Li Meng and Donald B. Rubin},
  doi = {10.1093/biomet/80.2.267},
  journal = {Biometrika},
  month = {6},
  number = {2},
  openalex = {W1967639437},
  pages = {267--278},
  publisher = {Oxford University Press},
  title = {Maximum likelihood estimation via the ECM algorithm: a general framework},
  volume = {80},
  year = {1993}
}

@phdthesis{Moller1993,
  abstract = {This thesis addresses the slow convergence rate problem in training feed-forward neural networks, especially for large scale problems. It describes different approaches to improve convergence, with the main results being the development of the Scaled Conjugate Gradient Algorithm and the stochastic version of this algorithm. The thesis concludes that conjugate gradient algorithms are very suitable for training of feed-forward networks, and that use of second order information by calculations on the Hessian matrix can be used to improve convergence.},
  address = {Denmark},
  author = {Møller, Martin Fodslette},
  doi = {10.7146/dpb.v22i464.6937},
  keywords = {neural networks, backpropagation, conjugate gradient, Hessian matrix, optimization, machine learning},
  month = {12},
  number = {464},
  openalex = {W1919233617},
  pdf = {https://tidsskrift.dk/daimipb/article/view/6937/5900},
  school = {Aarhus University},
  series = {DAIMI Report Series},
  title = {Efficient Training of Feed-Forward Neural Networks},
  type = {PhD thesis},
  url = {https://tidsskrift.dk/daimipb/article/view/6937},
  volume = {22},
  year = {1993}
}

@techreport{Neal1993,
  abstract = {Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The ``Metropolis algorithm'' has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of ``Gibbs sampling'' has been applied to problems of statistical inference.},
  author = {Neal, Radford M.},
  institution = {Department of Computer Science, University of Toronto, Canada},
  month = {9},
  number = {CRG-TR-93-1},
  pages = {144},
  pdf = {https://bayes.wustl.edu/Manual/RadfordNeal.review.pdf},
  title = {Probabilistic Inference Using Markov Chain Monte Carlo Methods},
  year = {1993}
}

@article{Bishop1992,
  abstract = {The elements of the Hessian matrix consist of the second derivatives of the error measure with respect to the weights and thresholds in the network. They are needed in Bayesian estimation of network regularization parameters, for estimation of error bars on the network outputs, for network pruning algorithms, and for fast retraining of the network following a small change in the training data. This paper presents an extended backpropagation algorithm that allows all elements of the Hessian matrix to be evaluated exactly for a feedforward network of arbitrary topology. Software implementation of the algorithm is straightforward.},
  author = {Bishop, Christopher M.},
  doi = {10.1162/neco.1992.4.4.494},
  journal = {Neural Computation},
  number = {4},
  openalex = {W2135520967},
  pages = {494--501},
  pdf = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-hessian-nc-92.pdf},
  publisher = {MIT Press},
  title = {Exact Calculation of the Hessian Matrix for the Multilayer Perceptron},
  volume = {4},
  year = {1992}
}

@inproceedings{Gilks1992,
  address = {Oxford},
  author = {Gilks, Walter R.},
  booktitle = {Bayesian Statistics 4: Proceedings of the Fourth Valencia International Meeting},
  editor = {Bernardo, José M. and Berger, James O. and Dawid, A. Philip and Smith, A. F. M.},
  isbn = {978-0-19-852266-9},
  pages = {641--649},
  publisher = {Oxford University Press},
  title = {Derivative-free adaptive rejection sampling for Gibbs sampling},
  year = {1992}
}

@article{Gilks1992b,
  abstract = {We propose a method for rejection sampling from any univariate log‐concave probability density function. The method is adaptive: As sampling proceeds, the rejection envelope and the squeezing function converge to the density function. The rejection envelope and squeezing function are piece‐wise exponential functions, the rejection envelope touching the density at previously sampled points, and the squeezing function forming arcs between those points of contact. The technique is intended for situations where evaluation of the density is computationally expensive, in particular for applications of Gibbs sampling to Bayesian models with non‐conjugacy. We apply the technique to a Gibbs sampling analysis of monoclonal antibody reactivity.},
  author = {Gilks, Walter R. and Wild, Pascal},
  doi = {10.2307/2347565},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  month = {6},
  number = {2},
  openalex = {W4388289942},
  pages = {337--348},
  title = {Adaptive Rejection Sampling for Gibbs Sampling},
  volume = {41},
  year = {1992}
}

@inproceedings{Hassibi1993,
  abstract = {We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems.},
  author = {Hassibi, Babak and Stork, David G.},
  booktitle = {Advances in Neural Information Processing Systems 5},
  editor = {Hanson, S. J. and Cowan, J. D. and Giles, C. L.},
  openalex = {W2125389748},
  pages = {164--171},
  pdf = {https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
  publisher = {Morgan Kaufmann},
  title = {Second Order Derivatives for Network Pruning: Optimal Brain Surgeon},
  url = {https://proceedings.neurips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html},
  year = {1992}
}

@article{MacKay1992,
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models.},
  author = {MacKay, David J. C.},
  doi = {10.1162/neco.1992.4.3.448},
  journal = {Neural Computation},
  month = {5},
  number = {3},
  openalex = {W2111051539},
  pages = {448--472},
  pdf = {https://authors.library.caltech.edu/13793/1/MACnc92b.pdf},
  publisher = {MIT Press},
  title = {A Practical Bayesian Framework for Back-propagation Networks},
  url = {https://direct.mit.edu/neco/article/4/3/448/5654/A-Practical-Bayesian-Framework-for-Backpropagation},
  volume = {4},
  year = {1992}
}

@article{Nowlan1992,
  abstract = {One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple Gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since the appropriate means or variances of the clusters are not known in advance, the parameters of the mixture model are allowed to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.},
  author = {Nowlan, Steven J. and Hinton, Geoffrey E.},
  doi = {10.1162/neco.1992.4.4.473},
  journal = {Neural Computation},
  month = {7},
  number = {4},
  pages = {473--493},
  pdf = {https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf},
  publisher = {MIT Press},
  title = {Simplifying Neural Networks by Soft Weight-Sharing},
  volume = {4},
  year = {1992}
}

@inproceedings{Simard1992,
  abstract = {In many machine learning applications, one has access not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For instance, the output of a character recognizer should be invariant with respect to small spatial distortions of the input image (translations, rotations, scale changes, etcetera). We present a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the network should perform. We introduce the concept of tangent vectors, which compactly represent the essence of transformation invariances, and present two classes of algorithms, tangent distance and tangent propagation, which make use of these invariances to improve performance.},
  address = {Denver, Colorado, USA},
  author = {Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
  booktitle = {Advances in Neural Information Processing Systems 4},
  editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
  isbn = {1-55860-222-4},
  month = {12},
  pages = {895--903},
  pdf = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},
  publisher = {Morgan Kaufmann},
  title = {Tangent Prop -- A Formalism for Specifying Selected Invariances in an Adaptive Network},
  year = {1992}
}

@article{Williams1992,
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation.},
  author = {Williams, Ronald J.},
  doi = {10.1007/BF00992696},
  journal = {Machine Learning},
  month = {5},
  number = {3--4},
  openalex = {W2119717200},
  pages = {229--256},
  publisher = {Springer},
  title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  url = {https://doi.org/10.1007/BF00992696},
  volume = {8},
  year = {1992}
}

@article{Comon1991,
  abstract = {Though it arouses more and more curiosity, the HJ iterative algorithm has never been derived in mathematical terms to date. We attempt in this paper to describe it from a statistical point of view. In fact, we show that the HJ algorithm is actually searching common zeros of n functionals by pipelined stochastic iterations. Based on simulation results, advantages and limitations as well as possible improvements are pointed out after a short theoretical analysis.},
  author = {Pierre Comon and Christian Jutten and Jeanny Hérault},
  doi = {10.1016/0165-1684(91)90080-3},
  journal = {Signal Processing},
  month = {7},
  number = {1},
  openalex = {W2155759219},
  pages = {11--20},
  title = {Blind separation of sources, part II: Problems statement},
  volume = {24},
  year = {1991}
}

@book{Cover1991,
  abstract = {This book introduces the basic concepts and fundamental theorems of information theory. Early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastic processes and data compression, and duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories.},
  address = {New York},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  edition = {First},
  isbn = {0471062596},
  month = {8},
  pages = {576},
  publisher = {Wiley-Interscience},
  series = {Wiley Series in Telecommunications and Signal Processing},
  title = {Elements of Information Theory},
  year = {1991}
}

@article{Jacobs1991,
  abstract = {We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  doi = {10.1162/neco.1991.3.1.79},
  journal = {Neural Computation},
  month = {3},
  number = {1},
  openalex = {W2150884987},
  pages = {79--87},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf},
  title = {Adaptive mixtures of local experts},
  volume = {3},
  year = {1991}
}

@article{Jutten1991,
  abstract = {The separation of independent sources from an array of sensors is a classical but difficult problem in signal processing. We propose an adaptive algorithm based on some biological observations to separate simultaneously all the unknown independent sources. The adaptive rule constitutes an independence test using non-linear functions and represents the main original point of this blind identification procedure.},
  author = {Christian Jutten and Jeanny H\érault},
  doi = {10.1016/0165-1684(91)90079-X},
  issn = {0165-1684},
  journal = {Signal Processing},
  keywords = {blind source separation, independent component analysis, neuromimetic architecture, adaptive algorithms},
  month = {7},
  number = {1},
  openalex = {W1996355918},
  pages = {1--10},
  pdf = {https://www.academia.edu/24288573/Blind_separation_of_sources_part_I_An_adaptive_algorithm_based_on_neuromimetic_architecture},
  title = {Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture},
  url = {https://www.sciencedirect.com/science/article/pii/016516849190079X},
  volume = {24},
  year = {1991}
}

@incollection{Fung1990,
  abstract = {Stochastic simulation approaches perform probabilistic inference in Bayesian networks by estimating the probability of an event based on the frequency that the event occurs in a set of simulation trials. This paper describes the evidence weighting mechanism, for augmenting the logic sampling stochastic simulation algorithm. Evidence weighting modifies the logic sampling algorithm by weighting each simulation trial by the likelihood of a network's evidence given the sampled state node values for that trial. We also describe an enhancement to the basic algorithm which uses the evidential integration technique. A comparison of the basic evidence weighting mechanism with the Markov blanket algorithm, the logic sampling algorithm, and the evidence integration algorithm is presented.},
  address = {Amsterdam},
  author = {Fung, Robert and Chang, Kuo-Chu},
  booktitle = {Uncertainty in Artificial Intelligence 5},
  doi = {10.48550/arXiv.1304.1504},
  editor = {Bonissone, Piero P. and Henrion, Max and Kanal, Laveen N. and Lemmer, John F.},
  isbn = {0-444-88738-5},
  note = {Originally presented at UAI 1989},
  pages = {208--219},
  publisher = {Elsevier},
  title = {Weighting and Integrating Evidence for Stochastic Simulation in Bayesian Networks},
  year = {1990}
}

@inproceedings{LeCun1990,
  abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
  address = {San Mateo, CA},
  author = {LeCun, Yann and Denker, John S. and Solla, Sara A. and Howard, R. E. and Jackel, L. D.},
  booktitle = {Advances in Neural Information Processing Systems 2},
  editor = {Touretzky, David S.},
  openalex = {W2963577451},
  pages = {598--605},
  pdf = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
  publisher = {Morgan Kaufmann},
  title = {Optimal Brain Damage},
  year = {1990}
}

@incollection{Shachter1990,
  abstract = {A number of algorithms have been developed to solve probabilistic inference problems on belief networks. These algorithms can be divided into two main groups: exact techniques which exploit conditional independence when the graph structure is relatively sparse, and probabilistic sampling techniques which exploit the conductance of an embedded Markov chain when conditional probabilities have non-extreme values. We investigate a family of forward Monte Carlo sampling techniques similar to Logic Sampling which appear to perform well even in some multiply connected networks with extreme conditional probabilities, making them generally applicable. We consider several enhancements which reduce the posterior variance using this approach and propose a framework and criteria for choosing when to use those enhancements.},
  address = {Amsterdam},
  author = {Shachter, Ross D. and Peot, Mark Alan},
  booktitle = {Uncertainty in Artificial Intelligence 5},
  doi = {10.1016/B978-0-444-88738-2.50026-6},
  editor = {Henrion, Max and Shachter, Ross D. and Kanal, Laveen N. and Lemmer, John F.},
  isbn = {978-0-444-88738-2},
  pages = {221--230},
  pdf = {https://arxiv.org/pdf/1304.1526.pdf},
  publisher = {North-Holland},
  series = {Machine Intelligence and Pattern Recognition},
  title = {Simulation Approaches to General Probabilistic Inference on Belief Networks},
  volume = {10},
  year = {1990}
}

@article{Baldi1989,
  abstract = {We address the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. In particular, we show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. In addition, we show that all the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  author = {Baldi, Pierre and Hornik, Kurt},
  doi = {10.1016/0893-6080(89)90014-2},
  journal = {Neural Networks},
  number = {1},
  openalex = {W2078626246},
  pages = {53--58},
  publisher = {Elsevier},
  title = {Neural networks and principal component analysis: Learning from examples without local minima},
  url = {https://doi.org/10.1016/0893-6080(89)90014-2},
  volume = {2},
  year = {1989}
}

@inproceedings{Becker1989,
  address = {San Mateo, CA},
  author = {Becker, S. and LeCun, Yann},
  booktitle = {Proceedings of the 1988 Connectionist Models Summer School},
  editor = {Touretzky, David S. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  openalex = {W19621276},
  pages = {29--37},
  publisher = {Morgan Kaufmann},
  title = {Improving the convergence of back-propagation learning with second order methods},
  year = {1989}
}

@article{Cybenko1989,
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of $n$ real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity.},
  author = {George V. Cybenko},
  doi = {10.1007/BF02551274},
  journal = {Mathematics of Control, Signals and Systems},
  number = {4},
  openalex = {W2103496339},
  pages = {303--314},
  publisher = {Springer-Verlag},
  title = {Approximation by superpositions of a sigmoidal function},
  url = {https://link.springer.com/article/10.1007/BF02551274},
  volume = {2},
  year = {1989}
}

@article{Funahashi1989,
  abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
  author = {Funahashi, Ken-ichi},
  doi = {10.1016/0893-6080(89)90003-8},
  journal = {Neural Networks},
  number = {3},
  openalex = {W1971735090},
  pages = {183--192},
  publisher = {Elsevier},
  title = {On the Approximate Realization of Continuous Mappings by Neural Networks},
  url = {https://doi.org/10.1016/0893-6080(89)90003-8},
  volume = {2},
  year = {1989}
}

@article{Hornik1989,
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  doi = {10.1016/0893-6080(89)90020-8},
  journal = {Neural Networks},
  number = {5},
  openalex = {W2137983211},
  pages = {359--366},
  publisher = {Elsevier BV},
  title = {Multilayer feedforward networks are universal approximators},
  volume = {2},
  year = {1989}
}

@article{LeCun1989,
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John S. and Henderson, Donnie and Howard, Richard E. and Hubbard, Wayne and Jackel, Lawrence D.},
  doi = {10.1162/neco.1989.1.4.541},
  issn = {0899-7667},
  journal = {Neural Computation},
  month = {12},
  number = {4},
  pages = {541--551},
  pdf = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
  publisher = {MIT Press},
  title = {Backpropagation Applied to Handwritten ZIP Code Recognition},
  volume = {1},
  year = {1989}
}

@book{McCullagh1989,
  abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications. The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion.},
  address = {London},
  author = {McCullagh, P. and Nelder, J. A.},
  doi = {10.1201/9780203753736},
  edition = {Second},
  isbn = {0-412-31760-5},
  pages = {xx + 511},
  publisher = {Chapman and Hall},
  series = {Monographs on Statistics and Applied Probability},
  title = {Generalized Linear Models},
  volume = {37},
  year = {1989}
}

@book{Anderson1988,
  abstract = {An essential guide to the concepts employed in neurocomputing that have been taken from disciplines as varied as neuroscience, psychology, cognitive science, engineering, and physics. The book contains 43 articles that include both historical and contemporary works in neural networks and neurocomputing, including pioneering contributions of McCulloch and Pitts, Hebb, and Lashley; innovative work by Von Neumann, Minsky and Papert, Cooper, Grossberg, and Kohonen.},
  address = {Cambridge, MA},
  editor = {Anderson, James A. and Rosenfeld, Edward},
  isbn = {978-0-262-01097-9},
  openalex = {W308556676},
  pages = {685},
  publisher = {MIT Press},
  title = {Neurocomputing: Foundations of Research},
  year = {1988}
}

@article{Bourlard1988,
  abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Loève transform.},
  author = {Bourlard, Hervé and Kamp, Yves},
  doi = {10.1007/BF00332918},
  journal = {Biological Cybernetics},
  number = {4-5},
  openalex = {W2017257315},
  pages = {291--294},
  pdf = {https://link.springer.com/content/pdf/10.1007/BF00332918.pdf},
  pmid = {3196773},
  publisher = {Springer},
  title = {Auto-association by multilayer perceptrons and singular value decomposition},
  volume = {59},
  year = {1988}
}

@article{Broomhead1988,
  abstract = {The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization' in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule.},
  author = {Broomhead, D. S. and Lowe, David},
  journal = {Complex Systems},
  number = {3},
  pages = {321--355},
  pdf = {https://content.wolfram.com/sites/13/2018/02/02-3-5.pdf},
  publisher = {Complex Systems Publications, Inc.},
  title = {Multivariable Functional Interpolation and Adaptive Networks},
  volume = {2},
  year = {1988}
}

@incollection{Henrion1988,
  abstract = {Bayesian belief networks and influence diagrams are attractive approaches for representing uncertain expert knowledge in coherent probabilistic form. However, current algorithms for propagating updates are either restricted to singly connected networks (Chow trees), as the scheme of Pearl and Kim, or they are liable to exponential complexity when dealing with multiply connected networks. Probabilistic logic sampling is a new scheme employing stochastic simulation which can make probabilistic inferences in large, multiply connected networks, with an arbitrary degree of precision controlled by the sample size. A prototype implementation, named Pulse, is illustrated, which provides efficient methods to estimate conditional probabilities, perform systematic sensitivity analysis, and compute evidence weights to explain inferences.},
  address = {Amsterdam},
  author = {Henrion, Max},
  booktitle = {Uncertainty in Artificial Intelligence 2},
  doi = {10.1016/B978-0-444-70396-5.50019-4},
  editor = {Lemmer, John F. and Kanal, Laveen N.},
  isbn = {978-0-444-70396-5},
  keywords = {Bayesian networks, probabilistic inference, logic sampling, stochastic simulation, uncertainty propagation},
  pages = {149--163},
  publisher = {Elsevier Science Publishers},
  series = {Machine Intelligence and Pattern Recognition},
  title = {Propagation of Uncertainty by Logic Sampling in Bayes' Networks},
  url = {https://www.sciencedirect.com/science/article/abs/pii/B9780444703965500194},
  volume = {5},
  year = {1988}
}

@book{Pearl1988,
  abstract = {This book provides a complete and accessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. This seminal work introduced and popularized what are now known as Bayesian networks, revolutionizing the field of artificial intelligence by establishing theoretical and practical foundations for using probabilistic methods in AI reasoning systems.},
  address = {San Mateo, California},
  author = {Pearl, Judea},
  isbn = {1558604797},
  note = {Bibliography: p. 521--538, includes indexes},
  pages = {xix + 552},
  publisher = {Morgan Kaufmann Publishers},
  series = {Series in Representation and Reasoning},
  title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
  year = {1988}
}

@inproceedings{Ricotti1988,
  abstract = {The authors show an example of an efficient and easy solution, using a neural network, of a problem that cannot be easily solved with rules. This example regards the localization of primary word stress.},
  address = {San Diego, CA, USA},
  author = {Ricotti, L. P. and Ragazzini, S. and Martinelli, G.},
  booktitle = {IEEE 1988 International Conference on Neural Networks},
  doi = {10.1109/ICNN.1988.23867},
  month = {7},
  pages = {355--361},
  publisher = {IEEE},
  title = {Learning of word stress in a sub-optimal second order back-propagation neural network},
  volume = {1},
  year = {1988}
}

@inproceedings{Zhou1988,
  abstract = {A method for computing optical flow using a neural network is presented. The authors use rotation invariant measurement primitives, intensity values and their principal curvatures, to compute the optical flow under the assumption that the changes in intensity are strictly due to the motion of the object. They first fit a 2-D polynomial to find a smooth continuous image intensity function in a window and estimate the subpixel intensity values and their principal curvatures. Under local rigidity assumption and smoothness constraints, a neural network is then employed to implement the computing procedure based on the estimated intensity values and their principal curvatures. A deterministic decision rule is used for the updating scheme. This method can detect both rotating and translating objects in the scene and obtain dense optical flow with subpixel accuracy.},
  address = {San Diego, CA, USA},
  author = {Zhou, Y. T. and Chellappa, R.},
  booktitle = {IEEE 1988 International Conference on Neural Networks},
  doi = {10.1109/ICNN.1988.23914},
  keywords = {Neural networks, Optical flow, Flow estimation, Image processing, Computer vision},
  month = {July},
  pages = {71--78},
  publisher = {IEEE},
  title = {Computation of optical flow using a neural network},
  url = {https://ieeexplore.ieee.org/document/23914/},
  volume = {2},
  year = {1988}
}

@book{Bartholomew1987,
  address = {London},
  author = {Bartholomew, David J.},
  isbn = {9780195206159},
  number = {40},
  pages = {193},
  publisher = {Charles Griffin},
  series = {Griffin's Statistical Monographs and Courses},
  title = {Latent Variable Models and Factor Analysis},
  year = {1987}
}

@article{Duane1987,
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan},
  doi = {10.1016/0370-2693(87)91197-X},
  journal = {Physics Letters B},
  month = {9},
  number = {2},
  openalex = {W2059448777},
  pages = {216--222},
  publisher = {Elsevier},
  title = {Hybrid Monte Carlo},
  url = {https://doi.org/10.1016/0370-2693(87)91197-X},
  volume = {195},
  year = {1987}
}

@book{Fletcher1987,
  address = {Chichester},
  author = {Fletcher, Roger},
  doi = {10.1002/9781118723203},
  edition = {Second},
  isbn = {9780471915478},
  note = {Bibliography: p. [417]--429. Includes index},
  pages = {xiv + 436},
  publisher = {John Wiley & Sons},
  series = {Wiley-Interscience publication},
  title = {Practical Methods of Optimization},
  year = {1987}
}

@article{Sirovich1987,
  abstract = {Two developments have altered the basic statistical framework of turbulence in the past decade: laboratory information which shows the existence of coherent structures and theoretical application of dynamical systems theory which suggests that these flows reside on relatively low dimensional attractors. The present work addresses both issues and their unification. A key ingredient is Lumley's idea that spatial velocity correlations be orthogonally decomposed as a rational quantitative method of identifying coherent structures. The method is applied to several flows and the results are discussed in detail. The implication for the dynamics is also addressed.},
  author = {Lawrence Sirovich},
  doi = {10.1090/qam/910462},
  journal = {Quarterly Applied Mathematics},
  month = {10},
  number = {3},
  pages = {561--571},
  title = {Turbulence and the Dynamics of Coherent Structures. I. Coherent Structures},
  volume = {45},
  year = {1987}
}

@incollection{Rumelhart1986,
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units.},
  address = {Cambridge, MA},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations},
  doi = {10.5555/104279.104293},
  editor = {Rumelhart, David E. and McClelland, James L. and the PDP Research Group},
  isbn = {978-0-262-68053-0},
  note = {Reprinted in Anderson and Rosenfeld (1988)},
  pages = {318--362},
  pdf = {https://www.cs.toronto.edu/~hinton/absps/pdp8.pdf},
  publisher = {MIT Press},
  series = {Bradford Books},
  title = {Learning internal representations by error propagation},
  volume = {1},
  year = {1986}
}

@book{Berger1985,
  abstract = {In this new edition the author has added substantial material on Bayesian analysis, including lengthy new sections on such important topics as empirical and hierarchical Bayes analysis, Bayesian calculation, Bayesian communication, and group decision making. With these changes, the book can be used as a self-contained introduction to Bayesian analysis. In addition, much of the decision-theoretic portion of the text was updated, including new sections covering such modern topics as minimax multivariate (Stein) estimation.},
  address = {New York},
  author = {Berger, James O.},
  doi = {10.1007/978-1-4757-4286-2},
  edition = {Second},
  isbn = {0387960988},
  openalex = {W1988520084},
  publisher = {Springer-Verlag},
  series = {Springer Series in Statistics},
  title = {Statistical Decision Theory and Bayesian Analysis},
  year = {1985}
}

@book{Everitt1984,
  abstract = {Latent variable models are used in many areas of the social and behavioural sciences. This book introduces such models to applied statisticians and research workers interested in exploring the structure of covariance and correlation matrices in terms of a small number of unobservable constructs. The emphasis is on the practical application of the procedures rather than on detailed discussion of their mathematical and statistical properties. The book includes chapters on factor analysis, the LISREL model, and latent variable models for categorical data.},
  address = {London},
  author = {Brian Sidney Everitt},
  doi = {10.1007/978-94-009-5564-6},
  isbn = {0412253100},
  pages = {viii + 108},
  publisher = {Chapman and Hall},
  series = {Monographs on Statistics and Applied Probability},
  title = {An Introduction to Latent Variable Models},
  year = {1984}
}

@article{Geman1984,
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  author = {Geman, Stuart and Geman, Donald},
  doi = {10.1109/TPAMI.1984.4767596},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month = {11},
  number = {6},
  openalex = {W2020999234},
  pages = {721--741},
  pdf = {https://cs.uwaterloo.ca/~mannr/cs886-w10/GemanandGeman84.pdf},
  title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
  volume = {6},
  year = {1984}
}

@article{Lloyd1982,
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy.},
  author = {Stuart P. Lloyd},
  doi = {10.1109/TIT.1982.1056489},
  journal = {IEEE Transactions on Information Theory},
  month = {3},
  note = {Originally appeared as Bell Telephone Labs Memorandum, Murray Hill, NJ},
  number = {2},
  pages = {129--137},
  publisher = {IEEE},
  title = {Least Squares Quantization in PCM},
  volume = {28},
  year = {1982}
}

@article{Rubin1982,
  abstract = {The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for a $q  imes q$ symmetric matrix where $q$ is the number of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation.},
  author = {Donald B. Rubin and Dorothy T. Thayer},
  doi = {10.1007/BF02293851},
  issn = {0033-3123},
  journal = {Psychometrika},
  month = {3},
  number = {1},
  pages = {69--76},
  publisher = {Springer-Verlag},
  title = {EM algorithms for ML factor analysis},
  url = {https://link.springer.com/article/10.1007/BF02293851},
  volume = {47},
  year = {1982}
}

@article{Adler1981,
  abstract = {I formulate a successive over-relaxation (SOR) procedure for the Monte Carlo evaluation of the Euclidean partition function for multiquadratic actions (such as the Yang-Mills action with canonical gauge fixing). The convergence analysis for the quadratic-action (Abelian) case shows that as thermalization proceeds the mean nodal fields relax according to the difference equation arising from the standard SOR analysis of the associated classical Euclidean field equation. Hence, SOR should accelerate the thermalization process, just as it accelerates convergence in the numerical solution of second-order elliptic differential equations.},
  author = {Adler, Stephen L.},
  doi = {10.1103/PhysRevD.23.2901},
  journal = {Physical Review D},
  month = {6},
  number = {12},
  openalex = {W1977671418},
  pages = {2901--2904},
  publisher = {American Physical Society},
  title = {Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions},
  volume = {23},
  year = {1981}
}

@article{Parisi1981,
  abstract = {If the equilibrium properties of a statistical system are obtained by solving numerically the associated Langevin equation describing the approach to equilibrium, the connected correlation functions can be computed directly with small effort and high precision.},
  author = {Parisi, Giorgio},
  doi = {10.1016/0550-3213(81)90056-0},
  journal = {Nuclear Physics B},
  month = {5},
  number = {3},
  pages = {378--384},
  publisher = {Elsevier},
  title = {Correlation functions and computer simulations},
  url = {https://www.sciencedirect.com/science/article/abs/pii/0550321381900560},
  volume = {180},
  year = {1981}
}

@article{Dawid1980,
  abstract = {A general calculus of conditional independence is developed, suitable for application to a wide range of statistical concepts such as sufficiency, parameter-identification, adequacy and ancillarity. A vehicle for this theory is the statistical operation, a structure-preserving map between statistical spaces. Concepts such as completeness and identifiability of mixtures arise naturally and play an important part. Some general theorems are exemplified by applications to ancillarity, including a study of a Bayesian definition of ancillarity in the presence of nuisance parameters.},
  author = {Dawid, A. Philip},
  doi = {10.1214/aos/1176345011},
  journal = {The Annals of Statistics},
  month = {5},
  number = {3},
  openalex = {W2089330921},
  pages = {598--617},
  title = {Conditional Independence for Statistical Operations},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-8/issue-3/Conditional-Independence-for-Statistical-Operations/10.1214/aos/1176345011.full},
  volume = {8},
  year = {1980}
}

@article{Fukushima1980,
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (gestalt) of their shapes without being affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells.},
  author = {Kunihiko Fukushima},
  doi = {10.1007/BF00344251},
  journal = {Biological Cybernetics},
  language = {english},
  number = {4},
  openalex = {W2101926813},
  pages = {193--202},
  pdf = {https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf},
  publisher = {Springer},
  title = {Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  volume = {36},
  year = {1980}
}

@article{Dawid1979,
  abstract = {Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building.},
  author = {Dawid, A. P.},
  doi = {10.1111/j.2517-6161.1979.tb01052.x},
  journal = {Journal of the Royal Statistical Society, Series B},
  number = {1},
  pages = {1--31},
  pdf = {https://people.csail.mit.edu/tdanford/discovering-causal-graphs-papers/dawid-79.pdf},
  title = {Conditional Independence in Statistical Theory (with discussion)},
  volume = {41},
  year = {1979}
}

@article{Dempster1977,
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  author = {Dempster, Arthur P. and Laird, Nan M. and Rubin, Donald B.},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  month = {9},
  number = {1},
  openalex = {W2049633694},
  pages = {1--38},
  pdf = {https://web.mit.edu/6.435/www/Dempster77.pdf},
  publisher = {Oxford University Press},
  title = {Maximum Likelihood from Incomplete Data Via the EM Algorithm},
  volume = {39},
  year = {1977}
}

@book{Duda1973,
  address = {New York},
  author = {Richard O. Duda and Peter E. Hart},
  isbn = {0471223611},
  note = {A Wiley-Interscience publication},
  pages = {xvii + 482},
  publisher = {Wiley-Interscience},
  title = {Pattern Classification and Scene Analysis},
  year = {1973}
}

@article{Nelder1972,
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
  author = {Nelder, John Ashworth and Wedderburn, Robert William Mead},
  doi = {10.2307/2344614},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  number = {3},
  pages = {370--384},
  title = {Generalized Linear Models},
  url = {https://www.jstor.org/stable/2344614},
  volume = {135},
  year = {1972}
}

@book{Feller1966,
  address = {New York},
  author = {Feller, William},
  edition = {Second},
  isbn = {0471257095},
  pages = {xxiv + 669},
  publisher = {John Wiley & Sons},
  title = {An Introduction to Probability Theory and its Applications},
  volume = {2},
  year = {1971}
}

@book{Rao1971,
  abstract = {This book provides a comprehensive treatment of generalized inverses of matrices and their applications. Topics covered include notations and preliminaries, generalized inverse of a matrix, three basic types of g-inverses, other special types of g-inverse, projectors and idempotent matrices, simultaneous reduction of hermitian forms, estimation of parameters in linear models, conditions for optimality and validity of least-squares theory, distribution of quadratic forms, miscellaneous applications of g-inverses, and computational methods. The book serves as a foundational reference in the field of generalized matrix inverses with applications to statistics and linear models.},
  address = {New York},
  author = {Rao, C. Radhakrishna and Mitra, Sujit Kumar},
  isbn = {978-0-471-70821-6},
  pages = {xiv + 240},
  publisher = {John Wiley & Sons},
  series = {Wiley Series in Probability and Mathematical Statistics},
  title = {Generalized Inverse of Matrices and Its Applications},
  year = {1971}
}

@article{Hastings1970,
  abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
  author = {Hastings, W. K.},
  doi = {10.1093/biomet/57.1.97},
  journal = {Biometrika},
  month = {4},
  number = {1},
  pages = {97--109},
  publisher = {Oxford University Press},
  title = {Monte Carlo sampling methods using Markov chains and their applications},
  volume = {57},
  year = {1970}
}

@book{Minsky1969,
  abstract = {Seminal work providing rigorous mathematical analysis of single-layer neural networks, demonstrating fundamental computational limitations of perceptrons including their inability to compute XOR and connectedness problems. The book proved that perceptrons cannot solve linearly non-separable problems and showed that increasing network complexity does not automatically overcome these constraints. This critical examination of early neural network models significantly influenced AI research direction in the 1970s and 1980s, contributing to a shift towards symbolic AI approaches.},
  address = {Cambridge, MA},
  author = {Minsky, Marvin L. and Papert, Seymour A.},
  isbn = {9780262630221},
  note = {Expanded edition 1990},
  openalex = {W2124018397},
  pages = {258},
  publisher = {MIT Press},
  title = {Perceptrons: An Introduction to Computational Geometry},
  year = {1969}
}

@book{Sagan1969,
  address = {Mineola, NY},
  author = {Hans Sagan},
  edition = {Revised},
  isbn = {9780486673660},
  note = {Originally published 1969, reprinted 1992},
  pages = {480},
  publisher = {Dover Publications},
  series = {Dover Books on Mathematics},
  title = {Introduction to the Calculus of Variations},
  year = {1969}
}

@article{Walker1969,
  abstract = {This paper considers a random sample of size n taken from a distribution having a density depending on a real parameter θ, where θ has an absolutely continuous prior distribution with density π(θ). Under suitable regularity conditions, it is proved rigorously that the posterior distribution of θ will be asymptotically normal when n tends to infinity, with mean equal to the maximum-likelihood estimator and variance equal to the reciprocal of the second derivative of the logarithm of the likelihood function evaluated at the maximum-likelihood estimator, independently of the form of π(θ).},
  author = {Walker, A. M.},
  doi = {10.1111/j.2517-6161.1969.tb00767.x},
  issn = {0035-9246},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  number = {1},
  pages = {80--88},
  publisher = {Wiley},
  title = {On the asymptotic behaviour of posterior distributions},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1969.tb00767.x},
  volume = {31},
  year = {1969}
}

@book{Lazarsfeld1968,
  address = {Boston},
  author = {Paul Felix Lazarsfeld and Neil W. Henry},
  note = {Foundational work in latent variable analysis and social research methodology},
  pages = {294},
  publisher = {Houghton Mifflin},
  title = {Latent Structure Analysis},
  year = {1968}
}

@article{Cover1967,
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R must be at least as great as the Bayes probability of error R*---the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M-category case that R* ≤ R ≤ R*(2 --MR*/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  author = {Thomas M. Cover and Peter E. Hart},
  doi = {10.1109/TIT.1967.1053964},
  journal = {IEEE Transactions on Information Theory},
  month = {1},
  number = {1},
  openalex = {W2122111042},
  pages = {21--27},
  title = {Nearest neighbor pattern classification},
  url = {https://ieeexplore.ieee.org/document/1053964/},
  volume = {13},
  year = {1967}
}

@inproceedings{MacQueen1967,
  abstract = {The main purpose of this paper is to describe a process for partitioning an N-dimensional population into $k$ sets on the basis of a sample. The process, which is called `$k$-means,' appears to give partitions which are reasonably efficient in the sense of within-class variance. Some mathematical properties of the process are discussed along with several applications in psychological and biological taxonomy.},
  address = {Berkeley, California},
  author = {MacQueen, James B.},
  booktitle = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
  doi = {10.1525/9780520313366-005},
  editor = {Le Cam, Lucien M. and Neyman, Jerzy},
  pages = {281--297},
  pdf = {https://digitalassets.lib.berkeley.edu/math/ucb/text/math_s5_v1_article-17.pdf},
  publisher = {University of California Press},
  series = {Berkeley Symposium on Mathematical Statistics and Probability},
  title = {Some methods for classification and analysis of multivariate observations},
  volume = {1},
  year = {1967}
}

@book{Abramowitz1965,
  abstract = {Comprehensive reference work providing scientific investigations with a self-contained summary of higher mathematical functions including Bessel, hypergeometric, and elliptic functions. Contains formulas, graphs, mathematical tables, differential equations, definite and indefinite integrals, inequalities, recurrence relations, and power series. Originally published by the National Bureau of Standards as Applied Mathematics Series Number 55, this handbook standardizes notations and normalizations for special mathematical functions.},
  author = {Abramowitz, Milton and Stegun, Irene A.},
  isbn = {9780486612720},
  note = {Reprint of National Bureau of Standards publication, 1964},
  number = {55},
  openalex = {W2798118393},
  pages = {1046},
  pdf = {https://personal.math.ubc.ca/~cbm/aands/abramowitz_and_stegun.pdf},
  publisher = {Dover Publications},
  series = {Applied Mathematics Series},
  title = {Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables},
  year = {1965}
}

@article{Anderson1963,
  abstract = {The asymptotic distribution of the characteristic roots and (normalized) vectors of a sample covariance matrix is given when the observations are from a multivariate normal distribution whose covariance matrix has characteristic roots of arbitrary multiplicity. The elements of each characteristic vector are the coefficients of a principal component (with sum of squares of coefficients being unity), and the corresponding characteristic root is the variance of the principal component. Tests of hypotheses of equality of population roots are treated, and confidence intervals are given for assumed equal roots.},
  author = {Anderson, T. W.},
  doi = {10.1214/aoms/1177704248},
  journal = {The Annals of Mathematical Statistics},
  month = {3},
  number = {1},
  openalex = {W2165918462},
  pages = {122--148},
  pdf = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-34/issue-1/Asymptotic-Theory-for-Principal-Component-Analysis/10.1214/aoms/1177704248.pdf},
  title = {Asymptotic Theory for Principal Component Analysis},
  volume = {34},
  year = {1963}
}

@book{Rosenblatt1962,
  abstract = {This book presents a comprehensive theory of brain mechanisms based on perceptron models. Rosenblatt's intention was not just to describe the perceptron machine, but to put forward a theory of neurodynamics. The work is divided into four parts: historical review of alternative approaches to brain modeling and basic perceptron concepts; three-layer series-coupled perceptrons; multi-layer and cross-coupled perceptrons; and back-coupled perceptrons with problems for future study. The book summarizes Rosenblatt's pioneering work on artificial neural networks and is considered a foundational text in the development of deep learning and artificial intelligence.},
  address = {Washington, DC},
  author = {Frank Rosenblatt},
  note = {Originally published as technical report No. 1196-G-8 by Cornell Aeronautical Laboratory in 1961},
  pages = {616},
  publisher = {Spartan Books},
  title = {Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms},
  url = {https://gwern.net/doc/ai/nn/1962-rosenblatt-principlesofneurodynamics.pdf},
  year = {1962}
}

@book{Bellman1961,
  abstract = {The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields.},
  author = {Bellman, Richard E.},
  doi = {10.1515/9781400874668},
  isbn = {9781400874668},
  openalex = {W2171029115},
  pages = {276},
  publisher = {Princeton University Press},
  series = {Princeton Legacy Library},
  title = {Adaptive Control Processes: A Guided Tour},
  volume = {2045},
  year = {1961}
}

@article{Hubel1959,
  abstract = {Recordings were made from single cells in the striate cortex of lightly anaesthetized cats. The retinas were stimulated separately or simultaneously with light spots of various sizes and shapes. In the light-adapted state cortical cells were active in the absence of additional light stimulation. Increasing the depth of anaesthesia tended to suppress this maintained activity. Restricted retinal areas which on illumination influenced the firing of single cortical units were called receptive fields. These fields were usually subdivided into mutually antagonistic excitatory and inhibitory regions.},
  author = {David H. Hubel and Torsten N. Wiesel},
  doi = {10.1113/jphysiol.1959.sp006308},
  journal = {The Journal of Physiology},
  number = {3},
  pages = {574--591},
  pmcid = {PMC1363130},
  pmid = {14403679},
  title = {Receptive fields of single neurones in the cat's striate cortex},
  volume = {148},
  year = {1959}
}

@incollection{Lawley1953,
  address = {Stockholm},
  author = {Lawley, Derrick Norman},
  booktitle = {Uppsala Symposium on Psychological Factor Analysis},
  month = {3},
  note = {Symposium held March 17--19, 1953},
  number = {3},
  pages = {35--42},
  publisher = {Almqvist & Wiksell},
  series = {Nordisk Psykologi Monograph Series},
  title = {A Modified Method of Estimation in Factor Analysis and Some Large Sample Results},
  year = {1953}
}

@article{Metropolis1953,
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  doi = {10.1063/1.1699114},
  journal = {The Journal of Chemical Physics},
  month = {6},
  number = {6},
  openalex = {W2056760934},
  pages = {1087--1092},
  publisher = {AIP Publishing},
  title = {Equation of State Calculations by Fast Computing Machines},
  url = {https://doi.org/10.1063/1.1699114},
  volume = {21},
  year = {1953}
}

@inproceedings{KuhnTucker1951,
  abstract = {This paper introduces the fundamental necessary conditions for optimality in nonlinear programming problems with inequality constraints. The conditions, now known as the Karush-Kuhn-Tucker (KKT) conditions, generalize the method of Lagrange multipliers to handle inequality constraints and form the theoretical foundation for modern constrained optimization theory.},
  address = {Berkeley},
  author = {Kuhn, Harold William and Tucker, Albert William},
  booktitle = {Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability},
  editor = {Neyman, Jerzy},
  openalex = {W2264885058},
  pages = {481--492},
  publisher = {University of California Press},
  title = {Nonlinear Programming},
  year = {1951}
}

@article{KullbackLeibler1951,
  abstract = {This paper introduces a measure of information for discrimination between two hypotheses or between two probability distributions. The measure is based on the concept of information provided by data for inference, and relates to the fundamental problem of sufficiency in statistics. The authors develop the theoretical framework for what would later become known as the Kullback-Leibler divergence, establishing important connections between information theory and statistical inference. The work demonstrates that the information deviation between finite measures cannot be increased by statistical operations, and provides key results on sufficiency for dominated sets, generalizing the Fisher-Neyman criterion for sufficient statistics.},
  author = {Kullback, Solomon and Leibler, Richard A.},
  doi = {10.1214/aoms/1177729694},
  journal = {The Annals of Mathematical Statistics},
  month = {3},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  title = {On Information and Sufficiency},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
  volume = {22},
  year = {1951}
}

@article{Metropolis1949,
  abstract = {We shall present here the motivation and a general description of a method dealing with a class of problems in mathematical physics. The method is, essentially, a statistical approach to the study of differential equations, or more generally, of integro-differential equations that occur in various branches of the natural sciences.},
  author = {Metropolis, Nicholas and Ulam, Stanislaw},
  doi = {10.1080/01621459.1949.10483310},
  journal = {Journal of the American Statistical Association},
  month = {9},
  number = {247},
  pages = {335--341},
  publisher = {Taylor & Francis},
  title = {The Monte Carlo method},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1949.10483310},
  volume = {44},
  year = {1949}
}

@article{Shannon1948,
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  author = {Shannon, Claude E.},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  issn = {1538-7305},
  journal = {The Bell System Technical Journal},
  month = {7},
  number = {3},
  openalex = {W2993383518},
  pages = {379--423 and 623--656},
  pdf = {https://ia803209.us.archive.org/27/items/bstj27-3-379/bstj27-3-379_text.pdf},
  publisher = {Wiley},
  title = {A Mathematical Theory of Communication},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x},
  volume = {27},
  year = {1948}
}

@article{Cox1946,
  abstract = {This foundational paper presents Cox's approach to deriving the laws of probability theory from logical postulates. Cox establishes the mathematical framework that connects logical reasoning with probability theory, laying the groundwork for what would later become known as Cox's theorem. The paper addresses the fundamental question of how degrees of reasonable expectation should be quantified and relates probability to frequency in a rigorous mathematical treatment.},
  author = {Richard Threlkeld Cox},
  doi = {10.1119/1.1990764},
  journal = {American Journal of Physics},
  month = {1},
  number = {1},
  openalex = {W1493677934},
  pages = {1--13},
  publisher = {American Association of Physics Teachers},
  title = {Probability, frequency and reasonable expectation},
  url = {https://pubs.aip.org/aapt/ajp/article/14/1/1/1033475},
  volume = {14},
  year = {1946}
}

@article{McCulloch1943,
  abstract = {Because of the all-or-none character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. These results are of interest as steps toward a statistical theory of the action of the central nervous system.},
  author = {McCulloch, Warren Sturgis and Pitts, Walter Harry},
  doi = {10.1007/BF02478259},
  journal = {Bulletin of Mathematical Biophysics},
  note = {Reprinted in Anderson and Rosenfeld (1988). Foundational work in artificial neural networks and computational neuroscience},
  openalex = {W1995341919},
  pages = {115--133},
  publisher = {Springer},
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  year = {1943}
}

@mastersthesis{Karush1939,
  abstract = {This master's thesis introduced the necessary conditions for optimization problems with inequality constraints, which later became fundamental to nonlinear programming theory. The work established what are now known as the Karush-Kuhn-Tucker (KKT) conditions, though the results remained unpublished and were independently rediscovered by Kuhn and Tucker in 1951. The thesis concerns the problem of minimizing a function of several variables subject to inequality constraints and provides the mathematical foundation for modern constrained optimization theory.},
  address = {Chicago, IL, USA},
  author = {Karush, William},
  keywords = {optimization, constrained optimization, KKT conditions, nonlinear programming},
  note = {Unpublished thesis that introduced what later became known as the Karush-Kuhn-Tucker (KKT) conditions for constrained optimization},
  school = {Department of Mathematics, University of Chicago},
  title = {Minima of Functions of Several Variables with Inequalities as Side Constraints},
  type = {Master's thesis},
  year = {1939}
}

@article{Hotelling1936,
  author = {Hotelling, Harold},
  doi = {10.1093/biomet/28.3-4.321},
  journal = {Biometrika},
  month = {12},
  number = {3-4},
  openalex = {W2025341678},
  pages = {321--377},
  title = {Relations between two sets of variates},
  url = {https://academic.oup.com/biomet/article-abstract/28/3-4/321/220073},
  volume = {28},
  year = {1936}
}

@article{Hotelling1933,
  abstract = {This paper presents the mathematical framework for principal component analysis (PCA), a method for reducing a complex of statistical variables to its essential features. The method replaces the original variables by a smaller number of derived variables, called principal components, which are linear combinations of the original variables that account for maximum variance. The paper details the problem statement, derives the method of analysis, shows its geometrical meaning, illustrates solution methods, and discusses derivative problems in multivariate statistical analysis.},
  author = {Harold Hotelling},
  doi = {10.1037/h0071325},
  journal = {Journal of Educational Psychology},
  month = {9},
  number = {6},
  pages = {417--441},
  publisher = {American Psychological Association},
  title = {Analysis of a Complex of Statistical Variables into Principal Components},
  volume = {24},
  year = {1933}
}

@article{Pearson1901,
  abstract = {This seminal paper presents the geometric foundations of what would later become known as principal component analysis (PCA). Pearson addresses the problem of constructing lines and planes of closest fit to systems of points in multidimensional space, defining best-fitting lines and planes as those that minimize the average squared perpendicular distance from the points. The work establishes the mathematical framework for finding low-dimensional subspaces that optimally represent high-dimensional data in the least squares sense, introducing concepts fundamental to modern multivariate statistics and dimensionality reduction techniques.},
  author = {Pearson, Karl},
  doi = {10.1080/14786440109462720},
  journal = {The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science},
  month = {11},
  number = {11},
  openalex = {W2294798173},
  pages = {559--572},
  publisher = {Taylor & Francis},
  series = {Sixth Series},
  title = {LIII. On lines and planes of closest fit to systems of points in space},
  url = {https://doi.org/10.1080/14786440109462720},
  volume = {2},
  year = {1901}
}
