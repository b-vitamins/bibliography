@inproceedings{wang2023optimal,
  abstract = {In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetrics structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.},
  author = {Tongzhou Wang and Antonio Torralba and Phillip Isola and Amy Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023optimal.pdf:pdf},
  mdate = {2024-07-31},
  pages = {36411--36430},
  pdf = {https://proceedings.mlr.press/v202/wang23al/wang23al.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning},
  url = {https://proceedings.mlr.press/v202/wang23al.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ren2023dimensionindependent,
  abstract = {Certified_Watermarks is the first to provide a watermark certificate against $l_2$-norm watermark removal attacks, by leveraging the randomized smoothing techniques for certified robustness to adversarial attacks. However, the randomized smoothing techniques suffer from hardness of certified robustness in high-dimensional space against $l_p$-norm attacks for large $p$ ($p>2$). The certified watermark method based on the randomized smoothing is no exception, i.e., fails to provide meaningful certificates in high-dimensional space against the $l_p$-norm watermark removal attacks ($p>2$). By leveraging mollifier theory, this paper proposes a mollifier smoothing method with dimension-independent certified radius of our proposed smooth classifier, for conducting the certified watermark problem against the $l_p$-norm watermark removal attacks ($1 \leq p \leq \infty$) for high parameter dimension $d$. Based on partial differential equation (PDE) theory, an approximation of mollifier smoothing is developed to alleviate the inefficiency of sampling and prediction in the randomized smoothing as well as numerical integration in the mollifier smoothing, while maintaining the certified watermark against the $l_p$-norm watermark removal attacks ($1 \leq p \leq \infty$).},
  author = {Jiaxiang Ren and Yang Zhou and Jiayin Jin and Lingjuan Lyu and Da Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ren2023dimensionindependent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28976--29008},
  pdf = {https://proceedings.mlr.press/v202/ren23c/ren23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dimension-independent Certified Neural Network Watermarks via Mollifier Smoothing},
  url = {https://proceedings.mlr.press/v202/ren23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023hyperbolic,
  abstract = {The non-Euclidean geometry of hyperbolic spaces has recently garnered considerable attention in the realm of representation learning. Current endeavors in hyperbolic representation largely presuppose that the underlying hierarchies can be automatically inferred and preserved through the adaptive optimization process. This assumption, however, is questionable and requires further validation. In this work, we first introduce a position-tracking mechanism to scrutinize existing prevalent hyperbolic models, revealing that the learned representations are sub-optimal and unsatisfactory. To address this, we propose a simple yet effective method, hyperbolic informed embedding (HIE), by incorporating cost-free hierarchical information deduced from the hyperbolic distance of the node to the origin (i.e., induced hyperbolic norm) to advance existing hyperbolic models. The proposed method HIE is both task-agnostic and model-agnostic, enabling its seamless integration with a broad spectrum of models and tasks. Extensive experiments across various models and different tasks demonstrate the versatility and adaptability of the proposed method. Remarkably, our method achieves a remarkable improvement of up to 21.4\% compared to the competing baselines.},
  author = {Menglin Yang and Min Zhou and Rex Ying and Yankai Chen and Irwin King},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023hyperbolic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39639--39659},
  pdf = {https://proceedings.mlr.press/v202/yang23u/yang23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyperbolic Representation Learning: Revisiting and Advancing},
  url = {https://proceedings.mlr.press/v202/yang23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023continuation,
  abstract = {Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.},
  author = {Xi Lin and Zhiyuan Yang and Xiaoyuan Zhang and Qingfu Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023continuation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21288--21311},
  pdf = {https://proceedings.mlr.press/v202/lin23n/lin23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continuation Path Learning for Homotopy Optimization},
  url = {https://proceedings.mlr.press/v202/lin23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{antoniadis2023paging,
  abstract = {Paging is a prototypical problem in the area of online algorithms. It has also played a central role in the development of learning-augmented algorithms. Previous work on learning-augmented paging has investigated predictions on (i) when the current page will be requested again (reoccurrence predictions), (ii) the current state of the cache in an optimal algorithm (state predictions), (iii) all requests until the current page gets requested again, and (iv) the relative order in which pages are requested. We study learning-augmented paging from the new perspective of requiring the least possible amount of predicted information. More specifically, the predictions obtained alongside each page request are limited to one bit only. We study two variants of such 1-bit predictions: discard predictions indicating whether a page can be safely evicted, and phase predictions indicating whether a page will be requested again within a phase. For both types of predictions, we develop algorithms that are consistent and robust, and we demonstrate that they have near-optimal performance.},
  author = {Antonios Antoniadis and Joan Boyar and Marek Elis and Lene Monrad Favrholdt and Ruben Hoeksma and Kim S. Larsen and Adam Polak and Bertrand Simon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/antoniadis2023paging.pdf:pdf},
  mdate = {2024-01-12},
  pages = {952--968},
  pdf = {https://proceedings.mlr.press/v202/antoniadis23a/antoniadis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Paging with Succinct Predictions},
  url = {https://proceedings.mlr.press/v202/antoniadis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vyas2023competitive,
  abstract = {We study the problem of convergence to a stationary point in zero-sum games. We propose competitive gradient optimization (CGO), a gradient-based method that incorporates the interactions between two players in zero-sum games for its iterative updates. We provide a continuous-time analysis of CGO and its convergence properties while showing that in the continuous limit, previous methods degenerate to their gradient descent ascent (GDA) variants. We further provide a rate of convergence to stationary points in the discrete-time setting. We propose a generalized class of $\alpha$-coherent functions and show that for strictly $\alpha$-coherent functions, CGO ensures convergence to a saddle point. Moreover, we propose optimistic CGO (oCGO), an optimistic variant, for which we show a convergence rate of $O(1/n)$ to saddle points for $\alpha$-coherent functions.},
  author = {Abhijeet Vyas and Brian Bullins and Kamyar Azizzadenesheli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vyas2023competitive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35243--35276},
  pdf = {https://proceedings.mlr.press/v202/vyas23a/vyas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Competitive Gradient Optimization},
  url = {https://proceedings.mlr.press/v202/vyas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jovanovi2023fare,
  abstract = {Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. Recent regulatory directives stress the need for FRL methods that provide practical certificates, i.e., provable upper bounds on the unfairness of any downstream classifier trained on preprocessed data, which directly provides assurance in a practical scenario. Creating such FRL methods is an important challenge that remains unsolved. In this work, we address that challenge and introduce FARE (Fairness with Restricted Encoders), the first FRL method with practical fairness certificates. FARE is based on our key insight that restricting the representation space of the encoder enables the derivation of practical guarantees, while still permitting favorable accuracy-fairness tradeoffs for suitable instantiations, such as one we propose based on fair trees.},
  author = {Nikola Jovanović and Mislav Balunović and Dimitar Iliev Dimitrov and Martin T. Vechev},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jovanovi2023fare.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15401--15420},
  pdf = {https://proceedings.mlr.press/v202/jovanovic23a/jovanovic23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FARE}: Provably Fair Representation Learning with Practical Certificates},
  url = {https://proceedings.mlr.press/v202/jovanovic23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{antoniadis2023mixing,
  abstract = {A major technique in learning-augmented online algorithms is combining multiple algorithms or predictors. Since the performance of each predictor may vary over time, it is desirable to use not the single best predictor as a benchmark, but rather a dynamic combination which follows different predictors at different times. We design algorithms that combine predictions and are competitive against such dynamic combinations for a wide class of online problems, namely, metrical task systems. Against the best (in hindsight) unconstrained combination of $\ell$ predictors, we obtain a competitive ratio of $O(\ell^2)$, and show that this is best possible. However, for a benchmark with slightly constrained number of switches between different predictors, we can get a $(1+\varepsilon)$-competitive algorithm. Moreover, our algorithms can be adapted to access predictors in a bandit-like fashion, querying only one predictor at a time.},
  author = {Antonios Antoniadis and Christian Coester and Marek Elis and Adam Polak and Bertrand Simon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/antoniadis2023mixing.pdf:pdf},
  mdate = {2024-01-12},
  pages = {969--983},
  pdf = {https://proceedings.mlr.press/v202/antoniadis23b/antoniadis23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mixing Predictions for Online Metric Algorithms},
  url = {https://proceedings.mlr.press/v202/antoniadis23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chiang2023tighter,
  abstract = {Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $\mathrm{TC}^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.},
  author = {David Chiang and Peter Cholak and Anand Pillay},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chiang2023tighter.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5544--5562},
  pdf = {https://proceedings.mlr.press/v202/chiang23a/chiang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tighter Bounds on the Expressivity of Transformer Encoders},
  url = {https://proceedings.mlr.press/v202/chiang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dehghani2023scaling,
  abstract = {The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for "LLM-like" scaling in vision, and provides key steps towards getting there.},
  author = {Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Peter Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme Ruiz and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin Fathy Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetic and Dustin Tran and Thomas Kipf and Mario Lucic and Xiaohua Zhai and Daniel Keysers and Jeremiah J. Harmsen and Neil Houlsby},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dehghani2023scaling.pdf:pdf},
  mdate = {2023-09-18},
  pages = {7480--7512},
  pdf = {https://proceedings.mlr.press/v202/dehghani23a/dehghani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Vision Transformers to 22 Billion Parameters},
  url = {https://proceedings.mlr.press/v202/dehghani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{morris2023wl,
  abstract = {Recently, many works studied the expressive power of graph neural networks ({GNN}s) by linking it to the 1-dimensional {Weisfeiler-Leman} algorithm (1-{WL}). Here, the 1-{WL} is a well-studied heuristic for the graph isomorphism problem, which iteratively colors or partitions a graph's vertex set. While this connection has led to significant advances in understanding and enhancing {GNN}s' expressive power, it does not provide insights into their generalization performance, i.e., their ability to make meaningful predictions beyond the training set. In this paper, we study {GNN}s' generalization ability through the lens of {Vapnik-Chervonenkis} ({VC}) dimension theory in two settings, focusing on graph-level predictions. First, when no upper bound on the graphs' order is known, we show that the bitlength of {GNN}s' weights tightly bounds their {VC} dimension. Further, we derive an upper bound for {GNN}s' {VC} dimension using the number of colors produced by the 1-{WL}. Secondly, when an upper bound on the graphs' order is known, we show a tight connection between the number of graphs distinguishable by the 1-{WL} and {GNN}s' {VC} dimension. Our empirical study confirms the validity of our theoretical findings.},
  author = {Christopher Morris and Floris Geerts and Jan Tönshoff and Martin Grohe},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/morris2023wl.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25275--25302},
  pdf = {https://proceedings.mlr.press/v202/morris23a/morris23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{WL} meet {VC}},
  url = {https://proceedings.mlr.press/v202/morris23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cardoso2023state,
  abstract = {Non-linear state-space models, also known as general hidden {Markov} models ({HMM}), are ubiquitous in statistical machine learning, being the most classical generative models for serial data and sequences. Learning in {HMM}, either via {Maximum Likelihood Estimation} ({MLE}) or {Markov Score Climbing} ({MSC}) requires the estimation of the- smoothing expectation of some additive functionals. Controlling the bias and the variance of this estimation is crucial to establish the convergence of learning algorithms. Our first contribution is to design a novel additive smoothing algorithm, the {Parisian} particle {Gibbs} ({PPG}) sampler, which can be viewed as a {PaRIS} ({Olsson}, {Westerborn} 2017) algorithm driven by conditional {SMC} moves, resulting in bias-reduced estimates of the targeted quantities. We substantiate the {PPG} algorithm with theoretical results, including new bounds on bias and variance as well as deviation inequalities. We then establish, in the learning context, and under standard assumptions, non-asymptotic bounds highlighting the value of bias reduction and the implicit {Rao--Blackwellization} of {PPG}. These are the first non-asymptotic results of this kind in this setting. We illustrate our theoretical results with numerical experiments supporting our claims.},
  author = {Gabriel Cardoso and Yazid Janati El Idrissi and Sylvain Le Corff and Eric Moulines and Jimmy Olsson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cardoso2023state.pdf:pdf},
  mdate = {2024-02-05},
  pages = {3625--3675},
  pdf = {https://proceedings.mlr.press/v202/cardoso23a/cardoso23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {State and parameter learning with {PARIS} particle {Gibbs}},
  url = {https://proceedings.mlr.press/v202/cardoso23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khler2023rigid,
  abstract = {Normalizing flows ({NF}) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal.},
  author = {Jonas Köhler and Michele Invernizzi and Pim de Haan and Frank Noé},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khler2023rigid.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17301--17326},
  pdf = {https://proceedings.mlr.press/v202/kohler23a/kohler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rigid Body Flows for Sampling Molecular Crystal Structures},
  url = {https://proceedings.mlr.press/v202/kohler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023differentially,
  abstract = {Training deep learning models with differential privacy ({DP}) results in a degradation of performance. The training dynamics of models with {DP} show a significant difference from standard training, whereas understanding the geometric properties of private learning remains largely unexplored. In this paper, we investigate sharpness, a key factor in achieving better generalization, in private learning. We show that flat minima can help reduce the negative effects of per-example gradient clipping and the addition of {Gaussian} noise. We then verify the effectiveness of {Sharpness-Aware Minimization} ({SAM}) for seeking flat minima in private learning. However, we also discover that {SAM} is detrimental to the privacy budget and computational time due to its two-step optimization. Thus, we propose a new sharpness-aware training method that mitigates the privacy-optimization trade-off. Our experimental results demonstrate that the proposed method improves the performance of deep learning models with {DP} from both scratch and fine-tuning.},
  author = {Jinseong Park and Hoki Kim and Yujin Choi and Jaewook Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023differentially.pdf:pdf},
  mdate = {2024-02-05},
  pages = {27204--27224},
  pdf = {https://proceedings.mlr.press/v202/park23g/park23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Sharpness-Aware Training},
  url = {https://proceedings.mlr.press/v202/park23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duan2023equivariant,
  abstract = {Recently, remarkable progress has been made by approximating {Nash} equilibrium ({NE}), correlated equilibrium ({CE}), and coarse correlated equilibrium ({CCE}) through function approximation that trains a neural network to predict equilibria from game representations. Furthermore, equivariant architectures are widely adopted in designing such equilibrium approximators in normal-form games. In this paper, we theoretically characterize the benefits and limitations of equivariant equilibrium approximators. For the benefits, we show that they enjoy better generalizability than general ones and can achieve better approximations when the payoff distribution is permutation-invariant. For the limitations, we discuss their drawbacks in terms of equilibrium selection and social welfare. Together, our results help to understand the role of equivariance in equilibrium approximators.},
  author = {Zhijian Duan and Yunxuan Ma and Xiaotie Deng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duan2023equivariant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8747--8778},
  pdf = {https://proceedings.mlr.press/v202/duan23d/duan23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Equivariant Equilibrium Approximators Beneficial?},
  url = {https://proceedings.mlr.press/v202/duan23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{munk2023uncertain,
  abstract = {We consider the problem of performing {Bayesian} inference in probabilistic models where observations are accompanied by uncertainty, referred to as 'uncertain evidence.' We explore how to interpret uncertain evidence, and by extension the importance of proper interpretation as it pertains to inference about latent variables.},
  author = {Andreas Munk and Alexander Mead and Frank Wood},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/munk2023uncertain.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25486--25500},
  pdf = {https://proceedings.mlr.press/v202/munk23a/munk23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Uncertain Evidence in Probabilistic Models and Stochastic Simulators},
  url = {https://proceedings.mlr.press/v202/munk23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023generalized,
  abstract = {We propose a new class of online learning algorithms, generalized implicit Follow-The-Regularized-Leader ({FTRL}), that expands the scope of {FTRL} framework. Generalized implicit {FTRL} can recover known algorithms, such as {FTRL} with linearized losses and implicit {FTRL}, and it allows the design of new update rules, as extensions of {aProx} and {Mirror-Prox} to {FTRL}. Our theory is constructive in the sense that it provides a simple unifying framework to design updates that directly improve the worst-case upper bound on the regret. The key idea is substituting the linearization of the losses with a {Fenchel-Young} inequality. We show the flexibility of the framework by proving that some known algorithms, like the {Mirror-Prox} updates, are instantiations of the generalized implicit {FTRL}. Finally, the new framework allows us to recover the temporal variation bound of implicit {OMD}, with the same computational complexity.},
  author = {Keyi Chen and Francesco Orabona},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023generalized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4826--4838},
  pdf = {https://proceedings.mlr.press/v202/chen23t/chen23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalized Implicit Follow-The-Regularized-Leader},
  url = {https://proceedings.mlr.press/v202/chen23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023correctness,
  abstract = {Recent work has shown that forward- and reverse- mode automatic differentiation ({AD}) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of {AD} when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which {AD} can be incorrect: the incorrect set on which the network is differentiable but {AD} does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. We further prove that {AD} always computes a {Clarke} subderivative even on the non-differentiable set. We also extend these results to neural networks possibly without bias parameters.},
  author = {Wonyeol Lee and Sejun Park and Alex Aiken},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023correctness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19094--19140},
  pdf = {https://proceedings.mlr.press/v202/lee23p/lee23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters},
  url = {https://proceedings.mlr.press/v202/lee23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023stratified,
  abstract = {Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection ({CPR}) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptive attacks. For instance, on {CIFAR}-10, {CPR} reduces the total robust loss (for different rejection losses) by at least 7.3\% under both seen and unseen attacks.},
  author = {Jiefeng Chen and Jayaram Raghuram and Jihye Choi and Xi Wu and Yingyu Liang and Somesh Jha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023stratified.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4867--4894},
  pdf = {https://proceedings.mlr.press/v202/chen23w/chen23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stratified Adversarial Robustness with Rejection},
  url = {https://proceedings.mlr.press/v202/chen23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023irnext,
  abstract = {We present {IRNeXt}, a simple yet effective convolutional network architecture for image restoration. Recently, {Transformer} models have dominated the field of image restoration due to the powerful ability of modeling long-range pixels interactions. In this paper, we excavate the potential of the convolutional neural network ({CNN}) and show that our {CNN}-based model can receive comparable or better performance than {Transformer} models with low computation overhead on several image restoration tasks. By re-examining the characteristics possessed by advanced image restoration algorithms, we discover several key factors leading to the performance improvement of restoration models. This motivates us to develop a novel network for image restoration based on cheap convolution operators. Comprehensive experiments demonstrate that {IRNeXt} delivers state-of-the-art performance among numerous datasets on a range of image restoration tasks with low computational complexity, including image dehazing, single-image defocus/motion deblurring, image deraining, and image desnowing.},
  author = {Yuning Cui and Wenqi Ren and Sining Yang and Xiaochun Cao and Alois Knoll},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023irnext.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6545--6564},
  pdf = {https://proceedings.mlr.press/v202/cui23d/cui23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{IRNeXt}: Rethinking Convolutional Network Design for Image Restoration},
  url = {https://proceedings.mlr.press/v202/cui23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{basu2023statistical,
  abstract = {Many modern high-performing machine learning models increasingly rely on scaling up models, e.g., transformer networks. Simultaneously, a parallel line of work aims to improve the model performance by augmenting an input instance with other (labeled) instances during inference. Examples of such augmentations include task-specific prompts and similar examples retrieved from the training data by a nonparametric component. Despite a growing literature showcasing the promise of these retrieval-based models, their theoretical underpinnings remain under-explored. In this paper, we present a formal treatment of retrieval-based models to characterize their performance via a novel statistical perspective. In particular, we study two broad classes of retrieval-based classification approaches: First, we analyze a local learning framework that employs an explicit local empirical risk minimization based on retrieved examples for each input instance. Interestingly, we show that breaking down the underlying learning task into local sub-tasks enables the model to employ a low complexity parametric component to ensure good overall performance. The second class of retrieval-based approaches we explore learns a global model using kernel methods to directly map an input instance and retrieved examples to a prediction, without explicitly solving a local learning task.},
  author = {Soumya Basu and Ankit Singh Rawat and Manzil Zaheer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/basu2023statistical.pdf:pdf},
  pages = {1852--1886},
  pdf = {https://proceedings.mlr.press/v202/basu23a/basu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Statistical Perspective on Retrieval-Based Models},
  url = {https://proceedings.mlr.press/v202/basu23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{weber2023global,
  abstract = {We study geodesically convex (g-convex) problems that can be written as a difference of {E}uclidean convex functions. This structure arises in key applications such as matrix scaling, {M}-estimators of scatter matrices, and {B}rascamp-{L}ieb inequalities. In particular, we exploit this structure to make use of the {C}onvex-{C}oncave {P}rocedure ({CCCP}), which helps us bypass potentially expensive {R}iemannian operations and leads to very competitive solvers. Importantly, unlike existing theory for {CCCP} that ensures convergence to stationary points, we exploit the overall g-convexity structure and provide iteration complexity results for global optimality. We illustrate our results by specializing them to a few concrete optimization problems that have been previously studied in the machine learning literature. We hope our work spurs the study of mixed {E}uclidean-{R}iemannian optimization algorithms.},
  author = {Melanie Weber and Suvrit Sra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/weber2023global.pdf:pdf},
  pages = {36790--36803},
  pdf = {https://proceedings.mlr.press/v202/weber23a/weber23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Global optimality for {E}uclidean {CCCP} under {R}iemannian convexity},
  url = {https://proceedings.mlr.press/v202/weber23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023power,
  abstract = {We propose ScaledGD($\lambda$), a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown and when the matrix is possibly ill-conditioned. Using overparameterized factor representations, ScaledGD($\lambda$) starts from a small random initialization and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. We show that under the Gaussian design, ScaledGD($\lambda$) converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimension. At the expense of light computational overhead incurred by preconditioners, ScaledGD($\lambda$) is remarkably robust to ill-conditioning compared to vanilla gradient descent (GD) even with overparameterization. This significantly improves over the convergence rate of vanilla GD which suffers from a polynomial dependency on the condition number.},
  author = {Xingyu Xu and Yandi Shen and Yuejie Chi and Cong Ma},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023power.pdf:pdf},
  pages = {38611--38654},
  pdf = {https://proceedings.mlr.press/v202/xu23o/xu23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing},
  url = {https://proceedings.mlr.press/v202/xu23o.html},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023transformed,
  abstract = {We study the problem of imputing missing values in a dataset, which has important applications in many domains. The key to missing value imputation is to capture the data distribution with incomplete samples and impute the missing values accordingly. In this paper, by leveraging the fact that any two batches of data with missing values come from the same data distribution, we propose to impute the missing values of two batches of samples by transforming them into a latent space through deep invertible functions and matching them distributionally. To learn the transformations and impute the missing values simultaneously, a simple and well-motivated algorithm is proposed. Our algorithm has fewer hyperparameters to fine-tune and generates high-quality imputations regardless of how missing values are generated. Extensive experiments over a large number of datasets and competing benchmark algorithms show that our method achieves state-of-the-art performance.},
  author = {He Zhao and Ke Sun and Amir Dezfouli and Edwin V. Bonilla},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023transformed.pdf:pdf},
  pages = {42159--42186},
  pdf = {https://proceedings.mlr.press/v202/zhao23h/zhao23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformed Distribution Matching for Missing Value Imputation},
  url = {https://proceedings.mlr.press/v202/zhao23h.html},
  volume = {202},
  year = {2023}
}

@inproceedings{cohen2023sequential,
  abstract = {We initiate the study of strategic behavior in screening processes with multiple classifiers. We focus on two contrasting settings: a conjunctive setting in which an individual must satisfy all classifiers simultaneously, and a sequential setting in which an individual to succeed must satisfy classifiers one at a time. We introduce the combination of strategic classification with screening processes and show that sequential screening pipelines exhibit new and surprising behavior where individuals can exploit the sequential ordering of the tests to "zig-zag" between classifiers without having to simultaneously satisfy all of them. We demonstrate that an individual can obtain a positive outcome using a limited manipulation budget even when far from the intersection of the positive regions of every classifier.},
  author = {Lee Cohen and Saeed Sharifi-Malvajerdi and Kevin Stangl and Ali Vakilian and Juba Ziani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cohen2023sequential.pdf:pdf},
  pages = {6279--6295},
  pdf = {https://proceedings.mlr.press/v202/cohen23a/cohen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Strategic Screening},
  url = {https://proceedings.mlr.press/v202/cohen23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{severo2023oneshot,
  abstract = {We present a one-shot method for compressing large labeled graphs called Random Edge Coding. When paired with a parameter-free model based on Pólya's Urn, the worst-case computational and memory complexities scale quasi-linearly and linearly with the number of observed edges, making it efficient on sparse graphs, and requires only integer arithmetic. Key to our method is bits-back coding, which is used to sample edges and vertices without replacement from the edge-list in a way that preserves the structure of the graph. Optimality is proven under a class of random graph models that are invariant to permutations of the edges and of vertices within an edge. Experiments indicate Random Edge Coding can achieve competitive compression performance on real-world network datasets and scales to graphs with millions of nodes and edges.},
  author = {Daniel Severo and James Townsend and Ashish J. Khisti and Alireza Makhzani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/severo2023oneshot.pdf:pdf},
  pages = {30633--30645},
  pdf = {https://proceedings.mlr.press/v202/severo23a/severo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-Shot Compression of Large Edge-Exchangeable Graphs using Bits-Back Coding},
  url = {https://proceedings.mlr.press/v202/severo23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{johnson2023rusure,
  abstract = {Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely. We propose Randomized Utility-driven Synthesis of Uncertain REgions ({R-U-SURE}), an approach for building uncertainty-aware suggestions based on a decision-theoretic model of goal-conditioned utility, using random samples from a generative model as a proxy for the unobserved possible intents of the end user. Our technique combines minimum-Bayes-risk decoding, dual decomposition, and decision diagrams in order to efficiently produce structured uncertainty summaries, given only sample access to an arbitrary generative model of code and an optional {AST} parser. We demonstrate {R-U-SURE} on three developer-assistance tasks, and show that it can be applied to different user interaction patterns without retraining the model and leads to more accurate uncertainty estimates than token-probability baselines.},
  author = {Daniel D. Johnson and Daniel Tarlow and Christian J. Walder},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/johnson2023rusure.pdf:pdf},
  pages = {15262--15306},
  pdf = {https://proceedings.mlr.press/v202/johnson23a/johnson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{R-U-SURE}? {U}ncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents},
  url = {https://proceedings.mlr.press/v202/johnson23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023what,
  abstract = {In online reinforcement learning ({RL}), instead of employing standard structural assumptions on Markov decision processes ({MDP}s), using a certain coverage condition (original from offline {RL}) is enough to ensure sample-efficient guarantees. We focus on this new direction by digging more possible and general coverage conditions, and studying the potential and the utility of them in efficient online {RL}. We identify concepts including the {$L^p$} variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be beneficial to sample-efficient online {RL}, achieving improved regret bounds. Our work contributes to the growing field of understanding how coverage conditions from offline {RL} can be adapted and generalized to improve the sample efficiency of online reinforcement learning algorithms with function approximation.},
  author = {Fanghui Liu and Luca Viano and Volkan Cevher},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023what.pdf:pdf},
  pages = {22063--22091},
  pdf = {https://proceedings.mlr.press/v202/liu23aj/liu23aj.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {What can online reinforcement learning with function approximation benefit from general coverage conditions?},
  url = {https://proceedings.mlr.press/v202/liu23aj.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023adversarial,
  abstract = {Adversarial attacks in reinforcement learning ({RL}) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk {MDP} in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk ({ACT}) to train Adversaries in this setting. We demonstrate that an Adversary trained with {ACT} still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing {RL} algorithms. More specifically, we show that an {ACT} Adversary is capable of harming performance by interfering with the learner's function approximation, or instead helping the Victim's performance by outputting useful features. Finally, we show that an {ACT} Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time.},
  author = {Chris Lu and Timon Willi and Alistair Letcher and Jakob Nicolaus Foerster},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023adversarial.pdf:pdf},
  pages = {22917--22941},
  pdf = {https://proceedings.mlr.press/v202/lu23h/lu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Cheap Talk},
  url = {https://proceedings.mlr.press/v202/lu23h.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hansen2023pretraining,
  abstract = {We revisit a simple Learning-from-Scratch ({LfS}) baseline that incorporates data augmentation and a shallow {C}onv{N}et, and find that this baseline is surprisingly competitive with recent approaches ({PVR}, {MVP}, {R3M}) that leverage frozen visual representations trained on large-scale vision datasets across a variety of algorithms, task domains, and metrics in simulation and on a real robot. Our results demonstrate that these methods are hindered by a significant domain gap between the pre-training datasets and current benchmarks for visuo-motor control, which is alleviated by finetuning. We evaluate {LfS} and pre-trained representations across 4 domains ({A}droit, {DM}Control, {P}ixMC, real robot), 3 algorithms (behavior cloning, on-policy {RL}, off-policy {RL}), and metrics like sample efficiency, asymptotic performance, robustness. Based on our findings, we provide recommendations for future research in pre-training for control and hope that our simple yet strong baseline will aid in accurately benchmarking progress in this area.},
  author = {Nicklas Hansen and Zhecheng Yuan and Yanjie Ze and Tongzhou Mu and Aravind Rajeswaran and Hao Su and Huazhe Xu and Xiaolong Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hansen2023pretraining.pdf:pdf},
  pages = {12511--12526},
  pdf = {https://proceedings.mlr.press/v202/hansen23c/hansen23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline},
  url = {https://proceedings.mlr.press/v202/hansen23c.html},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023doubly,
  abstract = {We consider online learning in multi-player smooth monotone games. Existing algorithms have limitations such as (1) being only applicable to strongly monotone games; (2) lacking the no-regret guarantee; (3) having only asymptotic or slow $\mathcal{O}(\frac{1}{\sqrt{T}})$ last-iterate convergence rate to a Nash equilibrium.},
  author = {Yang Cai and Weiqiang Zheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023doubly.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3507--3524},
  pdf = {https://proceedings.mlr.press/v202/cai23g/cai23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Doubly Optimal No-Regret Learning in Monotone Games},
  url = {https://proceedings.mlr.press/v202/cai23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023dpfast,
  abstract = {Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically characterizing how privacy affects the utility and computational cost in Bayesian inference. We empirically demonstrate the effectiveness and efficiency of our algorithm in various experiments.},
  author = {Wanrong Zhang and Ruqi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023dpfast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41847--41860},
  pdf = {https://proceedings.mlr.press/v202/zhang23aw/zhang23aw.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference},
  url = {https://proceedings.mlr.press/v202/zhang23aw.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023enforcing,
  abstract = {It is quite challenging to ensure the safety of reinforcement learning (RL) agents in an unknown and stochastic environment under hard constraints that require the system state not to reach certain specified unsafe regions.},
  author = {Yixuan Wang and Simon Sinong Zhan and Ruochen Jiao and Zhilu Wang and Wanxin Jin and Zhuoran Yang and Zhaoran Wang and Chao Huang and Qi Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023enforcing.pdf:pdf},
  mdate = {2023-12-27},
  pages = {36593--36604},
  pdf = {https://proceedings.mlr.press/v202/wang23as/wang23as.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments},
  url = {https://proceedings.mlr.press/v202/wang23as.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023sketching,
  abstract = {Projection maintenance is one of the core data structure tasks. Efficient data structures for projection maintenance have led to recent breakthroughs in many convex programming algorithms. In this work, we further extend this framework to the Kronecker product structure. Given a constraint matrix $\sf A$ and a positive semi-definite matrix $W\in \mathbb{R}^{n\times n}$ with a sparse eigenbasis, we consider the task of maintaining the projection in the form of $\sf B^\top(\sf B\sf B^\top)^{-1}\sf B$, where $\sf B=\sf A(W\otimes I)$ or $\sf B=\sf A(W^{1/2}\otimes W^{1/2})$. At each iteration, the weight matrix $W$ receives a low rank change and we receive a new vector $h$. The goal is to maintain the projection matrix and answer the query $\sf B^\top(\sf B\sf B^\top)^{-1}\sf Bh$ with good approximation guarantees. We design a fast dynamic data structure for this task and it is robust against an adaptive adversary. Following the beautiful and pioneering work of [Beimel, Kaplan, Mansour, Nissim, Saranurak and Stemmer, STOC'22], we use tools from differential privacy to reduce the randomness required by the data structure and further improve the running time.},
  author = {Zhao Song and Xin Yang and Yuanyuan Yang and Lichen Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023sketching.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32418--32462},
  pdf = {https://proceedings.mlr.press/v202/song23i/song23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sketching Meets Differential Privacy: Fast Algorithm for Dynamic Kronecker Projection Maintenance},
  url = {https://proceedings.mlr.press/v202/song23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023dualhsic,
  abstract = {Rehearsal-based approaches are a mainstay of continual learning (CL). They mitigate the catastrophic forgetting problem by maintaining a small fixed-size buffer with a subset of data from past tasks. While most rehearsal-based approaches study how to effectively exploit the knowledge from the buffered past data, little attention is paid to the inter-task relationships with the critical task-specific and task-invariant knowledge. By appropriately leveraging inter-task relationships, we propose a novel CL method named DualHSIC to boost the performance of existing rehearsal-based methods in a simple yet effective way. DualHSIC consists of two complementary components that stem from the so-called Hilbert Schmidt independence criterion (HSIC): HSIC-Bottleneck for Rehearsal (HBR) lessens the inter-task interference and HSIC Alignment (HA) promotes task-invariant knowledge sharing. Extensive experiments show that DualHSIC can be seamlessly plugged into existing rehearsal-based methods for consistent performance improvements, and also outperforms recent state-of-the-art regularization-enhanced rehearsal methods. Source code will be released.},
  author = {Zifeng Wang and Zheng Zhan and Yifan Gong and Yucai Shao and Stratis Ioannidis and Yanzhi Wang and Jennifer G. Dy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023dualhsic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36578--36592},
  pdf = {https://proceedings.mlr.press/v202/wang23ar/wang23ar.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning},
  url = {https://proceedings.mlr.press/v202/wang23ar.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023generalizedsmooth,
  abstract = {Various optimal gradient-based algorithms have been developed for smooth nonconvex optimization. However, many nonconvex machine learning problems do not belong to the class of smooth functions and therefore the existing algorithms are sub-optimal. Instead, these problems have been shown to satisfy certain generalized-smooth conditions, which have not been well understood in the existing literature. In this paper, we propose a notion of $\alpha$-symmetric generalized-smoothness that extends the existing notions and covers many important functions such as high-order polynomials and exponential functions. We study the fundamental properties and establish descent lemmas for the functions in this class. Then, to solve such a large class of nonconvex problems, we design a special deterministic normalized gradient descent algorithm that achieves the optimal iteration complexity $\mathcal{O}(\epsilon^{-2})$, and also prove that the popular SPIDER variance reduction algorithm achieves the optimal sample complexity $\mathcal{O}(\epsilon^{-3})$ in the stochastic setting. Our results show that solving generalized-smooth nonconvex problems is as efficient as solving smooth nonconvex problems.},
  author = {Ziyi Chen and Yi Zhou and Yingbin Liang and Zhaosong Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023generalizedsmooth.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5396--5427},
  pdf = {https://proceedings.mlr.press/v202/chen23ar/chen23ar.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalized-Smooth Nonconvex Optimization is As Efficient As Smooth Nonconvex Optimization},
  url = {https://proceedings.mlr.press/v202/chen23ar.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choi2023gread,
  abstract = {Graph neural networks (GNNs) are one of the most popular research topics for deep learning. GNN methods typically have been designed on top of the graph signal processing theory. In particular, diffusion equations have been widely used for designing the core processing layer of GNNs, and therefore they are inevitably vulnerable to the notorious oversmoothing problem. Recently, a couple of papers paid attention to reaction equations in conjunctions with diffusion equations. However, they all consider limited forms of reaction equations. To this end, we present a reaction-diffusion equation-based GNN method that considers all popular types of reaction equations in addition to one special reaction equation designed by us. To our knowledge, our paper is one of the most comprehensive studies on reaction-diffusion equation-based GNNs. In our experiments with 9 datasets and 28 baselines, our method, called GREAD, outperforms them in a majority of cases. Further synthetic data experiments show that it mitigates the oversmoothing problem and works well for various homophily rates.},
  author = {Jeongwhan Choi and Seoyoung Hong and Noseong Park and Sung-Bae Cho},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choi2023gread.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5722--5747},
  pdf = {https://proceedings.mlr.press/v202/choi23a/choi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {GREAD: Graph Neural Reaction-Diffusion Networks},
  url = {https://proceedings.mlr.press/v202/choi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023learning,
  abstract = {POMDPs capture a broad class of decision making problems, but hardness results suggest that learning is intractable even in simple settings due to the inherent partial observability. However, in many realistic problems, more information is either revealed or can be computed during some point of the learning process. Motivated by diverse applications ranging from robotics to data center scheduling, we formulate a Hindsight Observable Markov Decision Process (HOMDP) as a POMDP where the latent states are revealed to the learner in hindsight and only during training. We introduce new algorithms for the tabular and function approximation settings that are provably sample-efficient with hindsight observability, even in POMDPs that would otherwise be statistically intractable. We give a lower bound showing that the tabular algorithm is optimal in its dependence on latent state and observation cardinalities.},
  author = {Jonathan Lee and Alekh Agarwal and Christoph Dann and Tong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18733--18773},
  pdf = {https://proceedings.mlr.press/v202/lee23a/lee23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning in POMDPs is Sample-Efficient with Hindsight Observability},
  url = {https://proceedings.mlr.press/v202/lee23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023multiagent,
  abstract = {We consider the problem of fairly allocating a sequence of indivisible items that arrive online in an arbitrary order to a group of n agents with additive normalized valuation functions. We consider both the allocation of goods and chores and propose algorithms for approximating maximin share (MMS) allocations. When agents have identical valuation functions the problem coincides with the semi-online machine covering problem (when items are goods) and load balancing problem (when items are chores), for both of which optimal competitive ratios have been achieved. In this paper, we consider the case when agents have general additive valuation functions. For the allocation of goods, we show that no competitive algorithm exists even when there are only three agents and propose an optimal 0.5-competitive algorithm for the case of two agents. For the allocation of chores, we propose a (2-1/n)-competitive algorithm for n>=3 agents and a square root of 2 (approximately 1.414)-competitive algorithm for two agents. Additionally, we show that no algorithm can do better than 15/11 (approximately 1.364)-competitive for two agents.},
  author = {Shengwei Zhou and Rufan Bai and Xiaowei Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023multiagent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42506--42516},
  pdf = {https://proceedings.mlr.press/v202/zhou23a/zhou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-agent Online Scheduling: MMS Allocations for Indivisible Items},
  url = {https://proceedings.mlr.press/v202/zhou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023learning,
  abstract = {We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed Neural Constitutive Laws (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the motion observation. We validate NCLaw on various large-deformation dynamical systems, ranging from solids to fluids. After training on a single motion trajectory, our method generalizes to new geometries, initial/boundary conditions, temporal ranges, and even multi-physics systems. On these extremely out-of-distribution generalization tasks, NCLaw is orders-of-magnitude more accurate than previous NN approaches. Real-world experiments demonstrate our method's ability to learn constitutive laws from videos.},
  author = {Pingchuan Ma and Peter Yichen Chen and Bolei Deng and Joshua B. Tenenbaum and Tao Du and Chuang Gan and Wojciech Matusik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23279--23300},
  pdf = {https://proceedings.mlr.press/v202/ma23a/ma23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Neural Constitutive Laws from Motion Observations for Generalizable PDE Dynamics},
  url = {https://proceedings.mlr.press/v202/ma23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023weighted,
  abstract = {Local graph clustering methods aim to detect small clusters in very large graphs without the need to process the whole graph. They are fundamental and scalable tools for a wide range of tasks such as local community detection, node ranking and node embedding. While prior work on local graph clustering mainly focuses on graphs without node attributes, modern real-world graph datasets typically come with node attributes that provide valuable additional information.},
  author = {Shenghao Yang and Kimon Fountoulakis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023weighted.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39252--39276},
  pdf = {https://proceedings.mlr.press/v202/yang23d/yang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Weighted Flow Diffusion for Local Graph Clustering with Node Attributes: an Algorithm and Statistical Guarantees},
  url = {https://proceedings.mlr.press/v202/yang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiao2023random,
  abstract = {Non-local interactions play a vital role in boosting performance for image restoration. However, local window Transformer has been preferred due to its efficiency for processing high-resolution images. The superiority in efficiency comes at the cost of sacrificing the ability to model non-local interactions.},
  author = {Jie Xiao and Xueyang Fu and Man Zhou and Hongjian Liu and Zheng-Jun Zha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiao2023random.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38039--38058},
  pdf = {https://proceedings.mlr.press/v202/xiao23a/xiao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Shuffle Transformer for Image Restoration},
  url = {https://proceedings.mlr.press/v202/xiao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023evolving,
  abstract = {The paper addresses zero-shot learning (ZSL) by proposing a dynamic semantic prototype evolving (DSP) method to align predefined semantic prototypes with real prototypes. The key observation is that current semantic prototypes do not accurately represent real sample features, creating a visual-semantic domain shift problem. The proposed method refines sample features and semantic prototypes to improve generative ZSL methods.},
  author = {Shiming Chen and Wenjin Hou and Ziming Hong and Xiaohan Ding and Yibing Song and Xinge You and Tongliang Liu and Kun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023evolving.pdf:pdf},
  mdate = {2024-06-10},
  pages = {4611--4622},
  pdf = {https://proceedings.mlr.press/v202/chen23l/chen23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Evolving Semantic Prototype Improves Generative Zero-Shot Learning},
  url = {https://proceedings.mlr.press/v202/chen23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023data,
  abstract = {Focuses on multimodal models using visual and linguistic modalities and investigates data poisoning attacks across both modalities. The research addresses key questions about vulnerability of linguistic modality to poisoning attacks and which modality is most vulnerable. The research proposes three types of poisoning attacks and evaluates defenses to mitigate these attacks while preserving model utility.},
  author = {Ziqing Yang and Xinlei He and Zheng Li and Michael Backes and Mathias Humbert and Pascal Berrang and Yang Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023data.pdf:pdf},
  mdate = {2024-08-05},
  pages = {39299--39313},
  pdf = {https://proceedings.mlr.press/v202/yang23f/yang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data Poisoning Attacks Against Multimodal Encoders},
  url = {https://proceedings.mlr.press/v202/yang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023selfsupervised,
  abstract = {Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance.},
  author = {Weiwei Lin and Chenhang He and Man-Wai Mak and Youzhi Tu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023selfsupervised.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21065--21077},
  pdf = {https://proceedings.mlr.press/v202/lin23e/lin23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations},
  url = {https://proceedings.mlr.press/v202/lin23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gupta2023highdimensional,
  abstract = {In location estimation, we are given $n$ samples from a known distribution $f$ shifted by an unknown translation $\lambda$, and want to estimate $\lambda$ as precisely as possible. Asymptotically, the maximum likelihood estimate achieves the Cramér-Rao bound of error $\mathcal{N}(0, \frac{1}{n\mathcal{I}})$, where $\mathcal{I}$ is the Fisher information of $f$. However, the $n$ required for convergence depends on $f$, and may be arbitrarily large. We build on the theory using smoothed estimators to bound the error for finite $n$ in terms of $\mathcal{I}_r$, the Fisher information of the $r$-smoothed distribution.},
  author = {Shivam Gupta and Jasper C. H. Lee and Eric Price},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gupta2023highdimensional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12132--12164},
  pdf = {https://proceedings.mlr.press/v202/gupta23a/gupta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {High-dimensional Location Estimation via Norm Concentration for Subgamma Vectors},
  url = {https://proceedings.mlr.press/v202/gupta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023bidirectional,
  abstract = {The paper focuses on addressing the dense backward propagation issue for training efficiency of N:M fine-grained sparsity that preserves at most N out of M consecutive weights and achieves practical speedups supported by the N:M sparse tensor core. The authors present a novel method of Bi-directional Masks (Bi-Mask) with two central innovations: 1) Separate sparse masks in the two directions of forward and backward propagation to obtain training acceleration. 2) An efficient weight row permutation method to maintain performance.},
  author = {Yuxin Zhang and Yiting Luo and Mingbao Lin and Yunshan Zhong and Jingjing Xie and Fei Chao and Rongrong Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023bidirectional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41488--41497},
  pdf = {https://proceedings.mlr.press/v202/zhang23ae/zhang23ae.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bi-directional Masks for Efficient {N:M} Sparse Training},
  url = {https://proceedings.mlr.press/v202/zhang23ae.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dai2023refined,
  abstract = {We consider learning in an adversarial Markov Decision Process (MDP) where the loss functions can change arbitrarily over K episodes and the state space can be arbitrarily large. We assume that the Q-function of any policy is linear in some known features, that is, a linear function approximation exists. The best existing regret upper bound for this setting is of order Õ(K^{2/3}). This paper provides two algorithms that improve the regret to Õ(√K) in the same setting.},
  author = {Yan Dai and Haipeng Luo and Chen-Yu Wei and Julian Zimmert},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dai2023refined.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6726--6759},
  pdf = {https://proceedings.mlr.press/v202/dai23b/dai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Refined Regret for Adversarial {MDPs} with Linear Function Approximation},
  url = {https://proceedings.mlr.press/v202/dai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023polarity,
  abstract = {Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, artificial intelligences (AIs) typically learn with a prohibitive number of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we investigate the role of weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update, yet polarities are largely kept unchanged.},
  author = {Qingyang Wang and Michael Alan Powell and Eric W. Bridgeford and Ali Geisa and Joshua T. Vogelstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023polarity.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36264--36284},
  pdf = {https://proceedings.mlr.press/v202/wang23ae/wang23ae.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Polarity Is All You Need to Learn and Transfer Faster},
  url = {https://proceedings.mlr.press/v202/wang23ae.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rodriguezsanchez2023rlang,
  abstract = {We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to single elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic partial world model and policy that can be exploited by an RL agent.},
  author = {Rafael Rodriguez-Sanchez and Benjamin Adin Spiegel and Jennifer Wang and Roma Patel and Stefanie Tellex and George Konidaris},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rodriguezsanchez2023rlang.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29161--29178},
  pdf = {https://proceedings.mlr.press/v202/rodriguez-sanchez23a/rodriguez-sanchez23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{RLang}: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents},
  url = {https://proceedings.mlr.press/v202/rodriguez-sanchez23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023coordinated,
  abstract = {In online ad markets, a rising number of advertisers are employing bidding agencies to participate in ad auctions. These agencies are specialized in designing online algorithms and bidding on behalf of their clients. Typically, an agency usually has information on multiple advertisers, so she can potentially coordinate bids to help her clients achieve higher utilities than those under independent bidding. In this work, we study coordinated online bidding algorithms in repeated second-price auctions with budgets. We propose algorithms that guarantee every client a higher utility than the best she can get under independent bidding. We also show that these algorithms achieve maximal social welfare and discuss bidders' incentives to misreport their budgets, in symmetric cases.},
  author = {Yurong Chen and Qian Wang and Zhijian Duan and Haoran Sun and Zhaohua Chen and Xiang Yan and Xiaotie Deng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023coordinated.pdf:pdf},
  mdate = {2024-08-16},
  pages = {5052--5086},
  pdf = {https://proceedings.mlr.press/v202/chen23ac/chen23ac.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coordinated Dynamic Bidding in Repeated Second-Price Auctions with Budgets},
  url = {https://proceedings.mlr.press/v202/chen23ac.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023towards,
  abstract = {Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. In this paper, we take the first step towards omni-generalizable neural methods for VRPs that generalize across problem sizes and distributions. We propose a meta-learning framework that enables models to quickly adapt to new problem instances with different characteristics. Our approach learns generalizable node and edge representations through a novel architecture that captures both local and global structural patterns in routing problems.},
  author = {Jianan Zhou and Yaoxin Wu and Wen Song and Zhiguang Cao and Jie Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023towards.pdf:pdf},
  mdate = {2024-07-19},
  pages = {42769--42789},
  pdf = {https://proceedings.mlr.press/v202/zhou23o/zhou23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Omni-generalizable Neural Methods for Vehicle Routing Problems},
  url = {https://proceedings.mlr.press/v202/zhou23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023sketching,
  abstract = {Sketching is a fundamental tool in large-scale machine learning that compresses high-dimensional problems into lower dimensions. In this work, we propose a novel sketching scheme for first-order methods in distributed learning that reduces communication costs while maintaining algorithmic convergence. By using a sketching matrix to compress gradient information from dimension d to a smaller dimension s, agents can communicate more efficiently in low-bandwidth channels. However, we also demonstrate that this method does not inherently protect data privacy and can be vulnerable to gradient attacks. To address this vulnerability, we show that adding random noise can make the algorithm differentially private, creating a communication-efficient and privacy-preserving approach for federated learning.},
  author = {Zhao Song and Yitan Wang and Zheng Yu and Lichen Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023sketching.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32365--32417},
  pdf = {https://proceedings.mlr.press/v202/song23h/song23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability},
  url = {https://proceedings.mlr.press/v202/song23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023surrogate,
  abstract = {In Federated Learning (FL), collaborators share network weights trained with local data after multiple iterations. However, this weight-sharing mechanism exposes privacy vulnerabilities that can be exploited through gradient inversion attacks. We propose Surrogate Model Extension (SME), a novel method that extends gradient inversion attacks to work on weight updates rather than gradients alone. Our approach demonstrates state-of-the-art performance in reconstructing private data from shared model weights while running up to 100x faster than previous baselines. This work reveals significant privacy risks in current federated learning protocols and highlights the need for stronger privacy-preserving mechanisms in distributed machine learning systems.},
  author = {Junyi Zhu and Ruicong Yao and Matthew B. Blaschko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023surrogate.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43228--43257},
  pdf = {https://proceedings.mlr.press/v202/zhu23m/zhu23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Surrogate Model Extension ({SME}): A Fast and Accurate Weight Update Attack on Federated Learning},
  url = {https://proceedings.mlr.press/v202/zhu23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023local,
  abstract = {Multi-agent reinforcement learning has attracted growing attention recently due to its broad applications. While policy optimization methods with function approximation have shown promising empirical performance, it remains elusive how to design such algorithms with statistical guarantees. In this work, we leverage a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization. We find that the localized action value function serves as an ideal descent direction for each local policy. Based on this observation, we present a multi-agent PPO algorithm where each agent's local policy is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. This represents the first provably convergent multi-agent PPO algorithm in cooperative Markov games.},
  author = {Yulai Zhao and Zhuoran Yang and Zhaoran Wang and Jason D. Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023local.pdf:pdf},
  mdate = {2023-12-27},
  pages = {42200--42226},
  pdf = {https://proceedings.mlr.press/v202/zhao23j/zhao23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/zhao23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023nearlyoptimal,
  abstract = {We consider the problem of finding a vector $x'$ such that $\|x'-x^*\|_\infty \leq (\epsilon/\sqrt{d}) \cdot \|Ax^*-b\|_2 \cdot \|A^\dagger\|$ where $x^*$ is the optimal solution to the regression problem $\|Ax-b\|_2$. To obtain $\ell_\infty$ guarantees for $\ell_2$ regression, we prove that one must use sketching matrices that are dense -- this is the first use case where dense sketching matrices are necessary. We prove there exists a distribution of dense sketching matrices with $m = \epsilon^{-2} d \log^3(n/\delta)$ such that solving the sketched regression problem gives the $\ell_\infty$ guarantee with probability at least $1-\delta$. Moreover, the matrix $SA$ can be computed in time $O(nd \log n)$. Our row count is nearly-optimal up to logarithmic factors and significantly improves upon prior work.},
  author = {Zhao Song and Mingquan Ye and Junze Yin and Lichen Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023nearlyoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32463--32482},
  pdf = {https://proceedings.mlr.press/v202/song23j/song23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Nearly-Optimal Bound for Fast Regression with $\ell_\infty$ Guarantee},
  url = {https://proceedings.mlr.press/v202/song23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023contextual,
  abstract = {We study contextual combinatorial bandits with probabilistically triggered arms (C²MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C²-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{min})$, where $d$ is the dimension of contexts, $p_{min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC²-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$.},
  author = {Xutong Liu and Jinhang Zuo and Siwei Wang and John C. S. Lui and Mohammad Hajiesmaili and Adam Wierman and Wei Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023contextual.pdf:pdf},
  mdate = {2024-01-30},
  pages = {22559--22593},
  pdf = {https://proceedings.mlr.press/v202/liu23bf/liu23bf.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contextual Combinatorial Bandits with Probabilistically Triggered Arms},
  url = {https://proceedings.mlr.press/v202/liu23bf.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023towards,
  abstract = {Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While such a model can generalize well to new data of the same distribution, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this work, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalizes well to out-of-distribution problems. Our model mimics the structure of classical optimization algorithms and leverages their mathematical foundations for better generalization. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model.},
  author = {Jialin Liu and Xiaohan Chen and Zhangyang Wang and Wotao Yin and HanQin Cai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21426--21449},
  pdf = {https://proceedings.mlr.press/v202/liu23e/liu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Constituting Mathematical Structures for Learning to Optimize},
  url = {https://proceedings.mlr.press/v202/liu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dang2023neural,
  abstract = {Neural Collapse (NC) is a recently discovered phenomenon in which the last-layer features and classifiers of deep networks converge to a very specific geometric configuration during the final phase of training. In this work, we study NC in deep linear networks and prove that NC can occur for both mean squared error (MSE) and cross entropy (CE) losses under certain conditions. We extend our study to imbalanced data for the MSE loss and present the first geometric analysis of NC under this setting. Our results demonstrate that last-layer features and classifiers converge to a geometry consisting of orthogonal vectors, whose lengths depend on the amount of data in their corresponding classes. We validate our theoretical findings on both synthetic data and practical network architectures.},
  author = {Hien Dang and Tho Tran Huu and Stanley J. Osher and Hung Tran-The and Nhat Ho and Tan Minh Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dang2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6873--6947},
  pdf = {https://proceedings.mlr.press/v202/dang23b/dang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data},
  url = {https://proceedings.mlr.press/v202/dang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiao2023forward,
  abstract = {We propose a new method to ensure that neural ordinary differential equations (Neural ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters and inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters and inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.},
  author = {Wei Xiao and Tsun-Hsuan Wang and Ramin M. Hasani and Mathias Lechner and Yutong Ban and Chuang Gan and Daniela Rus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiao2023forward.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38100--38124},
  pdf = {https://proceedings.mlr.press/v202/xiao23d/xiao23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Forward Invariance of Neural {ODEs}},
  url = {https://proceedings.mlr.press/v202/xiao23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023dropout,
  abstract = {Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training.},
  author = {Zhuang Liu and Zhiqiu Xu and Joseph Jin and Zhiqiang Shen and Trevor Darrell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023dropout.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22233--22248},
  pdf = {https://proceedings.mlr.press/v202/liu23aq/liu23aq.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dropout Reduces Underfitting},
  url = {https://proceedings.mlr.press/v202/liu23aq.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feng2023weighted,
  abstract = {The top-$k$ classification accuracy is a crucial metric in machine learning and is often used to evaluate the performance of deep neural networks. These networks are typically trained using the cross-entropy loss, which optimizes for top-$1$ classification and is considered optimal in the case of infinite data. However, in real-world scenarios, data is often noisy and limited, leading to the need for more robust losses. In this paper, we propose using the Weighted Sampling Without Replacement (WSWR) method as a learning objective for top-$k$ loss. While traditional methods for evaluating WSWR-based top-$k$ loss are computationally impractical, we show a novel connection between WSWR and Reinforcement Learning (RL) and apply well-established RL algorithms to estimate gradients. We compared our method with recently proposed top-$k$ losses in various regimes of noise and data size for the prevalent use case of $k = 5$. Our experimental results reveal that our method consistently outperforms all other methods on the top-$k$ metric for noisy datasets, has more robustness on extreme testing scenarios, and achieves competitive results on training with limited data.},
  author = {Dieqiao Feng and Yuanqi Du and Carla P. Gomes and Bart Selman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feng2023weighted.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9910--9920},
  pdf = {https://proceedings.mlr.press/v202/feng23a/feng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Weighted Sampling without Replacement for Deep Top-$k$ Classification},
  url = {https://proceedings.mlr.press/v202/feng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cao2023variational,
  abstract = {To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky (SIC) factors. We combine this variational approximation of the posterior with a similar and efficient SIC-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular SIC ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation (DKLGP) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.},
  author = {Jian Cao and Myeongjong Kang and Felix Jimenez and Huiyan Sang and Florian Tobias Schaefer and Matthias Katzfuss},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cao2023variational.pdf:pdf},
  mdate = {2024-12-16},
  pages = {3559--3576},
  pdf = {https://proceedings.mlr.press/v202/cao23b/cao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Sparse Inverse Cholesky Approximation for Latent Gaussian Processes via Double Kullback-Leibler Minimization},
  url = {https://proceedings.mlr.press/v202/cao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jain2023price,
  abstract = {We study the accuracy of differentially private mechanisms in the continual release model. A continual release mechanism receives a sensitive dataset as a stream of T inputs and produces, after receiving each input, an output that is accurate for all the inputs received so far. We provide the first strong lower bounds on the error of continual release mechanisms. In particular, for two fundamental problems that are closely related to empirical risk minimization and widely studied and used in the standard (batch) model, we prove that the worst case error of every continual release algorithm is Õ(T^{1/3}) times larger than that of the best batch algorithm. Previous work shows only a Ω(log T) gap between the worst case error achievable in these two models.},
  author = {Palak Jain and Sofya Raskhodnikova and Satchit Sivakumar and Adam D. Smith},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jain2023price.pdf:pdf},
  mdate = {2024-08-20},
  pages = {14654--14678},
  pdf = {https://proceedings.mlr.press/v202/jain23b/jain23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Price of Differential Privacy under Continual Observation},
  url = {https://proceedings.mlr.press/v202/jain23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023gromovwasserstein,
  abstract = {Graph coarsening is a technique for solving large-scale graph problems by working on a smaller version of the original graph, and possibly interpolating the results back to the original graph. It has a long history in scientific computing and has recently gained popularity in machine learning, particularly in methods that preserve the graph spectrum. The geometric approach is useful when working with a collection of graphs, such as in graph classification and regression. In this study, we consider a graph as an element on a metric space equipped with the Gromov-Wasserstein (GW) distance, and bound the difference between the distance of two graphs and their coarsened versions. Minimizing this difference can be done using the popular weighted kernel K-means method, which improves existing spectrum-preserving methods with the proper choice of the kernel. The study includes a set of experiments to support the theory and method, including approximating the GW distance, preserving the graph spectrum, classifying graphs using spectral information, and performing regression using graph convolutional networks.},
  author = {Yifan Chen and Rentian Yao and Yun Yang and Jie Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023gromovwasserstein.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5257--5281},
  pdf = {https://proceedings.mlr.press/v202/chen23ak/chen23ak.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Gromov-Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening},
  url = {https://proceedings.mlr.press/v202/chen23ak.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{das2023image,
  abstract = {The field of image generation has made significant progress thanks to the introduction of Diffusion Models, which learn to progressively reverse a given image corruption. Recently, a few studies introduced alternative ways of corrupting images in Diffusion Models, with an emphasis on blurring. However, these studies are purely empirical and it remains unclear what is the optimal procedure for corrupting an image. In this work, we hypothesize that the optimal procedure minimizes the length of the path taken when corrupting an image towards a given final state. We propose the Fisher metric for the path length, measured in the space of probability distributions. We show that the shortest path according to this metric corresponds to a combination of image sharpening, rather than blurring, and noise deblurring.},
  author = {Ayan Das and Stathi Fotiadis and Anil Batra and Farhang Nabiei and Fengting Liao and Sattar Vakili and Da-Shan Shiu and Alberto Bernacchia},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/das2023image.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7009--7024},
  pdf = {https://proceedings.mlr.press/v202/das23a/das23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Image generation with shortest path diffusion},
  url = {https://proceedings.mlr.press/v202/das23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mu2023pfns4bo,
  abstract = {In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) through in-context learning on any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1.},
  author = {Samuel M{\"u}ller and Matthias Feurer and Noah Hollmann and Frank Hutter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mu2023pfns4bo.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25444--25470},
  pdf = {https://proceedings.mlr.press/v202/muller23a/muller23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PFNs4BO}: In-Context Learning for {B}ayesian Optimization},
  url = {https://proceedings.mlr.press/v202/muller23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023prompting,
  abstract = {Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting.},
  author = {Biao Zhang and Barry Haddow and Alexandra Birch},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023prompting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41092--41110},
  pdf = {https://proceedings.mlr.press/v202/zhang23m/zhang23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Prompting Large Language Model for Machine Translation: A Case Study},
  url = {https://proceedings.mlr.press/v202/zhang23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiao2023collaborative,
  abstract = {Collaborative causal inference (CCI) aims to improve the estimation of the causal effect of treatment variables by utilizing data aggregated from multiple self-interested parties. Since their source data are valuable proprietary assets that can be costly or tedious to obtain, every party has to be incentivized to be willing to contribute to the collaboration, such as with a guaranteed fair and sufficiently valuable reward (than performing causal inference on its own). This paper presents a reward scheme designed using the unique statistical properties that are required by causal inference to guarantee certain desirable incentive criteria (e.g., fairness, benefit) for the parties based on their contributions. To achieve this, we propose a data valuation function to value parties' data for CCI based on the distributional closeness of its resulting treatment effect estimate to that utilizing all parties' data.},
  author = {Rui Qiao and Xinyi Xu and Bryan Kian Hsiang Low},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiao2023collaborative.pdf:pdf},
  mdate = {2024-02-05},
  pages = {28300--28320},
  pdf = {https://proceedings.mlr.press/v202/qiao23a/qiao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Collaborative Causal Inference with Fair Incentives},
  url = {https://proceedings.mlr.press/v202/qiao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023tilted,
  abstract = {Additive models have been burgeoning in data analysis due to their flexible representation and desirable interpretability. However, most existing approaches are constructed under empirical risk minimization (ERM), and thus perform poorly in situations where average performance is not a suitable criterion for the problems of interest, e.g., data with complex non-Gaussian noise, imbalanced labels or both of them. In this paper, a novel class of sparse additive models is proposed under tilted empirical risk minimization (TERM), which addresses the deficiencies in ERM by imposing tilted impact on individual losses, and is flexibly capable of achieving a variety of learning objectives, e.g., variable selection, robust estimation, imbalanced classification and multiobjective learning. On the theoretical side, a learning theory analysis which is centered around the generalization bound and function approximation error bound (under some specific data distributions) is conducted rigorously.},
  author = {Yingjie Wang and Hong Chen and Weifeng Liu and Fengxiang He and Tieliang Gong and Youcheng Fu and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023tilted.pdf:pdf},
  mdate = {2024-07-26},
  pages = {35579--35604},
  pdf = {https://proceedings.mlr.press/v202/wang23c/wang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tilted Sparse Additive Models},
  url = {https://proceedings.mlr.press/v202/wang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023ods,
  abstract = {Test-time adaptation (TTA) adapts a source model to the distribution shift in testing data without using any source data. There have been plenty of algorithms concentrated on covariate shift in the last decade, i.e., $\mathcal{D}_t(X)$, the distribution of the test data is different from the source data. Nonetheless, in real application scenarios, it is necessary to consider the influence of label distribution shift, i.e., both $\mathcal{D}_t(X)$ and $\mathcal{D}_t(Y)$ are shifted, which has not been sufficiently explored yet.},
  author = {Zhi Zhou and Lan-Zhe Guo and Lin-Han Jia and Dingchu Zhang and Yu-Feng Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023ods.pdf:pdf},
  pages = {42574--42588},
  pdf = {https://proceedings.mlr.press/v202/zhou23e/zhou23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ODS}: Test-Time Adaptation in the Presence of Open-World Data Shift},
  url = {https://proceedings.mlr.press/v202/zhou23e.html},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023towards,
  abstract = {To improve the efficiency and sustainability of learning deep models, we propose CREST, the first scalable framework with rigorous theoretical guarantees to identify the most valuable examples for training non-convex models, particularly deep networks. To guarantee convergence to a stationary point of a non-convex function, CREST models the non-convex loss as a series of quadratic functions and extracts a coreset for each quadratic sub-region. In addition, to ensure faster convergence of stochastic gradient methods such as (mini-batch) SGD, CREST iteratively extracts multiple mini-batch coresets from larger random subsets of training data, to ensure nearly-unbiased gradients with small variances. Finally, to further improve scalability and efficiency, CREST identifies and excludes the examples that are learned from the coreset selection pipeline. Our extensive experiments on several deep networks trained on vision and NLP datasets, including CIFAR-10, CIFAR-100, TinyImageNet, and SNLI, confirm that CREST speeds up training deep networks on very large datasets, by 1.7x to 2.5x with minimum loss in the performance. By analyzing the learning difficulty of the subsets selected by CREST, we show that deep models benefit the most by learning from subsets of increasing difficulty levels.},
  author = {Yu Yang and Hao Kang and Baharan Mirzasoleiman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023towards.pdf:pdf},
  pages = {39314--39330},
  pdf = {https://proceedings.mlr.press/v202/yang23g/yang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Sustainable Learning: Coresets for Data-efficient Deep Learning},
  url = {https://proceedings.mlr.press/v202/yang23g.html},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023mitigating,
  abstract = {Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the model's activation maps towards the actual class rather than the spurious attribute when present. In particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy 23\% higher than ERM on CLIP with a ResNet-50 backbone, and 32\% higher on CLIP with a ViT backbone, while maintaining the same average accuracy as ERM.},
  author = {Yu Yang and Besmira Nushi and Hamid Palangi and Baharan Mirzasoleiman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023mitigating.pdf:pdf},
  pages = {39365--39379},
  pdf = {https://proceedings.mlr.press/v202/yang23j/yang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning},
  url = {https://proceedings.mlr.press/v202/yang23j.html},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023rethinking,
  abstract = {Decoding seen images from brain activities has been an absorbing field. However, the reconstructed images still suffer from low quality with existing studies. This can be because our visual system is not like a camera that 'remembers' every pixel. Instead, only part of the information can be perceived with our selective attention, and the brain 'guesses' the rest to form what we think we see. Most existing approaches ignored the brain completion mechanism. In this work, we propose to reconstruct seen images with both the visual perception and the brain completion process, and design a simple, yet effective visual decoding framework to achieve this goal. Specifically, we first construct a shared discrete representation space for both brain signals and images. Then, a novel self-supervised token-to-token inpainting network is designed to implement visual content completion by building context and prior knowledge about the visual objects from the discrete latent space. Our approach improved the quality of visual reconstruction significantly and achieved state-of-the-art.},
  author = {Jiaxuan Chen and Yu Qi and Gang Pan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023rethinking.pdf:pdf},
  pages = {4856--4866},
  pdf = {https://proceedings.mlr.press/v202/chen23v/chen23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethinking Visual Reconstruction: Experience-Based Content Completion Guided by Visual Cues},
  url = {https://proceedings.mlr.press/v202/chen23v.html},
  volume = {202},
  year = {2023}
}

@inproceedings{sheng2023flexgen,
  abstract = {The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours.},
  author = {Ying Sheng and Lianmin Zheng and Binhang Yuan and Zhuohan Li and Max Ryabinin and Beidi Chen and Percy Liang and Christopher R{\'e} and Ion Stoica and Ce Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sheng2023flexgen.pdf:pdf},
  pages = {31094--31116},
  pdf = {https://proceedings.mlr.press/v202/sheng23a/sheng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FlexGen}: High-Throughput Generative Inference of Large Language Models with a Single {GPU}},
  url = {https://proceedings.mlr.press/v202/sheng23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023dinknet,
  abstract = {Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designed loss functions adopt the mini-batch data to optimize the clustering distribution even without performance drops. Both experimental results and theoretical analyses demonstrate the superiority of our method. Compared to the runner-up, Dink-Net achieves 9.62\% NMI improvement on the ogbn-papers100M dataset with 111 million nodes and 1.6 billion edges.},
  author = {Yue Liu and Ke Liang and Jun Xia and Sihang Zhou and Xihong Yang and Xinwang Liu and Stan Z. Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023dinknet.pdf:pdf},
  pages = {21794--21812},
  pdf = {https://proceedings.mlr.press/v202/liu23v/liu23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dink-Net: Neural Clustering on Large Graphs},
  url = {https://proceedings.mlr.press/v202/liu23v.html},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023perception,
  abstract = {We develop techniques for neurosymbolic program synthesis where perceptual input is first parsed by neural nets into a low-dimensional interpretable representation, which is then processed by a synthesized program. We explore several techniques for relaxing the problem and jointly learning all modules end-to-end with gradient descent: multitask learning; amortized inference; overparameterization; and a differentiable strategy for penalizing lengthy programs. Collectively this toolbox improves the stability of gradient-guided program search, and suggests ways of learning both how to parse continuous input into discrete abstractions, and how to process those abstractions via symbolic code.},
  author = {Hao Tang and Kevin Ellis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023perception.pdf:pdf},
  pages = {33616--33631},
  pdf = {https://proceedings.mlr.press/v202/tang23c/tang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Perception to Programs: Regularize, Overparameterize, and Amortize},
  url = {https://proceedings.mlr.press/v202/tang23c.html},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023blip2,
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods.},
  author = {Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023blip2.pdf:pdf},
  pages = {19730--19742},
  pdf = {https://proceedings.mlr.press/v202/li23q/li23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{BLIP-2}: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  url = {https://proceedings.mlr.press/v202/li23q.html},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023progressive,
  abstract = {Partial label learning (PLL) aims to train multiclass classifiers from the examples each annotated with a set of candidate labels where a fixed but unknown candidate label is correct. In the last few years, the instance-independent generation process of candidate labels has been extensively studied, on the basis of which many theoretical advances have been made in PLL. Nevertheless, the candidate labels are always instance-dependent in practice and there is no theoretical guarantee that the model trained on the instance-dependent PLL examples can converge to an ideal one. In this paper, a theoretically grounded and practically effective approach named POP, i.e. Progressive Purification for instance-dependent partial label learning, is proposed.},
  author = {Ning Xu and Biao Liu and Jiaqi Lv and Congyu Qiao and Xin Geng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023progressive.pdf:pdf},
  pages = {38551--38565},
  pdf = {https://proceedings.mlr.press/v202/xu23l/xu23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Progressive Purification for Instance-Dependent Partial Label Learning},
  url = {https://proceedings.mlr.press/v202/xu23l.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023contrastive,
  abstract = {Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperforms existing state-of-the-art algorithms. We also provide some examples of applying CEP for image synthesis to demonstrate the scalability of CEP on high-dimensional data.},
  author = {Cheng Lu and Huayu Chen and Jianfei Chen and Hang Su and Chongxuan Li and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023contrastive.pdf:pdf},
  pages = {22825--22855},
  pdf = {https://proceedings.mlr.press/v202/lu23d/lu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/lu23d.html},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023tight,
  abstract = {We study the top-$k$ selection problem under the differential privacy model: $m$ items are rated according to votes of a set of clients. We consider a setting in which algorithms can retrieve data via a sequence of accesses, each either a random access or a sorted access; the goal is to minimize the total number of data accesses. Our algorithm requires only $O(\sqrt{mk})$ expected accesses: to our knowledge, this is the first sublinear data-access upper bound for this problem. Our analysis also shows that the well-known exponential mechanism requires only $O(\sqrt{m})$ expected accesses. Accompanying this, we develop the first lower bounds for the problem, in three settings: only random accesses; only sorted accesses; a sequence of accesses of either kind. We show that, to avoid $\Omega(m)$ access cost, supporting *both* kinds of access is necessary, and that in this case our algorithm's access cost is optimal.},
  author = {Hao Wu and Olga Ohrimenko and Anthony Wirth},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023tight.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37635--37655},
  pdf = {https://proceedings.mlr.press/v202/wu23q/wu23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tight Data Access Bounds for Private Top-k Selection},
  url = {https://proceedings.mlr.press/v202/wu23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023beyond,
  abstract = {Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimizers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a 'bona-fide' discretization of an underlying gradient flow. Yet, many ML setups involving overparametrized models do not fall into this problem class, which has motivated research beyond the so-called "Edge of Stability" (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior. The incipient theoretical analysis of this phenomena has mainly focused in the overparametrized regime, where the effect of choosing a large learning rate may be associated to a 'Sharpness-Minimization' implicit regularization within the manifold of minimizers, under appropriate asymptotic limits. In contrast, in this work we directly examine the conditions for such unstable convergence, focusing on simple, yet representative, learning problems, via analysis of two-step gradient updates. Specifically, we characterize a local condition involving third-order derivatives that guarantees existence and convergence to fixed points of the two-step updates, and leverage such property in a teacher-student setting, under population loss. Finally, starting from Matrix Factorization, we provide observations of period-2 orbit of GD in high-dimensional settings with intuition of its dynamics, along with exploration into more general settings.},
  author = {Lei Chen and Joan Bruna},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4330--4391},
  pdf = {https://proceedings.mlr.press/v202/chen23b/chen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond the Edge of Stability via Two-step Gradient Updates},
  url = {https://proceedings.mlr.press/v202/chen23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023cooperative,
  abstract = {Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behavior diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended Learning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. An analysis of the learning process of the algorithm shows that it can efficiently overcome cooperative incompatibility. The experimental results in the Overcooked game environment demonstrate that our method outperforms current state-of-the-art methods when coordinating with different-level partners.},
  author = {Yang Li and Shao Zhang and Jichen Sun and Yali Du and Ying Wen and Xinbing Wang and Wei Pan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023cooperative.pdf:pdf},
  mdate = {2024-02-05},
  pages = {20470--20484},
  pdf = {https://proceedings.mlr.press/v202/li23au/li23au.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cooperative Open-ended Learning Framework for Zero-Shot Coordination},
  url = {https://proceedings.mlr.press/v202/li23au.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aamand2023data,
  abstract = {The paper studies statistical/computational tradeoffs for the following density estimation problem: given $k$ distributions $v_1, \ldots, v_k$ over a discrete domain of size $n$, and sampling access to a distribution $p$, identify $v_i$ that is ``close'' to $p$. Their main result is the first data structure that, given a sublinear (in $n$) number of samples from $p$, identifies $v_i$ in time sublinear in $k$. They also give an improved version of the algorithm of Acharya et al. (2018) that reports $v_i$ in time linear in $k$. The experimental evaluation of the latter algorithm shows that it achieves a significant reduction in the number of operations needed to achieve a given accuracy compared to prior work.},
  author = {Anders Aamand and Alexandr Andoni and Justin Y. Chen and Piotr Indyk and Shyam Narayanan and Sandeep Silwal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aamand2023data.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1--18},
  pdf = {https://proceedings.mlr.press/v202/aamand23a/aamand23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data Structures for Density Estimation},
  url = {https://proceedings.mlr.press/v202/aamand23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{abbas2023clusterfug,
  abstract = {We propose a graph clustering formulation based on multicut (a.k.a. weighted correlation clustering) on the complete graph. Our formulation does not need specification of the graph topology as in the original sparse formulation of multicut, making our approach simpler and potentially better performing. In contrast to unweighted correlation clustering we allow for a more expressive weighted cost structure. In dense multicut, the clustering objective is given in a factorized form as inner products of node feature vectors. This allows for an efficient formulation and inference in contrast to multicut/weighted correlation clustering, which has at least quadratic representation and computation complexity when working on the complete graph.},
  author = {Ahmed Abbas and Paul Swoboda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/abbas2023clusterfug.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19--30},
  pdf = {https://proceedings.mlr.press/v202/abbas23a/abbas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ClusterFuG: Clustering Fully connected Graphs by Multicut},
  url = {https://proceedings.mlr.press/v202/abbas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{abbe2023generalization,
  abstract = {This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. The authors study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. The min-degree-interpolator means an interpolator of the training data that has minimal Fourier mass on the higher degree basis elements. These findings lead to two implications: (1) an explanation to the length generalization problem for Boolean functions; (2) the introduction of a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.},
  author = {Emmanuel Abbe and Samy Bengio and Aryo Lotfi and Kevin Rizk},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/abbe2023generalization.pdf:pdf},
  mdate = {2023-08-28},
  note = {Outstanding Paper Award},
  pages = {31--60},
  pdf = {https://proceedings.mlr.press/v202/abbe23a/abbe23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  url = {https://proceedings.mlr.press/v202/abbe23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{abedsoltan2023toward,
  abstract = {Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks (DNNs) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of DNNs is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce EigenPro 3.0, an algorithm based on projected dual preconditioned SGD and show scaling to model and data sizes which have not been possible with existing kernel methods.},
  author = {Amirhesam Abedsoltan and Mikhail Belkin and Parthe Pandit},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/abedsoltan2023toward.pdf:pdf},
  mdate = {2023-08-28},
  pages = {61--78},
  pdf = {https://proceedings.mlr.press/v202/abedsoltan23a/abedsoltan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Toward Large Kernel Models},
  url = {https://proceedings.mlr.press/v202/abedsoltan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{abels2023expertise,
  abstract = {Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.},
  author = {Axel Abels and Tom Lenaerts and Vito Trianni and Ann Now{\'e}},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/abels2023expertise.pdf:pdf},
  mdate = {2023-08-28},
  pages = {79--90},
  pdf = {https://proceedings.mlr.press/v202/abels23a/abels23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making},
  url = {https://proceedings.mlr.press/v202/abels23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{acharki2023comparison,
  abstract = {Conditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multi-valued treatments. We consider different meta-learners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weaknesses of those methods with synthetic and semi-synthetic datasets.},
  author = {Naoufal Acharki and Ramiro Lugo and Antoine Bertoncello and Josselin Garnier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/acharki2023comparison.pdf:pdf},
  mdate = {2023-08-28},
  pages = {91--132},
  pdf = {https://proceedings.mlr.press/v202/acharki23a/acharki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects},
  url = {https://proceedings.mlr.press/v202/acharki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{adams2023bnndp,
  abstract = {In this paper, we introduce BNN-DP, an efficient algorithmic framework for analysis of adversarial robustness of Bayesian Neural Networks (BNNs). Given a compact set of input points $T \subset \mathbb{R}^n$, BNN-DP computes lower and upper bounds on the BNN's predictions for all the points in $T$. The framework is based on an interpretation of BNNs as stochastic dynamical systems, which enables the use of Dynamic Programming (DP) algorithms to bound the prediction range along the layers of the network. Specifically, the method uses bound propagation techniques and convex relaxations to derive a backward recursion procedure to over-approximate the prediction range of the BNN with piecewise affine functions. The algorithm is general and can handle both regression and classification tasks. On a set of experiments on various regression and classification tasks and BNN architectures, we show that BNN-DP outperforms state-of-the-art methods by up to four orders of magnitude in both tightness of the bounds and computational efficiency.},
  author = {Steven Adams and Andrea Patane and Morteza Lahijanian and Luca Laurenti},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/adams2023bnndp.pdf:pdf},
  mdate = {2023-08-28},
  pages = {133--151},
  pdf = {https://proceedings.mlr.press/v202/adams23a/adams23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {BNN-DP: Robustness Certification of Bayesian Neural Networks via Dynamic Programming},
  url = {https://proceedings.mlr.press/v202/adams23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{agarwala2023sam,
  abstract = {The Sharpness Aware Minimization ({SAM}) optimization algorithm has been shown to control large eigenvalues of the loss {H}essian and provide generalization benefits in a variety of settings.},
  author = {Atish Agarwala and Yann N. Dauphin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/agarwala2023sam.pdf:pdf},
  mdate = {2023-08-28},
  pages = {152--168},
  pdf = {https://proceedings.mlr.press/v202/agarwala23a/agarwala23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SAM} operates far from home: eigenvalue regularization as a dynamical phenomenon},
  url = {https://proceedings.mlr.press/v202/agarwala23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{agarwala2023secondorder,
  abstract = {Recent studies of gradient descent with large step sizes have shown that there is often a regime with an initial increase in the largest eigenvalue of the loss {H}essian (progressive sharpening), followed by a stabilization of the eigenvalue near the maximum value which allows convergence (edge of stability).},
  author = {Atish Agarwala and Fabian Pedregosa and Jeffrey Pennington},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/agarwala2023secondorder.pdf:pdf},
  mdate = {2023-08-28},
  pages = {169--195},
  pdf = {https://proceedings.mlr.press/v202/agarwala23b/agarwala23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Second-order regression models exhibit progressive sharpening to the edge of stability},
  url = {https://proceedings.mlr.press/v202/agarwala23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{agazzi2023global,
  abstract = {We analyze {E}lman-type recurrent neural networks ({RNN}s) and their training in the mean-field regime. Specifically, we show convergence of gradient descent training dynamics of the {RNN} to the corresponding mean-field formulation in the large width limit.},
  author = {Andrea Agazzi and Jianfeng Lu and Sayan Mukherjee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/agazzi2023global.pdf:pdf},
  mdate = {2023-08-28},
  pages = {196--227},
  pdf = {https://proceedings.mlr.press/v202/agazzi23a/agazzi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Global optimality of {E}lman-type {RNN}s in the mean-field regime},
  url = {https://proceedings.mlr.press/v202/agazzi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aggarwal2023semsupxc,
  abstract = {Extreme classification ({XC}) involves predicting over large numbers of classes (thousands to millions), with real-world applications like news article classification and e-commerce product tagging. The zero-shot version of this task requires generalization to novel classes without additional supervision.},
  author = {Pranjal Aggarwal and Ameet Deshpande and Karthik R. Narasimhan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aggarwal2023semsupxc.pdf:pdf},
  mdate = {2023-08-28},
  pages = {228--247},
  pdf = {https://proceedings.mlr.press/v202/aggarwal23a/aggarwal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SemSup-XC}: Semantic Supervision for Zero and Few-shot Extreme Classification},
  url = {https://proceedings.mlr.press/v202/aggarwal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aghabozorgi2023adaptive,
  abstract = {Despite their success on large datasets, {GAN}s have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, {GAN}s tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place.},
  author = {Mehran Aghabozorgi and Shichong Peng and Ke Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aghabozorgi2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {248--264},
  pdf = {https://proceedings.mlr.press/v202/aghabozorgi23a/aghabozorgi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive {IMLE} for Few-shot Pretraining-free Generative Modelling},
  url = {https://proceedings.mlr.press/v202/aghabozorgi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aghajanyan2023scaling,
  abstract = {Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them.},
  author = {Armen Aghajanyan and Lili Yu and Alexis Conneau and Wei-Ning Hsu and Karen Hambardzumyan and Susan Zhang and Stephen Roller and Naman Goyal and Omer Levy and Luke Zettlemoyer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aghajanyan2023scaling.pdf:pdf},
  mdate = {2025-02-11},
  pages = {265--279},
  pdf = {https://proceedings.mlr.press/v202/aghajanyan23a/aghajanyan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Laws for Generative Mixed-Modal Language Models},
  url = {https://proceedings.mlr.press/v202/aghajanyan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aghbalou2023hypothesis,
  abstract = {Hypothesis transfer learning ({HTL}) contrasts domain adaptation by allowing for a previous task leverage without requiring access to the source data.},
  author = {Anass Aghbalou and Guillaume Staerman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aghbalou2023hypothesis.pdf:pdf},
  mdate = {2023-08-28},
  pages = {280--303},
  pdf = {https://proceedings.mlr.press/v202/aghbalou23a/aghbalou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hypothesis Transfer Learning with Surrogate Classification Losses: Generalization Bounds through Algorithmic Stability},
  url = {https://proceedings.mlr.press/v202/aghbalou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aglietti2023constrained,
  abstract = {We propose constrained causal Bayesian optimization (c{CBO}), an approach for finding interventions in a known causal graph that optimize a target variable under some constraints.},
  author = {Virginia Aglietti and Alan Malek and Ira Ktena and Silvia Chiappa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aglietti2023constrained.pdf:pdf},
  mdate = {2023-08-28},
  pages = {304--321},
  pdf = {https://proceedings.mlr.press/v202/aglietti23a/aglietti23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Causal Bayesian Optimization},
  url = {https://proceedings.mlr.press/v202/aglietti23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{agoritsas2023explaining,
  abstract = {In this paper, we quantify the impact of using non-convergent {M}arkov chains to train Energy-Based models ({EBM}s). In particular, we show analytically that {EBM}s trained with non-persistent short runs to estimate the gradient can perfectly reproduce a set of empirical statistics of the data, not at the level of the equilibrium measure, but through a precise dynamical process.},
  author = {Elisabeth Agoritsas and Giovanni Catania and Aurélien Decelle and Beatriz Seoane},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/agoritsas2023explaining.pdf:pdf},
  mdate = {2023-08-28},
  pages = {322--336},
  pdf = {https://proceedings.mlr.press/v202/agoritsas23a/agoritsas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Explaining the effects of non-convergent {MCMC} in the training of Energy-Based Models},
  url = {https://proceedings.mlr.press/v202/agoritsas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aher2023large,
  abstract = {We introduce a new type of test, called a {T}uring Experiment ({TE}), for evaluating to what extent a given language model can simulate different aspects of human behavior.},
  author = {Gati V. Aher and Rosa I. Arriaga and Adam Tauman Kalai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aher2023large.pdf:pdf},
  mdate = {2023-08-28},
  pages = {337--371},
  pdf = {https://proceedings.mlr.press/v202/aher23a/aher23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies},
  url = {https://proceedings.mlr.press/v202/aher23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ahuja2023interventional,
  abstract = {Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure.},
  author = {Kartik Ahuja and Divyat Mahajan and Yixin Wang and Yoshua Bengio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ahuja2023interventional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {372--407},
  pdf = {https://proceedings.mlr.press/v202/ahuja23a/ahuja23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Interventional Causal Representation Learning},
  url = {https://proceedings.mlr.press/v202/ahuja23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ailer2023sequential,
  abstract = {Instrumental variable (IV) methods are used to estimate causal effects in settings with unobserved confounding, where we cannot directly experiment on the treatment variable. Instruments are variables which only affect the outcome indirectly via the treatment variable(s). Most IV applications focus on low-dimensional treatments and crucially require at least as many instruments as treatments. This assumption is restrictive: in the natural sciences we often seek to infer causal effects of high-dimensional treatments (e.g., the effect of gene expressions or microbiota on health and disease), but can only run few experiments with a limited number of instruments (e.g., drugs or antibiotics). In such under-specified problems, the full treatment effect is not identifiable in a single experiment even in the linear case. We show that one can still reliably recover the projection of the treatment effect onto the instrumented subspace and develop techniques to consistently combine such partial estimates from different sets of instruments. We then leverage our combined estimators in an algorithm that iteratively proposes the most informative instruments at each round of experimentation to maximize the overall information about the full causal effect.},
  author = {Elisabeth Ailer and Jason Hartford and Niki Kilbertus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ailer2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {408--420},
  pdf = {https://proceedings.mlr.press/v202/ailer23a/ailer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Underspecified Instrument Selection for Cause-Effect Estimation},
  url = {https://proceedings.mlr.press/v202/ailer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aitchison2023atari5,
  abstract = {The Arcade Learning Environment (ALE) has become an essential benchmark for assessing the performance of reinforcement learning algorithms. However, the computational cost of generating results on the entire 57-game dataset limits ALE's use and makes the reproducibility of many results infeasible. We propose a novel solution to this problem in the form of a principled methodology for selecting small but representative subsets of environments within a benchmark suite. We applied our method to identify a subset of five ALE games, we call Atari-5, which produces 57-game median score estimates within 10% of their true values. Extending the subset to 10-games recovers 80% of the variance for log-scores for all games within the 57-game set. We show this level of compression is possible due to a high degree of correlation between many of the games in ALE.},
  author = {Matthew Aitchison and Penny Sweetser and Marcus Hutter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aitchison2023atari5.pdf:pdf},
  mdate = {2023-08-28},
  pages = {421--438},
  pdf = {https://proceedings.mlr.press/v202/aitchison23a/aitchison23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Atari-5: Distilling the Arcade Learning Environment down to Five Games},
  url = {https://proceedings.mlr.press/v202/aitchison23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{akhtar2023towards,
  abstract = {Originally inspired by game-theory, path attribution framework stands out among the post-hoc model interpretation tools due to its axiomatic nature. However, recent developments show that this framework can still suffer from counter-intuitive results. Moreover, specifically for deep visual models, the existing path-based methods also fall short on conforming to the original intuitions that are the basis of the claimed axiomatic properties of this framework. We address these problems with a systematic investigation, and pinpoint the conditions in which the counter-intuitive results can be avoided for deep visual model interpretation with the path attribution strategy. We also devise a scheme to preclude the conditions in which visual model interpretation can invalidate the axiomatic properties of path attribution. These insights are combined into a method that enables reliable visual model interpretation. Our findings are establish empirically with multiple datasets, models and evaluation metrics. Extensive experiments show a consistent performance gain of our method over the baselines.},
  author = {Naveed Akhtar and Mohammad A. A. K. Jalwana},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/akhtar2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {439--457},
  pdf = {https://proceedings.mlr.press/v202/akhtar23a/akhtar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards credible visual model interpretation with path attribution},
  url = {https://proceedings.mlr.press/v202/akhtar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{alacaoglu2023convergence,
  abstract = {We analyze classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\tilde{O}(t^{-1/4})$ and complexity $\tilde{O}(\varepsilon^{-4})$ for achieving an $\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantees require i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for constrained smooth nonconvex optimization with dependent data from $\tilde{O}(\varepsilon^{-8})$ to $\tilde{O}(\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, adaptive stochastic gradient algorithm AdaGrad and stochastic gradient algorithm with heavy ball momentum. As an application, we obtain first online nonnegative matrix factorization algorithms for dependent data based on stochastic projected gradient methods with adaptive step sizes and optimal rate of convergence.},
  author = {Ahmet Alacaoglu and Hanbaek Lyu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/alacaoglu2023convergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {458--489},
  pdf = {https://proceedings.mlr.press/v202/alacaoglu23a/alacaoglu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data},
  url = {https://proceedings.mlr.press/v202/alacaoglu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{alam2023recasting,
  abstract = {In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $O(T^2)$ memory and $O(T^2H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $O(TH \log H)$ time complexity, $O(TH)$ space complexity, and convergence in 10$\times$ fewer epochs.},
  author = {Mohammad Mahmudul Alam and Edward Raff and Stella Biderman and Tim Oates and James Holt},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/alam2023recasting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {490--507},
  pdf = {https://proceedings.mlr.press/v202/alam23a/alam23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Recasting Self-Attention with Holographic Reduced Representations},
  url = {https://proceedings.mlr.press/v202/alam23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{alghamdi2023saddlepoint,
  abstract = {We characterize the differential privacy guarantees of privacy mechanisms in the large-composition regime, i.e., when a privacy mechanism is sequentially applied a large number of times to sensitive data. Via exponentially tilting the privacy loss random variable, we derive a new formula for the privacy curve expressing it as a contour integral over an integration path that runs parallel to the imaginary axis with a free real-axis intercept. Then, using the method of steepest descent from mathematical physics, we demonstrate that the choice of saddle-point as the real-axis intercept yields closed-form accurate approximations of the desired contour integral. This procedure---dubbed the saddle-point accountant (SPA)---yields a constant-time accurate approximation of the privacy curve.},
  author = {Wael Alghamdi and Juan Felipe G{\'o}mez and Shahab Asoodeh and Fl{\'a}vio P. Calmon and Oliver Kosut and Lalitha Sankar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/alghamdi2023saddlepoint.pdf:pdf},
  mdate = {2023-08-28},
  pages = {508--528},
  pdf = {https://proceedings.mlr.press/v202/alghamdi23a/alghamdi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Saddle-Point Method in Differential Privacy},
  url = {https://proceedings.mlr.press/v202/alghamdi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{allingham2023simple,
  abstract = {Contrastively trained text-image models have the remarkable ability to perform zero-shot classification, that is, classifying previously unseen images into categories that the model has never been explicitly trained to identify. However, these zero-shot classifiers need prompt engineering to achieve high accuracy. Prompt engineering typically requires hand-crafting a set of prompts for individual downstream tasks. In this work, we aim to automate this prompt engineering and improve zero-shot accuracy through prompt ensembling. In particular, we ask ``Given a large pool of prompts, can we automatically score the prompts and ensemble those that are most suitable for a particular downstream dataset, without needing access to labeled validation data?''. We demonstrate that this is possible. In doing so, we identify several pathologies in a naive prompt scoring method where the score can be easily overconfident due to biases in pre-training and test data, and we propose a novel prompt scoring method that corrects for the biases. Using our proposed scoring method to create a weighted average prompt ensemble, our method outperforms equal average ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its variants, and 11 fine-grained classification benchmarks, all while being fully automatic, optimization-free, and not requiring access to labeled validation data.},
  author = {James Urquhart Allingham and Jie Ren and Michael W. Dusenberry and Xiuye Gu and Yin Cui and Dustin Tran and Jeremiah Zhe Liu and Balaji Lakshminarayanan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/allingham2023simple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {547--568},
  pdf = {https://proceedings.mlr.press/v202/allingham23a/allingham23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models},
  url = {https://proceedings.mlr.press/v202/allingham23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{allouah2023privacyrobustnessutility,
  abstract = {The ubiquity of distributed machine learning (ML) in sensitive public domain applications calls for algorithms that protect data privacy, while being robust to faults and adversarial behaviors. Although privacy and robustness have been extensively studied independently in distributed ML, their synthesis remains poorly understood. We present the first tight analysis of the error incurred by any algorithm ensuring robustness against a fraction of adversarial machines, as well as differential privacy (DP) for honest machines' data against any other curious entity. Our analysis exhibits a fundamental trade-off between privacy, robustness, and utility. To prove our lower bound, we consider the case of mean estimation, subject to distributed DP and robustness constraints, and devise reductions to centralized estimation of one-way marginals. We prove our matching upper bound by presenting a new distributed ML algorithm using a high-dimensional robust aggregation rule.},
  author = {Youssef Allouah and Rachid Guerraoui and Nirupam Gupta and Rafael Pinot and John Stephan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/allouah2023privacyrobustnessutility.pdf:pdf},
  mdate = {2023-09-30},
  pages = {569--626},
  pdf = {https://proceedings.mlr.press/v202/allouah23a/allouah23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Privacy-Robustness-Utility Trilemma in Distributed Learning},
  url = {https://proceedings.mlr.press/v202/allouah23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{alparslan2023differentially,
  abstract = {We propose a novel Bayesian inference framework for distributed differentially private linear regression. We consider a distributed setting where multiple parties hold parts of the data and share certain summary statistics of their portions in privacy-preserving noise. We develop a novel generative statistical model for privately shared statistics, which exploits a useful distributional relation between the summary statistics of linear regression. We propose Bayesian estimation of the regression coefficients, mainly using Markov chain Monte Carlo algorithms, while also providing a fast version that performs approximate Bayesian estimation in one iteration. The proposed methods have computational advantages over their competitors. We provide numerical results on both real and simulated data, which demonstrate that the proposed algorithms provide well-rounded estimation and prediction.},
  author = {Baris Alparslan and Sinan Yildirim and S. Ilker Birbil},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/alparslan2023differentially.pdf:pdf},
  mdate = {2024-02-05},
  pages = {627--641},
  pdf = {https://proceedings.mlr.press/v202/alparslan23a/alparslan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Distributed Bayesian Linear Regression with MCMC},
  url = {https://proceedings.mlr.press/v202/alparslan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{altamirano2023robust,
  abstract = {This paper proposes an online, provably robust, and scalable Bayesian approach for changepoint detection. The resulting algorithm has key advantages over previous work: it provides provable robustness by leveraging the generalised Bayesian perspective, and also addresses the scalability issues of previous attempts. Specifically, the proposed generalised Bayesian formalism leads to conjugate posteriors whose parameters are available in closed form by leveraging diffusion score matching. The resulting algorithm is exact, can be updated through simple algebra, and is more than 10 times faster than its closest competitor.},
  author = {Matias Altamirano and Fran{\c{c}}ois-Xavier Briol and Jeremias Knoblauch},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/altamirano2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {642--663},
  pdf = {https://proceedings.mlr.press/v202/altamirano23a/altamirano23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust and Scalable {B}ayesian Online Changepoint Detection},
  url = {https://proceedings.mlr.press/v202/altamirano23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{altekru2023neural,
  abstract = {Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks (NNs). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative NNs which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy.},
  author = {Fabian Altekr{\"u}ger and Johannes Hertrich and Gabriele Steidl},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/altekru2023neural.pdf:pdf},
  mdate = {2025-03-03},
  pages = {664--690},
  pdf = {https://proceedings.mlr.press/v202/altekruger23a/altekruger23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural {W}asserstein Gradient Flows for Discrepancies with {R}iesz Kernels},
  url = {https://proceedings.mlr.press/v202/altekruger23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{amani2023distributed,
  abstract = {We study distributed contextual linear bandits with stochastic contexts, where N agents act cooperatively to solve a linear bandit-optimization problem with d-dimensional features over the course of T rounds. For this problem, we derive the first ever information-theoretic lower bound Ω(dN) on the communication cost of any algorithm that performs optimally in a regret minimization setup. We then propose a distributed batch elimination version of the LinUCB algorithm, DisBE-LUCB, where the agents share information among each other through a central server. We prove that the communication cost of DisBE-LUCB matches our lower bound up to logarithmic factors. In particular, for scenarios with known context distribution, the communication cost of DisBE-LUCB is only Õ(dN) and its regret is Õ(√(dNT)), which is of the same order as that incurred by an optimal single-agent algorithm for NT rounds.},
  author = {Sanae Amani and Tor Lattimore and Andr{\'a}s Gy{\"o}rgy and Lin F. Yang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/amani2023distributed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {691--717},
  pdf = {https://proceedings.mlr.press/v202/amani23a/amani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distributed Contextual Linear Bandits with Minimax Optimal Communication Cost},
  url = {https://proceedings.mlr.press/v202/amani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{amin2023kernelized,
  abstract = {Generative models of biological sequences are a powerful tool for learning from complex sequence data, predicting the effects of mutations, and designing novel biomolecules with desired properties. To evaluate generative models it is important to accurately measure differences between high-dimensional distributions. In this paper we propose the KSD-B, a novel divergence measure for distributions over biological sequences that is based on the kernelized Stein discrepancy (KSD). The KSD-B can be evaluated even when the normalizing constant of the model is unknown; it allows for variable length sequences and can take into account biological notions of sequence distance. Unlike previous KSDs over discrete spaces the KSD-B (a) is theoretically guaranteed to detect convergence and non-convergence of distributions over sequence space and (b) can be efficiently estimated in practice.},
  author = {Alan Nawzad Amin and Eli N. Weinstein and Debora Susan Marks},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/amin2023kernelized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {718--767},
  pdf = {https://proceedings.mlr.press/v202/amin23a/amin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Kernelized {S}tein Discrepancy for Biological Sequences},
  url = {https://proceedings.mlr.press/v202/amin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{amortila2023optimal,
  abstract = {Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such approximation factors -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.},
  author = {Philip Amortila and Nan Jiang and Csaba Szepesv{\'a}ri},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/amortila2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {768--790},
  pdf = {https://proceedings.mlr.press/v202/amortila23a/amortila23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation},
  url = {https://proceedings.mlr.press/v202/amortila23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{amos2023meta,
  abstract = {We study the use of amortized optimization to predict optimal transport (OT) maps from the input measures, which we call Meta OT. This helps repeatedly solve similar OT problems between different measures by leveraging the knowledge and information present from past problems to rapidly predict and solve new problems. Otherwise, standard methods ignore the knowledge of the past solutions and suboptimally re-solve each problem from scratch. We instantiate Meta OT models in discrete and continuous settings between grayscale images, spherical data, classification labels, and color palettes and use them to improve the computational time of standard OT solvers.},
  author = {Brandon Amos and Giulia Luise and Samuel Cohen and Ievgen Redko},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/amos2023meta.pdf:pdf},
  mdate = {2023-08-28},
  pages = {791--813},
  pdf = {https://proceedings.mlr.press/v202/amos23a/amos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Meta Optimal Transport},
  url = {https://proceedings.mlr.press/v202/amos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{anagnostides2023nearoptimal,
  abstract = {In this paper, we establish efficient and uncoupled learning dynamics so that, when employed by all players in multiplayer perfect-recall imperfect-information extensive-form games, the trigger regret of each player grows as $O(\log T)$ after $T$ repetitions of play. This improves exponentially over the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a recent open question by Bai et al. (2022). As an immediate consequence, we guarantee convergence to the set of extensive-form correlated equilibria and coarse correlated equilibria at a near-optimal rate of $\log T/T$.},
  author = {Ioannis Anagnostides and Gabriele Farina and Tuomas Sandholm},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/anagnostides2023nearoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {814--839},
  pdf = {https://proceedings.mlr.press/v202/anagnostides23a/anagnostides23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-Optimal $\Phi$-Regret Learning in Extensive-Form Games},
  url = {https://proceedings.mlr.press/v202/anagnostides23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{andriushchenko2023modern,
  abstract = {Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on ImageNet and CIFAR-10 to fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlated with generalization depending on the setup. Interestingly, in multiple cases, we observe a consistent negative correlation of sharpness with out-of-distribution error implying that sharper minima can generalize better. Finally, we illustrate on a simple model that the right sharpness measure is highly data-dependent, and that we do not understand well this aspect for realistic data distributions.},
  author = {Maksym Andriushchenko and Francesco Croce and Maximilian M{\"u}ller and Matthias Hein and Nicolas Flammarion},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/andriushchenko2023modern.pdf:pdf},
  mdate = {2023-08-28},
  pages = {840--902},
  pdf = {https://proceedings.mlr.press/v202/andriushchenko23a/andriushchenko23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Modern Look at the Relationship between Sharpness and Generalization},
  url = {https://proceedings.mlr.press/v202/andriushchenko23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{andriushchenko2023sgd,
  abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) may lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics that biases it implicitly toward simple predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used: the regularization effect comes solely from the SGD dynamics influenced by the large step sizes schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks.},
  author = {Maksym Andriushchenko and Aditya Vardhan Varre and Loucas Pillaud-Vivien and Nicolas Flammarion},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/andriushchenko2023sgd.pdf:pdf},
  mdate = {2023-08-28},
  pages = {903--925},
  pdf = {https://proceedings.mlr.press/v202/andriushchenko23b/andriushchenko23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SGD} with Large Step Sizes Learns Sparse Features},
  url = {https://proceedings.mlr.press/v202/andriushchenko23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ansari2023neural,
  abstract = {Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved imputation and forecasting performance of NCDSSM over existing models.},
  author = {Abdul Fatir Ansari and Alvin Heng and Andre Lim and Harold Soh},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ansari2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {926--951},
  pdf = {https://proceedings.mlr.press/v202/ansari23a/ansari23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series},
  url = {https://proceedings.mlr.press/v202/ansari23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aouali2023exponential,
  abstract = {Off-policy learning (OPL) aims at finding improved policies from logged bandit data, often by minimizing the inverse propensity scoring (IPS) estimator of the risk. In this work, we investigate a smooth regularization for IPS, for which we derive a two-sided PAC-Bayes generalization bound.},
  author = {Imad Aouali and Victor-Emmanuel Brunel and David Rohde and Anna Korba},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aouali2023exponential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {984--1017},
  pdf = {https://proceedings.mlr.press/v202/aouali23a/aouali23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exponential Smoothing for Off-Policy Learning},
  url = {https://proceedings.mlr.press/v202/aouali23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{arbas2023polynomial,
  abstract = {We study the problem of privately estimating the parameters of $d$-dimensional Gaussian Mixture Models (GMMs) with $k$ components. For this, we develop a technique to reduce the problem to its non-private counterpart. This allows us to privatize existing non-private algorithms in a blackbox manner, while incurring only a small overhead in sample complexity and running time.},
  author = {Jamil Arbas and Hassan Ashtiani and Christopher Liaw},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/arbas2023polynomial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1018--1040},
  pdf = {https://proceedings.mlr.press/v202/arbas23a/arbas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Polynomial Time and Private Learning of Unbounded Gaussian Mixture Models},
  url = {https://proceedings.mlr.press/v202/arbas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{arisaka2023principled,
  abstract = {Iterative methods are ubiquitous in large-scale scientific computing applications, and a number of approaches based on meta-learning have been recently proposed to accelerate them. However, a systematic study of these approaches and how they differ from meta-learning is lacking. In this paper, we propose a framework to analyze such learning-based acceleration approaches, where one can immediately identify a departure from classical meta-learning. We show that this departure may lead to arbitrary deterioration of model performance. Based on our analysis, we introduce a novel training method for learning-based acceleration of iterative methods. Furthermore, we theoretically prove that the proposed method improves upon the existing methods, and demonstrate its significant advantage and versatility through various numerical applications.},
  author = {Sohei Arisaka and Qianxiao Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/arisaka2023principled.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1041--1059},
  pdf = {https://proceedings.mlr.press/v202/arisaka23a/arisaka23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Principled Acceleration of Iterative Numerical Methods Using Machine Learning},
  url = {https://proceedings.mlr.press/v202/arisaka23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{arora2023faster,
  author = {Raman Arora and Raef Bassily and Toms Gonzlez and Cristbal Guzmn and Michael Menart and Enayat Ullah},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/arora2023faster.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1060--1092},
  pdf = {https://proceedings.mlr.press/v202/arora23a/arora23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Faster Rates of Convergence to Stationary Points in Differentially Private Optimization},
  url = {https://proceedings.mlr.press/v202/arora23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{asadi2023prototypesample,
  abstract = {In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks' data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities.},
  author = {Nader Asadi and MohammadReza Davari and Sudhir P. Mudur and Rahaf Aljundi and Eugene Belilovsky},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/asadi2023prototypesample.pdf:pdf},
  mdate = {2025-06-10},
  pages = {1093--1106},
  pdf = {https://proceedings.mlr.press/v202/asadi23a/asadi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning},
  url = {https://proceedings.mlr.press/v202/asadi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{asi2023nearoptimal,
  abstract = {We consider online learning problems in the realizable setting, where there is a zero-loss solution, and propose new Differentially Private (DP) algorithms that obtain near-optimal regret bounds. For the problem of online prediction from experts, we design new algorithms that obtain near-optimal regret $O(\varepsilon^{-1} \log^{1.5}{d})$ where $d$ is the number of experts. This significantly improves over the best existing regret bounds for the DP non-realizable setting which are $O(\varepsilon^{-1} \min\{d, T^{1/3}\log d\})$.},
  author = {Hilal Asi and Vitaly Feldman and Tomer Koren and Kunal Talwar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/asi2023nearoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1107--1120},
  pdf = {https://proceedings.mlr.press/v202/asi23a/asi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-Optimal Algorithms for Private Online Optimization in the Realizable Regime},
  url = {https://proceedings.mlr.press/v202/asi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{asi2023robustness,
  abstract = {We study the relationship between two desiderata of algorithms in statistical inference and machine learning: differential privacy and robustness to adversarial data corruptions. Their conceptual similarity was first observed by Dwork and Lei (STOC 2009), who observed that private algorithms satisfy robustness, and gave a general method for converting robust algorithms to private ones. However, all general methods for transforming robust algorithms into private ones lead to suboptimal error rates. Our work gives the first black-box transformation that converts any adversarially robust algorithm into one that satisfies pure differential privacy.},
  author = {Hilal Asi and Jonathan R. Ullman and Lydia Zakynthinou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/asi2023robustness.pdf:pdf},
  mdate = {2023-11-12},
  pages = {1121--1146},
  pdf = {https://proceedings.mlr.press/v202/asi23b/asi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Robustness to Privacy and Back},
  url = {https://proceedings.mlr.press/v202/asi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{attia2023sgd,
  abstract = {We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular adaptive (self-tuning) method for first-order stochastic optimization. Despite being well studied, existing analyses of this method suffer from various shortcomings: they either assume some knowledge of the problem parameters, impose strong global Lipschitz conditions, or fail to give bounds that hold with high probability. We provide a comprehensive analysis of this basic method without any of these limitations, in both the convex and non-convex (smooth) cases, that additionally supports a general "affine variance" noise model and provides sharp rates of convergence in both the low-noise and high-noise regimes.},
  author = {Amit Attia and Tomer Koren},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/attia2023sgd.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1147--1171},
  pdf = {https://proceedings.mlr.press/v202/attia23a/attia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance},
  url = {https://proceedings.mlr.press/v202/attia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{attias2023adversarially,
  abstract = {We study robustness to test-time adversarial attacks in the regression setting with $\ell_p$ losses and arbitrary perturbation sets. We address the question of which function classes are PAC learnable in this setting. We show that classes of finite fat-shattering dimension are learnable in both the realizable and agnostic settings. Moreover, for convex function classes, they are even properly learnable. In contrast, some non-convex function classes provably require improper learning algorithms. Our main technique is based on a construction of an adversarially robust sample compression scheme of a size determined by the fat-shattering dimension.},
  author = {Idan Attias and Steve Hanneke},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/attias2023adversarially.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1172--1199},
  pdf = {https://proceedings.mlr.press/v202/attias23a/attias23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarially Robust PAC Learnability of Real-Valued Functions},
  url = {https://proceedings.mlr.press/v202/attias23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{atzeni2023infusing,
  abstract = {The Abstraction and Reasoning Corpus (ARC) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks.},
  author = {Mattia Atzeni and Mrinmaya Sachan and Andreas Loukas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/atzeni2023infusing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1200--1217},
  pdf = {https://proceedings.mlr.press/v202/atzeni23a/atzeni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning},
  url = {https://proceedings.mlr.press/v202/atzeni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{atzmon2023learning,
  abstract = {Training agents to control a dynamic environment is a fundamental task in AI. In many environments, the dynamics can be summarized by a small set of events that capture the semantic behavior of the system. Typically, these events form chains or cascades. We often wish to change the system behavior using a single intervention that propagates through the cascade. For instance, one may trigger a biochemical cascade to switch the state of a cell or, in logistics, reroute a truck to meet an unexpected, urgent delivery. We introduce a new supervised learning setup called Cascade. An agent observes a system with known dynamics evolving from some initial state. The agent is given a structured semantic instruction and needs to make an intervention that triggers a cascade of events, such that the system reaches an alternative (counterfactual) behavior. We combine semantic tree search with an event-driven forward model and devise an algorithm that learns to efficiently search in exponentially large semantic trees. We demonstrate that our approach learns to follow instructions to intervene in new complex scenes. When provided with an observed cascade of events, it can also reason about alternative outcomes.},
  author = {Yuval Atzmon and Eli A. Meirom and Shie Mannor and Gal Chechik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/atzmon2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1218--1243},
  pdf = {https://proceedings.mlr.press/v202/atzmon23a/atzmon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Initiate and Reason in Event-Driven Cascading Processes},
  url = {https://proceedings.mlr.press/v202/atzmon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{aubert2023convergence,
  abstract = {When fitting the learning data of an individual to algorithm-like learning models, the observations are so dependent and non-stationary that one may wonder what the classical Maximum Likelihood Estimator (MLE) could do, even if it is the usual tool applied to experimental cognition. Our objective in this work is to show that the estimation of the learning rate cannot be efficient if the learning rate is constant in the classical Exp3 (Exponential weights for Exploration and Exploitation) algorithm. The paper addresses the challenge of using Maximum Likelihood Estimation for learning rate estimation in multi-armed bandit algorithms, specifically focusing on the theoretical properties and convergence behavior when dealing with dependent and non-stationary observations typical in human learning experiments.},
  author = {Julien Aubert and Luc Lehricy and Patricia Reynaud-Bouret},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/aubert2023convergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1244--1275},
  pdf = {https://proceedings.mlr.press/v202/aubert23a/aubert23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the convergence of the MLE as an estimator of the learning rate in the Exp3 algorithm},
  url = {https://proceedings.mlr.press/v202/aubert23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{avdeyev2023dirichlet,
  abstract = {Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirichlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also solve Sudoku, including hard puzzles, without additional training. Finally, we applied this approach to develop the first human promoter DNA sequence design model and showed that designed sequences share similar properties with natural promoter sequences.},
  author = {Pavel Avdeyev and Chenlai Shi and Yuhao Tan and Kseniia Dudnyk and Jian Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/avdeyev2023dirichlet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1276--1301},
  pdf = {https://proceedings.mlr.press/v202/avdeyev23a/avdeyev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dirichlet Diffusion Score Model for Biological Sequence Generation},
  url = {https://proceedings.mlr.press/v202/avdeyev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{axiotis2023gradient,
  abstract = {We show that running gradient descent with variable learning rate guarantees loss $f(x) \leq 1.1 \cdot f(x^*) + \epsilon$ for the logistic regression objective, where the error $\epsilon$ decays exponentially with the number of iterations and polynomially with the magnitude of the entries of an arbitrary fixed solution $x^*$. This is in contrast to the common intuition that the absence of strong convexity precludes linear convergence of first-order methods, and highlights the importance of variable learning rates for gradient descent. We also apply our ideas to sparse logistic regression, where they lead to an exponential improvement of the sparsity-error tradeoff.},
  author = {Kyriakos Axiotis and Maxim Sviridenko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/axiotis2023gradient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1302--1319},
  pdf = {https://proceedings.mlr.press/v202/axiotis23a/axiotis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient Descent Converges Linearly for Logistic Regression on Separable Data},
  url = {https://proceedings.mlr.press/v202/axiotis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ayme2023naive,
  abstract = {Two different approaches exist to handle missing values for prediction: either imputation, prior to fitting any predictive algorithms, or dedicated methods able to natively incorporate missing values. While imputation is widely (and easily) used, it is unfortunately biased when low-capacity predictors (such as linear models) are applied afterward. However, in practice, naive imputation exhibits good predictive performance. In this paper, we study the impact of imputation in a high-dimensional linear model with MCAR missing data. We prove that zero imputation performs an implicit regularization closely related to the ridge method, often used in high-dimensional problems. Leveraging on this connection, we establish that the imputation bias is controlled by a ridge bias, which vanishes in high dimension.},
  author = {Alexis Ayme and Claire Boyer and Aymeric Dieuleveut and Erwan Scornet},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ayme2023naive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1320--1340},
  pdf = {https://proceedings.mlr.press/v202/ayme23a/ayme23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Naive imputation implicitly regularizes high-dimensional linear models},
  url = {https://proceedings.mlr.press/v202/ayme23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{azabou2023halfhop,
  abstract = {Message passing neural networks have shown a lot of success on graph-structured data. However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes. In this work, we introduce a simple yet general framework for improving learning in message passing neural networks. Our approach essentially upsamples edges in the original graph by adding "slow nodes" at each edge that can mediate communication between a source and a target node. Our method only modifies the input graph, making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing, we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks, and show improvements across the board, notably in heterophilic conditions where adjacent nodes are more likely to have different labels.},
  author = {Mehdi Azabou and Venkataramana Ganesh and Shantanu Thakoor and Chi-Heng Lin and Lakshmi Sathidevi and Ran Liu and Michal Valko and Petar Velickovic and Eva L. Dyer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/azabou2023halfhop.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1341--1360},
  pdf = {https://proceedings.mlr.press/v202/azabou23a/azabou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Half-Hop: A graph upsampling approach for slowing down message passing},
  url = {https://proceedings.mlr.press/v202/azabou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{azad2023clutr,
  abstract = {Reinforcement Learning (RL) algorithms are often known for sample inefficiency and difficult generalization. Recently, Unsupervised Environment Design (UED) emerged as a new paradigm for zero-shot generalization by simultaneously learning a task distribution and agent policies on the generated tasks. This is a non-stationary process where the task distribution evolves along with agent policies; creating an instability over time. While past works demonstrated the potential of such approaches, sampling effectively from the task space remains an open challenge, bottlenecking these approaches. To this end, we introduce CLUTR: a novel unsupervised curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization. It first trains a recurrent variational autoencoder on randomly generated tasks to learn a latent task manifold. Next, a teacher agent creates a curriculum by maximizing a minimax REGRET-based objective on a set of latent tasks sampled from this manifold. Using the fixed-pretrained task manifold, we show that CLUTR successfully overcomes the non-stationarity problem and improves stability. Our experimental results show CLUTR outperforms PAIRED, a principled and popular UED method, in the challenging CarRacing and navigation environments: achieving 10.6X and 45% improvement in zero-shot generalization, respectively.},
  author = {Abdus Salam Azad and Izzeddin Gur and Jasper Emhoff and Nathaniel Alexis and Aleksandra Faust and Pieter Abbeel and Ion Stoica},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/azad2023clutr.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1361--1395},
  pdf = {https://proceedings.mlr.press/v202/azad23a/azad23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {CLUTR: Curriculum Learning via Unsupervised Task Representation Learning},
  url = {https://proceedings.mlr.press/v202/azad23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{baek2023personalized,
  abstract = {The paper addresses scenarios where subgraphs of a larger global graph may be distributed across multiple devices, and only locally accessible due to privacy restrictions, although there may be links between subgraphs. Recently proposed subgraph Federated Learning (FL) methods deal with those missing links across local subgraphs while distributively training Graph Neural Networks (GNNs) on them. However, they have overlooked the inevitable heterogeneity between subgraphs comprising different communities of a global graph, consequently collapsing the incompatible knowledge from local GNN models. To this end, we introduce a new subgraph FL problem, personalized subgraph FL, which focuses on the joint improvement of the interrelated local GNNs rather than learning a single global model, and propose a novel framework, FEDerated Personalized sUBgraph learning (FED-PUB), to tackle it. FED-PUB utilizes functional embeddings of the local GNNs using random graphs as inputs to compute similarities between them, and use the similarities to perform weighted averaging for server-side aggregation. Further, it learns a personalized sparse mask at each client to select and update only the subgraph-relevant subset of the aggregated parameters.},
  author = {Jinheon Baek and Wonyong Jeong and Jiongdao Jin and Jaehong Yoon and Sung Ju Hwang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/baek2023personalized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1396--1415},
  pdf = {https://proceedings.mlr.press/v202/baek23a/baek23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Personalized Subgraph Federated Learning},
  url = {https://proceedings.mlr.press/v202/baek23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{baevski2023efficient,
  abstract = {Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8% with a ViT-L model trained for 150 epochs.},
  author = {Alexei Baevski and Arun Babu and Wei-Ning Hsu and Michael Auli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/baevski2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1416--1429},
  pdf = {https://proceedings.mlr.press/v202/baevski23a/baevski23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language},
  url = {https://proceedings.mlr.press/v202/baevski23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{baey2023efficient,
  abstract = {Latent variable models are powerful tools for modeling complex phenomena involving in particular partially observed data, unobserved variables or underlying complex unknown structures. Inference is often difficult due to the latent structure of the model. To deal with parameter estimation in the presence of latent variables, well-known efficient methods exist, such as gradient-based and EM-type algorithms, but with practical and theoretical limitations. In this paper, we propose as an alternative for parameter estimation an efficient preconditioned stochastic gradient algorithm. Our method includes a preconditioning step based on a positive definite Fisher information matrix estimate. We prove convergence results for the proposed algorithm under mild assumptions for very general latent variables models. We illustrate through relevant simulations the performance of the proposed methodology in a nonlinear mixed effects model and in a stochastic block model.},
  author = {Charlotte Baey and Maud Delattre and Estelle Kuhn and Jean-Benoist Leger and Sarah Lemler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/baey2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1430--1453},
  pdf = {https://proceedings.mlr.press/v202/baey23a/baey23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient preconditioned stochastic gradient descent for estimation in latent variable models},
  url = {https://proceedings.mlr.press/v202/baey23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bagi2023generative,
  abstract = {Conventional supervised learning methods typically assume i.i.d samples and are found to be sensitive to out-of-distribution (OOD) data. We propose Generative Causal Representation Learning (GCRL) which leverages causality to facilitate knowledge transfer under distribution shifts. While we evaluate the effectiveness of our proposed method in human trajectory prediction models, GCRL can be applied to other domains as well. First, we propose a novel causal model that explains the generative factors in motion forecasting datasets using features that are common across all environments and with features that are specific to each environment. Selection variables are used to determine which parts of the model can be directly transferred to a new environment without fine-tuning. Second, we propose an end-to-end variational learning paradigm to learn the causal mechanisms that generate observations from features. GCRL is supported by strong theoretical results that imply identifiability of the causal model under certain assumptions. Experimental results on synthetic and real-world motion forecasting datasets show the robustness and effectiveness of our proposed method for knowledge transfer under zero-shot and low-shot settings by substantially outperforming the prior motion forecasting models on out-of-distribution prediction.},
  author = {Shayan Shirahmad Gale Bagi and Zahra Gharaee and Oliver Schulte and Mark Crowley},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bagi2023generative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1596--1612},
  pdf = {https://proceedings.mlr.press/v202/shirahmad-gale-bagi23a/shirahmad-gale-bagi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting},
  url = {https://proceedings.mlr.press/v202/shirahmad-gale-bagi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bai2023feed,
  abstract = {Modern machine learning models deployed in the wild can encounter both covariate and semantic shifts, giving rise to the problems of out-of-distribution (OOD) generalization and OOD detection respectively. While both problems have received significant research attention lately, they have been pursued independently. This may not be surprising, since the two tasks have seemingly conflicting goals. This paper provides a new unified approach that is capable of simultaneously generalizing to covariate shifts while robustly detecting semantic shifts. We propose a margin-based learning framework that exploits freely available unlabeled data in the wild that captures the environmental test-time OOD distributions under both covariate and semantic shifts. We show both empirically and theoretically that the proposed margin constraint is the key to achieving both OOD generalization and detection. Extensive experiments show the superiority of our framework, outperforming competitive baselines that specialize in either OOD generalization or OOD detection.},
  author = {Haoyue Bai and Gregory Canal and Xuefeng Du and Jeongyeol Kwon and Robert D. Nowak and Yixuan Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bai2023feed.pdf:pdf},
  mdate = {2025-02-06},
  pages = {1454--1471},
  pdf = {https://proceedings.mlr.press/v202/bai23a/bai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection},
  url = {https://proceedings.mlr.press/v202/bai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bai2023answering,
  abstract = {Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries and may not generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are needed.},
  author = {Yushi Bai and Xin Lv and Juanzi Li and Lei Hou 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bai2023answering.pdf:pdf},
  mdate = {2023-11-15},
  pages = {1472--1491},
  pdf = {https://proceedings.mlr.press/v202/bai23b/bai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization},
  url = {https://proceedings.mlr.press/v202/bai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bai2023linear,
  abstract = {Optimal transport (OT) has gained popularity due to its various applications in fields such as machine learning, statistics, and signal processing. However, the balanced mass requirement limits its performance in practical problems. To address these limitations, variants of the OT problem, including unbalanced OT, Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have been proposed. In this paper, we propose the Linear optimal partial transport (LOPT) embedding, which extends the (local) linearization technique on OT and HK to the OPT problem. The proposed embedding allows for faster computation of OPT distance between pairs of positive measures. Besides our theoretical contributions, we demonstrate the LOPT embedding technique in point-cloud interpolation and PCA analysis.},
  author = {Yikun Bai and Ivan Vladimir Medri and Rocio Diaz Martin and Rana Muhammad Shahroz Khan and Soheil Kolouri},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bai2023linear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1492--1520},
  pdf = {https://proceedings.mlr.press/v202/bai23c/bai23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linear optimal partial transport embedding},
  url = {https://proceedings.mlr.press/v202/bai23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{baker2023implicit,
  abstract = {Implicit graph neural networks (IGNNs) solve a fixed-point equilibrium equation using Picard iteration for representation learning and have shown remarkable performance in learning long-range dependencies (LRD) in the underlying graphs. However, IGNNs suffer from several issues, including 1) their expressivity is limited by their parameterizations for the well-posedness guarantee, 2) IGNNs are unstable in learning LRD, and 3) IGNNs become computationally inefficient when learning LRD. In this paper, we provide a new well-posedness characterization for IGNNs leveraging monotone operator theory, resulting in a much more expressive parameterization than the existing one. We also propose an orthogonal parameterization for IGNN based on Cayley transform to stabilize learning LRD. Furthermore, we leverage Anderson-accelerated operator splitting schemes to efficiently solve for the fixed point of the equilibrium equation of IGNN with monotone or orthogonal parameterization. We verify the computational efficiency and accuracy of the new models over existing IGNNs on various graph learning tasks at both graph and node levels.},
  author = {Justin M. Baker and Qingsong Wang and Cory D. Hauck and Bao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/baker2023implicit.pdf:pdf},
  mdate = {2024-09-11},
  pages = {1521--1548},
  pdf = {https://proceedings.mlr.press/v202/baker23a/baker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Implicit Graph Neural Networks: A Monotone Operator Viewpoint},
  url = {https://proceedings.mlr.press/v202/baker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bakshi2023tensor,
  abstract = {Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a relative of modern tensor decomposition methods for learning latent variable models.},
  author = {Ainesh Bakshi and Allen Liu and Ankur Moitra and Morris Yau},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bakshi2023tensor.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1549--1563},
  pdf = {https://proceedings.mlr.press/v202/bakshi23a/bakshi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems},
  url = {https://proceedings.mlr.press/v202/bakshi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{balabanov2023block,
  abstract = {This article introduces a novel structured random matrix composed blockwise from subsampled randomized Hadamard transforms (SRHTs). The block SRHT is expected to outperform well-known dimension reduction maps, including SRHT and Gaussian matrices on distributed architectures. We prove that a block SRHT with enough rows is an oblivious subspace embedding, i.e., an approximate isometry for an arbitrary low-dimensional subspace with high probability. Our estimate of the required number of rows is similar to that of the standard SRHT. This suggests that the two transforms should provide the same accuracy of approximation in the algorithms. The block SRHT can be readily incorporated into randomized methods for computing a low-rank approximation of a large-scale matrix, such as the Nystr{\"o}m method. For completeness, we revisit this method with a discussion of its implementation on distributed architectures.},
  author = {Oleg Balabanov and Matthias Beaupre and Laura Grigori and Victor Lederer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/balabanov2023block.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1564--1576},
  pdf = {https://proceedings.mlr.press/v202/balabanov23a/balabanov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Block Subsampled Randomized Hadamard Transform for Nystr{\"o}m Approximation on Distributed Architectures},
  url = {https://proceedings.mlr.press/v202/balabanov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ball2023efficient,
  abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories.},
  author = {Philip J. Ball and Laura M. Smith and Ilya Kostrikov and Sergey Levine},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ball2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1577--1594},
  pdf = {https://proceedings.mlr.press/v202/ball23a/ball23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Online Reinforcement Learning with Offline Data},
  url = {https://proceedings.mlr.press/v202/ball23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ballu2023mirror,
  abstract = {Optimal transport is an important tool in machine learning, allowing to capture geometric properties of the data through a linear program on transport polytopes. We present a single-loop optimization algorithm for minimizing general convex objectives on these domains, utilizing the principles of Sinkhorn matrix scaling and mirror descent. The proposed algorithm is robust to noise, and can be used in an online setting. We provide theoretical guarantees for convex objectives and experimental results showcasing it effectiveness on both synthetic and real-world data.},
  author = {Marin Ballu and Quentin Berthet},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ballu2023mirror.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1595--1613},
  pdf = {https://proceedings.mlr.press/v202/ballu23a/ballu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes},
  url = {https://proceedings.mlr.press/v202/ballu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{andra2023functional,
  abstract = {Model stitching where the internal representations of two neural networks are aligned linearly helped demonstrate that the representations of different neural networks for the same task are surprisingly similar in a functional sense. Here, we investigate the functional similarity of robust and non-robust representations for image classification with the help of model stitching. We find that robust and non-robust networks indeed have different representations. However, these representations are compatible regarding accuracy. From the point of view of robust accuracy, compatibility decreases quickly after the first few layers but the representations become compatible again in the last layers, in the sense that the properties of the front model can be recovered. Moreover, this is true even in the case of cross-task stitching. Our results suggest that stitching in the initial, preprocessing layers and the final, abstract layers test different kinds of compatibilities. In particular, the final layers are easy to match, because their representations depend mostly on the same abstract task specification, in our case, the classification of the input into n classes.},
  author = {Andr{\'a}s Balogh and M{\'a}rk Jelasity},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/andra2023functional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1614--1635},
  pdf = {https://proceedings.mlr.press/v202/balogh23a/balogh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Functional Similarity of Robust and Non-Robust Neural Representations},
  url = {https://proceedings.mlr.press/v202/balogh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{balseiro2023robust,
  abstract = {Major Internet advertising platforms offer budget pacing tools as a standard service for advertisers to manage their ad campaigns. Given the inherent non-stationarity in an advertiser's value and also competing advertisers' values over time, a commonly used approach is to learn a target expenditure plan that specifies a target spend as a function of time, and then run a controller that tracks this plan. This raises the question: how many historical samples are required to learn a good expenditure plan? We study this question by considering an advertiser repeatedly participating in $T$ second-price auctions, where the tuple of her value and the highest competing bid is drawn from an unknown time-varying distribution. The advertiser seeks to maximize her total utility subject to her budget constraint. Prior work has shown the sufficiency of $T\log T$ samples per distribution to achieve the optimal $O(\sqrt{T})$-regret. We dramatically improve this state-of-the-art and show that just one sample per distribution is enough to achieve the near-optimal $\tilde O(\sqrt{T})$-regret, while still being robust to noise in the sampling distributions.},
  author = {Santiago R. Balseiro and Rachitesh Kumar and Vahab Mirrokni and Balasubramanian Sivan and Di Wang 0005},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/balseiro2023robust.pdf:pdf},
  mdate = {2024-08-18},
  pages = {1636--1659},
  pdf = {https://proceedings.mlr.press/v202/balseiro23a/balseiro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Budget Pacing with a Single Sample},
  url = {https://proceedings.mlr.press/v202/balseiro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{banihashem2023dynamic,
  abstract = {Maximizing a monotone submodular function under cardinality constraint $k$ is a core problem in machine learning and database with many basic applications, including video and data summarization, recommendation systems, feature extraction, exemplar clustering, and coverage problems.},
  author = {Kiarash Banihashem and Leyla Biabani and Samira Goudarzi and MohammadTaghi Hajiaghayi and Peyman Jabbarzade and Morteza Monemizadeh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/banihashem2023dynamic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1660--1691},
  pdf = {https://proceedings.mlr.press/v202/banihashem23a/banihashem23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dynamic Constrained Submodular Optimization with Polylogarithmic Update Time},
  url = {https://proceedings.mlr.press/v202/banihashem23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bao2023one,
  abstract = {This paper proposes a unified diffusion framework (dubbed {UniDiffuser}) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities.},
  author = {Fan Bao and Shen Nie and Kaiwen Xue and Chongxuan Li and Shi Pu and Yaole Wang and Gang Yue and Yue Cao and Hang Su 0006 and Jun Zhu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bao2023one.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1692--1717},
  pdf = {https://proceedings.mlr.press/v202/bao23a/bao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale},
  url = {https://proceedings.mlr.press/v202/bao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bao2023optimizing,
  abstract = {In federated learning ({FL}), multiple clients collaborate to train machine learning models together while keeping their data decentralized. Through utilizing more training data, {FL} suffers from the potential negative transfer problem: the global {FL} model may even perform worse than the models trained with local data only.},
  author = {Wenxuan Bao and Haohan Wang and Jun Wu 0019 and Jingrui He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bao2023optimizing.pdf:pdf},
  mdate = {2023-11-12},
  pages = {1718--1736},
  pdf = {https://proceedings.mlr.press/v202/bao23b/bao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimizing the Collaboration Structure in Cross-Silo Federated Learning},
  url = {https://proceedings.mlr.press/v202/bao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bartal2023multidiffusion,
  abstract = {Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present {MultiDiffusion}, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that {MultiDiffusion} can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.},
  author = {Omer Bar-Tal and Lior Yariv and Yaron Lipman and Tali Dekel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bartal2023multidiffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1737--1752},
  pdf = {https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MultiDiffusion}: Fusing Diffusion Paths for Controlled Image Generation},
  url = {https://proceedings.mlr.press/v202/bar-tal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{barakat2023reinforcement,
  abstract = {We consider the reinforcement learning ({RL}) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward {RL} setting, this problem includes as particular cases constrained {RL}, pure exploration and learning from demonstrations among others.},
  author = {Anas Barakat and Ilyas Fatkhullin and Niao He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/barakat2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1753--1800},
  pdf = {https://proceedings.mlr.press/v202/barakat23a/barakat23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space},
  url = {https://proceedings.mlr.press/v202/barakat23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{barbiero2023interpretable,
  abstract = {Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. We propose the Deep Concept Reasoner ({DCR}), the first interpretable concept-based model that builds upon concept embeddings.},
  author = {Pietro Barbiero and Gabriele Ciravegna and Francesco Giannini and Mateo Espinosa Zarlenga and Lucie Charlotte Magister and Alberto Tonda and Pietro Lio and Frdric Precioso and Mateja Jamnik and Giuseppe Marra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/barbiero2023interpretable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1801--1825},
  pdf = {https://proceedings.mlr.press/v202/barbiero23a/barbiero23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Interpretable Neural-Symbolic Concept Reasoning},
  url = {https://proceedings.mlr.press/v202/barbiero23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bartan2023moccasin,
  abstract = {The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. We develop a new constraint programming formulation called {Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph.},
  author = {Burak Bartan and Haoming Li 0002 and Harris Teague and Christopher Lott and Bistra Dilkina},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bartan2023moccasin.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1826--1837},
  pdf = {https://proceedings.mlr.press/v202/bartan23a/bartan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Moccasin}: Efficient Tensor Rematerialization for Neural Networks},
  url = {https://proceedings.mlr.press/v202/bartan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bassily2023userlevel,
  abstract = {We study the problem of differentially private ({DP}) stochastic convex optimization ({SCO}) under the notion of user-level differential privacy. In this problem, there are $n$ users, each contributing $m>1$ samples to the input dataset of the private {SCO} algorithm, and the notion of indistinguishability embedded in {DP} is w.r.t. replacing the entire local dataset of any given user.},
  author = {Raef Bassily and Ziteng Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bassily2023userlevel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1838--1851},
  pdf = {https://proceedings.mlr.press/v202/bassily23a/bassily23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {User-level Private Stochastic Convex Optimization with Optimal Rates},
  url = {https://proceedings.mlr.press/v202/bassily23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bauer2023humantimescale,
  abstract = {Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning ({RL}). In this work, we demonstrate that training an {RL} agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans.},
  author = {Jakob Bauer and Kate Baumli and Feryal M. P. Behbahani and Avishkar Bhoopchand and Nathalie Bradley-Schmieg and Michael Chang and Natalie Clay and Adrian Collister and Vibhavari Dasagi and Lucy Gonzalez and Karol Gregor and Edward Hughes 0001 and Sheleem Kashem and Maria Loks-Thompson and Hannah Openshaw and Jack Parker-Holder and Shreya Pathak and Nicolas Perez Nieves and Nemanja Rakicevic and Tim Rocktschel and Yannick Schroecker and Satinder Singh 0001 and Jakub Sygnowski and Karl Tuyls and Sarah York and Alexander Zacherl and Lei M. Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bauer2023humantimescale.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1887--1935},
  pdf = {https://proceedings.mlr.press/v202/bauer23a/bauer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Human-Timescale Adaptation in an Open-Ended Task Space},
  url = {https://proceedings.mlr.press/v202/bauer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{baum2023kernel,
  abstract = {We propose a goodness-of-fit measure for probability densities modeling observations with varying dimensionality, such as text documents of differing lengths or variable-length sequences. The proposed measure is an instance of the kernel Stein discrepancy (KSD), which has been used to construct goodness-of-fit tests for unnormalized densities. The KSD is defined by its Stein operator: current operators used in testing apply to fixed-dimensional spaces. As our main contribution, we extend the KSD to the variable-dimension setting by identifying appropriate Stein operators, and propose a novel KSD goodness-of-fit test. As with the previous variants, the proposed KSD does not require the density to be normalized, allowing the evaluation of a large class of models. Our test is shown to perform well in practice on discrete sequential data benchmarks.},
  author = {Jerome Baum and Heishiro Kanagawa and Arthur Gretton},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/baum2023kernel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1936--1953},
  pdf = {https://proceedings.mlr.press/v202/baum23a/baum23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Kernel {S}stein Test of Goodness of Fit for Sequential Models},
  url = {https://proceedings.mlr.press/v202/baum23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bechavod2023individually,
  abstract = {We consider an online learning problem with one-sided feedback, in which the learner is able to observe the true label only for positively predicted instances. On each round, $k$ instances arrive and receive classification outcomes according to a randomized policy deployed by the learner, whose goal is to maximize accuracy while deploying individually fair policies. We first present a novel auditing scheme, capable of utilizing feedback from dynamically-selected panels of multiple, possibly inconsistent, auditors regarding fairness violations. In particular, we show how our proposed auditing scheme allows for algorithmically exploring the resulting accuracy-fairness frontier, with no need for additional feedback from auditors. We then present an efficient reduction from our problem of online learning with one-sided feedback and a panel reporting fairness violations to the contextual combinatorial semi-bandit problem with side observations.},
  author = {Yahav Bechavod and Aaron Roth},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bechavod2023individually.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1954--1977},
  pdf = {https://proceedings.mlr.press/v202/bechavod23a/bechavod23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Individually Fair Learning with One-Sided Feedback},
  url = {https://proceedings.mlr.press/v202/bechavod23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{so2023predicting,
  abstract = {We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.},
  author = {S{\"o}ren Becker and Michal Klein and Alexander Neitz and Giambattista Parascandolo and Niki Kilbertus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/so2023predicting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {1978--2002},
  pdf = {https://proceedings.mlr.press/v202/becker23a/becker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Predicting Ordinary Differential Equations with Transformers},
  url = {https://proceedings.mlr.press/v202/becker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{beechey2023explaining,
  abstract = {For reinforcement learning systems to be widely adopted, their users must understand and trust them. We present a theoretical analysis of explaining reinforcement learning using Shapley values, following a principled approach from game theory for identifying the contribution of individual players to the outcome of a cooperative game. We call this general framework Shapley Values for Explaining Reinforcement Learning (SVERL). Our analysis exposes the limitations of earlier uses of Shapley values in reinforcement learning. We then develop an approach that uses Shapley values to explain agent performance. In a variety of domains, SVERL produces meaningful explanations that match and supplement human intuition.},
  author = {Daniel Beechey and Thomas M. S. Smith and {\"O}zg{\"u}r {\c{S}}im{\c{s}}ek},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/beechey2023explaining.pdf:pdf},
  mdate = {2023-09-30},
  pages = {2003--2014},
  pdf = {https://proceedings.mlr.press/v202/beechey23a/beechey23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Explaining Reinforcement Learning with {S}hapley Values},
  url = {https://proceedings.mlr.press/v202/beechey23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zanellabe2023b,
  abstract = {Algorithms such as Differentially Private SGD enable training machine learning models with formal privacy guarantees. However, because these guarantees hold with respect to unrealistic adversaries, the protection afforded against practical attacks is typically much better. An emerging strand of work empirically estimates the protection afforded by differentially private training as a confidence interval for the privacy budget $\hat{\varepsilon}$ spent with respect to specific threat models. Existing approaches derive confidence intervals for $\hat{\varepsilon}$ from confidence intervals for false positive and false negative rates of membership inference attacks, which requires training an impractically large number of models to get intervals that are actionable. The paper proposes a novel, more efficient Bayesian approach that brings privacy estimates within the reach of practitioners.},
  author = {Santiago Zanella-B{\'e}guelin and Lukas Wutschitz and Shruti Tople and Ahmed Salem and Victor R{\"u}hle and Andrew Paverd and Mohammad Naseri and Boris K{\"o}pf and Daniel Jones},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zanellabe2023b.pdf:pdf},
  mdate = {2025-03-11},
  pages = {40624--40636},
  pdf = {https://proceedings.mlr.press/v202/zanella-beguelin23a/zanella-beguelin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{B}ayesian Estimation of Differential Privacy},
  url = {https://proceedings.mlr.press/v202/zanella-beguelin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{behmanesh2023tide,
  abstract = {A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to over smoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches.},
  author = {Maysam Behmanesh and Maximilian Krahn and Maks Ovsjanikov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/behmanesh2023tide.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2015--2030},
  pdf = {https://proceedings.mlr.press/v202/behmanesh23a/behmanesh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TIDE}: Time Derivative Diffusion for Deep Learning on Graphs},
  url = {https://proceedings.mlr.press/v202/behmanesh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{benbaki2023fast,
  abstract = {The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning.},
  author = {Riade Benbaki and Wenyu Chen and Xiang Meng and Hussein Hazimeh and Natalia Ponomareva and Zhe Zhao and Rahul Mazumder},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/benbaki2023fast.pdf:pdf},
  mdate = {2025-05-19},
  pages = {2031--2049},
  pdf = {https://proceedings.mlr.press/v202/benbaki23a/benbaki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast as {CHITA}: Neural Network Pruning with Combinatorial Optimization},
  url = {https://proceedings.mlr.press/v202/benbaki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bender2023continuously,
  abstract = {Mixture models are universal approximators of smooth densities but are difficult to utilize in complicated datasets due to restrictions on typically available modes and challenges with initialiations. We show that by continuously parameterizing a mixture of factor analyzers using a learned ordinary differential equation, we can improve the fit of mixture models over direct methods. Once trained, the mixture components can be extracted and the neural ODE can be discarded, leaving us with an effective, but low-resource model. We additionally explore the use of a training curriculum from an easy-to-model latent space extracted from a normalizing flow to the more complex input space and show that the smooth curriculum helps to stabilize and improve results with and without the continuous parameterization.},
  author = {Christopher M. Bender and Yifeng Shi and Marc Niethammer and Junier Oliva},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bender2023continuously.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2050--2062},
  pdf = {https://proceedings.mlr.press/v202/bender23a/bender23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continuously Parameterized Mixture Models},
  url = {https://proceedings.mlr.press/v202/bender23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bendinelli2023controllable,
  abstract = {In symbolic regression, the objective is to find an analytical expression that accurately fits experimental data with the minimal use of mathematical symbols such as operators, variables, and constants. However, the combinatorial space of possible expressions can make it challenging for traditional evolutionary algorithms to find the correct expression in a reasonable amount of time. To address this issue, Neural Symbolic Regression (NSR) algorithms have been developed that can quickly identify patterns in the data and generate analytical expressions. However, these methods, in their current form, lack the capability to incorporate user-defined prior knowledge, which is often required in natural sciences and engineering fields. To overcome this limitation, we propose a novel neural symbolic regression method, named Neural Symbolic Regression with Hypothesis (NSRwH) that enables the explicit incorporation of user-defined prior knowledge into the symbolic regression process.},
  author = {Tommaso Bendinelli and Luca Biggio and Pierre-Alexandre Kamienny},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bendinelli2023controllable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2063--2077},
  pdf = {https://proceedings.mlr.press/v202/bendinelli23a/bendinelli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controllable Neural Symbolic Regression},
  url = {https://proceedings.mlr.press/v202/bendinelli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bengs2023secondorder,
  abstract = {It is well known that accurate probabilistic predictors can be trained through empirical risk minimisation with proper scoring rules as loss functions. While such learners capture so-called aleatoric uncertainty of predictions, various machine learning methods have recently been developed with the goal to let the learner also represent its epistemic uncertainty, i.e., the uncertainty caused by a lack of knowledge and data. An emerging branch of the literature proposes the use of a second-order learner that provides predictions in terms of distributions on probability distributions. However, recent work has revealed serious theoretical shortcomings for second-order predictors based on loss minimisation. In this paper, we generalise these findings and prove a more fundamental result: There seems to be no loss function that provides an incentive for a second-order learner to faithfully represent its epistemic uncertainty in the same manner as proper scoring rules do for standard (first-order) learners.},
  author = {Viktor Bengs and Eyke H{\"u}llermeier and Willem Waegeman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bengs2023secondorder.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2078--2091},
  pdf = {https://proceedings.mlr.press/v202/bengs23a/bengs23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Second-Order Scoring Rules for Epistemic Uncertainty Quantification},
  url = {https://proceedings.mlr.press/v202/bengs23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bennouna2023certified,
  abstract = {Recent work have demonstrated that robustness (to "corruption") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar "robust overfitting" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption—data evasion and poisoning attacks—while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversarial training and comes with a negligible additional computational burden.},
  author = {M. Amine Bennouna and Ryan Lucas and Bart P. G. Van Parys},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bennouna2023certified.pdf:pdf},
  mdate = {2023-09-30},
  pages = {2092--2112},
  pdf = {https://proceedings.mlr.press/v202/bennouna23a/bennouna23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Certified Robust Neural Networks: Generalization and Corruption Resistance},
  url = {https://proceedings.mlr.press/v202/bennouna23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{berlinghieri2023gaussian,
  abstract = {Oceanographers are interested in predicting ocean currents and identifying divergences in a current vector field based on sparse observations of buoy velocities. Since we expect current dynamics to be smooth but highly non-linear, Gaussian processes (GPs) offer an attractive model. But we show that applying a GP with a standard stationary kernel directly to buoy data can struggle at both current prediction and divergence identification – due to some physically unrealistic prior assumptions. To better reflect known physical properties of currents, we propose to instead put a standard stationary kernel on the divergence and curl-free components of a vector field obtained through a Helmholtz decomposition. We show that, because this decomposition relates to the original vector field just via mixed partial derivatives, we can still perform inference given the original data with only a small constant multiple of additional computational expense. We illustrate the benefits of our method on synthetic and real oceans data.},
  author = {Renato Berlinghieri and Brian L. Trippe and David R. Burt and Ryan James Giordano and Kaushik Srinivasan and Tamay M. Özgökmen and Junfei Xia and Tamara Broderick},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/berlinghieri2023gaussian.pdf:pdf},
  mdate = {2024-10-06},
  pages = {2113--2163},
  pdf = {https://proceedings.mlr.press/v202/berlinghieri23a/berlinghieri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gaussian processes at the {H}elm(holtz): A more fluid model for ocean currents},
  url = {https://proceedings.mlr.press/v202/berlinghieri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bernasconi2023optimal,
  abstract = {Bayesian persuasion studies how an informed sender should influence beliefs of rational receivers that take decisions through Bayesian updating of a common prior. This paper focuses on the online Bayesian persuasion framework, in which the sender repeatedly faces one or more receivers with unknown and adversarially selected types. The paper presents three main contributions: (1) We show how to obtain a tight $\tilde O(T^{1/2})$ regret bound in the case where the sender faces a single receiver and has bandit feedback, improving over the best previously known bound of $\tilde O(T^{4/5})$. (2) We provide the first no-regret guarantees for the multi-receiver setting under bandit feedback. (3) We show how to design no-regret algorithms with polynomial per-iteration running time by exploiting type reporting, thereby circumventing known complexity results on online Bayesian persuasion. We provide efficient algorithms guaranteeing a $O(T^{1/2})$ regret upper bound both in the single- and multi-receiver scenario when type reporting is allowed.},
  author = {Martino Bernasconi and Matteo Castiglioni and Andrea Celli and Alberto Marchesi and Francesco Trovò and Nicola Gatti},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bernasconi2023optimal.pdf:pdf},
  mdate = {2024-03-20},
  pages = {2164--2183},
  pdf = {https://proceedings.mlr.press/v202/bernasconi23a/bernasconi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Rates and Efficient Algorithms for Online {B}ayesian Persuasion},
  url = {https://proceedings.mlr.press/v202/bernasconi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bernasconi2023constrained,
  abstract = {The computational study of equilibria involving constraints on players' strategies has been largely neglected. However, in real-world applications, players are usually subject to constraints ruling out the feasibility of some of their strategies, such as, e.g., safety requirements and budget caps. Computational studies on constrained versions of the Nash equilibrium have lead to some results under very stringent assumptions, while finding constrained versions of the correlated equilibrium (CE) is still unexplored. In this paper, we introduce and computationally characterize constrained Phi-equilibria—a more general notion than constrained CEs—in normal-form games. We show that computing such equilibria is in general computationally intractable, and also that the set of the equilibria may not be convex, providing a sharp divide with unconstrained CEs. Nevertheless, we provide a polynomial-time algorithm for computing a constrained (approximate) Phi-equilibrium maximizing a given linear function, when either the number of constraints or that of players' actions is fixed. Moreover, in the special case in which a player's constraints do not depend on other players' strategies, we show that an exact, function-maximizing equilibrium can be computed in polynomial time, while one (approximate) equilibrium can be found with an efficient decentralized no-regret learning algorithm.},
  author = {Martino Bernasconi and Matteo Castiglioni and Alberto Marchesi and Francesco Trovò and Nicola Gatti},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bernasconi2023constrained.pdf:pdf},
  mdate = {2024-03-20},
  pages = {2184--2205},
  pdf = {https://proceedings.mlr.press/v202/bernasconi23b/bernasconi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained {P}hi-Equilibria},
  url = {https://proceedings.mlr.press/v202/bernasconi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{berrevoets2023differentiable,
  abstract = {Directed acyclic graphs (DAGs) encode a lot of information about a particular distribution in their structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique—named NOTEARS—is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. We introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function while remaining fully differentiable.},
  author = {Jeroen Berrevoets and Nabeel Seedat and Fergus Imrie and Mihaela van der Schaar},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/berrevoets2023differentiable.pdf:pdf},
  mdate = {2024-10-06},
  pages = {2206--2233},
  pdf = {https://proceedings.mlr.press/v202/berrevoets23a/berrevoets23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentiable and Transportable Structure Learning},
  url = {https://proceedings.mlr.press/v202/berrevoets23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{berzins2023polyhedral,
  abstract = {A neural network consisting of piecewise affine building blocks, such as fully-connected layers and ReLU activations, is itself a piecewise affine function supported on a polyhedral complex. This complex has been previously studied to characterize theoretical properties of neural networks, but, in practice, extracting it remains a challenge due to its high combinatorial complexity. A natural idea described in previous works is to subdivide the regions via intersections with hyperplanes induced by each neuron. However, we argue that this view leads to computational redundancy. Instead of regions, we propose to subdivide edges, leading to a novel method for polyhedral complex extraction. A key to this are sign-vectors, which encode the combinatorial structure of the complex. Our approach allows to use standard tensor operations on a GPU, taking seconds for millions of cells on a consumer grade machine. Motivated by the growing interest in neural shape representation, we use the speed and differentiability of our method to optimize geometric properties of the complex.},
  author = {Arturs Berzins},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/berzins2023polyhedral.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2234--2244},
  pdf = {https://proceedings.mlr.press/v202/berzins23a/berzins23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Polyhedral Complex Extraction from {R}e{LU} Networks using Edge Subdivision},
  url = {https://proceedings.mlr.press/v202/berzins23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bthune2023robust,
  abstract = {The authors propose a new method, dubbed One Class Signed Distance Function (OCSDF), to perform One Class Classification (OCC) by provably learning the Signed Distance Function (SDF) to the boundary of the support of any distribution. The distance to the support can be interpreted as a normality score, and its approximation using 1-Lipschitz neural networks provides robustness bounds against l2 adversarial attacks, an under-explored weakness of deep learning-based OCC algorithms. As a result, OCSDF comes with a new metric, certified AUROC, that can be computed at the same cost as any classical AUROC. The authors show that OCSDF is competitive against concurrent methods on tabular and image data while being way more robust to adversarial attacks, illustrating its theoretical properties. Finally, as exploratory research perspectives, they theoretically and empirically show how OCSDF connects OCC with image generation and implicit neural surface parametrization.},
  author = {Louis Béthune and Paul Novello and Guillaume Coiffier and Thibaut Boissin and Mathieu Serrurier and Quentin Vincenot and Andres Troya-Galvis},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bthune2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2245--2271},
  pdf = {https://proceedings.mlr.press/v202/bethune23a/bethune23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust One-Class Classification with Signed Distance Function using 1-{L}ipschitz Neural Networks},
  url = {https://proceedings.mlr.press/v202/bethune23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bevilacqua2023neural,
  abstract = {Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. The authors ensure invariance in the next-step prediction across such inputs by employing a self-supervised objective derived by their observation, formalized in a causal graph. They prove that the resulting method, which they call Hint-ReLIC, improves the OOD generalization capabilities of the reasoner. They evaluate their method on the CLRS algorithmic reasoning benchmark, where they show up to 3× improvements on the OOD test data.},
  author = {Beatrice Bevilacqua and Kyriacos Nikiforou and Borja Ibarz and Ioana Bica and Michela Paganini and Charles Blundell and Jovana Mitrovic and Petar Veličković},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bevilacqua2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2272--2288},
  pdf = {https://proceedings.mlr.press/v202/bevilacqua23a/bevilacqua23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Algorithmic Reasoning with Causal Regularisation},
  url = {https://proceedings.mlr.press/v202/bevilacqua23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bharti2023optimallyweighted,
  abstract = {Likelihood-free inference methods typically make use of a distance between simulated and real data. A common example is the maximum mean discrepancy (MMD), which has previously been used for approximate Bayesian computation, minimum distance estimation, generalised Bayesian inference, and within the nonparametric learning framework. The MMD is commonly estimated at a root-m rate, where m is the number of simulated samples. This can lead to significant computational challenges since a large m is required to obtain an accurate estimate, which is crucial for parameter estimation. In this paper, we propose a novel estimator for the MMD with significantly improved sample complexity. The estimator is particularly well suited for computationally expensive smooth simulators with low- to mid-dimensional inputs.},
  author = {Ayush Bharti and Masha Naslidnyk and Oscar Key and Samuel Kaski and François-Xavier Briol},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bharti2023optimallyweighted.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2289--2312},
  pdf = {https://proceedings.mlr.press/v202/bharti23a/bharti23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimally-weighted Estimators of the Maximum Mean Discrepancy for Likelihood-Free Inference},
  url = {https://proceedings.mlr.press/v202/bharti23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bhaskara2023bandit,
  abstract = {We study variants of the online linear optimization (OLO) problem with bandit feedback, where the algorithm has access to external information about the unknown cost vector. Our motivation is the recent body of work on using such "hints" towards improving regret bounds for OLO problems in the full-information setting. Unlike in the full-information OLO setting, with bandit feedback, we first show that one cannot improve the standard regret bounds of $\tilde{O}(\sqrt{T})$ by using hints, even if they are always well-correlated with the cost vector. In contrast, if the algorithm is empowered to issue queries and if all the responses are correct, then we show $O(\log T)$ regret is achievable. We then show how to make this result more robust—when some of the query responses can be adversarial—by using a little feedback on the quality of the responses.},
  author = {Aditya Bhaskara and Ashok Cutkosky and Ravi Kumar and Manish Purohit},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bhaskara2023bandit.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2313--2336},
  pdf = {https://proceedings.mlr.press/v202/bhaskara23a/bhaskara23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bandit Online Linear Optimization with Hints and Queries},
  url = {https://proceedings.mlr.press/v202/bhaskara23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bhatnagar2023improved,
  abstract = {We develop online conformal prediction methods that minimize the strongly adaptive regret to provide performance guarantees across different time intervals. The methods achieve near-optimal strongly adaptive regret and demonstrate improved performance in tasks like time series forecasting and image classification under distribution shift. Our approach focuses on uncertainty quantification in online settings with changing data distributions.},
  author = {Aadyot Bhatnagar and Huan Wang and Caiming Xiong and Yu Bai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bhatnagar2023improved.pdf:pdf},
  mdate = {2024-08-14},
  pages = {2337--2363},
  pdf = {https://proceedings.mlr.press/v202/bhatnagar23a/bhatnagar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Online Conformal Prediction via Strongly Adaptive Online Learning},
  url = {https://proceedings.mlr.press/v202/bhatnagar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bhattacharjee2023datacopying,
  abstract = {There has been recent interest in detecting and addressing memorization of training data by deep neural networks. A formal framework for memorization in generative models, called 'data-copying' was proposed by Meehan et al. (2020). This paper develops and analyzes this formal framework for understanding when generative models copy their training data, providing theoretical foundations for detecting and preventing such memorization behavior.},
  author = {Robi Bhattacharjee and Sanjoy Dasgupta and Kamalika Chaudhuri},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bhattacharjee2023datacopying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2364--2396},
  pdf = {https://proceedings.mlr.press/v202/bhattacharjee23a/bhattacharjee23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data-Copying in Generative Models: A Formal Framework},
  url = {https://proceedings.mlr.press/v202/bhattacharjee23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{biderman2023pythia,
  abstract = {Pythia is a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. The goal is to study how large language models develop and evolve during training across different model scales. The paper provides case studies on memorization, term frequency effects on few-shot performance, and reducing gender bias, offering insights into LLM training dynamics.},
  author = {Stella Biderman and Hailey Schoelkopf and Quentin Gregory Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/biderman2023pythia.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2397--2430},
  pdf = {https://proceedings.mlr.press/v202/biderman23a/biderman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  url = {https://proceedings.mlr.press/v202/biderman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bihani2023stridernet,
  abstract = {StriderNET presents a graph reinforcement learning approach that learns a policy to displace atoms towards low energy configurations on highly rough and non-convex energy landscapes. The method was evaluated on binary Lennard-Jones particles, calcium silicate hydrates gel, and disordered silicon, outperforming classical optimization algorithms and enabling discovery of lower energy minima. StriderNET exhibits inductivity to unseen system sizes an order of magnitude different from training systems.},
  author = {Vaibhav Bihani and Sahil Manchanda and Srikanth Sastry and Sayan Ranu and N. M. Anoop Krishnan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bihani2023stridernet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2431--2451},
  pdf = {https://proceedings.mlr.press/v202/bihani23a/bihani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {StriderNet: A Graph Reinforcement Learning Approach to Optimize Atomic Structures on Rough Energy Landscapes},
  url = {https://proceedings.mlr.press/v202/bihani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bilos2023modeling,
  abstract = {We propose a solution for modeling stochastic processes that govern temporal data by defining denoising diffusion models in function space, naturally handling irregularly-sampled observations. The forward process gradually adds noise to functions while preserving continuity, and the learned reverse process removes noise to return new function samples. We show applications to multivariate probabilistic forecasting and imputation, demonstrating how the model can be interpreted as a neural process.},
  author = {Marin Bilos and Kashif Rasul and Anderson Schneider and Yuriy Nevmyvaka and Stephan G\"{u}nnemann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bilos2023modeling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2452--2470},
  pdf = {https://proceedings.mlr.press/v202/bilos23a/bilos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Modeling Temporal Data as Continuous Functions with Stochastic Process Diffusion},
  url = {https://proceedings.mlr.press/v202/bilos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bitterwolf2023out,
  abstract = {Out-of-distribution (OOD) detection identifies inputs unrelated to the in-distribution task. Most current test OOD datasets have severe issues: in some cases more than 50\% contain objects belonging to ImageNet-1K classes, heavily distorting evaluation. As a solution, we introduce the NINCO (No ImageNet Class Objects) dataset with 64 OOD classes and 5879 samples, selected to have no categorical overlap with ImageNet-1K classes, making it suitable for evaluating OOD detection on ImageNet-1K.},
  author = {Julian Bitterwolf and Maximilian M\"{u}ller and Matthias Hein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bitterwolf2023out.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2471--2506},
  pdf = {https://proceedings.mlr.press/v202/bitterwolf23a/bitterwolf23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation},
  url = {https://proceedings.mlr.press/v202/bitterwolf23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{biza2023invariant,
  abstract = {We present a method for incorporating spatial symmetries via slot-centric reference frames into Slot Attention by translating, scaling, and rotating position encodings. This addresses the challenge of capturing spatial symmetries in the visual world, leading to improved sample efficiency and better object discovery. We evaluate on synthetic benchmarks including CLEVR, Tetrominoes, CLEVRTex, Objects Room, MultiShapeNet, and show promising improvements on the real-world Waymo Open dataset.},
  author = {Ondrej Biza and Sjoerd van Steenkiste and Mehdi S. M. Sajjadi and Gamaleldin Fathy Elsayed and Aravindh Mahendran and Thomas Kipf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/biza2023invariant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2507--2527},
  pdf = {https://proceedings.mlr.press/v202/biza23a/biza23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames},
  url = {https://proceedings.mlr.press/v202/biza23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{black2023understanding,
  abstract = {We analyze oversquashing in message passing graph neural networks through effective resistance between nodes. Effective resistance captures the strength of connection between nodes by paths in the graph. We propose using total effective resistance as a bound for total oversquashing and provide theoretical justification. We develop algorithms to identify edges to add to minimize total effective resistance, thereby alleviating oversquashing, and provide empirical evidence of effectiveness.},
  author = {Mitchell Black and Zhengchao Wan and Amir Nayyeri and Yusu Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/black2023understanding.pdf:pdf},
  mdate = {2025-04-22},
  pages = {2528--2547},
  pdf = {https://proceedings.mlr.press/v202/black23a/black23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Oversquashing in GNNs through the Lens of Effective Resistance},
  url = {https://proceedings.mlr.press/v202/black23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{blake2023unit,
  abstract = {Unit scaling is a paradigm for designing deep learning models that simplifies the use of low-precision number formats. Training in FP16 or FP8 offers substantial efficiency gains but can lack sufficient range for out-of-the-box training. Unit scaling addresses this by introducing a principled approach to model numerics: seeking unit variance of all weights, activations and gradients at initialization, enabling effective low-precision training without manual tuning.},
  author = {Charlie Blake and Douglas Orr and Carlo Luschi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/blake2023unit.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2548--2576},
  pdf = {https://proceedings.mlr.press/v202/blake23a/blake23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unit Scaling: Out-of-the-Box Low-Precision Training},
  url = {https://proceedings.mlr.press/v202/blake23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{blanke2023flex,
  abstract = {FLEX is an exploration algorithm for nonlinear dynamics based on optimal experimental design. The policy maximizes information of the next step, resulting in an adaptive exploration algorithm compatible with arbitrary parametric learning models and requiring minimal computing resources. We test the method on nonlinear environments covering different settings including time-varying dynamics, demonstrating improved sample efficiency in model-based reinforcement learning.},
  author = {Matthieu Blanke and Marc Lelarge},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/blanke2023flex.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2577--2591},
  pdf = {https://proceedings.mlr.press/v202/blanke23a/blanke23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FLEX: an Adaptive Exploration Algorithm for Nonlinear Systems},
  url = {https://proceedings.mlr.press/v202/blanke23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bla2023not,
  abstract = {Probabilistic modeling is a central task in machine learning. Probabilistic models should be tractable, i.e., allowing tractable probabilistic inference, but also efficient, i.e., being able to represent a large set of probability distributions. Zhang et al. (ICML 2021) recently proposed a new model, probabilistic generating circuits. They raised the question whether every strongly Rayleigh distribution can be efficiently represented by such circuits. We prove that this question has a negative answer, there are strongly Rayleigh distributions that cannot be represented by polynomial-sized probabilistic generating circuits, assuming a widely accepted complexity theoretic conjecture.},
  author = {Markus Bl{\"a}ser},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/bla2023not.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2592-2602},
  pdf = {https://proceedings.mlr.press/v202/blaser23a/blaser23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Not all Strongly Rayleigh Distributions Have Small Probabilistic Generating Circuits},
  url = {https://proceedings.mlr.press/v202/blaser23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bleistein2023learning,
  abstract = {We address the problem of learning the dynamics of an unknown non-parametric system linking a target and a feature time series. The feature time series is measured on a sparse and irregular grid, while we have access to only a few points of the target time series.},
  author = {Linus Bleistein and Adeline Fermanian and Anne-Sophie Jannot and Agathe Guilloux},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/bleistein2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2603-2640},
  pdf = {https://proceedings.mlr.press/v202/bleistein23a/bleistein23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning the Dynamics of Sparsely Observed Interacting Systems},
  url = {https://proceedings.mlr.press/v202/bleistein23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boehmer2023subset,
  abstract = {We consider the problem of subset selection where one is given multiple rankings of items and the goal is to select the highest "quality" subset. Score functions from the multiwinner voting literature have been used to aggregate rankings into quality scores for subsets. We study this setting of subset selection problems when, in addition, rankings may contain systemic or unconscious biases toward a group of items. For a general model of input rankings and biases, we show that requiring the selected subset to satisfy group fairness constraints can improve the quality of the selection with respect to unbiased rankings. Importantly, we show that for fairness constraints to be effective, different multiwinner score functions may require a drastically different number of rankings: While for some functions, fairness constraints need an exponential number of rankings to recover a close-to-optimal solution, for others, this dependency is only polynomial.},
  author = {Niclas Boehmer and L. Elisa Celis and Lingxiao Huang and Anay Mehrotra and Nisheeth K. Vishnoi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/boehmer2023subset.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2641-2688},
  pdf = {https://proceedings.mlr.press/v202/boehmer23a/boehmer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions},
  url = {https://proceedings.mlr.press/v202/boehmer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boehmer2023properties,
  abstract = {The Mallows model is a popular distribution for ranked data. We empirically and theoretically analyze how the properties of rankings sampled from the Mallows model change when increasing the number of alternatives. We find that real-world data behaves differently from the Mallows model, yet is in line with its recent variant proposed by Boehmer et al. [IJCAI '21]. As part of our study, we issue several warnings about using the classic Mallows model. For instance, we find that one should be extremely careful when using the Mallows model to generate data for experiments with a varying number of alternatives, as observed trends in such experiments might be due to the changing nature of the generated data.},
  author = {Niclas Boehmer and Piotr Faliszewski and Sonja Kraiczy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/boehmer2023properties.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2689-2711},
  pdf = {https://proceedings.mlr.press/v202/boehmer23b/boehmer23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Properties of the Mallows Model Depending on the Number of Alternatives: A Warning for an Experimentalist},
  url = {https://proceedings.mlr.press/v202/boehmer23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boetius2023robust,
  abstract = {Counterexample-guided repair aims at creating neural networks with mathematical safety guarantees, facilitating the application of neural networks in safety-critical domains. However, whether counterexample-guided repair is guaranteed to terminate remains an open question. We approach this question by showing that counterexample-guided repair can be viewed as a robust optimisation algorithm. While termination guarantees for neural network repair itself remain beyond our reach, we prove termination for more restrained machine learning models and disprove termination in a general setting. We empirically study the practical implications of our theoretical results, demonstrating the suitability of common verifiers and falsifiers for repair despite a disadvantageous theoretical result. Additionally, we use our theoretical insights to devise a novel algorithm for repairing linear regression models based on quadratic programming, surpassing existing approaches.},
  author = {David Boetius and Stefan Leue and Tobias Sutter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/boetius2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2712-2737},
  pdf = {https://proceedings.mlr.press/v202/boetius23a/boetius23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Robust Optimisation Perspective on Counterexample-Guided Repair of Neural Networks},
  url = {https://proceedings.mlr.press/v202/boetius23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bombari2023beyond,
  abstract = {Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this universal law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel (NTK). For random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the NTK model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. Our analysis decouples the effect of the kernel of the model from an interaction matrix, which describes the interaction with the test data and captures the effect of the activation. Our theoretical results are corroborated by numerical evidence on both synthetic and standard datasets.},
  author = {Simone Bombari and Shayan Kiyani and Marco Mondelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/bombari2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2738-2776},
  pdf = {https://proceedings.mlr.press/v202/bombari23a/bombari23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels},
  url = {https://proceedings.mlr.press/v202/bombari23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cle2023slicedwasserstein,
  abstract = {When dealing with electro or magnetoencephalography records, many supervised prediction tasks are solved by working with covariance matrices to summarize the signals. Learning with these matrices requires using Riemanian geometry to account for their structure. In this paper, we propose a new method to deal with distributions of covariance matrices and demonstrate its computational efficiency on M/EEG multivariate time series. More specifically, we define a Sliced-Wasserstein distance between measures of symmetric positive definite matrices that comes with strong theoretical guarantees. Then, we take advantage of its properties and kernel methods to apply this distance to brain-age prediction from MEG data and compare it to state-of-the-art algorithms based on Riemannian geometry. Finally, we show that it is an efficient surrogate to the Wasserstein distance in domain adaptation for Brain Computer Interface applications.},
  author = {Cl{\'e}ment Bonet and Beno{\^i}t Mal{\'e}zieux and Alain Rakotomamonjy and Lucas Drumetz and Thomas Moreau and Matthieu Kowalski and Nicolas Courty},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/cle2023slicedwasserstein.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2777-2805},
  pdf = {https://proceedings.mlr.press/v202/bonet23a/bonet23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sliced-Wasserstein on Symmetric Positive Definite Matrices for {M/EEG} Signals},
  url = {https://proceedings.mlr.press/v202/bonet23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bonev2023spherical,
  abstract = {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates since they incorrectly assume a flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries.},
  author = {Boris Bonev and Thorsten Kurth and Christian Hundt and Jaideep Pathak and Maximilian Baust and Karthik Kashinath and Anima Anandkumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/bonev2023spherical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2806-2823},
  pdf = {https://proceedings.mlr.press/v202/bonev23a/bonev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere},
  url = {https://proceedings.mlr.press/v202/bonev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boone2023regret,
  abstract = {The first contribution of this paper is the introduction of a new performance measure of a RL algorithm that is more discriminating than the regret, that we call the regret of exploration that measures the asymptotic cost of exploration. The second contribution is a new performance test (PT) to end episodes in RL optimistic algorithms. This test is based on the performance of the current policy with respect to the best policy over the current confidence set. This is in contrast with all existing RL algorithms whose episode lengths are only based on the number of visits to the states. This modification does not harm the regret and brings an additional property. We show that while all current episodic RL algorithms have a linear regret of exploration, our method has a O(log T) regret of exploration for non-degenerate deterministic MDPs.},
  author = {Victor Boone and Bruno Gaujal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/boone2023regret.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2824-2856},
  pdf = {https://proceedings.mlr.press/v202/boone23a/boone23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Regret of Exploration and the Control of Bad Episodes in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/boone23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boopathy2023modelagnostic,
  abstract = {The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. We evaluate this measure on various image classification datasets and find that it accurately predicts the relative difficulty of tasks for different machine learning models, even those with varying architectures and training procedures.},
  author = {Akhilan Boopathy and Kevin Liu and Jaedong Hwang and Shu Ge and Asaad Mohammedsaleh and Ila Fiete},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/boopathy2023modelagnostic.pdf:pdf},
  mdate = {2023-09-06},
  pages = {2857-2884},
  pdf = {https://proceedings.mlr.press/v202/boopathy23a/boopathy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-agnostic Measure of Generalization Difficulty},
  url = {https://proceedings.mlr.press/v202/boopathy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bouabid2023returning,
  abstract = {A directed acyclic graph (DAG) provides valuable prior knowledge that is often discarded in regression tasks in machine learning. We show that the independences arising from the presence of collider structures in DAGs provide meaningful inductive biases, which constrain the regression hypothesis space and improve predictive performance. We introduce collider regression, a framework to incorporate probabilistic causal knowledge from a collider in a regression problem. When the hypothesis space is a reproducing kernel Hilbert space, we prove a strictly positive generalisation benefit under mild assumptions and provide closed-form estimators of the empirical risk minimiser. Experiments on synthetic and climate model data demonstrate performance gains of the proposed methodology.},
  author = {Shahine Bouabid and Jake Fawkes and Dino Sejdinovic},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bouabid2023returning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2885--2913},
  pdf = {https://proceedings.mlr.press/v202/bouabid23a/bouabid23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Returning The Favour: When Regression Benefits From Probabilistic Causal Knowledge},
  url = {https://proceedings.mlr.press/v202/bouabid23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boudiaf2023search,
  abstract = {Source-free domain adaptation (SFDA) is compelling because it allows adapting an off-the-shelf model to a new domain using only unlabelled data. In this work, we apply existing SFDA techniques to a challenging set of naturally-occurring distribution shifts in bioacoustics, which are very different from the ones commonly studied in computer vision. We find existing methods perform differently relative to each other than observed in vision benchmarks, and sometimes perform worse than no adaptation at all. We propose a new simple method which outperforms the existing methods on our new shifts while exhibiting strong performance on a range of vision datasets. Our findings suggest that existing SFDA methods are not as generalizable as previously thought and that considering diverse modalities can be a useful avenue for designing more robust models.},
  author = {Malik Boudiaf and Tom Denton and Bart van Merrienboer and Vincent Dumoulin and Eleni Triantafillou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/boudiaf2023search.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2914--2931},
  pdf = {https://proceedings.mlr.press/v202/boudiaf23a/boudiaf23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {In Search for a Generalizable Method for Source Free Domain Adaptation},
  url = {https://proceedings.mlr.press/v202/boudiaf23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bouland2023quantum,
  abstract = {We give a quantum algorithm for computing an ε-approximate Nash equilibrium of a zero-sum game in a m × n payoff matrix with bounded entries. Given a standard quantum oracle for accessing the payoff matrix our algorithm runs in time Õ(√(m + n)·ε^(-2.5) + ε^(-3)) and outputs a classical representation of the ε-approximate Nash equilibrium. This improves upon the best prior quantum runtime of Õ(√(m + n) · ε^(-3)) obtained by [vAG19] and the classic Õ((m + n) · ε^(-2)) runtime due to [GK95] whenever ε = Ω((m +n)^(-1)). We obtain this result by designing new quantum data structures for efficiently sampling from a slowly-changing Gibbs distribution.},
  author = {Adam Bouland and Yosheb M. Getachew and Yujia Jin and Aaron Sidford and Kevin Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bouland2023quantum.pdf:pdf},
  mdate = {2023-08-28},
  pages = {2932--2952},
  pdf = {https://proceedings.mlr.press/v202/bouland23a/bouland23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum Speedups for Zero-Sum Games via Improved Dynamic Gibbs Sampling},
  url = {https://proceedings.mlr.press/v202/bouland23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{boutin2023diffusion,
  abstract = {An important milestone for AI is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the 'diversity vs. recognizability' scoring framework from Boutin et al (2022) and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines remains -- in part explainable by discrepancies in visual strategies.},
  author = {Victor Boutin and Thomas Fel and Lakshya Singhal and Rishav Mukherji and Akash Nagaraj and Julien Colin and Thomas Serre},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/boutin2023diffusion.pdf:pdf},
  mdate = {2024-10-06},
  pages = {2953--3002},
  pdf = {https://proceedings.mlr.press/v202/boutin23a/boutin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?},
  url = {https://proceedings.mlr.press/v202/boutin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bowling2023settling,
  abstract = {The reward hypothesis posits that, "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)." We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.},
  author = {Michael Bowling and John D. Martin and David Abel and Will Dabney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bowling2023settling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3003--3020},
  pdf = {https://proceedings.mlr.press/v202/bowling23a/bowling23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Settling the Reward Hypothesis},
  url = {https://proceedings.mlr.press/v202/bowling23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{brack2023illume,
  abstract = {Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.},
  author = {Manuel Brack and Patrick Schramowski and Björn Deiseroth and Kristian Kersting},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/brack2023illume.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3021--3037},
  pdf = {https://proceedings.mlr.press/v202/brack23a/brack23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ILLUME: Rationalizing Vision-Language Models through Human Interactions},
  url = {https://proceedings.mlr.press/v202/brack23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{brady2023provably,
  abstract = {Learning structured representations of the visual world in terms of objects promises to significantly improve the generalization abilities of current machine learning models. While recent efforts to this end have shown promising empirical progress, a theoretical account of when unsupervised object-centric representation learning is possible is still lacking. Consequently, understanding the reasons for the success of existing object-centric methods as well as designing new theoretically grounded methods remains challenging. In the present work, we analyze when object-centric representations can provably be learned without supervision. To this end, we first introduce two assumptions on the generative process for scenes comprised of several objects, which we call compositionality and irreducibility. Under this generative process, we prove that the ground-truth object representations can be identified by an invertible and compositional inference model, even in the presence of dependencies between objects. We empirically validate our results through experiments on synthetic data. Finally, we provide evidence that our theory holds predictive power for existing object-centric models by showing a close correspondence between models' compositionality and invertibility and their empirical identifiability.},
  author = {Jack Brady and Roland S. Zimmermann and Yash Sharma and Bernhard Schölkopf and Julius von Kügelgen and Wieland Brendel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/brady2023provably.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3038--3062},
  pdf = {https://proceedings.mlr.press/v202/brady23a/brady23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Learning Object-Centric Representations},
  url = {https://proceedings.mlr.press/v202/brady23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hermsdorff2023quantifying,
  abstract = {We leverage the combinatorial structure of graphs to quantify human priors over relational data, with experiments focusing on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. Our approach provides insights into how humans structure and reason about complex relational information in these fundamental domains.},
  author = {Gecia Bravo Hermsdorff},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hermsdorff2023quantifying.pdf:pdf},
  mdate = {2023-10-19},
  pages = {3063--3105},
  pdf = {https://proceedings.mlr.press/v202/bravo-hermsdorff23a/bravo-hermsdorff23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantifying Human Priors over Social and Navigation Networks},
  url = {https://proceedings.mlr.press/v202/bravo-hermsdorff23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{brchet2023critical,
  abstract = {We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. The Hessian of this loss at low-rank matrices can theoretically blow up, which creates challenges to analyze convergence of gradient optimization methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss as well as convergence results for finite step size gradient descent under certain assumptions on the initial weights.},
  author = {Pierre Bréchet and Katerina Papagiannouli and Jing An and Guido Montúfar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/brchet2023critical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3106--3147},
  pdf = {https://proceedings.mlr.press/v202/brechet23a/brechet23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss},
  url = {https://proceedings.mlr.press/v202/brechet23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{breugel2023synthetic,
  abstract = {Generating synthetic data through generative models is gaining interest in the ML community and beyond, promising a future where datasets can be tailored to individual needs. Unfortunately, synthetic data is usually not perfect, resulting in potential errors in downstream tasks. In this work we explore how the generative process affects the downstream ML task. We show that the naive synthetic data approach -- using synthetic data as if it is real -- leads to downstream models and analyses that do not generalize well to real data. As a first step towards better ML in the synthetic data regime, we introduce Deep Generative Ensemble (DGE) -- a framework inspired by Deep Ensembles that aims to implicitly approximate the posterior distribution over the generative process model parameters. DGE improves downstream model training, evaluation, and uncertainty quantification, vastly outperforming the naive approach on average. The largest improvements are achieved for minority classes and low-density regions of the original data, for which the generative uncertainty is largest.},
  author = {Boris van Breugel and Zhaozhi Qian and Mihaela van der Schaar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/breugel2023synthetic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34793--34808},
  pdf = {https://proceedings.mlr.press/v202/van-breugel23a/van-breugel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Synthetic Data, Real Errors: How (Not) to Publish and Use Synthetic Data},
  url = {https://proceedings.mlr.press/v202/van-breugel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bricken2023emergence,
  abstract = {A hallmark of biological neural networks, which distinguishes them from their artificial counterparts, is the high degree of sparsity in their activations. This discrepancy raises three questions our work helps to answer: (i) Why are biological networks so sparse? (ii) What are the benefits of this sparsity? (iii) How can these benefits be utilized by deep learning models? Our answers to all of these questions center around training networks to handle random noise. We show that noisy training introduces three implicit loss terms that result in sparsely firing neurons, and demonstrate that adding noise to network inputs can lead to the emergence of sparse representations.},
  author = {Trenton Bricken and Rylan Schaeffer and Bruno A. Olshausen and Gabriel Kreiman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bricken2023emergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3148--3191},
  pdf = {https://proceedings.mlr.press/v202/bricken23a/bricken23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Emergence of Sparse Representations from Noise},
  url = {https://proceedings.mlr.press/v202/bricken23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bu2023differentially,
  abstract = {Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2∼1000× more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error.},
  author = {Zhiqi Bu and Yu-Xiang Wang and Sheng Zha and George Karypis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bu2023differentially.pdf:pdf},
  mdate = {2023-11-20},
  pages = {3192--3218},
  pdf = {https://proceedings.mlr.press/v202/bu23a/bu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Optimization on Large Model at Small Cost},
  url = {https://proceedings.mlr.press/v202/bu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{bukharin2023machine,
  abstract = {Machine learning force fields (MLFF) have been proposed to accelerate molecular dynamics (MD) simulation, which finds widespread applications in chemistry and biomedical research. Even for the most data-efficient MLFFs, reaching chemical accuracy can require hundreds of frames of force and energy labels generated by expensive quantum mechanical algorithms, which may scale as O(n³) to O(n⁷), with n proportional to the number of basis functions. To address this issue, we propose a multi-stage computational framework -- ASTEROID, which lowers the data cost of MLFFs by leveraging a combination of cheap inaccurate data and expensive accurate data.},
  author = {Alexander Bukharin and Tianyi Liu and Shengjie Wang and Simiao Zuo and Weihao Gao and Wen Yan and Tuo Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/bukharin2023machine.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3219--3232},
  pdf = {https://proceedings.mlr.press/v202/bukharin23a/bukharin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Machine Learning Force Fields with Data Cost Aware Training},
  url = {https://proceedings.mlr.press/v202/bukharin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{busafekete2023label,
  abstract = {We study differentially private mechanisms for sharing training data in machine learning settings. Our goal is to enable learning of an accurate predictive model while protecting the privacy of each user's label. Previous work established privacy guarantees that assumed the features are public and given exogenously, a setting known as label differential privacy. In some scenarios, this can be a strong assumption that removes the interplay between features and labels from the privacy analysis. We relax this approach and instead assume the features are drawn from a distribution that depends on the private labels.},
  author = {Robert Istvan Busa-Fekete and Andres Munoz Medina and Umar Syed and Sergei Vassilvitskii},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/busafekete2023label.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3233--3251},
  pdf = {https://proceedings.mlr.press/v202/busa-fekete23a/busa-fekete23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Label differential privacy and private training data release},
  url = {https://proceedings.mlr.press/v202/busa-fekete23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{buschoff2023acquisition,
  abstract = {As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children's developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development – stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.},
  author = {Luca M. Schulze Buschoff and Eric Schulz and Marcel Binz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/buschoff2023acquisition.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30321--30341},
  pdf = {https://proceedings.mlr.press/v202/schulze-buschoff23a/schulze-buschoff23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Acquisition of Physical Knowledge in Generative Neural Networks},
  url = {https://proceedings.mlr.press/v202/schulze-buschoff23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cabannes2023ssl,
  abstract = {Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in kernel regimes, and highlight several insights for SSL practitioners that arise from our theory.},
  author = {Vivien Cabannes and Bobak Toussi Kiani and Randall Balestriero and Yann LeCun and Alberto Bietti},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cabannes2023ssl.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3252--3298},
  pdf = {https://proceedings.mlr.press/v202/cabannes23a/cabannes23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The {SSL} Interplay: Augmentations, Inductive Bias, and Generalization},
  url = {https://proceedings.mlr.press/v202/cabannes23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cacciamani2023online,
  abstract = {We study the problem of designing mechanisms for information acquisition scenarios, which model strategic interactions between an uninformed receiver and a set of informed senders. In this model, the senders receive information about the underlying state of nature and communicate their observation (either truthfully or not) to the receiver, which, based on this information, selects an action. The goal is to design mechanisms maximizing the receiver's utility while incentivizing the senders to report truthfully their information. We focus on the online problem in which the receiver sequentially interacts in an unknown game, with the objective of minimizing the cumulative regret with respect to the optimal incentive compatible mechanism.},
  author = {Federico Cacciamani and Matteo Castiglioni and Nicola Gatti},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cacciamani2023online.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3299--3326},
  pdf = {https://proceedings.mlr.press/v202/cacciamani23a/cacciamani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Mechanism Design for Information Acquisition},
  url = {https://proceedings.mlr.press/v202/cacciamani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{caggiano2023myodex,
  abstract = {Human dexterity is a hallmark of motor control. Our hands can rapidly synthesize new behaviors despite the complexity (multi-articular and multi-joints, with 23 joints controlled by more than 40 muscles) of musculoskeletal sensory-motor circuits. In this work, we take inspiration from how human dexterity builds on a diversity of prior experiences, instead of being acquired through a single task. Motivated by this observation, we set out to develop agents that can build upon their previous experience to quickly acquire new (previously unattainable) behaviors. Specifically, our approach leverages multi-task learning to implicitly capture task-agnostic behavioral priors (MyoDex) for human-like dexterity, using a physiologically realistic human hand model - MyoHand.},
  author = {Vittorio Caggiano and Sudeep Dasari and Vikash Kumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/caggiano2023myodex.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3327--3346},
  pdf = {https://proceedings.mlr.press/v202/caggiano23a/caggiano23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MyoDex}: A Generalizable Prior for Dexterous Manipulation},
  url = {https://proceedings.mlr.press/v202/caggiano23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cagnetta2023what,
  abstract = {Understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the local and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performance, e.g., the rate of decay of the generalisation error with the number of training samples. In this paper, we study infinitely-wide deep CNNs in the kernel regime. First, we show that the spectrum of the corresponding kernel inherits the hierarchical structure of the network, and we characterise its asymptotics. Then, we use this result together with generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function.},
  author = {Francesco Cagnetta and Alessandro Favero and Matthieu Wyart},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cagnetta2023what.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3347--3379},
  pdf = {https://proceedings.mlr.press/v202/cagnetta23a/cagnetta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {What Can Be Learnt With Wide Convolutional Neural Networks?},
  url = {https://proceedings.mlr.press/v202/cagnetta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023causal,
  abstract = {Causal discovery with latent confounders is an important but challenging task in many scientific areas. Despite the success of some overcomplete independent component analysis (OICA) based methods in certain domains, they are computationally expensive and can easily get stuck into local optima. We notice that interestingly, by making use of higher-order cumulants, there exists a closed-form solution to OICA in specific cases, e.g., when the mixing procedure follows the One-Latent-Component structure. In light of the power of the closed-form solution to OICA corresponding to the One-Latent-Component structure, we formulate a way to estimate the mixing matrix using the higher-order cumulants, and further propose the testable One-Latent-Component condition to identify the latent variables and determine causal orders.},
  author = {Ruichu Cai and Zhiyi Huang and Wei Chen and Zhifeng Hao and Kun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023causal.pdf:pdf},
  mdate = {2023-09-30},
  pages = {3380--3407},
  pdf = {https://proceedings.mlr.press/v202/cai23a/cai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Discovery with Latent Confounders Based on Higher-Order Cumulants},
  url = {https://proceedings.mlr.press/v202/cai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023connection,
  abstract = {Graph Transformer (GT) recently has emerged as a new paradigm of graph learning algorithms, outperforming the previously popular Message Passing Neural Network (MPNN) on multiple benchmarks. Previous work shows that with proper position embedding, GT can approximate MPNN arbitrarily well, implying that GT is at least as powerful as MPNN. In this paper, we study the inverse connection and show that MPNN with virtual node (VN), a commonly used heuristic with little theoretical understanding, is powerful enough to arbitrarily approximate the self-attention layer of GT. In particular, we first show that if we consider one type of linear transformer, the so-called Performer/Linear Transformer, then MPNN + VN with only $\mathcal{O}(1)$ depth and $\mathcal{O}(1)$ width can approximate a self-attention layer in Performer/Linear Transformer. Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN with $\mathcal{O}(n^d)$ width and $\mathcal{O}(1)$ depth can approximate the self-attention layer arbitrarily well, where $d$ is the input feature dimension. Lastly, under some assumptions, we provide an explicit construction of MPNN + VN with $\mathcal{O}(1)$ width and $\mathcal{O}(n)$ depth approximating the self-attention layer in GT arbitrarily well. On the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly strong baseline, outperforming GT on the recently proposed Long Range Graph Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer and MPNN on the climate modeling task.},
  author = {Chen Cai and Truong Son Hy and Rose Yu and Yusu Wang 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023connection.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3408--3430},
  pdf = {https://proceedings.mlr.press/v202/cai23b/cai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Connection Between {MPNN} and Graph Transformer},
  url = {https://proceedings.mlr.press/v202/cai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023ske2grid,
  abstract = {This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively. We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. Code and models are available at https://github.com/OSVAI/Ske2Grid.},
  author = {Dongqi Cai and Yangyuxuan Kang and Anbang Yao and Yurong Chen 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023ske2grid.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3431--3441},
  pdf = {https://proceedings.mlr.press/v202/cai23c/cai23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition},
  url = {https://proceedings.mlr.press/v202/cai23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023extrapolated,
  abstract = {In this paper, we propose a novel tree-based algorithm named Extrapolated Random Tree for Regression (ERTR) that adapts to arbitrary smoothness of the regression function while maintaining the interpretability of the tree. We first put forward the homothetic random tree for regression (HRTR) that converges to the target function as the homothetic ratio approaches zero. Then ERTR uses a linear regression model to extrapolate HRTR estimations with different ratios to the ratio zero. From the theoretical perspective, we for the first time establish the optimal convergence rates for ERTR when the target function resides in the general Hölder space $C^{k,\alpha}$ for $k\in \mathbb{N}$, whereas the lower bound of the convergence rate of the random tree for regression (RTR) is strictly slower than ERTR in the space $C^{k,\alpha}$ for $k\geq 1$. This shows that ERTR outperforms RTR for the target function with high-order smoothness due to the extrapolation. In the experiments, we compare ERTR with state-of-the-art tree algorithms on real datasets to show the superior performance of our model. Moreover, promising improvements are brought by using the extrapolated trees as base learners in the extension of ERTR to ensemble methods.},
  author = {Yuchao Cai and Yuheng Ma 0001 and Yiwei Dong and Hanfang Yang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023extrapolated.pdf:pdf},
  mdate = {2025-02-13},
  pages = {3442--3468},
  pdf = {https://proceedings.mlr.press/v202/cai23d/cai23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Extrapolated Random Tree for Regression},
  url = {https://proceedings.mlr.press/v202/cai23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023cyclic,
  abstract = {Nonconvex optimization is central in solving many machine learning problems, in which block-wise structure is commonly encountered. In this work, the authors propose cyclic block coordinate methods for nonconvex optimization problems with non-asymptotic gradient norm guarantees. Their convergence analysis is based on a gradient Lipschitz condition with respect to a Mahalanobis norm, inspired by recent progress on cyclic block coordinate methods. In deterministic settings, their convergence guarantee matches gradient descent, but with the gradient Lipschitz constant defined with respect to a Mahalanobis norm. In stochastic settings, they use recursive variance reduction to decrease per-iteration cost and match the arithmetic operation complexity of current optimal stochastic full-gradient methods, with a unified analysis for both finite-sum and infinite-sum cases. They prove a faster linear convergence result when a Polyak-Łojasiewicz (PŁ) condition holds. To their knowledge, this is the first work to provide non-asymptotic convergence guarantees for a cyclic block coordinate method in general composite (smooth + nonsmooth) nonconvex settings. Their experimental results demonstrate the efficacy of the proposed cyclic scheme in training deep neural nets.},
  author = {Xufeng Cai and Chaobing Song and Stephen J. Wright 0001 and Jelena Diakonikolas},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023cyclic.pdf:pdf},
  mdate = {2024-02-01},
  pages = {3469--3494},
  pdf = {https://proceedings.mlr.press/v202/cai23e/cai23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cyclic Block Coordinate Descent With Variance Reduction for Composite Nonconvex Optimization},
  url = {https://proceedings.mlr.press/v202/cai23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cai2023robust,
  abstract = {Given a robust model trained to be resilient to one or multiple types of distribution shifts (e.g., natural image corruptions), how is that "robustness" encoded in the model weights, and how easily can it be disentangled and/or "zero-shot" transferred to some other models? This paper empirically suggests a surprisingly simple answer: linearly - by straightforward model weight arithmetic! We start by drawing several key observations: (i) assuming that we train the same model architecture on both a clean dataset and its corrupted version, a comparison between the two resultant models shows their weights to mostly differ in shallow layers; (ii) the weight difference after projection, which we call "Robust Weight Signature" (RWS), appears to be discriminative and indicative of different corruption types; (iii) perhaps most strikingly, for the same corruption type, the RWSs obtained by one model architecture are highly consistent and transferable across different datasets. Based on those RWS observations, we propose a minimalistic model robustness "patching" framework that carries a model trained on clean data together with its pre-extracted RWSs. In this way, injecting certain robustness to the model is reduced to directly adding the corresponding RWS to its weight. We experimentally verify our proposed framework to be remarkably (1) lightweight. since RWSs concentrate on the shallowest few layers and we further show they can be painlessly quantized, storing an RWS is up to 13 x more compact than storing the full weight copy; (2) in-situ adjustable. RWSs can be appended as needed and later taken off to restore the intact clean model. We further demonstrate one can linearly re-scale the RWS to control the patched robustness strength; (3) composable. Multiple RWSs can be added simultaneously to patch more comprehensive robustness at once; and (4) transferable. Even when the clean model backbone is continually adapted or updated, RWSs remain as effective patches due to their outstanding cross-dataset transferability.},
  author = {Ruisi Cai and Zhenyu Zhang 0015 and Zhangyang Wang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cai2023robust.pdf:pdf},
  mdate = {2023-12-19},
  pages = {3495--3506},
  pdf = {https://proceedings.mlr.press/v202/cai23f/cai23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?},
  url = {https://proceedings.mlr.press/v202/cai23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{caliskan2023multiagent,
  abstract = {A large body of the "Inverse Reinforcement Learning" (IRL) literature focuses on recovering the reward function from a set of demonstrations of an expert agent who acts optimally or noisily optimally. Nevertheless, some recent works move away from the optimality assumption to study the "Learning from a Learner (LfL)" problem, where the challenge is inferring the reward function of a learning agent from a sequence of demonstrations produced by progressively improving policies. In this work, we take one of the initial steps in addressing the multi-agent version of this problem and propose a new algorithm, MA-LfL (Multiagent Learning from a Learner). Unlike the state-of-the-art literature, which recovers the reward functions from trajectories produced by agents in some equilibrium, we study the problem of inferring the reward functions of interacting agents in a general sum stochastic game.},
  author = {Mine Melodi Caliskan and Francesco Chini and Setareh Maghsudi},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/caliskan2023multiagent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3525--3540},
  pdf = {https://proceedings.mlr.press/v202/caliskan23a/caliskan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Agent Learning from Learners},
  url = {https://proceedings.mlr.press/v202/caliskan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cao2023efficient,
  abstract = {Learning the long-range interactions on large-scale mesh-based physical systems with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. Therefore, there has been growing interest in the community to introduce multi-scale structures to GNNs for physics simulation. However, current state-of-the-art methods are limited by their reliance on the labor-heavy drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, bi-stride to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the Breadth-First-Search (BFS), without the need for the manual drawing of coarser meshes and, avoid wrong edges introduced by spatial proximity. Additionally, it enables a reduced number of MP times on each level and the non-parametrized pooling and unpooling by interpolations, similar to convolutional Neural Networks (CNNs), which significantly reduces computational requirements.},
  author = {Yadi Cao and Menglei Chai and Minchen Li and Chenfanfu Jiang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cao2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3541--3558},
  pdf = {https://proceedings.mlr.press/v202/cao23a/cao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network},
  url = {https://proceedings.mlr.press/v202/cao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cao2023learning,
  abstract = {Resource-constrained perception systems such as edge computing and vision-for-robotics require vision models to be both accurate and lightweight in computation and memory usage. While knowledge distillation is a proven strategy to enhance the performance of lightweight classification models, its application to structured outputs like object detection and instance segmentation remains a complicated task, due to the variability in outputs and complex internal network modules involved in the distillation process. In this paper, we propose a simple yet surprisingly effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings. To the best of our knowledge, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.},
  author = {Shengcao Cao and Mengtian Li and James Hays and Deva Ramanan and Yu-Xiong Wang and Liangyan Gui},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cao2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3577--3598},
  pdf = {https://proceedings.mlr.press/v202/cao23c/cao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation},
  url = {https://proceedings.mlr.press/v202/cao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cao2023onesided,
  abstract = {Given only a few observed entries from a low-rank matrix X, matrix completion is the problem of imputing the missing entries, and it formalizes a wide range of real-world settings that involve estimating missing data. However, when there are too few observed entries to complete the matrix, what other aspects of the underlying matrix can be reliably recovered? We study one such problem setting, that of "one-sided" matrix completion, where our goal is to recover the right singular vectors of X, even in the regime where recovering the left singular vectors is impossible, which arises when there are more rows than columns and very few observations. We propose a natural algorithm that involves imputing the missing values of the matrix X^TX and show that even with only two observations per row in X, we can provably recover X^TX as long as we have at least Ω(r² d log d) rows, where r is the rank and d is the number of columns.},
  author = {Steven Cao and Percy Liang and Gregory Valiant},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cao2023onesided.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3599--3624},
  pdf = {https://proceedings.mlr.press/v202/cao23d/cao23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-sided Matrix Completion from Two Observations Per Row},
  url = {https://proceedings.mlr.press/v202/cao23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{carta2023grounding,
  abstract = {Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. The paper investigates several key scientific questions using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? They study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.},
  author = {Thomas Carta and Clément Romac and Thomas Wolf and Sylvain Lamprier and Olivier Sigaud and Pierre-Yves Oudeyer},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/carta2023grounding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3676--3713},
  pdf = {https://proceedings.mlr.press/v202/carta23a/carta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/carta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{castanet2023stein,
  abstract = {In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has discontinuities and the reward is sparse, a majority of goals are difficult to reach. In this context, a curriculum over goals helps agents learn by adapting training tasks to their current capabilities. In this work, we propose Stein Variational Goal Generation (SVGG), which samples goals of intermediate difficulty for the agent, by leveraging a learned predictive model of its goal reaching capabilities. The distribution of goals is modeled with particles that are attracted in areas of appropriate difficulty using Stein Variational Gradient Descent. We show that SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in terms of success coverage in hard exploration problems.},
  author = {Nicolas Castanet and Olivier Sigaud and Sylvain Lamprier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/castanet2023stein.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3714--3731},
  pdf = {https://proceedings.mlr.press/v202/castanet23a/castanet23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/castanet23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{castellini2023scalable,
  abstract = {Algorithms for safely improving policies are important to deploy reinforcement learning approaches in real-world scenarios. In this work, we propose an algorithm, called MCTS-SPIBB, that computes safe policy improvement online using a Monte Carlo Tree Search based strategy. We theoretically prove that the policy generated by MCTS-SPIBB converges, as the number of simulations grows, to the optimal safely improved policy generated by Safe Policy Improvement with Baseline Bootstrapping (SPIBB), a popular algorithm based on policy iteration. Moreover, our empirical analysis performed on three standard benchmark domains shows that MCTS-SPIBB scales to significantly larger problems than SPIBB because it computes the policy online and locally, i.e., only in the states actually visited.},
  author = {Alberto Castellini and Federico Bianchi 0002 and Edoardo Zorzi and Thiago D. Simo and Alessandro Farinelli and Matthijs T. J. Spaan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/castellini2023scalable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3732--3756},
  pdf = {https://proceedings.mlr.press/v202/castellini23a/castellini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scalable Safe Policy Improvement via Monte Carlo Tree Search},
  url = {https://proceedings.mlr.press/v202/castellini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{castiglia2023lessvfl,
  abstract = {We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection approaches.},
  author = {Timothy Castiglia and Yi Zhou 0015 and Shiqiang Wang 0001 and Swanand Kadhe and Nathalie Baracaldo and Stacy Patterson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/castiglia2023lessvfl.pdf:pdf},
  mdate = {2024-01-28},
  pages = {3757--3781},
  pdf = {https://proceedings.mlr.press/v202/castiglia23a/castiglia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning},
  url = {https://proceedings.mlr.press/v202/castiglia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{catellier2023robustness,
  abstract = {A fundamental issue in machine learning is the robustness of the model with respect to changes in the input. In natural language processing, models typically contain a first embedding layer, transforming a sequence of tokens into vector representations. While the robustness with respect to changes of continuous inputs is well-understood, the situation is less clear when considering discrete changes, for instance replacing a word by another in an input sentence. Our work formally proves that popular embedding schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit robustness in the Hölder or Lipschitz sense with respect to the Hamming distance. We provide quantitative bounds for these schemes and demonstrate how the constants involved are affected by the length of the document.},
  author = {Rmi Catellier and Samuel Vaiter and Damien Garreau},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/catellier2023robustness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3782--3814},
  pdf = {https://proceedings.mlr.press/v202/catellier23a/catellier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Robustness of Text Vectorizers},
  url = {https://proceedings.mlr.press/v202/catellier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cervio2023learning,
  abstract = {Smoothness and low dimensional structures play central roles in improving generalization and stability in learning and statistics. This work combines techniques from semi-infinite constrained learning and manifold regularization to learn representations that are globally smooth on a manifold. We show that under typical conditions the problem of learning a Lipschitz continuous function on a manifold is equivalent to a dynamically weighted manifold regularization problem. This observation leads to a practical algorithm based on a weighted Laplacian penalty whose weights are adapted using stochastic gradient techniques. Under mild conditions, this method estimates the Lipschitz constant of the solution, learning a globally smooth solution as a byproduct.},
  author = {Juan Cervio and Luiz F. O. Chamon and Benjamin David Haeffele and Ren Vidal and Alejandro Ribeiro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cervio2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3815--3854},
  pdf = {https://proceedings.mlr.press/v202/cervino23a/cervino23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Globally Smooth Functions on Manifolds},
  url = {https://proceedings.mlr.press/v202/cervino23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cha2023tighter,
  abstract = {We study convergence lower bounds of without-replacement stochastic gradient descent (SGD) for solving smooth (strongly-)convex finite-sum minimization problems. Unlike most existing results focusing on final iterate lower bounds in terms of the number of components n and the number of epochs K, we seek bounds for arbitrary weighted average iterates that are tight in all factors including the condition number κ. For SGD with Random Reshuffling, we present lower bounds that have tighter κ dependencies than existing bounds. Our results are the first to perfectly close the gap between lower and upper bounds for weighted average iterates in both strongly-convex and convex cases. We also prove weighted average iterate lower bounds for arbitrary permutation-based SGD, which apply to all variants that carefully choose the best permutation.},
  author = {Jaeyoung Cha and Jaewook Lee and Chulhee Yun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cha2023tighter.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3855--3912},
  pdf = {https://proceedings.mlr.press/v202/cha23a/cha23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond},
  url = {https://proceedings.mlr.press/v202/cha23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cha2023orthogonalityenforced,
  abstract = {Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding.},
  author = {Jaehoon Cha and Jeyan Thiyagalingam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cha2023orthogonalityenforced.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3913--3948},
  pdf = {https://proceedings.mlr.press/v202/cha23b/cha23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Orthogonality-Enforced Latent Space in Autoencoders: An Approach to Learning Disentangled Representations},
  url = {https://proceedings.mlr.press/v202/cha23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chakraborty2023steering,
  abstract = {Directed Exploration is a crucial challenge in reinforcement learning (RL), especially when rewards are sparse. Information-directed sampling (IDS), which optimizes the information ratio, seeks to do so by augmenting regret with information gain. However, estimating information gain is computationally intractable or relies on restrictive assumptions which prohibit its use in many practical instances. In this work, we posit an alternative exploration incentive in terms of the integral probability metric (IPM) between a current estimate of the transition model and the unknown optimal, which under suitable conditions, can be computed in closed form with the kernelized Stein discrepancy (KSD). Based on KSD, we develop a novel algorithm STEERING: STEin information dirEcted exploration for model-based Reinforcement LearnING. To enable its derivation, we develop fundamentally new variants of KSD for discrete conditional distributions.},
  author = {Souradip Chakraborty and Amrit S. Bedi and Alec Koppel and Mengdi Wang and Furong Huang and Dinesh Manocha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chakraborty2023steering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3949--3978},
  pdf = {https://proceedings.mlr.press/v202/chakraborty23a/chakraborty23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {STEERING : Stein Information Directed Exploration for Model-Based Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/chakraborty23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chakraborty2023thompson,
  abstract = {We consider the stochastic linear contextual bandit problem with high-dimensional features. We analyze the Thompson sampling algorithm using special classes of sparsity-inducing priors (e.g., spike-and-slab) to model the unknown parameter and provide a nearly optimal upper bound on the expected cumulative regret. To the best of our knowledge, this is the first work that provides theoretical guarantees of Thompson sampling in high-dimensional and sparse contextual bandits. For faster computation, we use variational inference instead of Markov Chain Monte Carlo (MCMC) to approximate the posterior distribution. Extensive simulations demonstrate the improved performance of our proposed algorithm over existing ones.},
  author = {Sunrit Chakraborty and Saptarshi Roy and Ambuj Tewari},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chakraborty2023thompson.pdf:pdf},
  mdate = {2023-08-28},
  pages = {3979--4008},
  pdf = {https://proceedings.mlr.press/v202/chakraborty23b/chakraborty23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Thompson Sampling for High-Dimensional Sparse Linear Contextual Bandits},
  url = {https://proceedings.mlr.press/v202/chakraborty23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chandak2023representations,
  abstract = {Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments.},
  author = {Yash Chandak and Shantanu Thakoor and Zhaohan Daniel Guo and Yunhao Tang and Rmi Munos and Will Dabney and Diana L. Borsa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chandak2023representations.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4009--4034},
  pdf = {https://proceedings.mlr.press/v202/chandak23a/chandak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition},
  url = {https://proceedings.mlr.press/v202/chandak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chang2023memorybased,
  abstract = {Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.},
  author = {Paul Edmund Chang and Prakhar Verma and S. T. John and Arno Solin and Mohammad Emtiyaz Khan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chang2023memorybased.pdf:pdf},
  mdate = {2023-09-30},
  pages = {4035--4054},
  pdf = {https://proceedings.mlr.press/v202/chang23a/chang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Memory-Based Dual Gaussian Processes for Sequential Learning},
  url = {https://proceedings.mlr.press/v202/chang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chang2023muse,
  abstract = {We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse learns to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requires fewer sampling iterations; compared to autoregressive models such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, which translates to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing.},
  author = {Huiwen Chang and Han Zhang 0010 and Jarred Barber and Aaron Maschinot and Jos Lezama and Lu Jiang 0004 and Ming-Hsuan Yang 0001 and Kevin Patrick Murphy and William T. Freeman and Michael Rubinstein and Yuanzhen Li and Dilip Krishnan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chang2023muse.pdf:pdf},
  mdate = {2023-08-29},
  pages = {4055--4075},
  pdf = {https://proceedings.mlr.press/v202/chang23b/chang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Muse: Text-To-Image Generation via Masked Generative Transformers},
  url = {https://proceedings.mlr.press/v202/chang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chao2023investigating,
  abstract = {Existing Score-Based Models (SBMs) can be categorized into constrained SBMs (CSBMs) or unconstrained SBMs (USBMs) according to their parameterization approaches. CSBMs model probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USBMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSBMs may limit their modeling ability. In addition, we show that USBMs' inability to preserve the property of conservativeness may lead to degraded performance in practice. To address the above issues, we propose Quasi-Conservative Score-Based Models (QCSBMs) for keeping the advantages of both CSBMs and USBMs. Our theoretical derivations demonstrate that the training objective of QCSBMs can be efficiently integrated into the training processes by leveraging the Hutchinson's trace estimator. In addition, our experimental results on the CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets validate the effectiveness of QCSBMs. Finally, we justify the advantage of QCSBMs using an example of a one-layered autoencoder.},
  author = {Chen-Hao Chao and Wei-Fang Sun and Bo-Wun Cheng and Chun-Yi Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chao2023investigating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4076--4095},
  pdf = {https://proceedings.mlr.press/v202/chao23a/chao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Investigating the Conservative Property of Score-Based Generative Models},
  url = {https://proceedings.mlr.press/v202/chao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{charisopoulos2023robust,
  abstract = {In this paper, we study the stochastic linear bandit problem under the additional requirements of differential privacy, robustness and batched observations. In particular, we assume an adversary randomly chooses a constant fraction of the observed rewards in each batch, replacing them with arbitrary numbers. We present differentially private and robust variants of the arm elimination algorithm using logarithmic batch queries under two privacy models and provide regret bounds in both settings. In the first model, every reward in each round is reported by a potentially different client, which reduces to standard local differential privacy (LDP). In the second model, every action is owned by a different client, who may aggregate the rewards over multiple queries and privatize the aggregate response instead. To the best of our knowledge, our algorithms are the first simultaneously providing differential privacy and adversarial robustness in the stochastic linear bandits problem.},
  author = {Vasileios Charisopoulos and Hossein Esfandiari and Vahab Mirrokni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/charisopoulos2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4096--4115},
  pdf = {https://proceedings.mlr.press/v202/charisopoulos23a/charisopoulos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust and private stochastic linear bandits},
  url = {https://proceedings.mlr.press/v202/charisopoulos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chaturvedi2023streaming,
  abstract = {In this work, we study the problem of privately maximizing a submodular function in the streaming setting. Extensive work has been done on privately maximizing submodular functions in the general case when the function depends upon the private data of individuals. However, when the size of the data stream drawn from the domain of the objective function is large or arrives very fast, one must privately optimize the objective within the constraints of the streaming setting. We establish fundamental differentially private baselines for this problem and then derive better trade-offs between privacy and utility for the special case of decomposable submodular functions. A submodular function is decomposable when it can be written as a sum of submodular functions; this structure arises naturally when each summand function models the utility of an individual and the goal is to study the total utility of the whole population as in the well-known Combinatorial Public Projects Problem. Finally, we complement our theoretical analysis with experimental corroboration.},
  author = {Anamay Chaturvedi and Huy L. Nguyen and Thy Dinh Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chaturvedi2023streaming.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4116--4143},
  pdf = {https://proceedings.mlr.press/v202/chaturvedi23a/chaturvedi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Streaming Submodular Maximization with Differential Privacy},
  url = {https://proceedings.mlr.press/v202/chaturvedi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chaudhuri2023why,
  abstract = {When facing data with imbalanced classes or groups, practitioners follow an intriguing strategy to achieve best results. They throw away examples until the classes or groups are balanced in size, and then perform empirical risk minimization on the reduced training set. This opposes common wisdom in learning theory, where the expected error is supposed to decrease as the dataset grows in size. In this work, we leverage extreme value theory to address this apparent contradiction. Our results show that the tails of the data distribution play an important role in determining the worst-group-accuracy of linear classifiers. When learning on data with heavy tails, throwing away data restores the geometric symmetry of the resulting classifier, and therefore improves its worst-group generalization.},
  author = {Kamalika Chaudhuri and Kartik Ahuja and Martn Arjovsky and David Lopez-Paz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chaudhuri2023why.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4144--4188},
  pdf = {https://proceedings.mlr.press/v202/chaudhuri23a/chaudhuri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why does Throwing Away Data Improve Worst-Group Error?},
  url = {https://proceedings.mlr.press/v202/chaudhuri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chawla2023collaborative,
  abstract = {The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of N agents such that each agent is learning one of M stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.},
  author = {Ronshee Chawla and Daniel Vial and Sanjay Shakkottai and R. Srikant 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/chawla2023collaborative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4189--4217},
  pdf = {https://proceedings.mlr.press/v202/chawla23a/chawla23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits},
  url = {https://proceedings.mlr.press/v202/chawla23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{che2023correcting,
  abstract = {The policy gradient theorem gives a convenient form of the policy gradient in terms of three factors: an action value, a gradient of the action likelihood, and a state distribution involving discounting called the discounted stationary distribution. But commonly used on-policy methods based on the policy gradient theorem ignores the discount factor in the state distribution, which is technically incorrect and may even cause degenerate learning behavior in some environments. An existing solution corrects this discrepancy by using γᵗ as a factor in the gradient estimate. However, this solution is not widely adopted and does not work well in tasks where the later states are similar to earlier states. We introduce a novel distribution correction to account for the discounted stationary distribution that can be plugged into many existing gradient estimators.},
  author = {Fengdi Che and Gautham Vasan and A. Rupam Mahmood},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/che2023correcting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4218--4240},
  pdf = {https://proceedings.mlr.press/v202/che23a/che23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Correcting discount-factor mismatch in on-policy policy gradient methods},
  url = {https://proceedings.mlr.press/v202/che23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{che2023fast,
  abstract = {Federated machine unlearning (FMU) aims to remove the influence of a specified subset of training data upon request from a trained federated learning model. Despite achieving remarkable performance, existing FMU techniques suffer from inefficiency due to two sequential operations of training and retraining/unlearning on large-scale datasets. Our prior study, PCMU, was proposed to improve the efficiency of centralized machine unlearning (CMU) with certified guarantees, by simultaneously executing the training and unlearning operations. This paper proposes a fast FMU algorithm, FFMU, for improving the FMU efficiency while maintaining the unlearning quality. The PCMU method is leveraged to train a local machine learning (MU) model on each edge device. We propose to employ nonlinear functional analysis techniques to refine the local MU models as output functions of a Nemytskii operator. We conduct theoretical analysis to derive that the Nemytskii operator has a global Lipschitz constant, which allows us to bound the difference between two MU models regarding the distance between their gradients. Based on the Nemytskii operator and average smooth local gradients, the global MU model on the server is guaranteed to achieve close performance to each local MU model with the certified guarantees.},
  author = {Tianshi Che and Yang Zhou 0001 and Zijie Zhang 0001 and Lingjuan Lyu and Ji Liu 0003 and Da Yan 0001 and Dejing Dou and Jun Huan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/che2023fast.pdf:pdf},
  mdate = {2024-01-04},
  pages = {4241--4268},
  pdf = {https://proceedings.mlr.press/v202/che23b/che23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Federated Machine Unlearning with Nonlinear Functional Theory},
  url = {https://proceedings.mlr.press/v202/che23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheikhi2023statistical,
  abstract = {Given a dataset on actions and resulting long-term rewards, a direct estimation approach fits value functions that minimize prediction error on the training data. Temporal difference learning (TD) methods instead fit value functions by minimizing the degree of temporal inconsistency between estimates made at successive time-steps. Focusing on finite state Markov chains, we provide a crisp asymptotic theory of the statistical advantages of this approach. First, we show that an intuitive inverse trajectory pooling coefficient completely characterizes the percent reduction in mean-squared error of value estimates. Depending on problem structure, the reduction could be enormous or nonexistent.},
  author = {David Cheikhi and Daniel Russo 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/cheikhi2023statistical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4269--4293},
  pdf = {https://proceedings.mlr.press/v202/cheikhi23a/cheikhi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Statistical Benefits of Temporal Difference Learning},
  url = {https://proceedings.mlr.press/v202/cheikhi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023contextaware,
  abstract = {Executing actions in a correlated manner is a common strategy for human coordination that often leads to better cooperation, which is also potentially beneficial for cooperative multi-agent reinforcement learning (MARL). However, the recent success of MARL relies heavily on the convenient paradigm of purely decentralized execution, where there is no action correlation among agents for scalability considerations. In this work, we introduce a Bayesian network to inaugurate correlations between agents' action selections in their joint policy. Theoretically, we establish a theoretical justification for why action dependencies are beneficial by deriving the multi-agent policy gradient formula under such a Bayesian network joint policy and proving its global convergence to Nash equilibria under tabular softmax policy parameterization in cooperative Markov games. Further, by equipping existing MARL algorithms with a recent method of differentiable directed acyclic graphs (DAGs), we develop practical algorithms to learn the context-aware Bayesian network policies in scenarios with partial observability and various difficulty. We also dynamically decrease the sparsity of the learned DAG throughout the training process, which leads to weakly or even purely independent policies for decentralized execution. Empirical results on a range of MARL benchmarks show the benefits of our approach.},
  author = {Dingyang Chen and Qi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023contextaware.pdf:pdf},
  mdate = {2023-10-16},
  month = {7},
  pages = {5327--5350},
  pdf = {https://proceedings.mlr.press/v202/chen23an/chen23an.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Context-Aware Bayesian Network Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/chen23an.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023bidirectional,
  abstract = {Offline model-based optimization aims to maximize a black-box objective function with a static dataset of designs and their scores. In this paper, we focus on biological sequence design to maximize some sequence score. A recent approach employs bidirectional learning, combining a forward mapping for exploitation and a backward mapping for constraint, and it relies on the neural tangent kernel (NTK) of an infinitely wide network to build a proxy model. Though effective, the NTK cannot learn features because of its parametrization, and its use prevents the incorporation of powerful pre-trained Language Models (LMs) that can capture the rich biophysical information in millions of biological sequences. We adopt an alternative proxy model, adding a linear head to a pre-trained LM, and propose a linearization scheme. This yields a closed-form loss and also takes into account the biophysical information in the pre-trained LM. In addition, the forward mapping and the backward mapping play different roles and thus deserve different weights during sequence optimization. To achieve this, we train an auxiliary model and leverage its weak supervision signal via a bi-level optimization framework to effectively learn how to balance the two mappings. Further, by extending the framework, we develop the first learning rate adaptation module Adaptive-$\eta$, which is compatible with all gradient-based algorithms for offline model-based optimization. Experimental results on DNA/protein sequence design tasks verify the effectiveness of our algorithm.},
  author = {Can Chen and Yingxue Zhang and Xue Liu and Mark Coates},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023bidirectional.pdf:pdf},
  mdate = {2024-02-08},
  month = {7},
  pages = {5351--5366},
  pdf = {https://proceedings.mlr.press/v202/chen23ao/chen23ao.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bidirectional Learning for Offline Model-based Biological Sequence Design},
  url = {https://proceedings.mlr.press/v202/chen23ao.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023multilayer,
  abstract = {To characterize the functions spaces explored by multi-layer neural networks (NNs), we introduce Neural Hilbert Ladders (NHLs), a collection of reproducing kernel Hilbert spaces (RKHSes) that are defined iteratively and adaptive to training. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning the NHL based on a new complexity measure.},
  author = {Zhengdao Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023multilayer.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4294--4329},
  pdf = {https://proceedings.mlr.press/v202/chen23a/chen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Layer Neural Networks as Trainable Ladders of Hilbert Spaces},
  url = {https://proceedings.mlr.press/v202/chen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023differentially,
  abstract = {We study (ε,δ)-differentially private (DP) stochastic convex optimization under an r-th quantile loss function taking the form c(u) = ru⁺ + (1-r)(-u)⁺. The function is non-smooth, and we propose to approximate it with a smooth function obtained by convolution smoothing, which enjoys both structure and bandwidth flexibility and can address outliers. This leads to a better approximation than those obtained from existing methods such as Moreau Envelope. We then design private algorithms based on DP stochastic gradient descent and objective perturbation, and show that both algorithms achieve (near) optimal excess generalization risk O(max{1/√n, √(d ln(1/δ))/(nε)}). Through objective perturbation, we further derive an upper bound O(max{√(d/n), √(d ln(1/δ))/(nε)}) on the parameter estimation error under mild assumptions on data generating processes. Some applications in private quantile regression and private inventory control will be discussed.},
  author = {Du Chen and Geoffrey A. Chua},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023differentially.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4435--4461},
  pdf = {https://proceedings.mlr.press/v202/chen23d/chen23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Stochastic Convex Optimization under a Quantile Loss Function},
  url = {https://proceedings.mlr.press/v202/chen23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023trompt,
  abstract = {Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The experimental results demonstrate that Trompt outperforms state-of-the-art deep neural networks and is comparable to tree-based models.},
  author = {Kuan-Yu Chen and Ping-Han Chiang and Hsin-Rung Chou and Ting-Wei Chen and Tien-Hao Chang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023trompt.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4392--4434},
  pdf = {https://proceedings.mlr.press/v202/chen23c/chen23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trompt: Towards a Better Deep Neural Network for Tabular Data},
  url = {https://proceedings.mlr.press/v202/chen23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023restorationdegradation,
  abstract = {Several recent works have analyzed stochastic samplers used for diffusion generative modeling and shown polynomial-time convergence to the target distribution. Unfortunately, the techniques used to analyze stochastic samplers are often not applicable to deterministic samplers because deriving concentration inequalities for deterministic settings is much more challenging. In this work, we develop the first polynomial-time convergence bounds for deterministic samplers. We analyze not just linear diffusion processes, but also more general, non-linear forward processes. Our key insight is to introduce a new operational interpretation for deterministic sampling. We show that one step along the probability flow ODE can be expressed as two steps: first, a restoration step that runs gradient ascent on the conditional log-likelihood at some infinitesimally previous time, and second, a degradation step that runs the forward process using noise pointing back towards the current iterate. This perspective allows us to extend Denoising Diffusion Implicit Models to general, non-linear forward processes and prove polynomial convergence for these samplers.},
  author = {Sitan Chen and Giannis Daras and Alex Dimakis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023restorationdegradation.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4462--4484},
  pdf = {https://proceedings.mlr.press/v202/chen23e/chen23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For {DDIM}-type Samplers},
  url = {https://proceedings.mlr.press/v202/chen23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023provably,
  abstract = {The Schrödinger bridge problem is gaining increasing attention in generative modeling and showing promising potential even in comparison with score-based generative models. SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, the Schrödinger bridge is implemented with approximated projections, and it is unclear whether such approximated algorithm converges and how fast it converges. In this work, we provide the first convergence analysis of the Schrödinger bridge algorithm based on approximated projections. Moreover, we propose to apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed method achieves the state-of-the-art performance on healthcare and environmental data.},
  author = {Yu Chen and Wei Deng and Shikai Fang and Fengpei Li and Nicole Tianjiao Yang and Yikai Zhang and Kashif Rasul and Shandian Zhe and Anderson Schneider and Yuriy Nevmyvaka},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023provably.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4485--4513},
  pdf = {https://proceedings.mlr.press/v202/chen23f/chen23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Convergent {S}chr\"{o}dinger Bridge with Applications to Probabilistic Time Series Imputation},
  url = {https://proceedings.mlr.press/v202/chen23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023edbatch,
  abstract = {Batching has a fundamental influence on the efficiency of deep neural network (DNN) execution. However, for dynamic DNNs, efficient batching is particularly challenging as the dataflow graph varies per input instance. As a result, state-of-the-art frameworks use heuristics that result in suboptimal batching decisions. We provide an approach for batching dynamic DNNs based on finite state machines, which enables the automatic discovery of batching policies specialized for each DNN via reinforcement learning. Moreover, memory planning that is aware of the batching policy can save significant data movement overheads, which is automated by a PQ tree-based algorithm we introduce. Experimental results show that our framework speeds up state-of-the-art frameworks by on average 1.15x, 1.39x, and 2.45x for chain-based, tree-based, and lattice-based DNNs across CPU platforms.},
  author = {Siyuan Chen and Pratik Pramod Fegade and Tianqi Chen and Phillip B. Gibbons and Todd C. Mowry},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023edbatch.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4514--4528},
  pdf = {https://proceedings.mlr.press/v202/chen23g/chen23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ED-Batch}: Efficient Automatic Batching of Dynamic Neural Networks via Learned Finite State Machines},
  url = {https://proceedings.mlr.press/v202/chen23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023learning,
  abstract = {Likelihood-free inference (LFI) is a set of techniques for inference in implicit statistical models. A longstanding question in LFI has been how to design or learn good summary statistics of data, but this might now seem unnecessary due to the advent of recent end-to-end (i.e. neural network-based) LFI methods. In this work, we rethink this question with a new method for learning summary statistics. We show that learning sufficient statistics may be easier than direct posterior inference, as the former problem can be reduced to a set of low-dimensional, easy-to-solve learning problems. This suggests us to explicitly decouple summary statistics learning from posterior inference in LFI. Experiments on diverse inference tasks with different data types validate our hypothesis.},
  author = {Yanzhi Chen and Michael U. Gutmann and Adrian Weller},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023learning.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4529--4544},
  pdf = {https://proceedings.mlr.press/v202/chen23h/chen23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Is Learning Summary Statistics Necessary for Likelihood-free Inference?},
  url = {https://proceedings.mlr.press/v202/chen23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023subequivariant,
  abstract = {Learning a shared policy that guides the locomotion of different agents is of core interest in Reinforcement Learning (RL), which leads to the study of morphology-agnostic RL. However, existing benchmarks are highly restrictive in the choice of starting point and target point, constraining the movement of the agents within 2D space. In this work, we propose a novel setup for morphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments (3D-SGRL). Specifically, we first introduce a new set of more practical yet challenging benchmarks in 3D space that allows the agent to have full Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary configurations. Moreover, to optimize the policy over the enlarged state-action space, we propose to inject geometric symmetry, i.e., subequivariance, into the modeling of the policy and Q-function such that the policy can generalize to all directions, improving exploration efficiency. This goal is achieved by a novel SubEquivariant Transformer (SET) that permits expressive message exchange. Finally, we evaluate the proposed method on the proposed benchmarks, where our method consistently and significantly outperforms existing approaches on single-task, multi-task, and zero-shot generalization scenarios. Extensive ablations are also conducted to verify our design.},
  author = {Runfa Chen and Jiaqi Han and Fuchun Sun and Wenbing Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023subequivariant.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {4545--4565},
  pdf = {https://proceedings.mlr.press/v202/chen23i/chen23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Subequivariant Graph Reinforcement Learning in {3D} Environments},
  url = {https://proceedings.mlr.press/v202/chen23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023efficient,
  abstract = {Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. The paper proposes {EDGE}, a new diffusion-based graph generation model that uses a discrete diffusion process to remove edges, focuses on a portion of nodes at each denoising step, explicitly models node degrees, is more efficient than competing methods, and can generate large graphs with thousands of nodes.},
  author = {Xiaohui Chen and Jiaxing He and Xu Han 0012 and Liping Liu 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023efficient.pdf:pdf},
  mdate = {2023-12-08},
  pages = {4585--4610},
  pdf = {https://proceedings.mlr.press/v202/chen23k/chen23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling},
  url = {https://proceedings.mlr.press/v202/chen23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023guardhfl,
  abstract = {Heterogeneous federated learning ({HFL}) enables clients with different computation and communication capabilities to collaboratively train their own customized models via a query-response paradigm on auxiliary datasets. However, such paradigm raises serious privacy issues due to the leakage of highly sensitive query samples and response predictions. We put forth {GuardHFL}, the first-of-its-kind efficient and privacy-preserving {HFL} framework. {GuardHFL} is equipped with a novel {HFL}-friendly secure querying scheme that is built on lightweight secret sharing and symmetric-key techniques. Its core is a set of customized multiplication and comparison protocols, which substantially boost the execution efficiency. Extensive evaluations demonstrate that {GuardHFL} outperforms the state-of-the-art works by up to two orders of magnitude in efficiency.},
  author = {Hanxiao Chen 0001 and Meng Hao 0001 and Hongwei Li 0001 and Kangjie Chen and Guowen Xu and Tianwei Zhang 0004 and Xilin Zhang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023guardhfl.pdf:pdf},
  mdate = {2025-05-26},
  pages = {4566--4584},
  pdf = {https://proceedings.mlr.press/v202/chen23j/chen23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GuardHFL}: Privacy Guardian for Heterogeneous Federated Learning},
  url = {https://proceedings.mlr.press/v202/chen23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023decentralized,
  abstract = {Bilevel optimization recently has received tremendous attention due to its great success in solving important machine learning problems like meta learning, reinforcement learning, and hyperparameter optimization. Extending single-agent training on bilevel problems to the decentralized setting is a natural generalization, and there has been a flurry of work studying decentralized bilevel optimization algorithms. However, it remains unknown how to design the distributed algorithm with sample complexity and convergence rate comparable to {SGD} for stochastic optimization, and at the same time without directly computing the exact {Hessian} or {Jacobian} matrices. In this paper we propose such an algorithm. More specifically, we propose a novel decentralized stochastic bilevel optimization ({DSBO}) algorithm that only requires first order stochastic oracle, {Hessian}-vector product and {Jacobian}-vector product oracle. The sample complexity of our algorithm matches the currently best known results for {DSBO}, and the advantage of our algorithm is that it does not require estimating the full {Hessian} and {Jacobian} matrices, thereby having improved per-iteration complexity.},
  author = {Xuxing Chen and Minhui Huang and Shiqian Ma and Krishna Balasubramanian},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023decentralized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4641--4671},
  pdf = {https://proceedings.mlr.press/v202/chen23n/chen23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Decentralized Stochastic Bilevel Optimization with Improved per-Iteration Complexity},
  url = {https://proceedings.mlr.press/v202/chen23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023explore,
  abstract = {The proliferation of pretrained models, as a result of advancements in pretraining techniques, has led to the emergence of a vast zoo of publicly available models. Effectively utilizing these resources to obtain models with robust out-of-distribution generalization capabilities for downstream tasks has become a crucial area of research. Previous research has primarily focused on identifying the most powerful models within the model zoo, neglecting to fully leverage the diverse inductive biases contained within. This paper argues that the knowledge contained in weaker models is valuable and presents a method for leveraging the diversity within the model zoo to improve out-of-distribution generalization capabilities. Specifically, we investigate the behaviors of various pretrained models across different domains of downstream tasks by characterizing the variations in their encoded representations in terms of two dimensions: diversity shift and correlation shift. This characterization enables us to propose a new algorithm for integrating diverse pretrained models, not limited to the strongest models, in order to achieve enhanced out-of-distribution generalization performance. Our proposed method demonstrates state-of-the-art empirical results on a variety of datasets, thus validating the benefits of utilizing diverse knowledge.},
  author = {Yimeng Chen and Tianyang Hu and Fengwei Zhou and Zhenguo Li and Zhi-Ming Ma},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023explore.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4623--4640},
  pdf = {https://proceedings.mlr.press/v202/chen23m/chen23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Explore and Exploit the Diverse Knowledge in Model Zoo for Domain Generalization},
  url = {https://proceedings.mlr.press/v202/chen23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023score,
  abstract = {Diffusion models achieve state-of-the-art performance in various generation tasks. However, their theoretical foundations fall far behind. This paper studies score approximation, estimation, and distribution recovery of diffusion models, when data are supported on an unknown low-dimensional linear subspace. Our result provides sample complexity bounds for distribution estimation using diffusion models. We show that with a properly chosen neural network architecture, the score function can be both accurately approximated and efficiently estimated. Further, the generated distribution based on the estimated score function captures the data geometric structures and converges to a close vicinity of the data distribution. The convergence rate depends on subspace dimension, implying that diffusion models can circumvent the curse of data ambient dimensionality.},
  author = {Minshuo Chen and Kaixuan Huang and Tuo Zhao and Mengdi Wang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023score.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4672--4712},
  pdf = {https://proceedings.mlr.press/v202/chen23o/chen23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data},
  url = {https://proceedings.mlr.press/v202/chen23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023sample,
  abstract = {We rigorously quantify the improvement in the sample complexity of variational divergence estimations for group-invariant distributions. In the cases of the {Wasserstein-1} metric and the {Lipschitz}-regularized α-divergences, the reduction of sample complexity is proportional to the group size if the group is finite. For the maximum mean discrepancy ({MMD}), the improvement of sample complexity is more nuanced, as it depends on not only the group size but also the choice of kernel. Numerical simulations verify our theories.},
  author = {Ziyu Chen and Markos A. Katsoulakis and Luc Rey-Bellet and Wei Zhu 0007},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023sample.pdf:pdf},
  mdate = {2024-12-14},
  pages = {4713--4734},
  pdf = {https://proceedings.mlr.press/v202/chen23p/chen23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sample Complexity of Probability Divergences under Group Symmetry},
  url = {https://proceedings.mlr.press/v202/chen23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023improved,
  abstract = {This paper provides an improved theoretical analysis of score-based generative modeling. Under a score estimate with small {L²} error (averaged across timesteps), we provide efficient convergence guarantees for any data distribution with second-order moment, by either employing early stopping or assuming smoothness condition on the score function of the data distribution. The result does not rely on any log-concavity or functional inequality assumption and has a logarithmic dependence on the smoothness. Under only a finite second moment condition, approximating the following in reverse {KL} divergence in ε-accuracy can be done in {Õ}(d log(1/δ)/ε) steps: 1) the variance-δ {Gaussian} perturbation of any data distribution; 2) data distributions with 1/δ-smooth score functions.},
  author = {Hongrui Chen and Holden Lee and Jianfeng Lu 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4735--4763},
  pdf = {https://proceedings.mlr.press/v202/chen23q/chen23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions},
  url = {https://proceedings.mlr.press/v202/chen23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023bidirectional,
  abstract = {The paper addresses the importance of optimizers in deep learning, noting that {SGD} and {Adam} are classical effective optimizers with many proposed variants like {SGDM} and {RAdam}. The authors innovatively combine the backward-looking and forward-looking aspects of the optimizer algorithm and propose a novel {Admeta} (A Double exponential Moving averagE To Adaptive and non-adaptive momentum) optimizer framework. The backward-looking component proposes a {DEMA} variant scheme, which is motivated by a metric in the stock market, to replace the common exponential moving average scheme. The forward-looking component presents a dynamic lookahead strategy which asymptotically approaches a set value, maintaining its speed at early stage and high convergence performance at final stage.},
  author = {Yineng Chen and Zuchao Li and Lefei Zhang and Bo Du 0001 and Hai Zhao 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023bidirectional.pdf:pdf},
  mdate = {2024-03-21},
  pages = {4764--4803},
  pdf = {https://proceedings.mlr.press/v202/chen23r/chen23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers},
  url = {https://proceedings.mlr.press/v202/chen23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023harsanyinet,
  abstract = {The {Shapley} value is widely regarded as a trustworthy attribution metric. However, when people use {Shapley} values to explain the attribution of input variables of a deep neural network ({DNN}), it usually requires a very high computational cost to approximate relatively accurate {Shapley} values in real-world applications. Therefore, we propose a novel network architecture, the {HarsanyiNet}, which makes inferences on the input sample and simultaneously computes the exact {Shapley} values of the input variables in a single forward propagation. The {HarsanyiNet} is designed on the theoretical foundation that the {Shapley} value can be reformulated as the redistribution of {Harsanyi} interactions encoded by the network. The {HarsanyiNet} uses different intermediate-layer neurons to represent different {Harsanyi} interactions and is an interpretable network architecture which simultaneously performs model inference and computes exact {Shapley} values in a single forward propagation.},
  author = {Lu Chen and Siyu Lou and Keyan Zhang and Jin Huang and Quanshi Zhang},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023harsanyinet.pdf:pdf},
  mdate = {2024-10-06},
  pages = {4804--4825},
  pdf = {https://proceedings.mlr.press/v202/chen23s/chen23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation},
  url = {https://proceedings.mlr.press/v202/chen23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023fisher,
  abstract = {Attention-based graph neural networks ({GNNs}), such as graph attention networks ({GATs}), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of {Gaussian} mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into generalizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based {GNNs}.},
  author = {Dexiong Chen and Paolo Pellizzoni and Karsten M. Borgwardt},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023fisher.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4839--4855},
  pdf = {https://proceedings.mlr.press/v202/chen23u/chen23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fisher Information Embedding for Node and Graph Learning},
  url = {https://proceedings.mlr.press/v202/chen23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023optimistic,
  abstract = {This paper investigates optimistic online mirror descent (OMD) for the Stochastically Extended Adversarial (SEA) model, which bridges stochastic and adversarial online convex optimization. For convex and smooth functions, we obtain a regret bound of $\mathcal{O}(\sqrt{\sigma_{1:T}^2} + \sqrt{\Sigma_{1:T}^2})$ without requiring individual function convexity. For strongly convex functions, we establish $\mathcal{O}(\min\{\log(\sigma_{1:T}^2 + \Sigma_{1:T}^2), (\sigma_{\max}^2 + \Sigma_{\max}^2) \log T\})$ bound. For exp-concave functions, we achieve $\mathcal{O}(d\log(\sigma_{1:T}^2 + \Sigma_{1:T}^2))$ bound. Additionally, we establish dynamic regret bounds for non-stationary scenarios.},
  author = {Sijia Chen and Wei-Wei Tu and Peng Zhao and Lijun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023optimistic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5002--5035},
  pdf = {https://proceedings.mlr.press/v202/chen23aa/chen23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization},
  url = {https://proceedings.mlr.press/v202/chen23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023multitask,
  abstract = {Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert data efficiency through identifying and transferring reusable basic skills across tasks. MH-AIRL synthesizes context-based multi-task learning, AIRL, and hierarchical policy learning, working with demonstrations without task or skill annotations.},
  author = {Jiayu Chen and Dipesh Tamboli and Tian Lan and Vaneet Aggarwal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023multitask.pdf:pdf},
  mdate = {2024-12-05},
  pages = {4895--4920},
  pdf = {https://proceedings.mlr.press/v202/chen23x/chen23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-task Hierarchical Adversarial Inverse Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/chen23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023layered,
  abstract = {We study the autonomous exploration (AX) problem proposed by Lim \& Auer (2012). In this setting, the objective is to discover a set of $\varepsilon$-optimal policies reaching a set $\mathcal{S}_L^{\rightarrow}$ of incrementally $L$-controllable states. We introduce a novel layered decomposition of the set of incrementally $L$-controllable states that is based on the iterative application of a state-expansion operator. We leverage these results to design Layered Autonomous Exploration (LAE), a novel algorithm for AX that provides improved sample complexity guarantees for autonomous exploration in reinforcement learning.},
  author = {Liyu Chen and Andrea Tirinzoni and Alessandro Lazaric and Matteo Pirotta},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023layered.pdf:pdf},
  mdate = {2023-08-28},
  pages = {4953--5001},
  pdf = {https://proceedings.mlr.press/v202/chen23z/chen23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Layered State Discovery for Incremental Autonomous Exploration},
  url = {https://proceedings.mlr.press/v202/chen23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023model,
  abstract = {Given an algorithmic predictor that is accurate on some source population consisting of strategic human decision subjects, will it remain accurate if the population responds to it? In our setting, an agent corresponds to a sample $(X,Y)$ drawn from a distribution and will face a model $h$ and its classification result $h(X)$. Agents can modify $X$ to adapt to $h$, which will incur a distribution shift on $(X,Y)$. We formalize the discussions of model transferability by studying how the performance of the model trained on the available source distribution translates to the performance on its induced domain, addressing fundamental questions in strategic machine learning.},
  author = {Yatong Chen and Zeyu Tang and Kun Zhang and Yang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023model.pdf:pdf},
  mdate = {2024-11-13},
  pages = {4921--4952},
  pdf = {https://proceedings.mlr.press/v202/chen23y/chen23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model Transferability with Responsive Decision Subjects},
  url = {https://proceedings.mlr.press/v202/chen23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023learning,
  abstract = {Many machine learning problems can be abstracted as solving game theory formulations and boil down to optimizing nested objectives, such as generative adversarial networks (GANs) and multi-agent reinforcement learning. Solving these games requires finding their stable fixed points or Nash equilibrium. However, existing algorithms for solving games suffer from empirical instability, hence demanding heavy ad-hoc tuning in practice. To tackle these challenges, we resort to the emerging scheme of Learning to Optimize (L2O), which discovers problem-specific efficient optimization algorithms through data-driven training. Our customized L2O framework for differentiable game theory problems, dubbed Learning to Play Games (L2PG), seeks a stable fixed point solution by predicting the fast update direction from the past trajectory, with a novel gradient stability-aware, sign-based loss function.},
  author = {Xuxi Chen and Nelson Vadori and Tianlong Chen and Zhangyang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023learning.pdf:pdf},
  mdate = {2025-05-12},
  pages = {5036--5051},
  pdf = {https://proceedings.mlr.press/v202/chen23ab/chen23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Optimize Differentiable Games},
  url = {https://proceedings.mlr.press/v202/chen23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023beats,
  abstract = {We introduce a self-supervised learning (SSL) framework BEATs for general audio representation pre-training, where we optimize an acoustic tokenizer and an audio SSL model by iterations. Unlike the previous audio SSL models that employ reconstruction loss for pre-training, our audio SSL model is trained with the discrete label prediction task, where the labels are generated by a semantic-rich acoustic tokenizer. We propose an iterative pipeline to jointly optimize the tokenizer and the pre-trained model, aiming to abstract high-level semantics and discard the redundant details for audio. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly.},
  author = {Sanyuan Chen and Yu Wu and Chengyi Wang and Shujie Liu and Daniel Tompkins and Zhuo Chen and Wanxiang Che and Xiangzhan Yu and Furu Wei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023beats.pdf:pdf},
  mdate = {2024-06-12},
  pages = {5178--5193},
  pdf = {https://proceedings.mlr.press/v202/chen23ag/chen23ag.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{BEATs}: Audio Pre-Training with Acoustic Tokenizers},
  url = {https://proceedings.mlr.press/v202/chen23ag.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023implicit,
  abstract = {Implicit Neural Spatial Representation (INSR) has emerged as an effective representation of spatially-dependent vector fields. This work explores solving time-dependent PDEs with INSR. Classical PDE solvers introduce both temporal and spatial discretizations. Common spatial discretizations include meshes and meshless point clouds, where each degree-of-freedom corresponds to a location in space. While these explicit spatial correspondences are intuitive to model and understand, these representations are not necessarily optimal for accuracy, memory usage, or adaptivity. Keeping the classical temporal discretization unchanged, we explore INSR as an alternative spatial discretization, where spatial information is implicitly stored in the neural network weights. Our approach exhibits higher accuracy and lower memory consumption compared to traditional representations, while being intrinsically adaptive.},
  author = {Honglin Chen and Rundi Wu and Eitan Grinspun and Changxi Zheng and Peter Yichen Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023implicit.pdf:pdf},
  mdate = {2024-10-06},
  pages = {5162--5177},
  pdf = {https://proceedings.mlr.press/v202/chen23af/chen23af.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Implicit Neural Spatial Representations for Time-dependent {PDEs}},
  url = {https://proceedings.mlr.press/v202/chen23af.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023semioffline,
  abstract = {Existing reinforcement learning (RL) mainly utilize online or offline settings. The online methods explore the environment with expensive time cost, and the offline methods efficiently obtain reward signals by sacrificing the exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. We apply our framework to text generation tasks and demonstrate that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.},
  author = {Changyu Chen and Xiting Wang and Yiqiao Jin and Victor Ye Dong and Li Dong and Jie Cao and Yi Liu and Rui Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023semioffline.pdf:pdf},
  mdate = {2024-03-28},
  pages = {5087--5103},
  pdf = {https://proceedings.mlr.press/v202/chen23ad/chen23ad.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi-Offline Reinforcement Learning for Optimized Text Generation},
  url = {https://proceedings.mlr.press/v202/chen23ad.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023learning,
  abstract = {We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm to our model, which achieves a $\mathcal{O}(K^2 \cdot T^{2/3})$ regret after $T$ iterations, where $K$ is the number of effort levels of the agent.},
  author = {Siyu Chen and Jibang Wu and Yifan Wu and Zhuoran Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023learning.pdf:pdf},
  mdate = {2025-04-10},
  pages = {5194--5218},
  pdf = {https://proceedings.mlr.press/v202/chen23ah/chen23ah.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model},
  url = {https://proceedings.mlr.press/v202/chen23ah.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023lower,
  abstract = {We establish strong PAC and regret lower bounds for learning in revealing POMDPs. While it is well-established that learning in Partially Observable Markov Decision Processes (POMDPs) requires exponentially many samples in the worst case, a surge of recent work shows that polynomial sample complexities are achievable under the revealing condition. However, the fundamental limits for learning in revealing POMDPs are much less understood. Our lower bounds scale polynomially in all relevant problem parameters in a multiplicative fashion, and achieve significantly smaller gaps against the current best upper bounds. For multi-step revealing POMDPs, we show that the latent state-space dependence is at least $\Omega(S^{1.5})$ in the PAC sample complexity, which is notably harder than the $\tilde{\Theta}(S)$ scaling for fully-observable MDPs.},
  author = {Fan Chen and Huan Wang and Caiming Xiong and Song Mei and Yu Bai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023lower.pdf:pdf},
  mdate = {2024-08-07},
  pages = {5104--5161},
  pdf = {https://proceedings.mlr.press/v202/chen23ae/chen23ae.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lower Bounds for Learning in Revealing {POMDPs}},
  url = {https://proceedings.mlr.press/v202/chen23ae.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023faster,
  abstract = {We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}[F(x;\xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}(L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3} + \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.},
  author = {Lesi Chen and Jing Xu and Luo Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023faster.pdf:pdf},
  mdate = {2023-08-29},
  pages = {5219--5233},
  pdf = {https://proceedings.mlr.press/v202/chen23ai/chen23ai.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization},
  url = {https://proceedings.mlr.press/v202/chen23ai.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023how,
  abstract = {In this paper, we study the problem of establishing the accountability and fairness of transparent machine learning models through monotonicity. Although there have been numerous studies on individual monotonicity, pairwise monotonicity is often overlooked in the existing literature. This paper studies transparent neural networks in the presence of three types of monotonicity: individual monotonicity, weak pairwise monotonicity, and strong pairwise monotonicity. As a means of achieving monotonicity while maintaining transparency, we propose the monotonic groves of neural additive models. As a result of empirical examples, we demonstrate that monotonicity is often violated in practice and that monotonic groves of neural additive models are transparent, accountable, and fair.},
  author = {Dangxing Chen and Weicheng Ye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5282--5295},
  pdf = {https://proceedings.mlr.press/v202/chen23al/chen23al.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How to address monotonicity for model risk management?},
  url = {https://proceedings.mlr.press/v202/chen23al.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023efficient,
  abstract = {Federated Learning (FL) aims to train machine learning models for multiple clients without sharing their own private data. Due to the heterogeneity of clients' local data distribution, recent studies explore the personalized FL that learns and deploys distinct local models with the help of auxiliary global models. However, the clients can be heterogeneous in terms of not only local data distribution, but also their computation and communication resources. The capacity and efficiency of personalized models are restricted by the lowest-resource clients, leading to sub-optimal performance and limited practicality of personalized FL. To overcome these challenges, we propose a novel approach named pFedGate for efficient personalized FL by adaptively and efficiently learning sparse local models.},
  author = {Daoyuan Chen and Liuyi Yao and Dawei Gao and Bolin Ding and Yaliang Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5234--5256},
  pdf = {https://proceedings.mlr.press/v202/chen23aj/chen23aj.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Personalized Federated Learning via Sparse Model-Adaptation},
  url = {https://proceedings.mlr.press/v202/chen23aj.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023learning,
  abstract = {Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network.},
  author = {Tianqi Chen and Mingyuan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5367--5382},
  pdf = {https://proceedings.mlr.press/v202/chen23ap/chen23ap.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling},
  url = {https://proceedings.mlr.press/v202/chen23ap.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023lifelong,
  abstract = {Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining.},
  author = {Wuyang Chen and Yanqi Zhou and Nan Du and Yanping Huang and James Laudon and Zhifeng Chen and Claire Cui},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023lifelong.pdf:pdf},
  mdate = {2023-12-01},
  pages = {5383--5395},
  pdf = {https://proceedings.mlr.press/v202/chen23aq/chen23aq.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lifelong Language Pretraining with Distribution-Specialized Experts},
  url = {https://proceedings.mlr.press/v202/chen23aq.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chen2023sketched,
  abstract = {Overparametrization often helps improve the generalization performance. This paper presents a dual view of overparametrization suggesting that downsampling may also help generalize. Focusing on the proportional regime $m\asymp n \asymp p$, where $m$ represents the sketching size, $n$ is the sample size, and $p$ is the feature dimensionality, we investigate two out-of-sample prediction risks of the sketched ridgeless least square estimator. Our findings challenge conventional beliefs by showing that downsampling does not always harm generalization but can actually improve it in certain cases. We identify the optimal sketching size that minimizes out-of-sample prediction risks and demonstrate that the optimally sketched estimator exhibits stabler risk curves, eliminating the peaks of those for the full-sample estimator.},
  author = {Xin Chen and Yicheng Zeng and Siyue Yang and Qiang Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chen2023sketched.pdf:pdf},
  mdate = {2025-02-06},
  pages = {5296--5326},
  pdf = {https://proceedings.mlr.press/v202/chen23am/chen23am.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sketched Ridgeless Linear Regression: The Role of Downsampling},
  url = {https://proceedings.mlr.press/v202/chen23am.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023mu2slam,
  abstract = {Mu$^2$SLAM is a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^2$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling objective (MLM) on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^2$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points.},
  author = {Yong Cheng and Yu Zhang and Melvin Johnson and Wolfgang Macherey and Ankur Bapna},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023mu2slam.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5504--5520},
  pdf = {https://proceedings.mlr.press/v202/cheng23e/cheng23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mu$^2${SLAM}: Multitask, Multilingual Speech and Language Models},
  url = {https://proceedings.mlr.press/v202/cheng23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023weakly,
  abstract = {This paper investigates an interesting weakly supervised regression setting called regression with interval targets (RIT). Although some of the previous methods on relevant regression settings can be adapted to RIT, they are not statistically consistent, and thus their empirical performance is not guaranteed. In this paper, we provide a thorough study on RIT. First, we proposed a novel statistical model to describe the data generation process for RIT and demonstrate its validity. Second, we analyze a simple selection method for RIT, which selects a particular value in the interval as the target value to train the model. Third, we propose a statistically consistent limiting method for RIT to train the model by limiting the predictions to the interval. We further derive an estimation error bound for our limiting method.},
  author = {Xin Cheng and Yuzhou Cao and Ximing Li and Bo An and Lei Feng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023weakly.pdf:pdf},
  mdate = {2024-08-04},
  pages = {5428--5448},
  pdf = {https://proceedings.mlr.press/v202/cheng23a/cheng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Weakly Supervised Regression with Interval Targets},
  url = {https://proceedings.mlr.press/v202/cheng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023play,
  abstract = {Layout design is an important task in various design fields, including user interface, document, and graphic design. As this task requires tedious manual effort by designers, prior works have attempted to automate this process using generative models, but commonly fell short of providing intuitive user controls and achieving design objectives. In this paper, we build a conditional latent diffusion model, PLay, that generates parametrically conditioned layouts in vector graphic space from user-specified guidelines, which are commonly used by designers for representing their design intents in current practices. Our method outperforms prior works across three datasets on metrics including FID and FD-VG, and in user study.},
  author = {Chin-Yi Cheng and Forrest Huang and Gang Li and Yang Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023play.pdf:pdf},
  mdate = {2024-02-08},
  pages = {5449--5471},
  pdf = {https://proceedings.mlr.press/v202/cheng23b/cheng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PLay}: Parametrically Conditioned Layout Generation using Latent Diffusion},
  url = {https://proceedings.mlr.press/v202/cheng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023identification,
  abstract = {Deep neural networks have been shown vulnerable to adversarial examples. Even though many defense methods have been proposed to enhance the robustness, it is still a long way toward providing an attack-free method to build a trustworthy machine learning system. In this paper, instead of enhancing the robustness, we take the investigator's perspective and propose a new framework to trace the first compromised model copy in a forensic investigation manner. Specifically, we focus on the following setting: the machine learning service provider provides model copies for a set of customers. However, one of the customers conducted adversarial attacks to fool the system. Therefore, the investigator's objective is to identify the first compromised copy by collecting and analyzing evidence from only available adversarial examples. To make the tracing viable, we design a random mask watermarking mechanism to differentiate adversarial examples from different copies.},
  author = {Minhao Cheng and Rui Min and Haochen Sun and Pin-Yu Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023identification.pdf:pdf},
  mdate = {2024-08-04},
  pages = {5472--5484},
  pdf = {https://proceedings.mlr.press/v202/cheng23c/cheng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Identification of the Adversary from a Single Adversarial Example},
  url = {https://proceedings.mlr.press/v202/cheng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023parallel,
  abstract = {Contextual bandit algorithms appear in several applications, such as online advertisement and recommendation systems like personalized education or personalized medicine. Individually-tailored recommendations boost the performance of the underlying application; nevertheless, providing individual suggestions becomes costly and even implausible as the number of users grows. As such, to efficiently serve the demands of several users in modern applications, it is imperative to identify the underlying users' clusters, i.e., the groups of users for which a single recommendation might be (near-)optimal. We propose CLUB-HG, a novel algorithm that integrates a game-theoretic approach into clustering inference. Our algorithm achieves Nash equilibrium at each inference step and discovers the underlying clusters. We also provide regret analysis within a standard linear stochastic noise setting.},
  author = {Xiaotong Cheng and Cheng Pan and Setareh Maghsudi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023parallel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5485--5503},
  pdf = {https://proceedings.mlr.press/v202/cheng23d/cheng23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Parallel Online Clustering of Bandits via Hedonic Game},
  url = {https://proceedings.mlr.press/v202/cheng23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cheng2023understanding,
  abstract = {In this paper, we study the role of feedback in online learning with switching costs. It has been shown that the minimax regret is $\widetilde{\Theta}(T^{2/3})$ under bandit feedback and improves to $\widetilde{\Theta}(\sqrt{T})$ under full-information feedback, where $T$ is the length of the time horizon. However, it remains largely unknown how the amount and type of feedback generally impact regret. To this end, we first consider the setting of bandit learning with extra observations; that is, in addition to the typical bandit feedback, the learner can freely make a total of $B_{\mathrm{ex}}$ extra observations. We fully characterize the minimax regret in this setting, which exhibits an interesting phase-transition phenomenon: when $B_{\mathrm{ex}} = O(T^{2/3})$, the regret remains $\widetilde{\Theta}(T^{2/3})$, but when $B_{\mathrm{ex}} = \Omega(T^{2/3})$, it becomes $\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$, which improves as the budget $B_{\mathrm{ex}}$ increases.},
  author = {Duo Cheng and Xingyu Zhou and Bo Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cheng2023understanding.pdf:pdf},
  mdate = {2023-09-22},
  pages = {5521--5543},
  pdf = {https://proceedings.mlr.press/v202/cheng23f/cheng23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding the Role of Feedback in Online Learning with Switching Costs},
  url = {https://proceedings.mlr.press/v202/cheng23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chidambaram2023provably,
  abstract = {Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show empirically that these theoretical insights extend to the practical settings of image benchmarks modified to have additional synthetic features.},
  author = {Muthu Chidambaram and Xiang Wang and Chenwei Wu and Rong Ge},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chidambaram2023provably.pdf:pdf},
  mdate = {2024-06-03},
  pages = {5563--5599},
  pdf = {https://proceedings.mlr.press/v202/chidambaram23a/chidambaram23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup},
  url = {https://proceedings.mlr.press/v202/chidambaram23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chidambaram2023hiding,
  abstract = {Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. In this work, we focus on this over-realized regime and study a masking-based sparse coding objective that models the case where not all training data is available. We prove that with high probability, this masking-based approach learns a dictionary where each atom is close to one of the ground-truth dictionary atoms.},
  author = {Muthu Chidambaram and Chenwei Wu and Yu Cheng and Rong Ge},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chidambaram2023hiding.pdf:pdf},
  mdate = {2024-08-02},
  pages = {5600--5615},
  pdf = {https://proceedings.mlr.press/v202/chidambaram23b/chidambaram23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hiding Data Helps: On the Benefits of Masking for Sparse Coding},
  url = {https://proceedings.mlr.press/v202/chidambaram23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chien2023pina,
  abstract = {The eXtreme Multi-label Classification (XMC) problem seeks to find relevant labels from an exceptionally large label space. Most of the existing XMC learners focus on the extraction of semantic features from input query text. However, conventional XMC studies usually neglect the side information of instances and labels, which can be of use in many real-world applications such as recommendation systems and e-commerce product search. We propose Predicted Instance Neighborhood Aggregation (PINA), a data enhancement method for the general XMC problem that leverages beneficial side information. Unlike most existing XMC frameworks that treat labels and input instances as featureless indicators and independent entries, PINA extracts information from the label metadata and the correlations among training instances. Extensive experimental results demonstrate the consistent gain of PINA on various XMC tasks compared to the state-of-the-art methods: PINA offers a gain in accuracy compared to standard XR-Transformers on five public benchmark datasets. Moreover, PINA achieves a ~5% gain in accuracy on the largest dataset LF-AmazonTitles-1.3M.},
  author = {Eli Chien and Jiong Zhang and Cho-Jui Hsieh and Jyun-Yu Jiang and Wei-Cheng Chang and Olgica Milenkovic and Hsiang-Fu Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chien2023pina.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5616--5630},
  pdf = {https://proceedings.mlr.press/v202/chien23a/chien23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PINA: Leveraging Side Information in eXtreme Multi-label Classification via Predicted Instance Neighborhood Aggregation},
  url = {https://proceedings.mlr.press/v202/chien23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chiu2023tight,
  abstract = {Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatically fewer variables comparable to much weaker LP methods. Despite nonconvexity, we show how off-the-shelf local optimization algorithms can be used to achieve and to certify global optimality in polynomial time. Our experiments find that the nonconvex relaxation almost completely closes the gap towards exact certification of adversarially trained models.},
  author = {Hong-Ming Chiu and Richard Y. Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chiu2023tight.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5631--5660},
  pdf = {https://proceedings.mlr.press/v202/chiu23a/chiu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations},
  url = {https://proceedings.mlr.press/v202/chiu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cho2023neural,
  abstract = {Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors, especially in lower dimensional space, than the baseline models. The time-warping model is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.},
  author = {Cheol Jun Cho and Edward F. Chang and Gopala Krishna Anumanchipalli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cho2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5661--5676},
  pdf = {https://proceedings.mlr.press/v202/cho23a/cho23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data},
  url = {https://proceedings.mlr.press/v202/cho23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cho2023convergence,
  abstract = {Federated Averaging (FedAvg) and its variants are the most popular optimization algorithms in federated learning (FL). Previous convergence analyses of FedAvg either assume full client participation or partial client participation where the clients can be uniformly sampled. However, in practical cross-device FL systems, only a subset of clients that satisfy local criteria such as battery status, network connectivity, and maximum participation frequency requirements (to ensure privacy) are available for training at a given time. As a result, client availability follows a natural cyclic pattern. We provide (to our knowledge) the first theoretical framework to analyze the convergence of FedAvg with cyclic client participation with several different client optimizers such as GD, SGD, and shuffled SGD. Our analysis discovers that cyclic client participation can achieve a faster asymptotic convergence rate than vanilla FedAvg with uniform client participation under suitable conditions, providing valuable insights into the design of client sampling protocols.},
  author = {Yae Jee Cho and Pranay Sharma and Gauri Joshi and Zheng Xu and Satyen Kale and Tong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cho2023convergence.pdf:pdf},
  mdate = {2024-10-06},
  pages = {5677--5721},
  pdf = {https://proceedings.mlr.press/v202/cho23b/cho23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Convergence of Federated Averaging with Cyclic Client Participation},
  url = {https://proceedings.mlr.press/v202/cho23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choi2023semiparametric,
  abstract = {Contextual dynamic pricing is a problem of setting prices based on current contextual information and previous sales history to maximize revenue. A popular approach is to postulate a distribution of customer valuation as a function of contextual information and the baseline valuation. A semi-parametric setting, where the context effect is parametric and the baseline is nonparametric, is of growing interest due to its flexibility. A challenge is that customer valuation is almost never observable in practice and is instead type-I interval censored by the offered price. To address this challenge, we propose a novel semi-parametric contextual pricing algorithm for stochastic contexts, called the epoch-based Cox proportional hazards Contextual Pricing (CoxCP) algorithm.},
  author = {Young-Geun Choi and Gi-Soo Kim and Yunseo Choi and Wooseong Cho and Myunghee Cho Paik and Min-hwan Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choi2023semiparametric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5771--5786},
  pdf = {https://proceedings.mlr.press/v202/choi23c/choi23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi-Parametric Contextual Pricing Algorithm using Cox Proportional Hazards Model},
  url = {https://proceedings.mlr.press/v202/choi23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choi2023overfitting,
  abstract = {Compact representation of multimedia signals using implicit neural representations (INRs) has advanced significantly over the past few years, and recent works address their applications to video. Existing studies on video INR have focused on network architecture design as all video information is contained within network parameters. Here, we propose a new paradigm in efficient INR for videos based on the idea of strong lottery ticket (SLT) hypothesis (Zhou et al., 2019), which demonstrates the possibility of finding an accurate subnetwork mask, called supermask, for a randomly initialized classification network without weight training. Specifically, we train multiple supermasks with a hierarchical structure for a randomly initialized image-wise video representation model without weight updates.},
  author = {Hee Min Choi and Hyoa Kang and Dokwan Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choi2023overfitting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5748--5770},
  pdf = {https://proceedings.mlr.press/v202/choi23b/choi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Is Overfitting Necessary for Implicit Video Representation?},
  url = {https://proceedings.mlr.press/v202/choi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choi2023restoration,
  abstract = {Denoising diffusion models (DDMs) have recently attracted increasing attention by showing impressive synthesis quality. DDMs are built on a diffusion process that pushes data to the noise distribution and the models learn to denoise. In this paper, we establish the interpretation of DDMs in terms of image restoration (IR). Integrating IR literature allows us to use an alternative objective and diverse forward processes, not confining to the diffusion process. By imposing prior knowledge on the loss function grounded on MAP-based estimation, we eliminate the need for the expensive sampling of DDMs. Also, we propose a multi-scale training, which improves the performance compared to the diffusion process, by taking advantage of the flexibility of the forward process.},
  author = {Jaemoo Choi and Yesom Park and Myungjoo Kang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choi2023restoration.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5787--5816},
  pdf = {https://proceedings.mlr.press/v202/choi23d/choi23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Restoration based Generative Models},
  url = {https://proceedings.mlr.press/v202/choi23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choi2023conceptbased,
  abstract = {Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe deployment of deep neural network (DNN) classifiers. While a myriad of methods have focused on improving the performance of OOD detectors, a critical gap remains in interpreting their decisions. We help bridge this gap by providing explanations for OOD detectors based on learned high-level concepts. We first propose two new metrics for assessing the effectiveness of a particular set of concepts for explaining OOD detectors: 1) detection completeness, which quantifies the sufficiency of concepts for explaining an OOD-detector's decisions, and 2) concept separability, which captures the distributional separation between in-distribution and OOD data in the concept space. Based on these metrics, we propose an unsupervised framework for learning a set of concepts that satisfy the desired properties of high detection completeness and concept separability, and demonstrate its effectiveness in providing concept-based explanations for diverse off-the-shelf OOD detectors.},
  author = {Jihye Choi and Jayaram Raghuram and Ryan Feng and Jiefeng Chen 0001 and Somesh Jha and Atul Prakash 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choi2023conceptbased.pdf:pdf},
  mdate = {2024-10-06},
  pages = {5817--5837},
  pdf = {https://proceedings.mlr.press/v202/choi23e/choi23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Concept-based Explanations for Out-of-Distribution Detectors},
  url = {https://proceedings.mlr.press/v202/choi23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choo2023active,
  abstract = {We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) G* while minimizing the number of interventions made. In our setting, we are additionally given side information about G* as advice, e.g. a DAG G purported to be G*. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on algorithms with predictions. When the advice is a DAG G, we design an adaptive search algorithm to recover G* whose intervention cost is at most O(max{1, log ψ}) times the cost for verifying G*; here, ψ is a distance measure between G and G* that is upper bounded by the number of edges that differ between the two graphs.},
  author = {Davin Choo and Themistoklis Gouleakis and Arnab Bhattacharyya 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choo2023active.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5838--5867},
  pdf = {https://proceedings.mlr.press/v202/choo23a/choo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Active causal structure learning with advice},
  url = {https://proceedings.mlr.press/v202/choo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choo2023new,
  abstract = {Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. With respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost.},
  author = {Davin Choo and Kirankumar Shiragur},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choo2023new.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5868--5903},
  pdf = {https://proceedings.mlr.press/v202/choo23b/choo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {New metrics and search algorithms for weighted causal DAGs},
  url = {https://proceedings.mlr.press/v202/choo23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chopin2023computational,
  abstract = {This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle filter, which involves Doob's $h$-transforms that are typically intractable. We propose a computational framework to approximate these $h$-transforms by solving the underlying backward Kolmogorov equations using nonlinear Feynman-Kac formulas and neural networks. The methodology allows one to train a locally optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more efficient than state-of-the-art particle filters in the regime of highly informative observations, when the observations are extreme under the model, and if the state dimension is large.},
  author = {Nicolas Chopin and Andras Fulop and Jeremy Heng and Alexandre H. Thiery},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chopin2023computational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5904--5923},
  pdf = {https://proceedings.mlr.press/v202/chopin23a/chopin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Computational Doob h-transforms for Online Filtering of Discretely Observed Diffusions},
  url = {https://proceedings.mlr.press/v202/chopin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choquettechoo2023multiepoch,
  abstract = {We introduce new differentially private (DP) mechanisms for gradient-based machine learning (ML) with multiple passes (epochs) over a dataset, substantially improving the achievable privacy-utility-computation tradeoffs. We formalize the problem of DP mechanisms for adaptive streams with multiple participations and introduce a non-trivial extension of online matrix factorization DP mechanisms to our setting. This includes establishing the necessary theory for sensitivity calculations and efficient computation of optimal matrices. For some applications like $>10,000$ SGD steps, applying these optimal techniques becomes computationally expensive. We thus design an efficient Fourier-transform-based mechanism with only a minor utility loss. Extensive empirical evaluation on both example-level DP for image classification and user-level DP for language modeling demonstrate substantial improvements over all previous methods, including the widely-used DP-SGD.},
  author = {Christopher A. Choquette-Choo and Hugh Brendan McMahan and J. Keith Rush and Abhradeep Guha Thakurta},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choquettechoo2023multiepoch.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5924--5963},
  pdf = {https://proceedings.mlr.press/v202/choquette-choo23a/choquette-choo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning},
  url = {https://proceedings.mlr.press/v202/choquette-choo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choromanski2023taming,
  abstract = {We introduce in this paper the mechanism of graph random features (GRFs). GRFs can be used to construct unbiased randomized estimators of several important kernels defined on graphs' nodes, in particular the regularized Laplacian kernel. As regular RFs for non-graph kernels, they provide means to scale up kernel methods defined on graphs to larger networks. Importantly, they give substantial computational gains also for smaller graphs, while applied in downstream applications. Consequently, GRFs address the notoriously difficult problem of cubic (in the number of the nodes of the graph) time complexity of graph kernels algorithms. We provide a detailed theoretical analysis of GRFs and an extensive empirical evaluation: from speed tests, through Frobenius relative error analysis to kmeans graph-clustering with graph kernels.},
  author = {Krzysztof Marcin Choromanski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choromanski2023taming.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5964--5977},
  pdf = {https://proceedings.mlr.press/v202/choromanski23a/choromanski23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Taming graph kernels with random features},
  url = {https://proceedings.mlr.press/v202/choromanski23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choromanski2023efficient,
  abstract = {We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct.},
  author = {Krzysztof Marcin Choromanski and Arijit Sehanobish and Han Lin and Yunfan Zhao and Eli Berger and Tetiana Parshakova and Alvin Pan and David Watkins and Tianyi Zhang and Valerii Likhosherstov and Somnath Basu Roy Chowdhury and Kumar Avinava Dubey and Deepali Jain and Tms Sarls and Snigdha Chaturvedi and Adrian Weller},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choromanski2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {5978--6004},
  pdf = {https://proceedings.mlr.press/v202/choromanski23b/choromanski23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Graph Field Integrators Meet Point Clouds},
  url = {https://proceedings.mlr.press/v202/choromanski23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{choshen2023contrabar,
  abstract = {In meta reinforcement learning (meta RL), an agent seeks a Bayes-optimal policy -- the optimal policy when facing an unknown task that is sampled from some known task distribution. Previous approaches tackled this problem by inferring a belief over task parameters, using variational inference methods. Motivated by recent successes of contrastive learning approaches in RL, such as contrastive predictive coding (CPC), we investigate whether contrastive methods can be used for learning Bayes-optimal behavior. We begin by proving that representations learned by CPC are indeed sufficient for Bayes optimality. Based on this observation, we propose a simple meta RL algorithm that uses CPC in lieu of variational belief inference. Our method, ContraBAR, achieves comparable performance to state-of-the-art in domains with state-based observation and circumvents the computational toll of future observation reconstruction, enabling learning in domains with image-based observations.},
  author = {Era Choshen and Aviv Tamar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/choshen2023contrabar.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6005--6027},
  pdf = {https://proceedings.mlr.press/v202/choshen23a/choshen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ContraBAR: Contrastive Bayes-Adaptive Deep RL},
  url = {https://proceedings.mlr.press/v202/choshen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chourasia2023forget,
  abstract = {Unlearning algorithms aim to remove deleted data's influence from trained models at a cost lower than full retraining. However, prior guarantees of unlearning in literature are flawed and don't protect the privacy of deleted records. We show that when users delete their data as a function of published models, records in a database become interdependent. So, even retraining a fresh model after deletion of a record doesn't ensure its privacy. Secondly, unlearning algorithms that cache partial computations to speed up the processing can leak deleted information over a series of releases, violating the privacy of deleted records in the long run. To address these, we propose a sound deletion guarantee and show that the privacy of existing records is necessary for the privacy of deleted records. Under this notion, we propose an accurate, computationally efficient, and secure machine unlearning algorithm based on noisy gradient descent.},
  author = {Rishav Chourasia and Neil Shah},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chourasia2023forget.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6028--6073},
  pdf = {https://proceedings.mlr.press/v202/chourasia23a/chourasia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Forget Unlearning: Towards True Data-Deletion in Machine Learning},
  url = {https://proceedings.mlr.press/v202/chourasia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chowdhury2023beam,
  abstract = {We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. Our experiments show that BT-Cell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps.},
  author = {Jishnu Ray Chowdhury and Cornelia Caragea},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chowdhury2023beam.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28768--28791},
  pdf = {https://proceedings.mlr.press/v202/ray-chowdhury23a/ray-chowdhury23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beam Tree Recursive Cells},
  url = {https://proceedings.mlr.press/v202/ray-chowdhury23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chowdhury2023monotonic,
  abstract = {We explore different ways to utilize position-based cross-attention in seq2seq networks to enable length generalization in algorithmic tasks. We show that a simple approach of interpolating the original and reversed encoded representations combined with relative attention allows near-perfect length generalization for both forward and reverse lookup tasks or copy tasks that had been generally hard to tackle. We also devise harder diagnostic tasks where the relative distance of the ideal attention position varies with timestep. In such settings, the simple interpolation trick with relative attention is not sufficient. We introduce novel variants of location attention building on top of Dubois et al. (2020) to address the new diagnostic tasks. We also show the benefits of our approaches for length generalization in SCAN (Lake \& Baroni, 2018) and CFQ (Keysers et al., 2020).},
  author = {Jishnu Ray Chowdhury and Cornelia Caragea},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chowdhury2023monotonic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28792--28808},
  pdf = {https://proceedings.mlr.press/v202/ray-chowdhury23b/ray-chowdhury23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Monotonic Location Attention for Length Generalization},
  url = {https://proceedings.mlr.press/v202/ray-chowdhury23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chowdhury2023patchlevel,
  abstract = {We show for the first time that patch-level mixture of experts (pMoE) provably reduces the required number of training samples to achieve desirable generalization by a factor in the polynomial order of n/l, where n is the number of patches and l is the number of patches routed to each expert. The advantage comes from the discriminative routing property that pMoE routers can filter label-irrelevant patches and route similar class-discriminative patches to the same expert. Our theoretical analysis is supported by experimental results on MNIST, CIFAR-10, and CelebA, showing that pMoE can avoid learning spurious correlations and achieve better sample efficiency compared to single-expert models.},
  author = {Mohammed Nowaz Rabbani Chowdhury and Shuai Zhang and Meng Wang and Sijia Liu and Pin-Yu Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chowdhury2023patchlevel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6074--6114},
  pdf = {https://proceedings.mlr.press/v202/chowdhury23a/chowdhury23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks},
  url = {https://proceedings.mlr.press/v202/chowdhury23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chowers2023what,
  abstract = {It has previously been reported that the representation that is learned in the first layer of deep Convolutional Neural Networks (CNNs) is highly consistent across initializations and architectures. In this work, we quantify this consistency by considering the first layer as a filter bank and measuring its energy distribution. We find that the energy distribution is very different from that of the initial weights and is remarkably consistent across random initializations, datasets, architectures and even when the CNNs are trained with random labels. We show that this consistency can be understood from a linear systems perspective, and that the first layer of these CNNs indeed perform approximate whitening of their inputs.},
  author = {Rhea Chowers and Yair Weiss},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chowers2023what.pdf:pdf},
  mdate = {2024-10-06},
  pages = {6115--6139},
  pdf = {https://proceedings.mlr.press/v202/chowers23a/chowers23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {What do {CNN}s Learn in the First Layer and Why? {A} Linear Systems Perspective},
  url = {https://proceedings.mlr.press/v202/chowers23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{christofidellis2023unifying,
  abstract = {Neural language models have been applied to chemistry, offering generative solutions for molecular design and synthesis planning, with potential to fuel data-driven automation in scientific discovery. However, specialized models are typically required for each task, and the main obstacle is the lack of unified representation between natural language and chemical representations. We propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. This unified approach can robustly and efficiently accelerate discovery in physical sciences by superseding problem-specific fine-tuning and enhancing human-model interactions.},
  author = {Dimitrios Christofidellis and Giorgio Giannone and Jannis Born and Ole Winther and Teodoro Laino and Matteo Manica},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/christofidellis2023unifying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6140--6157},
  pdf = {https://proceedings.mlr.press/v202/christofidellis23a/christofidellis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unifying Molecular and Textual Representations via Multi-task Language Modelling},
  url = {https://proceedings.mlr.press/v202/christofidellis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chu2023wasserstein,
  abstract = {Graph size generalization is hard for Message Passing Neural Networks (MPNNs). The graph-level classification performance of MPNNs degrades across various graph sizes. Recent theoretical studies reveal that a slow uncontrollable convergence rate with respect to graph size could adversely affect the size generalization. We use Wasserstein barycenters as graph-level consensus to combat node-level correlations. We introduce a Wasserstein barycenter matching (WBM) layer that represents an input graph by Wasserstein distances between its MPNN-filtered node embeddings versus some learned class-wise barycenters. Theoretically, we show that the convergence rate of an MPNN with a WBM layer is controllable and independent of the dimensionality of the signal-generating space. Thus MPNNs with WBM layers are less susceptible to slow uncontrollable convergence rate and size variations.},
  author = {Xu Chu and Yujie Jin and Xin Wang and Shanghang Zhang and Yasha Wang and Wenwu Zhu and Hong Mei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chu2023wasserstein.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6158--6184},
  pdf = {https://proceedings.mlr.press/v202/chu23a/chu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Wasserstein Barycenter Matching for Graph Size Generalization of Message Passing Neural Networks},
  url = {https://proceedings.mlr.press/v202/chu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chu2023shapeguided,
  abstract = {We present a shape-guided expert-learning framework to tackle the problem of unsupervised 3D anomaly detection. The method is established on the effectiveness of two specialized expert models and their synergy to localize anomalous regions from color and shape modalities. The first expert utilizes geometric information to probe 3D structural anomalies by modeling the implicit distance fields around local shapes. The second expert considers the 2D RGB features associated with the first expert to identify color appearance irregularities on the local shapes. The two experts are used to build dual memory banks from the anomaly-free training samples and perform shape-guided inference to pinpoint defects in the testing samples.},
  author = {Yu-Min Chu and Chieh Liu and Ting-I Hsieh and Hwann-Tzong Chen and Tyng-Luh Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chu2023shapeguided.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6185--6194},
  pdf = {https://proceedings.mlr.press/v202/chu23b/chu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Shape-Guided Dual-Memory Learning for {3D} Anomaly Detection},
  url = {https://proceedings.mlr.press/v202/chu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chu2023multiply,
  abstract = {Typical off-policy evaluation (OPE) and off-policy learning (OPL) are not well-defined problems under truncation by death, where the outcome of interest is not defined after some events such as death. The standard OPE no longer yields consistent estimators, and the standard OPL results in suboptimal policies. We formulate OPE and OPL using principal stratification under truncation by death and propose a survivor value function for a subpopulation whose outcomes are always defined regardless of treatment conditions. We establish a novel identification strategy under principal ignorability and derive the semiparametric efficiency bound of an OPE estimator. We propose multiply robust estimators for OPE and OPL and show that the proposed estimators are consistent and asymptotically normal even with flexible semi/nonparametric models for nuisance functions approximation.},
  author = {Jianing Chu and Shu Yang and Wenbin Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chu2023multiply.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6195--6227},
  pdf = {https://proceedings.mlr.press/v202/chu23c/chu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multiply Robust Off-policy Evaluation and Learning under Truncation by Death},
  url = {https://proceedings.mlr.press/v202/chu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chuang2023infoot,
  abstract = {Optimal transport aligns samples across distributions by minimizing the transportation cost between them. However, existing optimal transport methods ignore coherence structure in the data such as clusters, do not handle outliers well, and cannot integrate new data points. To address these drawbacks, we propose InfoOT, an information-theoretic extension of optimal transport that maximizes the mutual information between domains while minimizing geometric distances. The resulting objective can be formulated as a generalized optimal transport problem and solved efficiently by projected gradient descent. This formulation yields a new projection method that is robust to outliers and generalizes to unseen samples. Empirically, InfoOT improves the quality of alignments across benchmarks in domain adaptation, cross-domain retrieval, and single-cell alignment.},
  author = {Ching-Yao Chuang and Stefanie Jegelka and David Alvarez-Melis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chuang2023infoot.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6228--6242},
  pdf = {https://proceedings.mlr.press/v202/chuang23a/chuang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{InfoOT}: Information Maximizing Optimal Transport},
  url = {https://proceedings.mlr.press/v202/chuang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{chughtai2023toy,
  abstract = {Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.},
  author = {Bilal Chughtai and Lawrence Chan and Neel Nanda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/chughtai2023toy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6243--6267},
  pdf = {https://proceedings.mlr.press/v202/chughtai23a/chughtai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations},
  url = {https://proceedings.mlr.press/v202/chughtai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cirone2023neural,
  abstract = {Motivated by the paradigm of reservoir computing, we consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations (Neural CDEs), a unified architecture which encompasses both RNNs and ResNets. We show that in the infinite-width-depth limit and under proper scaling, these architectures converge weakly to Gaussian processes indexed on some spaces of continuous paths and with kernels satisfying certain partial differential equations (PDEs) varying according to the choice of activation function φ, extending the results of Hayou (2022); Hayou & Yang (2023) to the controlled and homogeneous case. In the special, homogeneous, case where φ is the identity, we show that the equation reduces to a linear PDE and the limiting kernel agrees with the signature kernel of Salvi et al. (2021a). We name this new family of limiting kernels neural signature kernels. Finally, we show that in the infinite-depth regime, finite-width controlled ResNets converge in distribution to Neural CDEs with random vector fields which, depending on whether the weights are shared across layers, are either time-independent and Gaussian or behave like a matrix-valued Brownian motion.},
  author = {Nicola Muca Cirone and Maud Lemercier and Cristopher Salvi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cirone2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25358--25425},
  pdf = {https://proceedings.mlr.press/v202/muca-cirone23a/muca-cirone23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural signature kernels as infinite-width-depth-limits of controlled {ResNets}},
  url = {https://proceedings.mlr.press/v202/muca-cirone23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{clarkson2023distribution,
  abstract = {Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure. We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We verify the efficacy of our approach across standard benchmark datasets using popular GNN models.},
  author = {Jase Clarkson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/clarkson2023distribution.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6268--6278},
  pdf = {https://proceedings.mlr.press/v202/clarkson23a/clarkson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distribution Free Prediction Sets for Node Classification},
  url = {https://proceedings.mlr.press/v202/clarkson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cohen2023fewsample,
  abstract = {We present a new method for few-sample supervised feature selection. Our method first learns the manifold of the feature space of each class using kernels capturing multi-feature associations. Then, based on Riemannian geometry, a composite kernel is computed, extracting the differences between the learned feature associations. Finally, a feature selection score based on spectral analysis is proposed. Considering multi-feature associations makes our method multivariate by design, which allows for the extraction of the hidden manifold underlying the features and avoids overfitting, facilitating few-sample feature selection. We demonstrate the efficacy of our method on illustrative examples and several benchmarks, where our method shows higher accuracy in selecting informative features compared to competing methods. Additionally, we show that our feature selection approach leads to improved classification and better generalization when applied to test data.},
  author = {David Cohen and Tal Shnitzer and Yuval Kluger and Ronen Talmon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cohen2023fewsample.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6296--6319},
  pdf = {https://proceedings.mlr.press/v202/cohen23b/cohen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Few-Sample Feature Selection via Feature Manifold Learning},
  url = {https://proceedings.mlr.press/v202/cohen23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cole2023spatial,
  abstract = {Estimating the geographical range of a species from sparse observations is a challenging and important geospatial prediction problem. Given a set of locations where a species has been observed, the goal is to build a model to predict whether the species is present or absent at any location. This problem has a long history in ecology, but traditional methods struggle to take advantage of emerging large-scale crowdsourced datasets which can include tens of millions of records for hundreds of thousands of species. In this work, we use Spatial Implicit Neural Representations (SINRs) to jointly estimate the geographical range of 47k species simultaneously. We find that our approach scales gracefully, making increasingly better predictions as we increase the number of species and the amount of data per species when training.},
  author = {Elijah Cole and Grant Van Horn and Christian Lange and Alexander Shepard and Patrick Leary and Pietro Perona and Scott Loarie and Oisin Mac Aodha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cole2023spatial.pdf:pdf},
  mdate = {2025-01-27},
  pages = {6320--6342},
  pdf = {https://proceedings.mlr.press/v202/cole23a/cole23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Spatial Implicit Neural Representations for Global-Scale Species Mapping},
  url = {https://proceedings.mlr.press/v202/cole23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{coletta2023kshap,
  abstract = {Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. We propose K-SHAP, a Policy Clustering algorithm that learns to group anonymous state-action pairs according to the agent policies. The method addresses the challenge of learning agent strategies from anonymous state-action pairs by proposing a novel policy-clustering method in which we group state-action observations that belong to agents sharing the same behavior or policy.},
  author = {Andrea Coletta and Svitlana Vyetrenko and Tucker Balch},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/coletta2023kshap.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6343--6363},
  pdf = {https://proceedings.mlr.press/v202/coletta23a/coletta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{K-SHAP}: Policy Clustering Algorithm for Anonymous Multi-Agent State-Action Pairs},
  url = {https://proceedings.mlr.press/v202/coletta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{compagnoni2023sde,
  abstract = {The {SAM} (Sharpness-Aware Minimization) optimizer has recently attracted significant attention due to its increased performance over more classical variants of stochastic gradient descent. The main contribution is the derivation of continuous-time models (in the form of {SDEs}) for {SAM} and two of its variants, both for the full-batch and mini-batch settings. We demonstrate that these {SDEs} are rigorous approximations of the real discrete-time algorithms (in a weak sense, scaling linearly with the learning rate). Using these models, we then offer an explanation of why {SAM} prefers flat minima over sharp ones by showing that it minimizes an implicitly regularized loss with a Hessian-dependent noise structure.},
  author = {Enea Monzio Compagnoni and Luca Biggio and Antonio Orvieto and Frank Norbert Proske and Hans Kersting and Aurlien Lucchi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/compagnoni2023sde.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25209--25253},
  pdf = {https://proceedings.mlr.press/v202/monzio-compagnoni23a/monzio-compagnoni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An {SDE} for Modeling {SAM}: Theory and Insights},
  url = {https://proceedings.mlr.press/v202/monzio-compagnoni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{connolly2023taskspecific,
  abstract = {Understanding causality should be a core requirement of any attempt to build real impact through {AI}. Due to the inherent unobservability of counterfactuals, large randomised trials ({RCTs}) are the standard for causal inference. However, large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to {RCTs}, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications.},
  author = {Bethany Connolly and Kim Moore and Tobias Schwedes and Alexander Adam and Gary Willis and Ilya Feige and Christopher Frye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/connolly2023taskspecific.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6384--6401},
  pdf = {https://proceedings.mlr.press/v202/connolly23a/connolly23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Task-specific experimental design for treatment effect estimation},
  url = {https://proceedings.mlr.press/v202/connolly23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cornacchia2023mathematical,
  abstract = {Curriculum learning ({CL}), training using samples that are generated and presented in a meaningful order, was introduced in the machine learning context around a decade ago, but has had very little mathematical justification for its advantages despite extensive empirical use. We introduce a {CL} model for learning the class of k-parities on d bits of a binary string with a neural network trained by stochastic gradient descent ({SGD}). Our key finding is that a wise choice of training examples, involving two or more product distributions, allows to reduce significantly the computational cost of learning this class of functions, compared to learning under the uniform distribution.},
  author = {Elisabetta Cornacchia and Elchanan Mossel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cornacchia2023mathematical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6402--6423},
  pdf = {https://proceedings.mlr.press/v202/cornacchia23a/cornacchia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Mathematical Model for Curriculum Learning for Parities},
  url = {https://proceedings.mlr.press/v202/cornacchia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{covert2023learning,
  abstract = {Feature selection is a valuable approach for reducing data acquisition costs in machine learning. The standard approach trains models with static feature subsets. We consider the dynamic feature selection ({DFS}) problem where a model sequentially queries features based on presently available information. While {DFS} is often addressed with reinforcement learning, we explore a simpler approach of greedily selecting features based on their conditional mutual information. We show that this approach can be surprisingly effective, performing comparably to or better than more complex methods while being orders of magnitude faster.},
  author = {Ian Connick Covert and Wei Qiu and Mingyu Lu and Nayoon Kim and Nathan J. White and Su-In Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/covert2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6424--6447},
  pdf = {https://proceedings.mlr.press/v202/covert23a/covert23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Maximize Mutual Information for Dynamic Feature Selection},
  url = {https://proceedings.mlr.press/v202/covert23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023rethinking,
  abstract = {Contrastive learning has shown outstanding performances in both supervised and unsupervised learning, and has recently been introduced to solve weakly supervised learning problems such as semi-supervised learning and noisy label learning. Despite empirical evidence showing that semi-supervised labels improve the representations of contrastive learning, it remains unknown if noisy supervised information can be directly used in training instead of after manual denoising. To explore the mechanical differences between semi-supervised and noisy-labeled information in helping contrastive learning, we establish a unified theoretical framework of contrastive learning under weak supervision. By translating the weakly supervised information into a similarity graph under the framework of spectral clustering based on the posterior probability of weak labels, we establish the downstream classification error bound. We prove that semi-supervised labels improve the downstream error bound whereas noisy labels have limited effects under such a paradigm.},
  author = {Jingyi Cui and Weiran Huang 0001 and Yifei Wang 0001 and Yisen Wang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023rethinking.pdf:pdf},
  mdate = {2023-09-06},
  pages = {6448--6467},
  pdf = {https://proceedings.mlr.press/v202/cui23a/cui23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethinking Weak Supervision in Helping Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/cui23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023bayesoptimal,
  abstract = {We consider the problem of learning a target function corresponding to a deep, extensive-width, non-linear neural network with random {G}aussian weights. We consider the asymptotic limit where the number of samples, the input dimension and the network width are proportionally large. We propose a closed-form expression for the {B}ayes-optimal test error, for regression and classification tasks. We further compute closed-form expressions for the test errors of ridge regression, kernel and random features regression. We find, in particular, that optimally regularized ridge regression, as well as kernel regression, achieve {B}ayes-optimal performances, while the logistic loss yields a near-optimal test error for classification.},
  author = {Hugo Cui and Florent Krzakala and Lenka Zdeborová},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023bayesoptimal.pdf:pdf},
  mdate = {2024-02-05},
  pages = {6468--6521},
  pdf = {https://proceedings.mlr.press/v202/cui23b/cui23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayes-optimal Learning of Deep Random Networks of Extensive-width},
  url = {https://proceedings.mlr.press/v202/cui23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023general,
  abstract = {The generalization performance of machine learning methods depends heavily on the quality of data representation. However, existing researches rarely consider representation learning from the perspective of generalization error. In this paper, we prove that generalization error of representation learning function can be estimated effectively by solving two convex optimization problems. Based on it, we propose a general representation learning framework. And then, we apply the proposed framework to two most commonly used nonlinear mapping methods, i.e., kernel based method and deep neural network ({DNN}), and thus design a kernel selection method and a {DNN} boosting framework, correspondingly. Finally, extensive experiments verify the effectiveness of the proposed methods.},
  author = {Junbiao Cui and Jianqing Liang and Qin Yue and Jiye Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023general.pdf:pdf},
  mdate = {2025-06-23},
  pages = {6522--6544},
  pdf = {https://proceedings.mlr.press/v202/cui23c/cui23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A General Representation Learning Framework with Generalization Performance Guarantees},
  url = {https://proceedings.mlr.press/v202/cui23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023scaling,
  abstract = {Dataset Distillation is a newly emerging area that aims to distill large datasets into much smaller and highly informative synthetic ones to accelerate training and reduce storage. Among various dataset distillation methods, trajectory-matching-based methods ({MTT}) have achieved {SOTA} performance in many tasks, e.g., on {CIFAR-10/100}. However, due to exorbitant memory consumption when unrolling optimization through {SGD} steps, {MTT} fails to scale to large-scale datasets such as {ImageNet-1K}. To answer the question whether we can scale this {SOTA} method to {ImageNet-1K} and whether its effectiveness on {CIFAR} transfers to {ImageNet-1K}, we first propose a procedure to exactly compute the unrolled gradient with constant memory complexity, which allows us to scale {MTT} to {ImageNet-1K} seamlessly with ~6x reduction in memory footprint. We further discover that it is challenging for {MTT} to handle datasets with a large number of classes, and propose a novel soft label assignment that drastically improves its convergence. The resulting algorithm sets new {SOTA} on {ImageNet-1K}: we can scale up to 50 {IPCs} (Image Per Class) on {ImageNet-1K} on a single {GPU} (all previous methods can only scale to 2 {IPCs} on {ImageNet-1K}), leading to the best accuracy (only 5.9% accuracy drop against full dataset training) while utilizing only 4.2% of the number of data points.},
  author = {Justin Cui and Ruochen Wang and Si Si and Cho-Jui Hsieh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023scaling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6565--6590},
  pdf = {https://proceedings.mlr.press/v202/cui23e/cui23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Up Dataset Distillation to {ImageNet-1K} with Constant Memory},
  url = {https://proceedings.mlr.press/v202/cui23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cui2023learning,
  abstract = {Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries, named as modulated queries, better capture the prior of object locations and categories in the different images. Equipped with our modulated queries, a wide range of {DETR}-based models achieve consistent and superior performance across multiple tasks (object detection, instance segmentation, panoptic segmentation) and on different benchmarks ({MS COCO}, {CityScapes}, {YoutubeVIS}).},
  author = {Yiming Cui and Linjie Yang and Haichao Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cui2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6591--6602},
  pdf = {https://proceedings.mlr.press/v202/cui23f/cui23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation},
  url = {https://proceedings.mlr.press/v202/cui23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{curth2023adaptive,
  abstract = {We study the problem of adaptively identifying patient subpopulations that benefit from a given treatment during a confirmatory clinical trial. This type of adaptive clinical trial has been thoroughly studied in biostatistics, but has been allowed only limited adaptivity so far. Here, we aim to relax classical restrictions on such designs and investigate how to incorporate ideas from the recent machine learning literature on adaptive and online experimentation to make trials more flexible and efficient.},
  author = {Alicia Curth and Alihan Hüyük and Mihaela van der Schaar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/curth2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6603--6622},
  pdf = {https://proceedings.mlr.press/v202/curth23a/curth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions},
  url = {https://proceedings.mlr.press/v202/curth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{curth2023search,
  abstract = {Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, and provide interesting insights into the relative (dis)advantages of different criteria alongside desiderata for the design of further illuminating empirical studies in this context.},
  author = {Alicia Curth and Mihaela van der Schaar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/curth2023search.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6623--6642},
  pdf = {https://proceedings.mlr.press/v202/curth23b/curth23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation},
  url = {https://proceedings.mlr.press/v202/curth23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cutkosky2023optimal,
  abstract = {We present new algorithms for optimizing non-smooth, non-convex stochastic objectives based on a novel analysis technique. This improves the current best-known complexity for finding a $(δ,ε)$-stationary point from $O(ε^{-4}δ^{-1})$ stochastic gradient queries to $O(ε^{-3}δ^{-1})$, which we also show to be optimal. Our primary technique is a reduction from non-smooth non-convex optimization to online learning, after which our results follow from standard regret bounds in online learning. For deterministic and second-order smooth objectives, applying more advanced optimistic online learning techniques enables a new complexity of $O(ε^{-1.5}δ^{-0.5})$. Our techniques also recover all optimal or best-known results for finding $ε$ stationary points of smooth or second-order smooth objectives in both stochastic and deterministic settings.},
  author = {Ashok Cutkosky and Harsh Mehta and Francesco Orabona},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cutkosky2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6643--6670},
  pdf = {https://proceedings.mlr.press/v202/cutkosky23a/cutkosky23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion},
  url = {https://proceedings.mlr.press/v202/cutkosky23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cuturi2023monge,
  abstract = {Optimal transport ({OT}) theory focuses, among all maps $T:\mathbb{R}^d\rightarrow \mathbb{R}^d$ that can morph a probability measure onto another, on those that are the "thriftiest", i.e. such that the averaged cost $c(x, T(x))$ between $x$ and its image $T(x)$ be as small as possible. Many computational approaches have been proposed to estimate such Monge maps when $c$ is the $\ell_2^2$ distance, e.g., using entropic maps or neural networks. We propose a new model for transport maps, built on a family of translation invariant costs $c(x, y):=h(x-y)$, where $h:=\tfrac{1}{2}\|\cdot\|_2^2+τ$ and $τ$ is a regularizer. We propose a generalization of the entropic map suitable for $h$, and highlight a surprising link tying it with the Bregman centroids of the divergence $D_h$ generated by $h$, and the proximal operator of $τ$. We show that choosing a sparsity-inducing norm for $τ$ results in maps that apply Occam's razor to transport, in the sense that the displacement vectors $\Delta(x):= T(x)-x$ they induce are sparse, with a sparsity pattern that varies depending on $x$. We showcase the ability of our method to estimate meaningful {OT} maps for high-dimensional single-cell transcription data, in the $34000$-$d$ space of gene counts for cells, without using dimensionality reduction, thus retaining the ability to interpret all displacements at the gene level.},
  author = {Marco Cuturi and Michal Klein and Pierre Ablin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cuturi2023monge.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6671--6682},
  pdf = {https://proceedings.mlr.press/v202/cuturi23a/cuturi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Monge, Bregman and Occam: Interpretable Optimal Transport in High-Dimensions with Feature-Sparse Maps},
  url = {https://proceedings.mlr.press/v202/cuturi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cyffers2023noisy,
  abstract = {We study differentially private ({DP}) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like {DP-SGD} and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers ({ADMM}) method, we use our general framework derive novel private {ADMM} algorithms for centralized, federated and fully decentralized learning. We establish strong privacy guarantees for these algorithms, leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees for the three algorithms using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.},
  author = {Edwige Cyffers and Aurélien Bellet and Debabrota Basu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cyffers2023noisy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6683--6711},
  pdf = {https://proceedings.mlr.press/v202/cyffers23a/cyffers23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Noisy Fixed-Point Iterations to Private {ADMM} for Centralized and Federated Learning},
  url = {https://proceedings.mlr.press/v202/cyffers23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dai2023chameleon,
  abstract = {In a federated learning ({FL}) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of {FL} backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrate that Chameleon significantly extends the backdoor lifespan over baselines by 1.2× ∼ 4×, for a wide range of image datasets, backdoor types, and model architectures.},
  author = {Yanbo Dai and Songze Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dai2023chameleon.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6712--6725},
  pdf = {https://proceedings.mlr.press/v202/dai23a/dai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning},
  url = {https://proceedings.mlr.press/v202/dai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dai2023multirobustbench,
  abstract = {We present the first unified framework for considering multiple attacks against machine learning models, allowing modeling of robustness against unforeseen attacks and robustness against unions of attacks. Using this framework, we present MultiRobustBench, the first leaderboard for benchmarking multi-attack evaluation which captures performance across attack types and attack strengths. We evaluate 16 defended models for robustness against 9 different attack types (L1, L2, Linf, Elastic, L1 JPEG, Linf JPEG, ReColor, StAdv, and LPIPS) at 20 different attack strengths (180 attacks total). Our analysis shows that while existing defenses have made progress in terms of average robustness across the set of attacks used, robustness against the worst-case attack is still a significant open problem.},
  author = {Sihui Dai and Saeed Mahloujifar and Chong Xiang 0001 and Vikash Sehwag and Pin-Yu Chen and Prateek Mittal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dai2023multirobustbench.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6760--6785},
  pdf = {https://proceedings.mlr.press/v202/dai23c/dai23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MultiRobustBench}: Benchmarking Robustness Against Multiple Attacks},
  url = {https://proceedings.mlr.press/v202/dai23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dai2023moderately,
  abstract = {Domain generalization (DG) aims to tackle the distribution shift between training domains and unknown target domains. Generating new domains is one of the most effective approaches, yet its performance gain depends on the distribution discrepancy between the generated and target domains. Distributionally robust optimization is promising for tackling distribution discrepancy by exploring domains in an uncertainty set. However, the uncertainty set may be overwhelmingly large, leading to low-confidence prediction in DG because a large uncertainty set could introduce domains containing semantically different factors from training domains. To address this issue, we propose to perform moderately distributional exploration (MODE) for domain generalization. Specifically, MODE performs distribution exploration in an uncertainty subset that shares the same semantic factors with the training domains. We show that MODE can endow models with provable generalization performance on unknown target domains, and experimental results demonstrate that MODE achieves competitive performance compared to state-of-the-art baselines.},
  author = {Rui Dai and Yonggang Zhang 0003 and Zhen Fang 0001 and Bo Han 0003 and Xinmei Tian 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dai2023moderately.pdf:pdf},
  mdate = {2025-01-29},
  pages = {6786--6817},
  pdf = {https://proceedings.mlr.press/v202/dai23d/dai23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Moderately Distributional Exploration for Domain Generalization},
  url = {https://proceedings.mlr.press/v202/dai23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{daley2023trajectoryaware,
  abstract = {Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios (traces) to combat the variance of the IS estimator. Unfortunately, once a trace has been cut, the effect cannot be easily reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time.},
  author = {Brett Daley and Martha White and Christopher Amato and Marlos C. Machado},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/daley2023trajectoryaware.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6818--6835},
  pdf = {https://proceedings.mlr.press/v202/daley23a/daley23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/daley23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{daneshmand2023efficient,
  abstract = {We consider particle gradient descent, which uses particles to represent a probability measure and performs gradient descent on particles in parallel, widely used to optimize functions of probability measures. This paper considers particle gradient descent with a finite number of particles and establishes its theoretical guarantees to optimize functions that are displacement convex in measures. Concretely, for Lipschitz displacement convex functions defined on probability over $\mathbb{R}^d$, we prove that $O(1/\varepsilon^2)$ particles and $O(d/\varepsilon^4)$ computations are sufficient to find the $\varepsilon$-optimal solutions. We further provide improved complexity bounds for optimizing smooth displacement convex functions. An application of our results proves the conjecture of no optimization-barrier up to permutation invariance, proposed by Entezari et al.},
  author = {Hadi Daneshmand and Jason D. Lee and Chi Jin 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/daneshmand2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6836--6854},
  pdf = {https://proceedings.mlr.press/v202/daneshmand23a/daneshmand23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient displacement convex optimization with particle gradient descent},
  url = {https://proceedings.mlr.press/v202/daneshmand23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dang2023multiple,
  abstract = {We propose a meta-ability decoupling (MAD) paradigm, which brings together various object navigation methods in an architecture system, allowing them to mutually enhance each other and evolve together. Based on the MAD paradigm, we design a multiple thinking (MT) model that leverages distinct thinking to abstract various meta-abilities. The method decouples meta-abilities from three aspects: input, encoding, and reward while employing the multiple thinking collaboration (MTC) module to promote mutual cooperation between thinking.},
  author = {Ronghao Dang and Lu Chen and Liuyi Wang and Zongtao He and Chengju Liu and Qijun Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dang2023multiple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6855--6872},
  pdf = {https://proceedings.mlr.press/v202/dang23a/dang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation},
  url = {https://proceedings.mlr.press/v202/dang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dann2023reinforcement,
  abstract = {This work studies whether directly incorporating multiple alternate reward formulations of the same task in a single agent can lead to faster learning. We analyze multi-reward extensions of action-elimination algorithms and prove more favorable instance-dependent regret bounds compared to their single-reward counterparts, both in multi-armed bandits and in tabular Markov decision processes. Our bounds scale for each state-action pair with the inverse of the largest gap among all reward functions. We demonstrate that learning with multiple rewards can indeed be more sample-efficient, as long as the rewards agree on an optimal policy.},
  author = {Christoph Dann and Yishay Mansour and Mehryar Mohri},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dann2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6948--6967},
  pdf = {https://proceedings.mlr.press/v202/dann23a/dann23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reinforcement Learning Can Be More Efficient with Multiple Rewards},
  url = {https://proceedings.mlr.press/v202/dann23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dann2023best,
  abstract = {Policy optimization methods are popular reinforcement learning algorithms in practice and recent works have build theoretical foundation for them by proving $\sqrt{T}$ regret bounds even when the losses are adversarial. Such bounds are tight in the worst case but often overly pessimistic. In this work, we show that by carefully designing the regularizer, bonus terms, and learning rates, one can achieve a more favorable $\text{polylog}(T)$ regret bound when the losses are stochastic, without sacrificing the worst-case guarantee in the adversarial regime. Specifically, we show the first best of both worlds guarantee for policy optimization in tabular MDPs by leveraging either a Tsallis entropy or a Shannon entropy regularizer.},
  author = {Christoph Dann and Chen-Yu Wei and Julian Zimmert},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dann2023best.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6968--7008},
  pdf = {https://proceedings.mlr.press/v202/dann23b/dann23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Best of Both Worlds Policy Optimization},
  url = {https://proceedings.mlr.press/v202/dann23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{das2023efficient,
  abstract = {We demonstrate the use of batches in studying list-decodable linear regression, in which only $\alpha\in (0,1]$ fraction of batches contain genuine samples from a common distribution and the rest can contain arbitrary or even adversarial samples. When genuine batches have $\ge \tilde\Omega(1/\alpha)$ samples each, our algorithm can efficiently find a small list of potential regression parameters, with a high probability that one of them is close to the true parameter. This is the first polynomial time algorithm for list-decodable linear regression, and its sample complexity scales nearly linearly with the dimension of the covariates.},
  author = {Abhimanyu Das and Ayush Jain 0001 and Weihao Kong and Rajat Sen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/das2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7025--7065},
  pdf = {https://proceedings.mlr.press/v202/das23b/das23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient List-Decodable Regression using Batches},
  url = {https://proceedings.mlr.press/v202/das23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{das2023beyond,
  abstract = {Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex functions when the Lipschitz constants are unbounded but have bounded moments, i.e., they are heavy-tailed.},
  author = {Rudrajit Das and Satyen Kale and Zheng Xu 0002 and Tong Zhang 0001 and Sujay Sanghavi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/das2023beyond.pdf:pdf},
  mdate = {2024-02-08},
  pages = {7066--7101},
  pdf = {https://proceedings.mlr.press/v202/das23c/das23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond Uniform Lipschitz Condition in Differentially Private Optimization},
  url = {https://proceedings.mlr.press/v202/das23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{das2023understanding,
  abstract = {Self-distillation (SD) is the process of first training a teacher model and then using its predictions to train a student model that has the same architecture. Specifically, the student's loss is $\xi*\ell(\text{teacher's predictions}, \text{ student's predictions}) + (1-\xi)*\ell(\text{given labels}, \text{ student's predictions})$, where $\ell$ is the loss function and $\xi$ is some parameter $\in [0,1]$. In this paper, we theoretically characterize the effect of SD in two supervised learning problems with noisy labels. We first analyze SD for regularized linear regression and show that in the high label noise regime, the optimal value of $\xi$ that minimizes the expected error in estimating the ground truth parameter is surprisingly greater than 1.},
  author = {Rudrajit Das and Sujay Sanghavi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/das2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7102--7140},
  pdf = {https://proceedings.mlr.press/v202/das23d/das23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Self-Distillation in the Presence of Label Noise},
  url = {https://proceedings.mlr.press/v202/das23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{datta2023interval,
  abstract = {Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between the available tasks and their respective interval bounds. We apply our framework to both model-agnostic meta-learning as well as prototype-based metric-learning paradigms. The efficacy of our proposed approach is evident from the improved performance on several datasets from diverse domains compared to current methods.},
  author = {Shounak Datta and Sankha Subhra Mullick and Anish Chakrabarty and Swagatam Das},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/datta2023interval.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7141--7166},
  pdf = {https://proceedings.mlr.press/v202/datta23a/datta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Interval Bound Interpolation for Few-shot Learning with Few Tasks},
  url = {https://proceedings.mlr.press/v202/datta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{daulton2023hypervolume,
  abstract = {Bayesian optimization is a popular method for sample efficient multi-objective optimization. However, existing Bayesian optimization techniques fail to effectively exploit common and often-neglected problem structure such as decoupled evaluations, where objectives can be queried independently from one another and each may consume different resources, or multi-fidelity evaluations, where lower fidelity-proxies of the objectives can be evaluated at lower cost. In this work, we propose a general one-step lookahead acquisition function based on the Knowledge Gradient that addresses the complex question of what to evaluate when and at which design points in a principled Bayesian decision-theoretic fashion. Hence, our approach naturally addresses decoupled, multi-fidelity, and standard multi-objective optimization settings in a unified Bayesian decision making framework. By construction, our method is the one-step Bayes-optimal policy for hypervolume maximization.},
  author = {Samuel Daulton and Maximilian Balandat and Eytan Bakshy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/daulton2023hypervolume.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7167--7204},
  pdf = {https://proceedings.mlr.press/v202/daulton23a/daulton23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hypervolume Knowledge Gradient: {A} Lookahead Approach for Multi-Objective {B}ayesian Optimization with Partial Information},
  url = {https://proceedings.mlr.press/v202/daulton23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{davies2023fast,
  abstract = {We introduce fast algorithms for correlation clustering with respect to the Min Max objective that provide constant factor approximations on complete graphs. These are the first purely combinatorial approximation algorithms for this problem. We construct a novel semi-metric on the set of vertices, called the correlation metric, that indicates to clustering algorithms whether pairs of nodes should be in the same cluster. We demonstrate empirically that, compared to prior work, our algorithms sacrifice little in objective quality to obtain significantly better run-time, and the algorithms scale to larger networks that are effectively intractable for known algorithms.},
  author = {Sami Davies and Benjamin Moseley and Heather Newman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/davies2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7205--7230},
  pdf = {https://proceedings.mlr.press/v202/davies23a/davies23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Combinatorial Algorithms for Min Max Correlation Clustering},
  url = {https://proceedings.mlr.press/v202/davies23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{davies2023predictive,
  abstract = {Recent work has shown that leveraging learned predictions can improve the running time of algorithms for bipartite matching and similar combinatorial problems. In this work, we build on this idea to improve the performance of the widely used Ford-Fulkerson algorithm for computing maximum flows by seeding Ford-Fulkerson with predicted flows. Our proposed method offers strong theoretical performance in terms of the quality of the prediction. We then consider image segmentation, a common use-case of flows in computer vision, and complement our theoretical analysis with strong empirical results.},
  author = {Sami Davies and Benjamin Moseley and Sergei Vassilvitskii and Yuyan Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/davies2023predictive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7231--7248},
  pdf = {https://proceedings.mlr.press/v202/davies23b/davies23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Predictive Flows for Faster Ford-Fulkerson},
  url = {https://proceedings.mlr.press/v202/davies23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{davies2023persistent,
  abstract = {Persistent homology is arguably the most successful technique in Topological Data Analysis. It combines homology, a topological feature of a data set, with persistence, which tracks the evolution of homology over different scales. The persistent Laplacian is a recent theoretical development that combines persistence with the combinatorial Laplacian, the higher-order extension of the well-known graph Laplacian. Crucially, the Laplacian encode both the homology of a data set, and some additional geometric information not captured by the homology. Here, we provide the first investigation into the efficacy of the persistence Laplacian as an embedding of data for downstream classification and regression tasks. We extend the persistent Laplacian to cubical complexes so it can be used on images, then evaluate its performance as an embedding method on the MNIST and MoleculeNet datasets, demonstrating that it consistently outperforms persistent homology across tasks.},
  author = {Thomas Davies and Zhengchao Wan and Rub{\'e}n J. S{\'a}nchez-Garc{\'i}a},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/davies2023persistent.pdf:pdf},
  mdate = {2023-09-05},
  pages = {7249--7263},
  pdf = {https://proceedings.mlr.press/v202/davies23c/davies23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Persistent Laplacian for Data Science: Evaluating Higher-Order Persistent Spectral Representations of Data},
  url = {https://proceedings.mlr.press/v202/davies23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{daw2023mitigating,
  abstract = {Despite the success of physics-informed neural networks (PINNs) in approximating partial differential equations (PDEs), PINNs can sometimes fail to converge to the correct solution in problems involving complicated PDEs. This is reflected in several recent studies on characterizing the failure modes of PINNs, although a thorough understanding of the connection between PINN failure modes and sampling strategies is missing. In this paper, we provide a novel perspective of failure modes of PINNs by hypothesizing that training PINNs relies on successful propagation of solution from initial and/or boundary condition points to interior points. We show that PINNs with poor sampling strategies can get stuck at trivial solutions if there are propagation failures, characterized by highly imbalanced PDE residual fields. We propose a novel Retain-Resample-Release (R3) sampling algorithm to mitigate propagation failures with minimal computational overhead. The R3 algorithm incrementally accumulates collocation points in regions of high PDE residuals, mitigating these failures with minimal computational overhead.},
  author = {Arka Daw and Jie Bu and Sifan Wang and Paris Perdikaris and Anuj Karpatne},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/daw2023mitigating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7264--7302},
  pdf = {https://proceedings.mlr.press/v202/daw23a/daw23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release ({R3}) Sampling},
  url = {https://proceedings.mlr.press/v202/daw23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dbouk2023robustness,
  abstract = {Randomized ensemble classifiers (RECs), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing RECs are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: ``When are RECs useful?'', ``What are their limits?'', and ``How do we train them?''. In this work, we first demystify RECs as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm (BARRE) for training robust RECs, and empirically demonstrate its effectiveness at defending against strong $\ell_\infty$ norm-bounded adversaries across various network architectures and datasets.},
  author = {Hassan Dbouk and Naresh R. Shanbhag},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dbouk2023robustness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7303--7328},
  pdf = {https://proceedings.mlr.press/v202/dbouk23a/dbouk23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Robustness of Randomized Ensembles to Adversarial Perturbations},
  url = {https://proceedings.mlr.press/v202/dbouk23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dedieu2023learning,
  abstract = {Noisy-OR Bayesian Networks (BNs) are a family of probabilistic graphical models which express rich statistical dependencies in binary data. Variational inference (VI) has been the main method proposed to learn noisy-OR BNs with complex latent structures, but the proposed VI approaches either (a) use a recognition network with standard amortized inference that cannot induce explaining-away; or (b) assume a simple mean-field (MF) posterior which is vulnerable to bad local optima. In this work, we propose to use max-product belief propagation (MP-BP) as an alternative to VI. We show that MP-BP can be used to learn noisy-OR BNs with arbitrary network structures, and we empirically demonstrate that it consistently outperforms VI approaches across a range of tasks.},
  author = {Antoine Dedieu and Guangyao Zhou and Dileep George and Miguel L{\'a}zaro-Gredilla},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dedieu2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7426--7448},
  pdf = {https://proceedings.mlr.press/v202/dedieu23a/dedieu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Noisy {OR} {B}ayesian Networks with Max-Product Belief Propagation},
  url = {https://proceedings.mlr.press/v202/dedieu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{defazio2023learningratefree,
  abstract = {The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance $D$ from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of $D$ yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.},
  author = {Aaron Defazio and Konstantin Mishchenko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/defazio2023learningratefree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7449--7479},
  pdf = {https://proceedings.mlr.press/v202/defazio23a/defazio23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning-Rate-Free Learning by {D}-Adaptation},
  url = {https://proceedings.mlr.press/v202/defazio23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{delattre2023efficient,
  abstract = {Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches.},
  author = {Blaise Delattre and Quentin Barth{\'e}lemy and Alexandre Araujo and Alexandre Allauzen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/delattre2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7513--7532},
  pdf = {https://proceedings.mlr.press/v202/delattre23a/delattre23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Bound of {L}ipschitz Constant for Convolutional Layers by {G}ram Iteration},
  url = {https://proceedings.mlr.press/v202/delattre23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{demirovic2023blossom,
  abstract = {We propose a simple algorithm to learn optimal decision trees of bounded depth. This algorithm is essentially an anytime version of the state-of-the-art dynamic programming approach. It has virtually no overhead compared to heuristic methods and is comparable to the best exact methods to prove optimality on most data sets. Experiments show that whereas existing exact methods hardly scale to deep trees, this algorithm learns trees comparable to standard heuristics without computational overhead, and can significantly improve their accuracy when given more computation time, even for deep trees.},
  author = {Emir Demirovic and Emmanuel Hebrard and Louis Jean},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/demirovic2023blossom.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7533--7562},
  pdf = {https://proceedings.mlr.press/v202/demirovic23a/demirovic23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Blossom: an Anytime Algorithm for Computing Optimal Decision Trees},
  url = {https://proceedings.mlr.press/v202/demirovic23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023optimizing,
  abstract = {Recently, an intriguing class of non-convex optimization problems has emerged in the context of learning directed acyclic graphs (DAGs). These problems involve minimizing a given loss or score function, subject to a non-convex continuous constraint that penalizes the presence of cycles in a graph. In this work, we delve into the optimization challenges associated with this class of non-convex programs. To address these challenges, we propose a bi-level algorithm that leverages the non-convex constraint in a novel way. The outer level of the algorithm optimizes over topological orders by iteratively swapping pairs of nodes within the topological order of a DAG. A key innovation of our approach is the development of an effective method for generating a set of candidate swapping pairs for each iteration. At the inner level, given a topological order, we utilize off-the-shelf solvers that can handle linear constraints. The key advantage of our proposed algorithm is that it is guaranteed to find a local minimum or a KKT point under weaker conditions compared to previous work and finds solutions with lower scores. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in terms of achieving a better score. Additionally, our method can also be used as a post-processing algorithm to significantly improve the score of other algorithms.},
  author = {Chang Deng and Kevin Bello and Bryon Aragam and Pradeep Kumar Ravikumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023optimizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7563--7595},
  pdf = {https://proceedings.mlr.press/v202/deng23a/deng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimizing {NOTEARS} Objectives via Topological Swaps},
  url = {https://proceedings.mlr.press/v202/deng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023uncertainty,
  abstract = {Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning. In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our proposed method consistently outperforms traditional EDL-related algorithms in multiple uncertainty estimation tasks, especially in the more challenging few-shot classification settings.},
  author = {Danruo Deng and Guangyong Chen and Yang Yu and Furui Liu and Pheng-Ann Heng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023uncertainty.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7596--7616},
  pdf = {https://proceedings.mlr.press/v202/deng23b/deng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Uncertainty Estimation by {Fisher} Information-based Evidential Deep Learning},
  url = {https://proceedings.mlr.press/v202/deng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023multichannel,
  abstract = {In digital online advertising, advertisers procure ad impressions simultaneously on multiple platforms, or so-called channels, such as Google Ads, Meta Ads Manager, etc., each of which consists of numerous ad auctions. We study how an advertiser maximizes total conversion (e.g. ad clicks) while satisfying aggregate return-on-investment (ROI) and budget constraints across all channels. In practice, an advertiser does not have control over, and thus cannot globally optimize, which individual ad auctions she participates in for each channel, and instead authorizes a channel to procure impressions on her behalf: the advertiser can only utilize two levers on each channel, namely setting a per-channel budget and per-channel target ROI. In this work, we first analyze the effectiveness of each of these levers for solving the advertiser's global multi-channel problem. An efficient learning algorithm is presented that produces per-channel budgets whose resulting conversion approximates that of the global optimal problem, and the paper shows that these results hold for both single-item and multi-item auctions.},
  author = {Yuan Deng and Negin Golrezaei and Patrick Jaillet and Jason Cheuk Nam Liang and Vahab Mirrokni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023multichannel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7617--7644},
  pdf = {https://proceedings.mlr.press/v202/deng23c/deng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-channel Autobidding with Budget and {ROI} Constraints},
  url = {https://proceedings.mlr.press/v202/deng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023surrogate,
  abstract = {Spiking neural networks provide an alternative solution to conventional artificial neural networks with energy-saving and high-efficiency characteristics after hardware implantation. However, due to its non-differentiable activation function and the temporally delayed accumulation in outputs, the direct training of SNNs is extraordinarily tough even adopting a surrogate gradient to mimic the backpropagation. For SNN training, this non-differentiability causes the intrinsic gradient error that would be magnified through layerwise backpropagation, especially through multiple layers. In this paper, we propose a novel approach to reducing gradient error from a new perspective called surrogate module learning (SML). Surrogate module learning tries to construct a shortcut path to back-propagate more accurate gradient to a certain SNN part utilizing the surrogate modules. Then, we develop a new loss function for concurrently training the network and enhancing the surrogate modules' surrogate capacity. We demonstrate that when the outputs of surrogate modules are close to the SNN output, the fraction of the gradient error drops significantly. Our method consistently and significantly enhances the performance of SNNs on all experiment datasets, including CIFAR-10/100, ImageNet, and ES-ImageNet. For example, for spiking ResNet-34 architecture on ImageNet, we increased the SNN accuracy by 3.46\%.},
  author = {Shikuang Deng and Hao Lin and Yuhang Li 0001 and Shi Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023surrogate.pdf:pdf},
  mdate = {2025-05-28},
  pages = {7645--7657},
  pdf = {https://proceedings.mlr.press/v202/deng23d/deng23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Surrogate Module Learning: Reduce the Gradient Error Accumulation in Training Spiking Neural Networks},
  url = {https://proceedings.mlr.press/v202/deng23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023confidence,
  abstract = {This work aims to assess how well a model performs under distribution shifts without using labels. While recent methods study prediction confidence, this work reports prediction dispersity is another informative cue. Confidence reflects whether the individual prediction is certain; dispersity indicates how the overall predictions are distributed across all categories. Our key insight is that a well-performing model should give predictions with high confidence and high dispersity. That is, we need to consider both properties so as to make more accurate estimates. To this end, we use the nuclear norm that has been shown to be effective in characterizing both properties. Extensive experiments validate the effectiveness of nuclear norm for various models (e.g., ViT and ConvNeXt), different datasets (e.g., ImageNet and CUB-200), and diverse types of distribution shifts (e.g., style shift and reproduction shift). We show that the nuclear norm is more accurate and robust in accuracy estimation than existing methods. Furthermore, we validate the feasibility of other measurements (e.g., mutual information maximization) for characterizing dispersity and confidence. Lastly, we investigate the limitation of the nuclear norm, study its improved variant under severe class imbalance, and discuss potential directions.},
  author = {Weijian Deng and Yumin Suh and Stephen Gould and Liang Zheng 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023confidence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7658--7674},
  pdf = {https://proceedings.mlr.press/v202/deng23e/deng23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Confidence and Dispersity Speak: Characterizing Prediction Matrix for Unsupervised Accuracy Estimation},
  url = {https://proceedings.mlr.press/v202/deng23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{deng2023great,
  abstract = {Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring the agreement between its latent space, and the latent space of a foundation model. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, e.g., arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a neighborhood agreement measure between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability.},
  author = {Ailin Deng and Miao Xiong and Bryan Hooi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/deng2023great.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7675--7693},
  pdf = {https://proceedings.mlr.press/v202/deng23f/deng23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement},
  url = {https://proceedings.mlr.press/v202/deng23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{desai2023hardwareaware,
  abstract = {Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, state-of-the-art techniques fall short in one or more desirable aspects of compression -- for instance, pruning does not reduce memory for training, quantization can only provide up to 32x compression, HashedNet is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by clubbing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern.},
  author = {Aditya Desai and Keren Zhou 0001 and Anshumali Shrivastava},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/desai2023hardwareaware.pdf:pdf},
  mdate = {2023-11-12},
  pages = {7732--7749},
  pdf = {https://proceedings.mlr.press/v202/desai23b/desai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hardware-Aware Compression with Random Operation Access Specific Tile ({ROAST}) Hashing},
  url = {https://proceedings.mlr.press/v202/desai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{desai2023hyperbolic,
  abstract = {Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept 'dog' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text datasets. Our results show that MERU learns a highly interpretable and structured representation space while being competitive with CLIP's performance on standard multi-modal tasks like image classification and image-text retrieval.},
  author = {Karan Desai and Maximilian Nickel and Tanmay Rajpurohit and Justin Johnson 0001 and Shanmukha Ramakrishna Vedantam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/desai2023hyperbolic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7694--7731},
  pdf = {https://proceedings.mlr.press/v202/desai23a/desai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyperbolic Image-text Representations},
  url = {https://proceedings.mlr.press/v202/desai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dettmers2023case,
  abstract = {Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2.},
  author = {Tim Dettmers and Luke Zettlemoyer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dettmers2023case.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7750--7774},
  pdf = {https://proceedings.mlr.press/v202/dettmers23a/dettmers23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The case for 4-bit precision: k-bit Inference Scaling Laws},
  url = {https://proceedings.mlr.press/v202/dettmers23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{devic2023fairness,
  abstract = {The prevalence and importance of algorithmic two-sided marketplaces has drawn attention to the issue of fairness in such settings. Algorithmic decisions are used in assigning students to schools, users to advertisers, and applicants to job interviews. These decisions should heed the preferences of individuals, and simultaneously be fair with respect to their merits (synonymous with fit, future performance, or need). Merits conditioned on observable features are always uncertain, a fact that is exacerbated by the widespread use of machine learning algorithms to infer merit from the observables. As our key contribution, we carefully axiomatize a notion of individual fairness in the two-sided marketplace setting which respects the uncertainty in the merits; indeed, it simultaneously recognizes uncertainty as the primary potential cause of unfairness and an approach to address it.},
  author = {Siddartha Devic and David Kempe 0001 and Vatsal Sharan and Aleksandra Korolova},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/devic2023fairness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7775--7794},
  pdf = {https://proceedings.mlr.press/v202/devic23a/devic23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fairness in Matching under Uncertainty},
  url = {https://proceedings.mlr.press/v202/devic23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dhawan2023efficient,
  abstract = {It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.},
  author = {Nikita Dhawan and Sicong Huang 0001 and Juhan Bae and Roger Baker Grosse},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/dhawan2023efficient.pdf:pdf},
  mdate = {2024-02-05},
  pages = {7795--7812},
  pdf = {https://proceedings.mlr.press/v202/dhawan23a/dhawan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Parametric Approximations of Neural Network Function Space Distance},
  url = {https://proceedings.mlr.press/v202/dhawan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dheur2023largescale,
  abstract = {Accurate probabilistic predictions are essential for optimal decision making. While neural network miscalibration has been studied primarily in classification, we investigate this in the less-explored domain of regression. We conduct the largest empirical study to date to assess the probabilistic calibration of neural networks. We also analyze the performance of recalibration, conformal, and regularization methods to enhance probabilistic calibration. Additionally, we introduce novel differentiable recalibration and regularization methods, uncovering new insights into their effectiveness. Our findings reveal that regularization methods offer a favorable tradeoff between calibration and sharpness. Post-hoc methods exhibit superior probabilistic calibration, which we attribute to the finite-sample coverage guarantee of conformal prediction. Furthermore, we demonstrate that quantile recalibration can be considered as a specific case of conformal prediction.},
  author = {Victor Dheur and Souhaib Ben Taieb},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/dheur2023largescale.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7813--7836},
  pdf = {https://proceedings.mlr.press/v202/dheur23a/dheur23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Large-Scale Study of Probabilistic Calibration in Neural Network Regression},
  url = {https://proceedings.mlr.press/v202/dheur23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{di2023nearly,
  abstract = {We study the Stochastic Shortest Path (SSP) problem with a linear mixture transition kernel, where an agent repeatedly interacts with a stochastic environment and seeks to reach certain goal state while minimizing the cumulative cost. Existing works often assume a strictly positive lower bound of the cost function or an upper bound of the expected length for the optimal policy. In this paper, we propose a new algorithm to eliminate these restrictive assumptions. Our algorithm is based on extended value iteration with a fine-grained variance-aware confidence set, where the variance is estimated recursively from high-order moments. Our algorithm achieves an Õ(dB*√K) regret bound, where d is the dimension of the feature mapping in the linear transition kernel, B* is the upper bound of the total cumulative cost for the optimal policy, and K is the number of episodes. Our regret upper bound matches the Ω(dB*√K) lower bound of linear mixture SSPs, which suggests that our algorithm is nearly minimax optimal.},
  author = {Qiwei Di and Jiafan He and Dongruo Zhou and Quanquan Gu},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/di2023nearly.pdf:pdf},
  mdate = {2024-12-24},
  pages = {7837--7864},
  pdf = {https://proceedings.mlr.press/v202/di23a/di23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic Shortest Path},
  url = {https://proceedings.mlr.press/v202/di23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{diakonikolas2023nearlylinear,
  abstract = {We study principal component analysis (PCA), where given a dataset in ℝᵈ from a distribution, the task is to find a unit vector v that approximately maximizes the variance of the distribution after being projected along v. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. The main contribution is to develop a nearly-linear time algorithm for robust PCA with near-optimal error guarantees.},
  author = {Ilias Diakonikolas and Daniel Kane 0001 and Ankit Pensia and Thanasis Pittas},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/diakonikolas2023nearlylinear.pdf:pdf},
  mdate = {2024-04-04},
  pages = {7886--7921},
  pdf = {https://proceedings.mlr.press/v202/diakonikolas23a/diakonikolas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly-Linear Time and Streaming Algorithms for Outlier-Robust {PCA}},
  url = {https://proceedings.mlr.press/v202/diakonikolas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{diakonikolas2023nearoptimal,
  abstract = {We study the task of agnostically learning halfspaces under the Gaussian distribution. Specifically, given labeled examples (x,y) from an unknown distribution on ℝⁿ × {±1}, whose marginal distribution on x is the standard Gaussian and the labels y can be arbitrary, the goal is to output a hypothesis with 0-1 loss OPT+ε, where OPT is the 0-1 loss of the best-fitting halfspace. We prove a near-optimal computational hardness result for this task, under the widely believed sub-exponential time hardness of the Learning with Errors (LWE) problem. Prior hardness results are either qualitatively suboptimal or apply to restricted families of algorithms. Our techniques extend to yield near-optimal lower bounds for related problems, including ReLU regression.},
  author = {Ilias Diakonikolas and Daniel Kane 0001 and Lisheng Ren},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/diakonikolas2023nearoptimal.pdf:pdf},
  mdate = {2024-04-04},
  pages = {7922--7938},
  pdf = {https://proceedings.mlr.press/v202/diakonikolas23b/diakonikolas23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-Optimal Cryptographic Hardness of Agnostically Learning Halfspaces and {ReLU} Regression under {Gaussian} Marginals},
  url = {https://proceedings.mlr.press/v202/diakonikolas23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{diamant2023improving,
  abstract = {Deep graph generative modeling has proven capable of learning the distribution of complex, multi-scale structures characterizing real-world graphs. However, one of the main limitations of existing methods is their large output space, which limits generation scalability and hinders accurate modeling of the underlying distribution. To overcome these limitations, we propose a novel approach that significantly reduces the output space of existing graph generative models. Specifically, starting from the observation that many real-world graphs have low graph bandwidth, we restrict graph bandwidth during training and generation. Our strategy improves both generation scalability and quality without increasing architectural complexity or reducing expressiveness.},
  author = {Nathaniel Lee Diamant and Alex M. Tseng and Kangway V. Chuang and Tommaso Biancalani and Gabriele Scalia},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/diamant2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7939--7959},
  pdf = {https://proceedings.mlr.press/v202/diamant23a/diamant23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Graph Generation by Restricting Graph Bandwidth},
  url = {https://proceedings.mlr.press/v202/diamant23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{diao2023forwardbackward,
  abstract = {Variational inference (VI) seeks to approximate a target distribution π by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian VI, which approximates π by minimizing the Kullback-Leibler (KL) divergence to π over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference (FB-GVI) algorithm to solve Gaussian VI. Our approach exploits the composite structure of the KL divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein (BW) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when π is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when π is only log-smooth.},
  author = {Michael Ziyang Diao and Krishna Balasubramanian and Sinho Chewi and Adil Salim},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/diao2023forwardbackward.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7960--7991},
  pdf = {https://proceedings.mlr.press/v202/diao23a/diao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Forward-Backward {Gaussian} Variational Inference via {JKO} in the {Bures-Wasserstein} Space},
  url = {https://proceedings.mlr.press/v202/diao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dick2023subsetbased,
  abstract = {We propose a new definition of instance optimality for differentially private estimation algorithms. Our definition requires an optimal algorithm to compete, simultaneously for every dataset D, with the best private benchmark algorithm that (a) knows D in advance and (b) is evaluated by its worst-case performance on large subsets of D. That is, the benchmark algorithm need not perform well when potentially extreme points are added to D; it only has to handle the removal of a small number of real data points that already exist. This makes our benchmark significantly stronger than those proposed in prior work. We nevertheless show, for real-valued datasets, how to construct private algorithms that achieve our notion of instance optimality when estimating a broad class of dataset properties, including means, quantiles, and ℓp-norm minimizers. For means in particular, we provide a detailed analysis and show that our algorithm simultaneously matches or exceeds the asymptotic performance of existing algorithms under a range of distributional assumptions.},
  author = {Travis Dick and Alex Kulesza and Ziteng Sun and Ananda Theertha Suresh},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/dick2023subsetbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7992--8014},
  pdf = {https://proceedings.mlr.press/v202/dick23a/dick23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Subset-Based Instance Optimality in Private Estimation},
  url = {https://proceedings.mlr.press/v202/dick23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dimitriadis2023pareto,
  abstract = {In Multi-Task Learning (MTL), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks' performance, and resort to optimality in the Pareto sense. Most MTL methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose Pareto Manifold Learning, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single training run, that allows to modulate the performance on each task during inference. Experiments on multi-task learning benchmarks, ranging from image classification to tabular datasets and scene understanding, show that Pareto Manifold Learning outperforms state-of-the-art single-point algorithms, while learning a better Pareto parameterization than multi-point baselines.},
  author = {Nikolaos Dimitriadis and Pascal Frossard and François Fleuret},
  booktitle = {ICML},
  file = {:/home/b/documents/inproceedings/dimitriadis2023pareto.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8015--8052},
  pdf = {https://proceedings.mlr.press/v202/dimitriadis23a/dimitriadis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Pareto} Manifold Learning: Tackling multiple tasks via ensembles of single-task models},
  url = {https://proceedings.mlr.press/v202/dimitriadis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ding2023entity,
  abstract = {We investigate the use of natural language to drive the generalization of policies in multi-agent settings. Unlike single-agent settings, the generalization of policies should also consider the influence of other agents. Besides, with the increasing number of entities in multi-agent settings, more agent-entity interactions are needed for language grounding, and the enormous search space could impede the learning process. Moreover, given a simple general instruction, e.g., beating all enemies, agents are required to decompose it into multiple subgoals and figure out the right one to focus on. We propose a novel framework for language grounding in multi-agent reinforcement learning, entity divider ({EnDi}), to address these challenges.},
  author = {Ziluo Ding and Wanpeng Zhang and Junpeng Yue and Xiangjun Wang and Tiejun Huang and Zongqing Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ding2023entity.pdf:pdf},
  mdate = {2025-04-09},
  pages = {8103--8119},
  pdf = {https://proceedings.mlr.press/v202/ding23d/ding23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Entity Divider with Language Grounding in Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/ding23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ding2023bayesian,
  abstract = {Recently, reward-conditioned reinforcement learning ({RCRL}) has gained popularity due to its simplicity, flexibility, and off-policy nature. However, we will show that current {RCRL} approaches are fundamentally limited and fail to address two critical challenges of {RCRL} -- improving generalization on high reward-to-go ({RTG}) inputs, and avoiding out-of-distribution ({OOD}) {RTG} queries during testing time. To address these challenges when training vanilla {RCRL} architectures, we propose Bayesian Reparameterized {RCRL} ({BR-RCRL}), a novel set of inductive biases for {RCRL} inspired by Bayes' theorem. {BR-RCRL} removes a core obstacle preventing vanilla {RCRL} from generalizing on high {RTG} inputs -- a tendency that the model treats different {RTG} inputs as independent values, which we term "{RTG} Independence". {BR-RCRL} also allows us to design an accompanying adaptive inference method, which maximizes total returns while avoiding {OOD} queries that yield unpredictable behaviors in vanilla {RCRL} methods. The paper shows that {BR-RCRL} achieves state-of-the-art performance on the Gym-Mujoco and Atari offline {RL} benchmarks, improving upon vanilla {RCRL} by up to 11\%.},
  author = {Wenhao Ding and Tong Che and Ding Zhao and Marco Pavone},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ding2023bayesian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8053--8066},
  pdf = {https://proceedings.mlr.press/v202/ding23a/ding23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayesian Reparameterization of Reward-Conditioned Reinforcement Learning with Energy-based Models},
  url = {https://proceedings.mlr.press/v202/ding23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ding2023dsgdceca,
  abstract = {Decentralized Stochastic Gradient Descent ({SGD}) is an emerging neural network training approach that enables multiple agents to train a model collaboratively and simultaneously. Rather than using a central parameter server to collect gradients from all the agents, each agent keeps a copy of the model parameters and communicates with a small number of other agents to exchange model updates. Their communication, governed by the communication topology and gossip weight matrices, facilitates the exchange of model updates. The state-of-the-art approach uses the dynamic one-peer exponential-2 topology, achieving faster training times and improved scalability than the ring, grid, torus, and hypercube topologies. However, this approach requires a power-of-2 number of agents, which is impractical at scale. In this paper, we remove this restriction and propose Decentralized {SGD} with Communication-optimal Exact Consensus Algorithm ({DSGD-CECA}), which works for any number of agents while still achieving state-of-the-art properties. In particular, {DSGD-CECA} incurs a unit per-iteration communication overhead and an $\tilde{O}(n^3)$ transient iteration complexity. Our proof is based on newly discovered properties of gossip weight matrices and a novel approach to combine them with {DSGD}'s convergence analysis. Numerical experiments show the efficiency of {DSGD-CECA}.},
  author = {Lisang Ding and Kexin Jin and Bicheng Ying and Kun Yuan and Wotao Yin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ding2023dsgdceca.pdf:pdf},
  mdate = {2025-05-28},
  pages = {8067--8089},
  pdf = {https://proceedings.mlr.press/v202/ding23b/ding23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DSGD-CECA}: Decentralized {SGD} with Communication-Optimal Exact Consensus Algorithm},
  url = {https://proceedings.mlr.press/v202/ding23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ding2023openvocabulary,
  abstract = {In this paper, we tackle an emerging computer vision task, open-vocabulary universal image segmentation, that aims to perform semantic/instance/panoptic segmentation (background semantic labeling + foreground instance segmentation) for arbitrary categories of text-based descriptions in inference time. We first build a baseline method by directly adopting pre-trained CLIP models without finetuning or distillation. We then develop MaskCLIP, a Transformer-based approach with a MaskCLIP Visual Encoder, which is an encoder-only module that seamlessly integrates mask tokens with a pre-trained ViT CLIP model for semantic/instance segmentation and class prediction. MaskCLIP learns to efficiently and effectively utilize pre-trained partial/dense CLIP features within the MaskCLIP Visual Encoder that avoids the time-consuming student-teacher training process. MaskCLIP outperforms previous methods for semantic/instance/panoptic segmentation on ADE20K and PASCAL datasets. We show qualitative illustrations for MaskCLIP with online custom categories.},
  author = {Zheng Ding and Jieke Wang and Zhuowen Tu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ding2023openvocabulary.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8090--8102},
  pdf = {https://proceedings.mlr.press/v202/ding23c/ding23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Open-Vocabulary Universal Image Segmentation with MaskCLIP},
  url = {https://proceedings.mlr.press/v202/ding23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dinh2023pixelasparam,
  abstract = {Diffusion models recently achieved state-of-the-art in image generation. They mainly utilize the denoising framework, which leverages the Langevin dynamics process for image sampling. Recently, the guidance method has modified this process to add conditional information to achieve a controllable generator. However, the current guidance on denoising processes suffers from the trade-off between diversity, image quality, and conditional information. In this work, we propose to view this guidance sampling process from a gradient view, where image pixels are treated as parameters being optimized, and each mathematical term in the sampling process represents one update direction. This perspective reveals more insights into the conflict problems between updated directions on the pixels, which cause the trade-off as mentioned previously. We investigate the conflict problems and propose to solve them by a simple projection method. The experimental results evidently improve over different baselines on datasets with various resolutions.},
  author = {AnhDung Dinh and Daochang Liu and Chang Xu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dinh2023pixelasparam.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8120--8137},
  pdf = {https://proceedings.mlr.press/v202/dinh23a/dinh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PixelAsParam}: A Gradient View on Diffusion Sampling with Guidance},
  url = {https://proceedings.mlr.press/v202/dinh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{doikov2023secondorder,
  abstract = {We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetic complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetic complexity of second-order algorithms by a factor $\sqrt{d}$.},
  author = {Nikita Doikov and El Mahdi Chayti and Martin Jaggi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/doikov2023secondorder.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8138--8161},
  pdf = {https://proceedings.mlr.press/v202/doikov23a/doikov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Second-Order Optimization with Lazy Hessians},
  url = {https://proceedings.mlr.press/v202/doikov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{doikov2023polynomial,
  abstract = {We study first-order methods with preconditioning for solving structured convex optimization problems. We propose a new family of preconditioners generated by the symmetric polynomials. They provide the first-order optimization methods with a provable improvement of the condition number, cutting the gaps between highest eigenvalues, without explicit knowledge of the actual spectrum. We give a stochastic interpretation of this preconditioning in terms of the coordinate volume sampling and compare it with other classical approaches, including the Chebyshev polynomials. We show how to incorporate a polynomial preconditioning into the Gradient and Fast Gradient Methods and establish their better global complexity bounds. Finally, we propose a simple adaptive search procedure that automatically ensures the best polynomial preconditioning for the Gradient Method, minimizing the objective along a low-dimensional Krylov subspace. Numerical experiments confirm the efficiency of our preconditioning strategies for solving various machine learning problems.},
  author = {Nikita Doikov and Anton Rodomanov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/doikov2023polynomial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8162--8187},
  pdf = {https://proceedings.mlr.press/v202/doikov23b/doikov23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Polynomial Preconditioning for Gradient Methods},
  url = {https://proceedings.mlr.press/v202/doikov23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dominguezolmedo2023data,
  abstract = {The geometric structure of data is an important inductive bias in machine learning. In this work, we characterize the data manifolds entailed by structural causal models. The strengths of the proposed framework are twofold: firstly, the geometric structure of the data manifolds is causally informed, and secondly, it enables causal reasoning about the data manifolds in an interventional and a counterfactual sense. We showcase the versatility of the proposed framework by applying it to the generation of causally-grounded counterfactual explanations for machine learning classifiers, measuring distances along the data manifold in a differential geometric-principled manner.},
  author = {Ricardo Dominguez-Olmedo and Amir-Hossein Karimi and Georgios Arvanitidis and Bernhard Sch\"{o}lkopf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dominguezolmedo2023data.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8188--8201},
  pdf = {https://proceedings.mlr.press/v202/dominguez-olmedo23a/dominguez-olmedo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Data Manifolds Entailed by Structural Causal Models},
  url = {https://proceedings.mlr.press/v202/dominguez-olmedo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023does,
  abstract = {Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) shows that even if a learner is given linear features in $\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\varepsilon$, searching for an $O(\varepsilon)$-optimal action requires pulling at least $\Omega(\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\varepsilon\sqrt{d})$-optimal solution can be learned within $\operatorname{poly}(d/\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break $\varepsilon\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\varepsilon)$-optimal actions by querying $\tilde{O}(\exp(m\varepsilon))$ actions, where $m$ is the sparsity parameter, removing the $\exp(d)$-dependence. We further show (with an information-theoretical lower bound) that this is the best possible if one demands an error $ m^{\delta}\varepsilon$ for $0<\delta<1$. We further show that $\operatorname{poly}(m/\varepsilon)$ bounds are possible when the linear features are "good". These results provide a nearly complete picture of how sparsity can help in misspecified bandit learning and provide a deeper understanding of when linear features are "useful" for bandit and reinforcement learning with misspecification.},
  author = {Jialin Dong and Lin Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023does.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8317--8333},
  pdf = {https://proceedings.mlr.press/v202/dong23g/dong23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Does Sparsity Help in Learning Misspecified Linear Bandits?},
  url = {https://proceedings.mlr.press/v202/dong23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023towards,
  abstract = {Graph neural networks (GNNs) have emerged as a powerful paradigm to learn from relational data mostly through applying the message passing mechanism. However, this approach may exhibit suboptimal performance when applied to graphs possessing various structural issues. In this work, we focus on understanding and alleviating the effect of graph structural noise on GNN performance. To evaluate the graph structural noise in real data, we propose edge signal-to-noise ratio (ESNR), a novel metric evaluating overall edge noise level with respect to data features or labels based on random matrix theory. We have found striking concordance between the proposed ESNR metric and the GNN performance in various simulated and real data. To reduce the effect of the noise, we propose GPS (Graph Propensity Score) graph rewiring, which estimates the edge likelihood for rewiring data graphs based on self-supervised link prediction. We provide a theoretical guarantee for GPS graph rewiring and demonstrate its efficacy by comprehensive benchmarks.},
  author = {Mingze Dong and Yuval Kluger},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8202--8226},
  pdf = {https://proceedings.mlr.press/v202/dong23a/dong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Understanding and Reducing Graph Structural Noise for {GNN}s},
  url = {https://proceedings.mlr.press/v202/dong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023speeddetr,
  abstract = {Vision Transformers (ViTs) have continuously achieved new milestones in object detection. However, the considerable computation and memory burden compromise their efficiency and generalization of deployment on resource-constraint devices. Besides, efficient transformer-based detectors designed by existing works can hardly achieve a realistic speedup, especially on multi-core processors (e.g., GPUs). The main issue is that the current literature solely concentrates on building algorithms with minimal computation, oblivious that the practical latency can also be affected by the memory access cost and the degree of parallelism. Therefore, we propose SpeedDETR, a novel speed-aware transformer for end-to-end object detectors, achieving high-speed inference on multiple devices. Specifically, we design a latency prediction model which can directly and accurately estimate the network latency by analyzing network properties, hardware memory access pattern, and degree of parallelism. Following the effective local-to-global visual modeling process and the guidance of the latency prediction model, we build our hardware-oriented architecture design and develop a new family of SpeedDETR. Experiments on the MS COCO dataset show SpeedDETR outperforms current DETR-based methods on Tesla V100. Even acceptable speed inference can be achieved on edge GPUs.},
  author = {Peiyan Dong and Zhenglun Kong and Xin Meng and Peng Zhang and Hao Tang 0005 and Yanzhi Wang and Chih-Hsien Chou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023speeddetr.pdf:pdf},
  mdate = {2024-04-03},
  pages = {8227--8243},
  pdf = {https://proceedings.mlr.press/v202/dong23b/dong23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SpeedDETR}: Speed-aware Transformers for End-to-end Object Detection},
  url = {https://proceedings.mlr.press/v202/dong23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023diversityenhancing,
  abstract = {Few-shot hypothesis adaptation (FHA) aims to train a classifier for the target domain with a few labeled target-domain data and a well-trained source-domain classifier (i.e., a source hypothesis). The existing methods for generating unlabeled data produce extremely similar or even identical data, and the strong dependency among the generated data leads to learning failure. To address this issue, we propose a diversity-enhancing generative network (DEG-Net) for the FHA problem, which can generate diverse unlabeled data with the help of a kernel independence measure: the Hilbert-Schmidt independence criterion (HSIC). Specifically, DEG-Net generates data by minimizing the HSIC value (i.e., maximizing the independence) among the semantic features of the generated data. The semantic features are obtained from the hidden-layer outputs of the well-trained source hypothesis. HSIC can be easily estimated by data samples, making it computationally efficient for measuring dependency among generated unlabeled data. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our proposed approach.},
  author = {Ruijiang Dong and Feng Liu 0003 and Haoang Chi and Tongliang Liu and Mingming Gong and Gang Niu 0001 and Masashi Sugiyama and Bo Han 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023diversityenhancing.pdf:pdf},
  mdate = {2024-02-14},
  pages = {8260--8275},
  pdf = {https://proceedings.mlr.press/v202/dong23d/dong23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation},
  url = {https://proceedings.mlr.press/v202/dong23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023understand,
  abstract = {Despite the effectiveness of ELECTRA-style pre-training, their performance is dependent on the careful selection of the model size for the auxiliary generator, leading to high trial-and-error costs. In this paper, we present the first systematic study of this problem. Our theoretical investigation highlights the importance of controlling the generator capacity in ELECTRA-style training. Meanwhile, we found it is not handled properly in the original ELECTRA design, leading to the sensitivity issue. Specifically, since adaptive optimizers like Adam will cripple the weighing of individual losses in the joint optimization, the original design fails to control the generator training effectively. To regain control over the generator, we modularize the generator optimization by decoupling the generator optimizer and discriminator optimizer completely, instead of simply relying on the weighted objective combination. Our simple technique reduced the sensitivity of ELECTRA training significantly and obtains considerable performance gain compared to the original design.},
  author = {Chengyu Dong and Liyuan Liu and Hao Cheng 0002 and Jingbo Shang and Jianfeng Gao 0001 and Xiaodong Liu 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023understand.pdf:pdf},
  mdate = {2024-08-02},
  pages = {8244--8259},
  pdf = {https://proceedings.mlr.press/v202/dong23c/dong23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understand and Modularize Generator Optimization in {ELECTRA}-style Pretraining},
  url = {https://proceedings.mlr.press/v202/dong23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023pasta,
  abstract = {We consider a class of assortment optimization problems in an offline data-driven setting. A firm does not know the underlying customer choice model but has access to an offline dataset consisting of the historically offered assortment set, customer choice, and revenue. The objective is to use the offline dataset to find an optimal assortment. Due to the combinatorial nature of assortment optimization, the problem of insufficient data coverage is likely to occur in the offline dataset. Therefore, designing a provably efficient offline learning algorithm becomes a significant challenge. To this end, based on the principle of pessimism, we propose a novel algorithm called Pessimistic ASsortment opTimizAtion (PASTA for short), which can correctly identify the optimal assortment by only requiring the offline data to cover the optimal assortment under general settings.},
  author = {Juncheng Dong and Weibin Mo and Zhengling Qi and Cong Shi 0001 and Ethan X. Fang and Vahid Tarokh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023pasta.pdf:pdf},
  mdate = {2025-04-28},
  pages = {8276--8295},
  pdf = {https://proceedings.mlr.press/v202/dong23e/dong23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PASTA}: Pessimistic Assortment Optimization},
  url = {https://proceedings.mlr.press/v202/dong23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dong2023adaptively,
  abstract = {Concept shift is a prevailing problem in natural tasks like medical image segmentation where samples usually come from different subpopulations with variant correlations between features and labels. One common type of concept shift in medical image segmentation is the information imbalance between label-sparse samples with few (if any) segmentation labels and label-dense samples with plentiful labeled pixels. Existing distributionally robust algorithms have focused on adaptively truncating/down-weighting the less informative (i.e., label-sparse in our context) samples. To exploit data features of label-sparse samples more efficiently, we propose an adaptively weighted online optimization algorithm -- AdaWAC -- to incorporate data augmentation consistency regularization in sample reweighting. Our method introduces a set of trainable weights to balance the supervised loss and unsupervised consistency regularization of each sample separately.},
  author = {Yijun Dong and Yuege Xie and Rachel A. Ward},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dong2023adaptively.pdf:pdf},
  mdate = {2024-01-04},
  pages = {8296--8316},
  pdf = {https://proceedings.mlr.press/v202/dong23f/dong23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptively Weighted Data Augmentation Consistency Regularization for Robust Optimization under Concept Shift},
  url = {https://proceedings.mlr.press/v202/dong23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023symmetryaware,
  abstract = {Robot design aims at learning to create robots that can be easily controlled and perform tasks efficiently. Previous works on robot design have proven its ability to generate robots for various tasks. However, these works searched the robots directly from the vast design space and ignored common structures, resulting in abnormal robots and poor performance. To tackle this problem, we propose a Symmetry-Aware Robot Design (SARD) framework that exploits the structure of the design space by incorporating symmetry searching into the robot design process. Specifically, we represent symmetries with the subgroups of the dihedral group and search for the optimal symmetry in structured subgroups. Then robots are designed under the searched symmetry. In this way, SARD can design efficient symmetric robots while covering the original design space, which is theoretically analyzed.},
  author = {Heng Dong 0001 and Junyu Zhang and Tonghan Wang 0001 and Chongjie Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023symmetryaware.pdf:pdf},
  mdate = {2025-06-26},
  pages = {8334--8355},
  pdf = {https://proceedings.mlr.press/v202/dong23h/dong23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Symmetry-Aware Robot Design with Structured Subgroups},
  url = {https://proceedings.mlr.press/v202/dong23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dorfman2023docofl,
  abstract = {Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients may appear only once during training and thus must download the model parameters. Accordingly, we propose DoCoFL -- a new framework for downlink compression in the cross-device setting. Importantly, DoCoFL can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that DoCoFL offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.},
  author = {Ron Dorfman and Shay Vargaftik and Yaniv Ben-Itzhak and Kfir Yehuda Levy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dorfman2023docofl.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8356--8388},
  pdf = {https://proceedings.mlr.press/v202/dorfman23a/dorfman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DoCoFL}: Downlink Compression for Cross-Device Federated Learning},
  url = {https://proceedings.mlr.press/v202/dorfman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dorrell2023metalearning,
  abstract = {Training data is always finite, making it unclear how to generalise to unseen situations. But, animals do generalise, wielding Occam's razor to select a parsimonious explanation of their observations. How they do this is called their inductive bias, and it is implicitly built into the operation of animals' neural circuits. This relationship between an observed circuit and its inductive bias is a useful explanatory window for neuroscience, allowing design choices to be understood normatively. However, it is generally very difficult to map circuit structure to inductive bias. Here, we present a neural network tool to bridge this gap. The tool meta-learns the inductive bias by learning functions that a neural circuit finds easy to generalise, since easy-to-generalise functions are exactly those the circuit chooses to explain incomplete data.},
  author = {Will Dorrell and Maria Yuffa and Peter E. Latham},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dorrell2023metalearning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8389--8402},
  pdf = {https://proceedings.mlr.press/v202/dorrell23a/dorrell23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Meta-Learning the Inductive Bias of Simple Neural Circuits},
  url = {https://proceedings.mlr.press/v202/dorrell23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{doshi2023selfrepellent,
  abstract = {We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real $\alpha$, we prove that the empirical distribution of the process converges almost surely to the target (stationary) distribution of the underlying Markov chain kernel.},
  author = {Vishwaraj Doshi and Jie Hu and Do Young Eun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/doshi2023selfrepellent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8403--8423},
  pdf = {https://proceedings.mlr.press/v202/doshi23a/doshi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear {M}arkov Chains},
  url = {https://proceedings.mlr.press/v202/doshi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dowling2023linear,
  abstract = {Latent Gaussian process (GP) models are widely used in neuroscience to uncover hidden state evolutions from sequential observations, mainly in neural activity recordings. While latent GP models provide a principled and powerful solution in theory, the intractable posterior in non-conjugate settings necessitates approximate inference schemes, which may lack scalability. In this work, we propose cvHM, a general inference framework for latent GP models leveraging Hida-Matérn kernels and conjugate computation variational inference (CVI). With cvHM, we are able to perform variational inference of latent neural trajectories with linear time complexity for arbitrary likelihoods. The reparameterization of stationary kernels using Hida-Matérn GPs helps us connect the latent variable models that encode prior assumptions through dynamical systems to those that encode trajectory assumptions through GPs.},
  author = {Matthew Dowling and Yuan Zhao 0004 and Il Memming Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dowling2023linear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8424--8448},
  pdf = {https://proceedings.mlr.press/v202/dowling23a/dowling23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linear Time {GPs} for Inferring Latent Trajectories from Neural Spike Trains},
  url = {https://proceedings.mlr.press/v202/dowling23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{draxler2023convergence,
  abstract = {Gaussianization is a simple generative model that can be trained without backpropagation. It has shown compelling performance on low dimensional data. As the dimension increases, however, it has been observed that the convergence speed slows down. In this work, the authors analytically show that the layer count scales linearly with dimension for Gaussian input. The model struggles to capture inter-dimensional dependencies. Empirical findings show similar linear cost increase for different input distributions.},
  author = {Felix Draxler and Lars Khmichel and Armand Rousselot and Jens M{\"u}ller and Christoph Schn{\"o}rr and Ullrich K{\"o}the},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/draxler2023convergence.pdf:pdf},
  mdate = {2024-09-04},
  pages = {8449--8468},
  pdf = {https://proceedings.mlr.press/v202/draxler23a/draxler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Convergence Rate of {G}aussianization with Random Rotations},
  url = {https://proceedings.mlr.press/v202/draxler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{driess2023palme,
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multimodal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  author = {Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/driess2023palme.pdf:pdf},
  mdate = {2024-11-12},
  pages = {8469--8488},
  pdf = {https://proceedings.mlr.press/v202/driess23a/driess23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PaLM-E: An Embodied Multimodal Language Model},
  url = {https://proceedings.mlr.press/v202/driess23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023reduce,
  abstract = {Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the use of new compositional operators and more sophisticated, Metropolis-corrected samplers. Intriguingly we find these samplers lead to notable improvements in compositional generation across a wide set of problems such as classifier-guided ImageNet modeling and compositional text-to-image generation.},
  author = {Yilun Du and Conor Durkan and Robin Strudel and Joshua B. Tenenbaum and Sander Dieleman and Rob Fergus and Jascha Sohl-Dickstein and Arnaud Doucet and Will Sussman Grathwohl},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023reduce.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8489--8510},
  pdf = {https://proceedings.mlr.press/v202/du23a/du23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC},
  url = {https://proceedings.mlr.press/v202/du23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023multitask,
  abstract = {Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandit (RepBAI-LB) and best policy identification in contextual linear bandit (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks.},
  author = {Yihan Du and Longbo Huang and Wen Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023multitask.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8511--8564},
  pdf = {https://proceedings.mlr.press/v202/du23b/du23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-task Representation Learning for Pure Exploration in Linear Bandits},
  url = {https://proceedings.mlr.press/v202/du23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023nonparametric,
  abstract = {Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric fashion, demonstrating its great potential.},
  author = {Chao Du and Tianbo Li and Tianyu Pang and Shuicheng Yan and Min Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023nonparametric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8565--8584},
  pdf = {https://proceedings.mlr.press/v202/du23c/du23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonparametric Generative Modeling with Conditional Sliced-Wasserstein Flows},
  url = {https://proceedings.mlr.press/v202/du23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023subsample,
  abstract = {The paper studies subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty λ and the limiting subsample aspect ratio φₛ (the ratio of the feature size to the subsample size), they characterize contours in the (λ, φₛ)-plane at any achievable risk. As a consequence, they prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, they prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.},
  author = {Jin-Hong Du and Pratik Patil and Arun K. Kuchibhotla},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023subsample.pdf:pdf},
  mdate = {2023-11-12},
  pages = {8585--8631},
  pdf = {https://proceedings.mlr.press/v202/du23d/du23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation},
  url = {https://proceedings.mlr.press/v202/du23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023unimodal,
  abstract = {We abstract the features (i.e. learned representations) of multi-modal data into 1) uni-modal features, which can be learned from uni-modal training, and 2) paired features, which can only be learned from cross-modal interactions. Multi-modal models are expected to benefit from cross-modal interactions on the basis of ensuring uni-modal feature learning. However, recent supervised multi-modal late-fusion training approaches still suffer from insufficient learning of uni-modal features on each modality. We prove that this phenomenon does hurt the model's generalization ability. To this end, we propose to choose a targeted late-fusion learning method for the given supervised multi-modal task from Uni-Modal Ensemble (UME) and the proposed Uni-Modal Teacher (UMT), according to the distribution of uni-modal and paired features.},
  author = {Chenzhuang Du and Jiaye Teng and Tingle Li and Yichen Liu and Tianyuan Yuan and Yue Wang and Yang Yuan and Hang Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023unimodal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8632--8656},
  pdf = {https://proceedings.mlr.press/v202/du23e/du23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Uni-Modal Feature Learning in Supervised Multi-Modal Learning},
  url = {https://proceedings.mlr.press/v202/du23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023guiding,
  abstract = {Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually match or improve performance on a range of downstream tasks.},
  author = {Yuqing Du and Olivia Watkins and Zihan Wang and C{\'e}dric Colas and Trevor Darrell and Pieter Abbeel and Abhishek Gupta and Jacob Andreas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023guiding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8657--8677},
  pdf = {https://proceedings.mlr.press/v202/du23f/du23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Guiding Pretraining in Reinforcement Learning with Large Language Models},
  url = {https://proceedings.mlr.press/v202/du23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{du2023flexible,
  abstract = {Denoising diffusion (score-based) generative models have become a popular choice for modeling complex data. Recently, a deep connection between forward-backward stochastic differential equations (SDEs) and diffusion-based models has been established, leading to the development of new SDE variants such as sub-VP and critically-damped Langevin. Despite the empirical success of some hand-crafted forward SDEs, many potentially promising forward SDEs remain unexplored. In this work, we propose a general framework for parameterizing diffusion models, particularly the spatial part of forward SDEs, by leveraging the symplectic and Riemannian geometry of the data manifold. We introduce a systematic formalism with theoretical guarantees and connect it with previous diffusion models.},
  author = {Weitao Du and He Zhang and Tao Yang and Yuanqi Du},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/du2023flexible.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8678--8696},
  pdf = {https://proceedings.mlr.press/v202/du23g/du23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Flexible Diffusion Model},
  url = {https://proceedings.mlr.press/v202/du23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duan2023fast,
  abstract = {Based on the offset Rademacher complexity, this work outlines a systematic framework for deriving sharp excess risk bounds in statistical learning without Bernstein condition. In addition to recovering fast rates in a unified way for some parametric and nonparametric supervised learning models with minimum identifiability assumptions, we also obtain new and improved results for LAD (sparse) linear regression and deep logistic regression with deep ReLU neural networks, respectively. The framework demonstrates applications to both parametric and nonparametric learning models and provides new results for specific methods like LAD linear regression and deep logistic regression with ReLU neural networks.},
  author = {Chenguang Duan and Yuling Jiao and Lican Kang and Xiliang Lu and Jerry Zhijian Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duan2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8697--8716},
  pdf = {https://proceedings.mlr.press/v202/duan23a/duan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Excess Risk Rates via Offset Rademacher Complexity},
  url = {https://proceedings.mlr.press/v202/duan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duan2023diffusion,
  abstract = {Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.},
  author = {Jinhao Duan and Fei Kong and Shiqi Wang and Xiaoshuang Shi and Kaidi Xu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duan2023diffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8717--8730},
  pdf = {https://proceedings.mlr.press/v202/duan23b/duan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Diffusion Models Vulnerable to Membership Inference Attacks?},
  url = {https://proceedings.mlr.press/v202/duan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duan2023bayesian,
  abstract = {Deep topic models have shown an impressive ability to extract multi-layer document latent representations and discover hierarchical semantically meaningful topics. However, most deep topic models are limited to the single-step generative process, despite the fact that the progressive generative process has achieved impressive performance in modeling image data. In this paper, we propose a novel progressive deep topic model that consists of a knowledge-informed textural data coarsening process and a corresponding progressive generative model. The former is used to build multi-level observations ranging from concrete to abstract, while the latter is used to generate more concrete observations gradually. The model includes a graph-enhanced decoder to capture the semantic relationships among words at different levels of observation. We provide theoretical analysis based on the principle of information theory, showing how the proposed model can alleviate the well-known latent variable collapse problem. Extensive experiments demonstrate that our proposed model effectively improves the ability of deep topic models, resulting in higher-quality latent document representations and topics.},
  author = {Zhibin Duan and Xinyang Liu and Yudi Su and Yishi Xu and Bo Chen and Mingyuan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duan2023bayesian.pdf:pdf},
  mdate = {2025-04-17},
  pages = {8731--8746},
  pdf = {https://proceedings.mlr.press/v202/duan23c/duan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayesian Progressive Deep Topic Model with Knowledge Informed Textual Data Coarsening Process},
  url = {https://proceedings.mlr.press/v202/duan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dubois2023evaluating,
  abstract = {Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs few-shot) by trading off error components. All results and pretrained models are at https://github.com/YannDubs/SSL-Risk-Decomposition.},
  author = {Yann Dubois and Tatsunori Hashimoto and Percy Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dubois2023evaluating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8779--8820},
  pdf = {https://proceedings.mlr.press/v202/dubois23a/dubois23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Evaluating Self-Supervised Learning via Risk Decomposition},
  url = {https://proceedings.mlr.press/v202/dubois23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duetting2023fully,
  abstract = {Maximizing monotone submodular functions under a matroid constraint is a classic algorithmic problem with multiple applications in data mining and machine learning. In this paper we study this classic problem in the fully dynamic setting, where elements can be both inserted and deleted. Our main result is a randomized algorithm that maintains a solution with expected approximation ratio 4 using a data structure with O(k log^2 n) amortized update time, where k is the rank of the matroid and n is the number of elements. This is the first algorithm that provides sublinear update time in the fully dynamic setting for this problem. We also provide a lower bound of Omega(k) for the update time of any algorithm that maintains any O(1)-approximation. Additionally, we present experimental results on synthetic and real-world data that show the practical efficiency of our algorithm.},
  author = {Paul Duetting and Federico Fusco and Silvio Lattanzi and Ashkan Norouzi-Fard and Morteza Zadimoghaddam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duetting2023fully.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8821--8835},
  pdf = {https://proceedings.mlr.press/v202/duetting23a/duetting23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fully Dynamic Submodular Maximization over Matroids},
  url = {https://proceedings.mlr.press/v202/duetting23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duetting2023optimal,
  abstract = {Inspired by applications in pricing and contract design, we study the maximization of one-sided Lipschitz functions, which only provide the (weaker) guarantee that they do not grow too quickly in one direction. We show that it is possible to learn a maximizer for such a function while incurring O(log log T) total regret (with a universal constant independent of the number of discontinuities / complexity of the function). This regret bound is asymptotically optimal in T due to a lower bound of Kleinberg and Leighton. By applying this algorithm, we show that one can sell digital goods to multiple buyers and learn the optimal linear contract in the principal-agent setting while incurring at most O(log log T) regret.},
  author = {Paul Duetting and Guru Guruganesh and Jon Schneider and Joshua Ruizhi Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duetting2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8836--8850},
  pdf = {https://proceedings.mlr.press/v202/duetting23b/duetting23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal No-Regret Learning for One-Sided Lipschitz Functions},
  url = {https://proceedings.mlr.press/v202/duetting23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dufumier2023integrating,
  abstract = {Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are defined and, ultimately, the quality of the learned representation. In this work, we open the door to new perspectives for CL by integrating prior knowledge, given either by generative models -- viewed as prior representations -- or weak attributes in the positive and negative sampling. We use kernel theory to propose a novel loss, called decoupled uniformity, that i) allows the integration of prior knowledge and ii) removes the positive-negative coupling in the original InfoNCE loss. We draw a connection between contrastive learning and the conditional mean embedding theory to derive tight bounds on the downstream classification loss. In an unsupervised setting, we empirically demonstrate that CL benefits from generative models to improve its representation both on natural and medical images. In a weakly supervised setting, our framework outperforms other methods.},
  author = {Benoit Dufumier and Carlo Alberto Barbano and Robin Louiset and Edouard Duchesnay and Pietro Gori},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dufumier2023integrating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8851--8878},
  pdf = {https://proceedings.mlr.press/v202/dufumier23a/dufumier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Integrating Prior Knowledge in Contrastive Learning with Kernel},
  url = {https://proceedings.mlr.press/v202/dufumier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dugan2023qflow,
  abstract = {Studying open quantum system dynamics is enabling breakthroughs in fundamental physics as well as quantum applications. The density matrix ρ (fundamental for describing such systems) is high-dimensional, so we can attempt to use deep generative neural networks to model it. However, the complex-valued nature of ρ, normalization constraints, and its complicated dynamics prohibit a seamless connection to the recent advances in deep generative modeling. We overcome this fundamental limitation by utilizing a reformulation of open quantum system dynamics to a partial differential equation (PDE) for a corresponding probability distribution Q, the Husimi Q function. Thus, we can model the Q function seamlessly with off-the-shelf deep generative models such as normalizing flows. Our method, called Q-Flow, outperforms both state-of-the-art classical and quantum solvers on challenging dynamics tasks, while offering unique capabilities such as learning from noisy data, building surrogate models, and computing observables.},
  author = {Owen M. Dugan and Peter Y. Lu and Rumen Dangovski and Di Luo and Marin Soljacic},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dugan2023qflow.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8879--8901},
  pdf = {https://proceedings.mlr.press/v202/dugan23a/dugan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows},
  url = {https://proceedings.mlr.press/v202/dugan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duong2023adaptive,
  abstract = {Statistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. Sign-constraining the gains improves robustness of the network to ill-conditioned inputs. A generalization of the circuit achieves a form of local whitening in convolutional populations, such as those found throughout the visual or auditory systems.},
  author = {Lyndon R. Duong and David Lipshutz and David J. Heeger and Dmitri B. Chklovskii and Eero P. Simoncelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duong2023adaptive.pdf:pdf},
  mdate = {2024-10-06},
  pages = {8902--8921},
  pdf = {https://proceedings.mlr.press/v202/duong23a/duong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Whitening in Neural Populations with Gain-modulating Interneurons},
  url = {https://proceedings.mlr.press/v202/duong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dupuis2023generalization,
  abstract = {Providing generalization guarantees for modern neural networks has been a crucial task in statistical learning. Recently, several studies have attempted to analyze the generalization error in such settings by using tools from fractal geometry. While these works have successfully introduced new mathematical tools to apprehend generalization, they heavily rely on a Lipschitz continuity assumption, which in general does not hold for neural networks and might make the bounds vacuous. In this work, we address this issue and prove fractal geometry-based generalization bounds without requiring any Lipschitz assumption. To achieve this goal, we build upon a classical covering argument in learning theory and introduce a data-dependent fractal dimension. Despite introducing technical complications, this new notion allows us to control the generalization error along with certain mutual information terms. Finally, we make a rigorous connection between the proposed data-dependent dimension and topological data analysis tools, which enables us to compute the dimension in a numerically efficient way.},
  author = {Benjamin Dupuis and George Deligiannidis and Umut Simsekli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dupuis2023generalization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8922--8968},
  pdf = {https://proceedings.mlr.press/v202/dupuis23a/dupuis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalization Bounds using Data-Dependent Fractal Dimensions},
  url = {https://proceedings.mlr.press/v202/dupuis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dushatskiy2023multiobjective,
  abstract = {Population Based Training (PBT) is a well-established method for hyperparameter optimization that dynamically adjusts hyperparameters during training. However, PBT is designed for single-objective optimization and does not handle scenarios where multiple competing objectives need to be optimized simultaneously. We propose Multi-Objective Population Based Training (MO-PBT), which extends PBT to the multi-objective setting. Our method maintains a population of models and uses Pareto dominance to guide the exploitation and exploration phases, allowing practitioners to discover a set of Pareto-optimal solutions representing different trade-offs between competing objectives. We demonstrate the effectiveness of MO-PBT on various machine learning tasks where practitioners need to balance multiple performance criteria, such as accuracy and computational efficiency.},
  author = {Arkadiy Dushatskiy and Alexander Chebykin and Tanja Alderliesten and Peter A. N. Bosman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dushatskiy2023multiobjective.pdf:pdf},
  mdate = {2023-08-28},
  pages = {8969--8989},
  pdf = {https://proceedings.mlr.press/v202/dushatskiy23a/dushatskiy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Objective Population Based Training},
  url = {https://proceedings.mlr.press/v202/dushatskiy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{dutordoir2023neural,
  abstract = {Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.},
  author = {Vincent Dutordoir and Alan Saul and Zoubin Ghahramani and Fergus Simpson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/dutordoir2023neural.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {8990--9012},
  pdf = {https://proceedings.mlr.press/v202/dutordoir23a/dutordoir23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Diffusion Processes},
  url = {https://proceedings.mlr.press/v202/dutordoir23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{duval2023faenet,
  abstract = {Applications of machine learning techniques for materials modeling typically involve functions that are known to be equivariant or invariant to specific symmetries. While graph neural networks (GNNs) have proven successful in such applications, conventional GNN approaches that enforce symmetries via the model architecture often reduce expressivity, scalability or comprehensibility. In this paper, we introduce (1) a flexible, model-agnostic framework based on stochastic frame averaging that enforces E(3) equivariance or invariance, without any architectural constraints; (2) FAENet: a simple, fast and expressive GNN that leverages stochastic frame averaging to process geometric information without constraints. We prove the validity of our method theoretically and demonstrate its superior accuracy and computational scalability in materials modeling on the OC20 dataset (S2EF, IS2RE) as well as common molecular modeling tasks (QM9, QM7-X).},
  author = {Alexandre Duval and Victor Schmidt and Alex Hernández-García and Santiago Miret and Fragkiskos D. Malliaros and Yoshua Bengio and David Rolnick},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/duval2023faenet.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9013--9033},
  pdf = {https://proceedings.mlr.press/v202/duval23a/duval23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FAENet}: Frame Averaging Equivariant {GNN} for Materials Modeling},
  url = {https://proceedings.mlr.press/v202/duval23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eiben2023computational,
  abstract = {Hypersphere classification is a classical and foundational method that can provide easy-to-process explanations for the classification of real-valued as well as binary data. However, obtaining an (ideally concise) explanation via hypersphere classification is much more difficult when dealing with binary data as opposed to real-valued data. In this paper, we perform the first complexity-theoretic study of the hypersphere classification problem for binary data. We use the fine-grained parameterized complexity paradigm to analyze the impact of structural properties that may be present in the input data as well as potential conciseness constraints. Our results include not only stronger lower bounds but also a number of new fixed-parameter algorithms for hypersphere classification of binary data, which can find an exact and concise explanation when one exists.},
  author = {Eduard Eiben and Robert Ganian and Iyad A. Kanj and Sebastian Ordyniak and Stefan Szeider},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eiben2023computational.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9060--9070},
  pdf = {https://proceedings.mlr.press/v202/eiben23a/eiben23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Computational Complexity of Concise Hypersphere Classification},
  url = {https://proceedings.mlr.press/v202/eiben23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eijkelboom2023equivariant,
  abstract = {This paper presents E(n) Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an E(n) equivariant fashion. EMPSNs simultaneously generalize E(n) Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks, thereby serving as a proof of concept for combining geometric and topological information in graph learning. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method individually, being on par with state-of-the-art approaches for learning on geometric graphs. Moreover, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high-dimensional simplicial structures.},
  author = {Floor Eijkelboom and Rob Hesselink and Erik J. Bekkers},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eijkelboom2023equivariant.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9071--9081},
  pdf = {https://proceedings.mlr.press/v202/eijkelboom23a/eijkelboom23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {E(n) Equivariant Message Passing Simplicial Networks},
  url = {https://proceedings.mlr.press/v202/eijkelboom23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eilat2023performative,
  abstract = {The primary goal in recommendation is to suggest relevant content to users, but optimizing for accuracy often results in recommendations that lack diversity. To remedy this, conventional approaches such as re-ranking improve diversity by presenting more diverse items. Here we argue that to promote inherent and prolonged diversity, the system must encourage its creation. Towards this, we harness the performative nature of recommendation, and show how learning can incentivize strategic content creators to create diverse content. Our approach relies on a novel form of regularization that anticipates strategic changes to content, and penalizes for content homogeneity. We provide analytic and empirical results that demonstrate when and how diversity can be incentivized, and experimentally demonstrate the utility of our approach on synthetic and semi-synthetic data.},
  author = {Itay Eilat and Nir Rosenfeld},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eilat2023performative.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9082--9103},
  pdf = {https://proceedings.mlr.press/v202/eilat23a/eilat23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Performative Recommendation: Diversifying Content via Strategic Incentives},
  url = {https://proceedings.mlr.press/v202/eilat23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eimer2023hyperparameters,
  abstract = {In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent's final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We demonstrate that adopting HPO tools and practices has the potential to make RL research more efficient, accessible and reproducible, comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead.},
  author = {Theresa Eimer and Marius Lindauer and Roberta Raileanu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eimer2023hyperparameters.pdf:pdf},
  mdate = {2023-11-12},
  month = {7},
  pages = {9104--9149},
  pdf = {https://proceedings.mlr.press/v202/eimer23a/eimer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyperparameters in Reinforcement Learning and How To Tune Them},
  url = {https://proceedings.mlr.press/v202/eimer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eliasof2023graph,
  abstract = {Two main families of node feature augmentation schemes have been explored for enhancing GNNs: random features and spectral positional encoding. Surprisingly, however, there is still no clear understanding of the relation between these two augmentation schemes. Here we propose a novel family of positional encoding schemes which draws a link between the above two approaches and improves over both. The new approach, named Random Feature Propagation (RFP), is inspired by the power iteration method and its generalizations. It concatenates several intermediate steps of an iterative algorithm for computing the dominant eigenvectors of a propagation matrix, starting from random node features. Notably, these propagation steps are based on graph-dependent propagation operators that can be either predefined or learned.},
  author = {Moshe Eliasof and Fabrizio Frasca and Beatrice Bevilacqua and Eran Treister and Gal Chechik and Haggai Maron},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eliasof2023graph.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9202--9223},
  pdf = {https://proceedings.mlr.press/v202/eliasof23a/eliasof23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Positional Encoding via Random Feature Propagation},
  url = {https://proceedings.mlr.press/v202/eliasof23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eliasof2023improving,
  abstract = {Graph Neural Networks (GNNs) are limited in their propagation operators. In many cases, these operators often contain non-negative elements only and are shared across channels, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs. In this paper, we bridge these gaps by incorporating trainable channel-wise weighting factors ω to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called ωGNN, and is easy to implement. We study two variants: ωGCN and ωGAT. For ωGCN, we theoretically analyse its behaviour and the impact of ω on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth.},
  author = {Moshe Eliasof and Lars Ruthotto and Eran Treister},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eliasof2023improving.pdf:pdf},
  mdate = {2024-02-05},
  month = {7},
  pages = {9224--9245},
  pdf = {https://proceedings.mlr.press/v202/eliasof23b/eliasof23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Graph Neural Networks with Learnable Propagation Operators},
  url = {https://proceedings.mlr.press/v202/eliasof23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{elimelech2023phase,
  abstract = {The study examines the problem of detecting correlation between two Gaussian databases X∈ℝⁿˣᵈ and Y∈ℝⁿˣᵈ, each composed of n users with d features. This problem has relevance in analyzing social media, computational biology, etc. The authors formulate this as a hypothesis testing problem where under the null hypothesis, these two databases are statistically independent. The paper focuses on understanding phase transitions in the detection capabilities when trying to identify correlations between large databases, which is an important problem in machine learning and statistics with applications in social media analysis and computational biology.},
  author = {Dor Elimelech and Wasim Huleihel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/elimelech2023phase.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9246--9266},
  pdf = {https://proceedings.mlr.press/v202/elimelech23a/elimelech23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Phase Transitions in the Detection of Correlated Databases},
  url = {https://proceedings.mlr.press/v202/elimelech23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{elkin2023new,
  abstract = {Given a reference set R of n points and a query set Q of m points in a metric space, this paper studies an important problem of finding k-nearest neighbors of every point q ∈ Q in the set R in a near-linear time. In the paper at ICML 2006, Beygelzimer, Kakade, and Langford introduced a cover tree and attempted to prove that this tree can be built in O(n log n) time while the nearest neighbor search can be done O(n log m) time with a hidden dimensionality factor. In 2015, section 5.3 of Curtin's PhD pointed out that the proof of the latter claim can have a serious gap in time complexity estimation. A paper at TopoInVis 2022 reported explicit counterexamples for a key step in the proofs of both claims. This paper fills a substantial gap in the past proofs of time complexity by defining a simpler compressed cover tree on the reference set R. The first new algorithm constructs a compressed cover tree in O(n log n) time. The second new algorithm finds all k-nearest neighbors of all points from Q using a compressed cover tree in time O(m(k+log n)log k) with a hidden dimensionality factor depending on point distributions of the given sets R,Q but not on their sizes.},
  author = {Yury Elkin and Vitaliy Kurlin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/elkin2023new.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {9267--9311},
  pdf = {https://proceedings.mlr.press/v202/elkin23a/elkin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A New Near-Linear Time Algorithm for k-Nearest Neighbor Search Using a Compressed Cover Tree},
  url = {https://proceedings.mlr.press/v202/elkin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{endo2023motion,
  abstract = {We propose {HumanMotionQA}, a new task to evaluate complex, multi-step reasoning abilities on long-form human motion sequences. Our task requires models to detect motor cues in small portions of motion sequences, reason temporally about when events occur, and query specific motion attributes. Additionally, we propose {NSPose}, a neuro-symbolic method that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. {NSPose} achieves strong performance on our task while remaining interpretable and generalizable to new compositions of motion concepts at test time.},
  author = {Mark Endo and Joy Hsu and Jiaman Li and Jiajun Wu 0001},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/endo2023motion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9312--9328},
  pdf = {https://proceedings.mlr.press/v202/endo23a/endo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Motion Question Answering via Modular Motion Programs},
  url = {https://proceedings.mlr.press/v202/endo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{enguehard2023learning,
  abstract = {Explaining predictions based on multivariate time series data carries the additional difficulty of handling not only multiple features, but also time dependencies. It matters not only what happened, but also when, and the same feature could have a very different impact on a prediction depending on this time information. Previous work has used perturbation-based saliency methods to tackle this issue, perturbing an input using a trainable mask to discover which features at which times are driving the predictions. However these methods introduce fixed perturbations, inspired from similar methods on static data, while there seems to be little motivation to do so on temporal data. In this work, we aim to explain predictions by learning not only masks, but also associated perturbations.},
  author = {Joseph Enguehard},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/enguehard2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9329--9342},
  pdf = {https://proceedings.mlr.press/v202/enguehard23a/enguehard23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Perturbations to Explain Time Series Predictions},
  url = {https://proceedings.mlr.press/v202/enguehard23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{erez2023regret,
  abstract = {An abundance of recent impossibility results establish that regret minimization in Markov games with adversarial opponents is both statistically and computationally intractable. Nevertheless, none of these results preclude the possibility of regret minimization under the assumption that all parties adopt the same learning procedure. In this work, we present the first (to our knowledge) algorithm for learning in general-sum Markov games that provides sublinear regret guarantees when executed by all agents. The bounds we obtain are for swap regret, and thus, along the way, imply convergence to a correlated equilibrium. Our algorithm is decentralized, computationally efficient, and does not require any communication between agents.},
  author = {Liad Erez and Tal Lancewicki and Uri Sherman and Tomer Koren and Yishay Mansour},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/erez2023regret.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9343--9373},
  pdf = {https://proceedings.mlr.press/v202/erez23a/erez23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regret Minimization and Convergence to Equilibria in General-sum Markov Games},
  url = {https://proceedings.mlr.press/v202/erez23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{esposito2023delayed,
  abstract = {We study a $K$-armed bandit with delayed feedback and intermediate observations. We consider a model, where intermediate observations have a form of a finite state, which is observed immediately after taking an action, whereas the loss is observed after an adversarially chosen delay. We show that the regime of the mapping of states to losses determines the complexity of the problem, irrespective of whether the mapping of actions to states is stochastic or adversarial. If the mapping of states to losses is adversarial, then the regret rate is of order $\sqrt{(K+d)T}$ (within log factors), where $T$ is the time horizon and $d$ is a fixed delay. This matches the regret rate of a $K$-armed bandit with delayed feedback and without intermediate observations, implying that intermediate observations are not helpful. However, if the mapping of states to losses is stochastic, we show that the regret grows at a rate of $\sqrt{(K+\min\{|S|,d\})T}$ (within log factors), implying that if the number $|S|$ of states is smaller than the delay, then intermediate observations help.},
  author = {Emmanuel Esposito and Saeed Masoudian and Hao Qiu and Dirk van der Hoeven and Nicol Cesa-Bianchi and Yevgeny Seldin},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/esposito2023delayed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9374--9395},
  pdf = {https://proceedings.mlr.press/v202/esposito23a/esposito23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Delayed Bandits: When Do Intermediate Observations Help?},
  url = {https://proceedings.mlr.press/v202/esposito23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{esteves2023scaling,
  abstract = {Spherical CNNs generalize CNNs to functions on the sphere, by using spherical convolutions as the main linear operation. The most accurate and efficient way to compute spherical convolutions is in the spectral domain (via the convolution theorem), which is still costlier than the usual planar convolutions. For this reason, applications of spherical CNNs have so far been limited to small problems that can be approached with low model capacity. In this work, we show how spherical CNNs can be scaled for much larger problems. To achieve this, we make critical improvements including novel variants of common model components, an implementation of core operations to exploit hardware accelerator characteristics, and application-specific input representations that exploit the properties of our model.},
  author = {Carlos Esteves and Jean-Jacques E. Slotine and Ameesh Makadia},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/esteves2023scaling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9396--9411},
  pdf = {https://proceedings.mlr.press/v202/esteves23a/esteves23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Spherical CNNs},
  url = {https://proceedings.mlr.press/v202/esteves23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{even2023stochastic,
  abstract = {We study a variation of vanilla stochastic gradient descent where the optimizer only has access to a Markovian sampling scheme. These schemes encompass applications that range from decentralized optimization with a random walker (token algorithms), to RL and online system identification problems. We focus on obtaining rates of convergence under the least restrictive assumptions possible on the underlying Markov chain and on the functions optimized. We first unveil the theoretical lower bound for methods that sample stochastic gradients along the path of a Markov chain, making appear a dependency in the hitting time of the underlying Markov chain. We then study Markov chain SGD (MC-SGD) under much milder regularity assumptions than prior works (e.g., no bounded gradients or domain, and infinite state spaces).},
  author = {Mathieu Even},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/even2023stochastic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9412--9439},
  pdf = {https://proceedings.mlr.press/v202/even23a/even23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stochastic Gradient Descent under Markovian Sampling Schemes},
  url = {https://proceedings.mlr.press/v202/even23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{evron2023continual,
  abstract = {We analyze continual learning on a sequence of separable linear classification tasks with binary labels. We show theoretically that learning with weak regularization reduces to solving a sequential max-margin problem, corresponding to a special case of the Projection Onto Convex Sets (POCS) framework. We then develop upper bounds on the forgetting and other quantities of interest under various settings with recurring tasks, including cyclic and random orderings of tasks. We discuss several practical implications to popular training practices like regularization scheduling and weighting.},
  author = {Itay Evron and Edward Moroshko and Gon Buzaglo and Maroun Khriesh and Badea Marjieh and Nathan Srebro and Daniel Soudry},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/evron2023continual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9440--9484},
  pdf = {https://proceedings.mlr.press/v202/evron23a/evron23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continual Learning in Linear Classification on Separable Data},
  url = {https://proceedings.mlr.press/v202/evron23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{eysenbach2023connection,
  abstract = {As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One class of methods, known as one-step RL, perform just one step of policy improvement. These methods, which include advantage-weighted regression and conditional behavioral cloning, are thus simple and stable, but can have limited asymptotic performance. A second class of methods, known as critic regularization, perform many steps of policy improvement with a regularized objective. These methods typically require more compute but have appealing lower-bound guarantees. In this paper, we draw a connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL.},
  author = {Benjamin Eysenbach and Matthieu Geist and Sergey Levine and Ruslan Salakhutdinov},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/eysenbach2023connection.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9485--9507},
  pdf = {https://proceedings.mlr.press/v202/eysenbach23a/eysenbach23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Connection between One-Step RL and Critic Regularization in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/eysenbach23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{faber2023neural,
  abstract = {We study the problem of learning comparisons between numbers with neural networks. Despite comparisons being a seemingly simple problem, we find that both general-purpose models such as multilayer perceptrons (MLPs) as well as arithmetic architectures such as the Neural Arithmetic Logic Unit (NALU) struggle with learning comparisons. Neither architecture can extrapolate to much larger numbers than those seen in the training set. We propose a novel differentiable architecture, the Neural Status Register (NSR) to solve this problem. We can combine the NSR with other neural models to solve interesting problems such as piecewise-defined arithmetic, comparison of digit images, recurrent problems, or finding shortest paths in graphs.},
  author = {Lukas Faber and Roger Wattenhofer},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/faber2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9508--9522},
  pdf = {https://proceedings.mlr.press/v202/faber23a/faber23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Status Registers},
  url = {https://proceedings.mlr.press/v202/faber23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fahrbach2023learning,
  abstract = {We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret.},
  author = {Matthew Fahrbach and Adel Javanmard and Vahab Mirrokni and Pratik Worah},
  booktitle = {International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fahrbach2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9523--9546},
  pdf = {https://proceedings.mlr.press/v202/fahrbach23a/fahrbach23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Rate Schedules in the Presence of Distribution Shift},
  url = {https://proceedings.mlr.press/v202/fahrbach23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{faletto2023predicting,
  author = {Gregory Faletto and Jacob Bien},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/faletto2023predicting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9547--9602},
  pdf = {https://proceedings.mlr.press/v202/faletto23a/faletto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Predicting Rare Events by Shrinking Towards Proportional Odds},
  url = {https://proceedings.mlr.press/v202/faletto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fan2023optimizing,
  abstract = {We propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting in sample quality comparable to or even surpassing that of the full-step model across various datasets.},
  author = {Ying Fan and Kangwook Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fan2023optimizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9623--9639},
  pdf = {https://proceedings.mlr.press/v202/fan23b/fan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimizing {DDPM} Sampling with Shortcut Fine-Tuning},
  url = {https://proceedings.mlr.press/v202/fan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fan2023lsds,
  abstract = {The k-means++ initialization algorithm has driven new acceleration strategies and theoretical analysis for solving the k-means clustering problem. The state-of-the-art variant, called LocalSearch++, adds extra local search steps upon k-means++ to achieve constant approximation error in expectation. In this paper, we propose a new variant named LSDS++, which improves the sampling efficiency of LocalSearch++ via a strategy called dual sampling. By defining a new capture graph based on the concept of coreset, the proposed LSDS++ is able to achieve the same expected constant error with reduced complexity. Experiments are conducted to justify the benefit of LSDS++ in practice.},
  author = {Chenglin Fan and Ping Li and Xiaoyun Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fan2023lsds.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9640--9649},
  pdf = {https://proceedings.mlr.press/v202/fan23c/fan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LSDS++}: Dual Sampling for Accelerated k-means++},
  url = {https://proceedings.mlr.press/v202/fan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fan2023freeform,
  abstract = {Gaussian process state-space models (GPSSMs) provide a principled and flexible approach to modeling the dynamics of a latent state, which is observed at discrete-time points via a likelihood model. However, inference in GPSSMs is computationally and statistically challenging due to the large number of latent variables in the model and the strong temporal dependencies between them. We propose a new method for inference in Bayesian GPSSMs, which overcomes the drawbacks of previous approaches, namely over-simplified assumptions, and high computational requirements. Our method is based on free-form variational inference via stochastic gradient Hamiltonian Monte Carlo within the inducing-variable formalism. By exploiting our proposed variational distribution, we provide a collapsed extension of our method where the inducing variables are marginalized analytically. We also showcase results when combining our framework with particle MCMC methods. We show that, on six real-world datasets, our approach can learn transition dynamics and latent states more accurately than competing methods.},
  author = {Xuhui Fan and Edwin V. Bonilla and Terence J. O'Kane and Scott A. Sisson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fan2023freeform.pdf:pdf},
  mdate = {2024-01-22},
  pages = {9603--9622},
  pdf = {https://proceedings.mlr.press/v202/fan23a/fan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Free-Form Variational Inference for {G}aussian Process State-Space Models},
  url = {https://proceedings.mlr.press/v202/fan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fan2023smart,
  abstract = {The simplex method, introduced by Dantzig more than half a century ago, is still one of the most efficient methods for solving large-scale linear programming (LP) problems. While the simplex method is known to have the finite termination property under mild assumptions, the number of iterations until optimality largely depends on the choice of initial basis. Existing strategies for selecting an advanced initial basis are mostly rule-based. These rules usually require extensive expert knowledge and empirical study to develop. Yet, many of them fail to exhibit consistent improvement, even for LP problems that arise in a single application scenario. In this paper, we propose a learning-based approach for initial basis selection. We employ graph neural networks as a building block and develop a model that attempts to capture the relationship between LP problems and their optimal bases. We leverage the fact that we often encounter sets of LP problems that exhibit considerable similarities, such as in scenarios like a manufacturer's daily production planning or an airport's hourly flight scheduling.},
  author = {Zhenan Fan and Xinglu Wang and Oleksandr Yakovenko and Abdullah Ali Sivas and Owen Ren and Yong Zhang and Zirui Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fan2023smart.pdf:pdf},
  mdate = {2023-12-01},
  pages = {9650--9664},
  pdf = {https://proceedings.mlr.press/v202/fan23d/fan23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Smart Initial Basis Selection for Linear Programs},
  url = {https://proceedings.mlr.press/v202/fan23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fanaskov2023general,
  abstract = {Replacing classical partial differential equation (PDE) integrators with neural networks is a challenging task. The popular strategy involves generating input-output pairs with a PDE solver, training neural networks in a regression setting, and using the trained model as a cheap surrogate for the solver. However, the bottleneck is the number of expensive queries of a PDE solver needed to generate the dataset. We propose a computationally cheap augmentation strategy based on general covariance and simple random coordinate transformations. Our approach relies on the fact that physical laws are independent of the coordinate choice, so changing the coordinate system preserves the type of a parametric PDE and only changes PDE's data. For the neural networks and partial differential equations tested, our augmentation improved test error by 23\% on average. The worst observed result was a 17\% increase in test error for multilayer perceptron, while the best case showed an 80\% decrease for dilated residual network.},
  author = {Vladimir Fanaskov and Tianchi Yu and Alexander Rudikov and Ivan V. Oseledets},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fanaskov2023general.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9665--9688},
  pdf = {https://proceedings.mlr.press/v202/fanaskov23a/fanaskov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {General Covariance Data Augmentation for Neural {PDE} Solvers},
  url = {https://proceedings.mlr.press/v202/fanaskov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fandina2023fast,
  abstract = {The Johnson-Lindenstrauss lemma is a cornerstone result in dimensionality reduction, stating it is possible to embed a set of n points in d-dimensional Euclidean space into optimal k=O(ε^{-2} ln n) dimensions, while preserving all pairwise distances to within a factor (1 ± ε). The original Fast Johnson-Lindenstrauss (Fast JL) transform supports computing the embedding of a data point in O(d ln d + k ln^2 n) time, where the d ln d term comes from multiplication with a d × d Hadamard matrix and the k ln^2 n term comes from multiplication with a sparse k × d matrix. We provide a surprising new analysis of the Fast JL transform, showing that the k ln^2 n term in the embedding time can be improved to (k ln^2 n)/α for an α = Ω(min{ε^{-1}ln(1/ε), ln n}). This is significant because despite the Fast JL transform being more than a decade old, it is one of the fastest dimensionality reduction techniques for many tradeoffs between ε, d and n.},
  author = {Ora Nova Fandina and Mikael M{\o}ller H{\o}gsgaard and Kasper Green Larsen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fandina2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9689--9715},
  pdf = {https://proceedings.mlr.press/v202/fandina23a/fandina23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Fast {J}ohnson-{L}indenstrauss Transform Is Even Faster},
  url = {https://proceedings.mlr.press/v202/fandina23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fang2023regression,
  abstract = {The assumption that response and predictor belong to the same statistical unit may be violated in practice. Unbiased estimation and recovery of true label ordering based on unlabeled data are challenging tasks and have attracted increasing attention in the recent literature. In this paper, we present a relatively complete analysis of label permutation problem for the generalized linear model with multivariate responses. The theory is established under different scenarios: with knowledge of true parameters, with partial knowledge of underlying label permutation matrix and without any knowledge. Our results remove the stringent conditions required by the current literature and are further extended to the missing observation setting which has never been considered in the field of label permutation problem. On the computational side, we propose two methods, maximum likelihood estimation algorithm and two-step estimation algorithm, to accommodate for different settings.},
  author = {Guanhua Fang and Ping Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fang2023regression.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9716--9760},
  pdf = {https://proceedings.mlr.press/v202/fang23a/fang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regression with Label Permutation in Generalized Linear Model},
  url = {https://proceedings.mlr.press/v202/fang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{farhadkhani2023robust,
  abstract = {Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates.},
  author = {Sadegh Farhadkhani and Rachid Guerraoui and Nirupam Gupta and L{\^e}-Nguy{\^e}n Hoang and Rafael Pinot and John Stephan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/farhadkhani2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9761--9813},
  pdf = {https://proceedings.mlr.press/v202/farhadkhani23a/farhadkhani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Collaborative Learning with Linear Gradient Overhead},
  url = {https://proceedings.mlr.press/v202/farhadkhani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fasina2023neural,
  abstract = {Data diffusion embeddings are common in unsupervised learning, but they are inherently limited due to their discrete nature. To address this, we propose neural FIM, a method for computing the Fisher information metric (FIM) from point cloud data - allowing for a continuous manifold model for the data. Neural FIM creates an extensible metric space from discrete point cloud data such that information from the metric can inform us of manifold characteristics such as volume and geodesics. We demonstrate Neural FIM's utility in selecting parameters for the PHATE visualization method as well as its ability to obtain information pertaining to local volume illuminating branching points and cluster centers embeddings of a toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs (immune cells).},
  author = {Oluwadamilola Fasina and Guillaume Huguet and Alexander Tong and Yanlei Zhang and Guy Wolf and Maximilian Nickel and Ian Adelstein and Smita Krishnaswamy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fasina2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9814--9826},
  pdf = {https://proceedings.mlr.press/v202/fasina23a/fasina23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural {FIM} for learning {F}isher information metrics from point cloud data},
  url = {https://proceedings.mlr.press/v202/fasina23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fatkhullin2023stochastic,
  abstract = {Recently, the impressive empirical success of policy gradient (PG) methods has catalyzed the development of their theoretical foundations. Despite the huge efforts directed at the design of efficient stochastic PG-type algorithms, the understanding of their convergence to a globally optimal policy is still limited. In this work, we develop improved global convergence guarantees for a general class of Fisher-non-degenerate parameterized policies which allows to address the case of continuous state action spaces. First, we propose a Normalized Policy Gradient method with Implicit Gradient Transport (N-PG-IGT) and derive a $\tilde{\mathcal{O}}(\varepsilon^{-2.5})$ sample complexity of this method for finding a global $\varepsilon$-optimal policy. Improving over the previously known $\tilde{\mathcal{O}}(\varepsilon^{-3})$ complexity, this algorithm does not require the use of importance sampling or second-order information and samples only one trajectory per iteration. Second, we further improve this complexity to $\tilde{\mathcal{\mathcal{O}}}(\varepsilon^{-2})$ by considering a Hessian-Aided Recursive Policy Gradient ((N)-HARPG) algorithm enhanced with a correction based on a Hessian-vector product. Interestingly, both algorithms are (i) simple and easy to implement: single-loop, do not require large batches of trajectories and sample at most two trajectories per iteration; (ii) computationally and memory efficient: they do not require expensive subroutines at each iteration and can be implemented with memory linear in the dimension of parameters.},
  author = {Ilyas Fatkhullin and Anas Barakat and Anastasia Kireeva and Niao He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fatkhullin2023stochastic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9827--9869},
  pdf = {https://proceedings.mlr.press/v202/fatkhullin23a/fatkhullin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies},
  url = {https://proceedings.mlr.press/v202/fatkhullin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feldstein2023parallel,
  abstract = {Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model. However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.},
  author = {Jonathan Feldstein and Modestas Jurcius and Efthymia Tsamoura},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feldstein2023parallel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9870--9885},
  pdf = {https://proceedings.mlr.press/v202/feldstein23a/feldstein23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Parallel Neurosymbolic Integration with Concordia},
  url = {https://proceedings.mlr.press/v202/feldstein23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fellows2023why,
  abstract = {Temporal difference methods that use infrequently updated target values for policy evaluation in Markov Decision Processes have been integral to recent successes in deep reinforcement learning, yet a complete theoretical explanation for the effectiveness of target networks remains elusive. We provide an analysis of this popular class of algorithms to finally answer the question: why do target networks stabilise TD learning? To do so, we formalize the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms.},
  author = {Mattie Fellows and Matthew J. A. Smith and Shimon Whiteson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fellows2023why.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9886--9909},
  pdf = {https://proceedings.mlr.press/v202/fellows23a/fellows23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why Target Networks Stabilise Temporal Difference Methods},
  url = {https://proceedings.mlr.press/v202/fellows23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023improved,
  abstract = {We investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. In the myopic advertiser setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight O(√T) regret in the worst-case and negative regret when the values are static across all auctions and there is a gap between the highest expected value (i.e. value multiplied by their CTR) and second highest expected value ad.},
  author = {Zhe Feng 0004 and Christopher Liaw and Zixin Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023improved.pdf:pdf},
  mdate = {2025-04-24},
  pages = {9921--9937},
  pdf = {https://proceedings.mlr.press/v202/feng23b/feng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Online Learning Algorithms for CTR Prediction in Ad Auctions},
  url = {https://proceedings.mlr.press/v202/feng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feng2023fractional,
  abstract = {Coordinate denoising as a promising 3D molecular pre-training method has achieved remarkable performance in downstream drug discovery tasks, with an objective theoretically equivalent to learning force fields. However, there exist two key challenges: low coverage samples and isotropic force fields, caused by molecular distributions assumed by existing denoising methods failing to capture the anisotropic characteristics of molecules. To tackle the challenges, we propose a novel hybrid noise strategy that includes noises on both dihedral angles and coordinates. The fractional denoising (Frad) framework decouples noise design from the constraints imposed by force learning equivalence, making the noise customizable and allowing for incorporating chemical priors to significantly improve molecular distribution modeling. Experiments demonstrate that our framework consistently outperforms existing methods, establishing state-of-the-art results across force prediction, quantum chemical properties, and binding affinity tasks, with refined noise design enhancing force accuracy and sampling coverage.},
  author = {Shikun Feng and Yuyan Ni and Yanyan Lan and Zhi-Ming Ma and Wei-Ying Ma},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feng2023fractional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9938--9961},
  pdf = {https://proceedings.mlr.press/v202/feng23c/feng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fractional Denoising for 3D Molecular Pre-training},
  url = {https://proceedings.mlr.press/v202/feng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feng2023improved,
  abstract = {We study streaming algorithms in the white-box adversarial stream model, where the internal state of the streaming algorithm is revealed to an adversary who adaptively generates the stream updates, but the algorithm obtains fresh randomness unknown to the adversary at each time step. We incorporate cryptographic assumptions to construct robust algorithms against such adversaries. We propose efficient algorithms for sparse recovery of vectors, low rank recovery of matrices and tensors, as well as low rank plus sparse recovery of matrices (robust PCA). Unlike deterministic algorithms, our algorithms can report when the input is not sparse or low rank even in the presence of such an adversary. We give the first efficient algorithm for outputting a matching in a graph with insertions and deletions to its edges provided the matching size is small, and otherwise declare the matching size is large.},
  author = {Ying Feng and David P. Woodruff},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feng2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9962--9975},
  pdf = {https://proceedings.mlr.press/v202/feng23d/feng23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Algorithms for White-Box Adversarial Streams},
  url = {https://proceedings.mlr.press/v202/feng23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feng2023nonstationary,
  abstract = {General function approximation is a powerful tool to handle large state and action spaces in a broad range of reinforcement learning (RL) scenarios. However, theoretical understanding of non-stationary MDPs with general function approximation is still limited. This work makes the first such an attempt to address this gap. We introduce a new complexity metric called dynamic Bellman Eluder (DBE) dimension for non-stationary MDPs, which subsumes majority of existing tractable RL problems in static MDPs as well as non-stationary MDPs. We propose a novel confidence-set based model-free algorithm called SW-OPEA, which features a sliding window mechanism and a new confidence set design for non-stationary MDPs. We provide an upper bound on the dynamic regret for our algorithm, showing that SW-OPEA is provably efficient as long as the variation budget is not significantly large.},
  author = {Songtao Feng and Ming Yin 0003 and Ruiquan Huang and Yu-Xiang Wang 0003 and Jing Yang 0002 and Yingbin Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feng2023nonstationary.pdf:pdf},
  mdate = {2023-11-20},
  pages = {9976--10007},
  pdf = {https://proceedings.mlr.press/v202/feng23e/feng23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Non-stationary Reinforcement Learning under General Function Approximation},
  url = {https://proceedings.mlr.press/v202/feng23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{feofanov2023random,
  abstract = {We propose a theoretical framework to analyze semi-supervised classification under the low density separation assumption in a high-dimensional regime. We introduce QLDS, a linear classification model, where the low density separation assumption is implemented via quadratic margin maximization. The algorithm has an explicit solution with rich theoretical properties, showing that particular cases include the least-square support vector machine (supervised case), spectral clustering (fully unsupervised regime), and semi-supervised graph-based approaches. QLDS establishes a smooth bridge between these supervised and unsupervised learning methods. Using recent advances in random matrix theory, we formally derive a theoretical evaluation of the classification error in the asymptotic regime.},
  author = {Vasilii Feofanov and Malik Tiomoko and Aladin Virmaux},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/feofanov2023random.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10008--10033},
  pdf = {https://proceedings.mlr.press/v202/feofanov23a/feofanov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Matrix Analysis to Balance between Supervised and Unsupervised Learning under the Low Density Separation Assumption},
  url = {https://proceedings.mlr.press/v202/feofanov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ferber2023surco,
  abstract = {Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose SurCo that learns linear SURrogate costs which can be used in existing COmbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three SurCo variants: SurCo-zero for individual nonlinear problems, SurCo-prior for problem distributions, and SurCo-hybrid to combine both distribution and problem-specific information.},
  author = {Aaron M. Ferber and Taoan Huang and Daochen Zha and Martin Schubert and Benoit Steiner and Bistra Dilkina and Yuandong Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ferber2023surco.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10034--10052},
  pdf = {https://proceedings.mlr.press/v202/ferber23a/ferber23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SurCo: Learning Linear SURrogates for COmbinatorial Nonlinear Optimization Problems},
  url = {https://proceedings.mlr.press/v202/ferber23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fernandes2023scaling,
  abstract = {We present a large-scale empirical study of the scaling properties of multilingual neural machine translation models. We examine how increases in model size affect model performance and investigate the role of the training mixture composition on the scaling behavior. We find that changing the weightings of individual language pairs in the training mixture only affects the multiplicative factor of the scaling law. Multilingual models trained using different mixing rates all exhibit the same scaling exponent. We formulate a novel joint scaling law that, by using a single function, allows us to compute the effective number of parameters allocated to each language pair and examine the role of language similarity in the scaling behavior. We find little evidence that language similarity has any impact on scaling behavior. We also find that the direction of multilinguality plays a significant role in scaling behavior.},
  author = {Patrick Fernandes and Behrooz Ghorbani and Xavier Garcia and Markus Freitag and Orhan Firat},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fernandes2023scaling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10053--10071},
  pdf = {https://proceedings.mlr.press/v202/fernandes23a/fernandes23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Laws for Multilingual Neural Machine Translation},
  url = {https://proceedings.mlr.press/v202/fernandes23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fichtenberger2023constant,
  abstract = {The paper studies fine-grained error bounds for differentially private algorithms for counting under continual observation. Their main insight is that the matrix mechanism when using lower-triangular matrices can be used in the continual observation model. They provide an explicit factorization for the counting matrix M_count and upper bound the error explicitly with a fine-grained analysis, specifying the exact constant in the upper bound. The analysis is based on upper and lower bounds of the completely bounded norm (cb-norm) of M_count, improving the best-known bound of 28 years by Mathias (SIAM Journal on Matrix Analysis and Applications, 1993) on the cb-norm of M_count for a large range of the dimension. The authors are the first to give concrete error bounds for various problems under continual observation such as binary counting, maintaining a histogram, releasing an approximately cut-preserving synthetic graph, many graph-based statistics, and substring and episode counting.},
  author = {Hendrik Fichtenberger and Monika Henzinger and Jalaj Upadhyay},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fichtenberger2023constant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10072--10092},
  pdf = {https://proceedings.mlr.press/v202/fichtenberger23a/fichtenberger23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constant Matters: Fine-grained Error Bound on Differentially Private Continual Observation},
  url = {https://proceedings.mlr.press/v202/fichtenberger23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fiegel2023adapting,
  abstract = {Imperfect information games (IIG) are games in which each player only partially observes the current game state. We study how to learn ε-optimal strategies in a zero-sum IIG through self-play with trajectory feedback. We give a problem-independent lower bound Õ(H(A_X+B_Y)/ε²) on the required number of realizations to learn these strategies with high probability, where H is the length of the game, A_X and B_Y are the total number of actions for the two players. We propose two Follow the Regularized leader (FTRL) algorithms for this setting: Balanced FTRL which matches this lower bound, but requires the knowledge of the information set structure beforehand to define the regularization; and Adaptive FTRL which needs Õ(H²(A_X+B_Y)/ε²) realizations without this requirement by progressively adapting the regularization to the observations.},
  author = {Cme Fiegel and Pierre Mnard and Tadashi Kozuno and Rmi Munos and Vianney Perchet and Michal Valko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fiegel2023adapting.pdf:pdf},
  mdate = {2023-08-28},
  note = {Outstanding Paper Award},
  pages = {10093--10135},
  pdf = {https://proceedings.mlr.press/v202/fiegel23a/fiegel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adapting to game trees in zero-sum imperfect information games},
  url = {https://proceedings.mlr.press/v202/fiegel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{finzi2023userdefined,
  abstract = {Diffusion models are a class of probabilistic generative models that have been widely used as a prior for image processing tasks like text conditional generation and inpainting. The authors demonstrate that these models can be adapted to make predictions and provide uncertainty quantification for chaotic dynamical systems. In these applications, diffusion models can implicitly represent knowledge about outliers and extreme events; however, querying that knowledge through conditional sampling or measuring probabilities is surprisingly difficult. Existing methods for conditional sampling at inference time seek mainly to enforce the constraints, which is insufficient to match the statistics of the distribution or compute the probability of the chosen events.},
  author = {Marc Anton Finzi and Anudhyan Boral and Andrew Gordon Wilson and Fei Sha and Leonardo Zepeda-Núñez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/finzi2023userdefined.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10136--10152},
  pdf = {https://proceedings.mlr.press/v202/finzi23a/finzi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems},
  url = {https://proceedings.mlr.press/v202/finzi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fonseca2023continuous,
  abstract = {The authors present the Continuous Spatiotemporal Transformer (CST), a new transformer architecture designed for modeling continuous systems. The motivation stems from a fundamental limitation of standard transformers: they are discrete time and space models with no guarantees regarding continuous sampling when modeling continuous dynamical systems. The CST framework guarantees continuous and smooth output via optimization in Sobolev space. The authors benchmarked CST against traditional transformers and other spatiotemporal dynamics modeling methods, achieving superior performance on both synthetic and real systems, including learning brain dynamics. CST extends transformers to continuous data by adding Sobolev-based regularization that enforces smooth outputs, enabling accurate interpolation of irregular, noisy time-series.},
  author = {Antonio Henrique de Oliveira Fonseca and Emanuele Zappala and Josue Ortega Caro and David van Dijk},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fonseca2023continuous.pdf:pdf},
  mdate = {2024-10-06},
  pages = {7343--7365},
  pdf = {https://proceedings.mlr.press/v202/de-oliveira-fonseca23a/de-oliveira-fonseca23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continuous Spatiotemporal Transformer},
  url = {https://proceedings.mlr.press/v202/de-oliveira-fonseca23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fontanella2023acat,
  abstract = {In some medical imaging tasks where only small parts of the image are informative for classification, traditional CNNs can struggle to generalise. While manually annotated Regions of Interest (ROI) are sometimes used to isolate informative parts, these are expensive to collect and may vary significantly across annotators. The authors propose a framework that employs saliency maps to obtain soft spatial attention masks that modulate the image features at different scales, referring to their method as Adversarial Counterfactual Attention (ACAT). ACAT increases the baseline classification accuracy of lesions in brain CT scans from 71.39% to 72.55% and of COVID-19 related findings in lung CT scans from 67.71% to 70.84% and exceeds the performance of competing methods. In the task of localising lesion location out of 6 possible regions, they obtain a score of 65.05% on brain CT scans, improving the score of 61.29% obtained with the best competing method.},
  author = {Alessandro Fontanella and Antreas Antoniou and Wenwen Li and Joanna M. Wardlaw and Grant Mair and Emanuele Trucco and Amos J. Storkey},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fontanella2023acat.pdf:pdf},
  mdate = {2024-10-30},
  pages = {10153--10169},
  pdf = {https://proceedings.mlr.press/v202/fontanella23a/fontanella23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ACAT}: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging},
  url = {https://proceedings.mlr.press/v202/fontanella23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{forel2023explainable,
  abstract = {Data-driven optimization uses contextual information and machine learning algorithms to find solutions to decision problems with uncertain parameters. While a vast body of work is dedicated to interpreting machine learning models in the classification setting, explaining decision pipelines involving learning algorithms remains unaddressed. This lack of interpretability can block the adoption of data-driven solutions as practitioners may not understand or trust the recommended decisions. The authors introduce a counterfactual explanation methodology tailored to explain solutions to data-driven problems, introducing two classes of explanations and developing methods to find nearest explanations of random forest and nearest-neighbor predictors. They demonstrate the approach by explaining key problems in operations management such as inventory management and routing.},
  author = {Alexandre Forel and Axel Parmentier and Thibaut Vidal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/forel2023explainable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10170--10187},
  pdf = {https://proceedings.mlr.press/v202/forel23a/forel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Explainable Data-Driven Optimization: From Context to Decision and Back Again},
  url = {https://proceedings.mlr.press/v202/forel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{foster2023hardness,
  abstract = {The authors consider the problem of decentralized multi-agent reinforcement learning in Markov games. A fundamental question is whether there exist algorithms that, when run independently by all agents, lead to no-regret for each player, analogous to celebrated convergence results for no-regret learning in normal-form games. While recent work has shown that such algorithms exist for restricted settings (notably, when regret is defined with respect to deviations to Markov policies), the question of whether independent no-regret learning can be achieved in the standard Markov game framework was open. They provide a decisive negative resolution to this problem, both from a computational and statistical perspective. Under the complexity-theoretic assumption that PPAD ≠ P, there is no polynomial-time algorithm that attains no-regret in two-player general-sum Markov games when executed independently by all players. When the game is unknown, no algorithm can achieve no-regret without observing exponentially many episodes in the number of players.},
  author = {Dylan J. Foster and Noah Golowich and Sham M. Kakade},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/foster2023hardness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10188--10221},
  pdf = {https://proceedings.mlr.press/v202/foster23a/foster23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games},
  url = {https://proceedings.mlr.press/v202/foster23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fotiadis2023disentangled,
  abstract = {Deep neural networks for modeling system dynamics face challenges with long-term prediction accuracy and out-of-distribution generalization. The authors address these challenges by considering the parameters of dynamical systems as factors of variation of the data and leverage their ground-truth values to disentangle the representations learned by generative models. Their experimental results in phase-space and observation-space dynamics demonstrate the effectiveness of latent-space supervision in producing disentangled representations, leading to improved long-term prediction accuracy and out-of-distribution robustness. The research focuses on treating domain parameters of dynamical systems as factors of variation of the data generating process and leveraging ideas from supervised disentanglement and causal factorization to separate the domain parameters from the dynamics in the latent space of generative models.},
  author = {Stathi Fotiadis and Mario Lino Valencia and Shunlong Hu and Stef Garasto and Chris D. Cantwell and Anil Anthony Bharath},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fotiadis2023disentangled.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10222--10248},
  pdf = {https://proceedings.mlr.press/v202/fotiadis23a/fotiadis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Disentangled Generative Models for Robust Prediction of System Dynamics},
  url = {https://proceedings.mlr.press/v202/fotiadis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fournier2023can,
  abstract = {Forward Gradients - the idea of using directional derivatives in forward differentiation mode - have recently been shown to be utilizable for neural network training while avoiding problems generally associated with backpropagation gradient computation, such as locking and memorization requirements. The cost is the requirement to guess the step direction, which is hard in high dimensions. The authors propose to strongly bias gradient guesses in directions that are much more promising, such as feedback obtained from small, local auxiliary networks. For a standard computer vision neural network, they conduct a rigorous study systematically covering a variety of combinations of gradient targets and gradient guesses, including those previously presented in the literature.},
  author = {Louis Fournier and Stéphane Rivaud and Eugene Belilovsky and Michael Eickenberg and Edouard Oyallon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fournier2023can.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10249--10264},
  pdf = {https://proceedings.mlr.press/v202/fournier23a/fournier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Can Forward Gradient Match Backpropagation?},
  url = {https://proceedings.mlr.press/v202/fournier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{foussoul2023last,
  abstract = {The paper builds on the Last Switch Dependent (LSD) bandits model introduced by Laforgue et al., which captures nonstationary phenomena from player-environment interaction, including examples like satiation (decreased performance from consecutive plays) and deprivation (increased payoff after inactivity intervals). The work focuses on the planning problem for LSD bandits under complete model knowledge, designing the first efficient constant approximation algorithm that nearly matches state-of-the-art guarantees under monotonicity assumptions on payoffs.},
  author = {Ayoub Foussoul and Vineet Goyal and Orestis Papadigenopoulos and Assaf Zeevi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/foussoul2023last.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10265--10284},
  pdf = {https://proceedings.mlr.press/v202/foussoul23a/foussoul23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Last Switch Dependent Bandits with Monotone Payoff Functions},
  url = {https://proceedings.mlr.press/v202/foussoul23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{francazi2023theoretical,
  abstract = {Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on the direction of the gradients: the minority class suffers from a higher directional noise, which reduces the effectiveness of the per-class gradient normalization. Our findings not only allow us to understand the potential and limitations of strategies involving the per-class gradients, but also the reason for the effectiveness of previously used solutions for class imbalance such as oversampling.},
  author = {Emanuele Francazi and Marco Baity-Jesi and Aurélien Lucchi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/francazi2023theoretical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10285--10322},
  pdf = {https://proceedings.mlr.press/v202/francazi23a/francazi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Theoretical Analysis of the Learning Dynamics under Class Imbalance},
  url = {https://proceedings.mlr.press/v202/francazi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{frantar2023sparsegpt,
  abstract = {We show for the first time that large-scale generative pretrained transformer ({GPT}) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called {SparseGPT}, specifically designed to work efficiently and accurately on massive {GPT}-family models. We can execute {SparseGPT} on the largest available open-source models, {OPT}-175B and {BLOOM}-176B, in under 4.5 hours, and can reach 60\% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. {SparseGPT} generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches.},
  author = {Elias Frantar and Dan Alistarh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/frantar2023sparsegpt.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10323--10337},
  pdf = {https://proceedings.mlr.press/v202/frantar23a/frantar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SparseGPT}: Massive Language Models Can be Accurately Pruned in One-Shot},
  url = {https://proceedings.mlr.press/v202/frantar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{freed2023learning,
  abstract = {Agents that can build temporally abstract representations of their environment are better able to understand their world and make plans on extended time scales, with limited computational power and modeling capacity. However, existing methods for automatically learning temporally abstract world models usually require millions of online environmental interactions and incentivize agents to reach every accessible environmental state, which is infeasible for most real-world robots both in terms of data efficiency and hardware safety. In this paper, we present an approach for simultaneously learning sets of skills and temporally abstract, skill-conditioned world models purely from offline data, enabling agents to perform zero-shot online planning of skill sequences for new tasks. We show that our approach performs comparably to or better than a wide array of state-of-the-art offline {RL} algorithms on a number of simulated robotics locomotion and manipulation benchmarks, while offering a higher degree of adaptability to new goals.},
  author = {Benjamin Freed and Siddarth Venkatraman and Guillaume Adrien Sartoretti and Jeff Schneider and Howie Choset},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/freed2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10338--10356},
  pdf = {https://proceedings.mlr.press/v202/freed23a/freed23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Temporally Abstract World Models without Online Experimentation},
  url = {https://proceedings.mlr.press/v202/freed23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{freund2023coupled,
  abstract = {In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler ({KL}) divergence, for distribution matching based imitation learning.},
  author = {Gideon Joseph Freund and Elad Sarafian and Sarit Kraus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/freund2023coupled.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10357--10372},
  pdf = {https://proceedings.mlr.press/v202/freund23a/freund23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Coupled Flow Approach to Imitation Learning},
  url = {https://proceedings.mlr.press/v202/freund23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023simple,
  abstract = {State space models ({SSMs}) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match {SSMs} in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover {SSM} performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop {FlashButterfly}, an {IO}-aware algorithm to improve the runtime performance of long convolutions. {FlashButterfly} appeals to classic Butterfly decompositions of the convolution to reduce {GPU} memory {IO} and increase {FLOP} utilization.},
  author = {Daniel Y. Fu and Elliot L. Epstein and Eric Nguyen and Armin W. Thomas and Michael Zhang and Tri Dao and Atri Rudra and Christopher Ré},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023simple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10373--10391},
  pdf = {https://proceedings.mlr.press/v202/fu23a/fu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simple Hardware-Efficient Long Convolutions for Sequence Modeling},
  url = {https://proceedings.mlr.press/v202/fu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023mononerf,
  abstract = {We propose a generalizable neural radiance fields - {MonoNeRF}, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. {MonoNeRF} follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane {NeRF} representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis.},
  author = {Yang Fu and Ishan Misra and Xiaolong Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023mononerf.pdf:pdf},
  mdate = {2024-05-07},
  pages = {10392--10404},
  pdf = {https://proceedings.mlr.press/v202/fu23b/fu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MonoNeRF}: Learning Generalizable {NeRFs} from Monocular Videos without Camera Poses},
  url = {https://proceedings.mlr.press/v202/fu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023beyond,
  abstract = {Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called {GoBI} - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging {Minigrid} navigation tasks and improves the sample efficiency on locomotion tasks from {DeepMind} Control Suite.},
  author = {Yao Fu and Run Peng and Honglak Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10405--10420},
  pdf = {https://proceedings.mlr.press/v202/fu23c/fu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Go Beyond Imagination: Maximizing Episodic Reachability with World Models},
  url = {https://proceedings.mlr.press/v202/fu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023specializing,
  abstract = {The surprising ability of Large Language Models ({LLMs}) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models. We show that such abilities can, in fact, be distilled down from {GPT}-3.5 (≥ 175B) to {T5} variants (≤ 11B). We propose model specialization, to specialize the model's ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power such that they can perform a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we specialize their capacity towards a target task, the model can achieve decent performance improvements. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1) balancing language model's performance on multiple tasks is a delicate matter, as improvements on one task may compromise other tasks; (2) yet by intentionally paying the price of decreased generic ability, we can clearly improve across different model scales smaller than 10B towards a specialized multi-step math reasoning ability.},
  author = {Yao Fu and Hao Peng and Litu Ou and Ashish Sabharwal and Tushar Khot},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023specializing.pdf:pdf},
  mdate = {2024-02-26},
  pages = {10421--10430},
  pdf = {https://proceedings.mlr.press/v202/fu23d/fu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Specializing Smaller Language Models towards Multi-Step Reasoning},
  url = {https://proceedings.mlr.press/v202/fu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023accelerated,
  abstract = {Non-convex optimization plays a key role in a growing number of machine learning applications. This motivates the identification of specialized structure that enables sharper theoretical analysis. One such identified structure is quasar-convexity, a non-convex generalization of convexity that subsumes convex functions. Existing algorithms for minimizing quasar-convex functions in the stochastic setting have either high complexity or slow convergence, which prompts us to derive a new class of stochastic methods for optimizing smooth quasar-convex functions. We demonstrate that our algorithms have fast convergence and outperform existing algorithms on several examples, including the classical problem of learning linear dynamical systems. We also present a unified analysis of our newly proposed algorithms and a previously studied deterministic algorithm.},
  author = {Qiang Fu and Dongchu Xu and Ashia Camage Wilson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023accelerated.pdf:pdf},
  mdate = {2025-05-26},
  pages = {10431--10460},
  pdf = {https://proceedings.mlr.press/v202/fu23e/fu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accelerated Stochastic Optimization Methods under Quasar-convexity},
  url = {https://proceedings.mlr.press/v202/fu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023nerfool,
  abstract = {Generalizable Neural Radiance Fields ({GNeRF}) are one of the most promising real-world solutions for novel view synthesis, thanks to their cross-scene generalization capability and thus the possibility of instant rendering on new scenes. While adversarial robustness is essential for real-world applications, little study has been devoted to understanding its implication on {GNeRF}. We hypothesize that because {GNeRF} is implemented by conditioning on the source views from new scenes, which are often acquired from the Internet or third-party providers, there are potential new security concerns regarding its real-world applications. Meanwhile, existing understanding and solutions for neural networks' adversarial robustness may not be applicable to {GNeRF}, due to its 3D nature and uniquely diverse operations. To this end, we present {NeRFool}, which to the best of our knowledge is the first work that sets out to understand the adversarial robustness of {GNeRF}. Specifically, {NeRFool} unveils the vulnerability patterns and important insights regarding {GNeRF}'s adversarial robustness. Built upon the above insights gained from {NeRFool}, we further develop {NeRFool}+, which integrates two techniques capable of effectively attacking {GNeRF} across a wide range of target views, and provide guidelines for defending against our proposed attacks.},
  author = {Yonggan Fu and Ye Yuan and Souvik Kundu and Shang Wu and Shunyao Zhang and Yingyan Celine Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023nerfool.pdf:pdf},
  mdate = {2025-05-27},
  pages = {10482--10493},
  pdf = {https://proceedings.mlr.press/v202/fu23g/fu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{NeRFool}: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations},
  url = {https://proceedings.mlr.press/v202/fu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{fu2023metalearning,
  abstract = {We propose a novel parameterized skill-learning algorithm that aims to learn transferable parameterized skills and synthesize them into a new action space that supports efficient learning in long-horizon tasks. We propose to leverage off-policy Meta-RL combined with a trajectory-centric smoothness term to learn a set of parameterized skills.},
  author = {Haotian Fu and Shangqun Yu and Saket Tiwari and Michael Littman and George Konidaris},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/fu2023metalearning.pdf:pdf},
  mdate = {2023-10-09},
  pages = {10461--10481},
  pdf = {https://proceedings.mlr.press/v202/fu23f/fu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Meta-learning Parameterized Skills},
  url = {https://proceedings.mlr.press/v202/fu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{furelosblanco2023hierarchies,
  abstract = {Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.},
  author = {Daniel Furelos-Blanco and Mark Law and Anders Jonsson and Krysia Broda and Alessandra Russo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/furelosblanco2023hierarchies.pdf:pdf},
  mdate = {2024-01-24},
  pages = {10494--10541},
  pdf = {https://proceedings.mlr.press/v202/furelos-blanco23a/furelos-blanco23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchies of Reward Machines},
  url = {https://proceedings.mlr.press/v202/furelos-blanco23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gadhikar2023why,
  abstract = {Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one by constraining the search to a fixed random mask. We demonstrate the feasibility of this approach in experiments for different pruning methods and propose particularly effective choices of initial layer-wise sparsity ratios of the random source network. As a special case, we show theoretically and experimentally that random source networks also contain strong lottery tickets.},
  author = {Advait Harshal Gadhikar and Sohom Mukherjee and Rebekka Burkholz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gadhikar2023why.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10542--10570},
  pdf = {https://proceedings.mlr.press/v202/gadhikar23a/gadhikar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why Random Pruning Is All We Need to Start Sparse},
  url = {https://proceedings.mlr.press/v202/gadhikar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{galloue2023cellfree,
  abstract = {Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. The authors show that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. They introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning that can be flexibly combined with any strategy for learning a latent representation. The results show that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Revenge.},
  author = {Quentin Gallou{\\'{e}}dec and Emmanuel Dellandr{\\'{e}}a},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/galloue2023cellfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10571--10586},
  pdf = {https://proceedings.mlr.press/v202/gallouedec23a/gallouedec23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cell-Free Latent Go-Explore},
  url = {https://proceedings.mlr.press/v202/gallouedec23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rodri2023role,
  abstract = {The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making all these methods stable when training with smaller batch sizes.},
  author = {Borja Rodr{\'{i}}guez G{\'{a}}lvez and Arno Blaas and Pau Rodr{\'{i}}guez and Adam Golinski and Xavier Suau and Jason Ramapuram and Dan Busbridge and Luca Zappella},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rodri2023role.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29143--29160},
  pdf = {https://proceedings.mlr.press/v202/rodri-guez-galvez23a/rodri-guez-galvez23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning},
  url = {https://proceedings.mlr.press/v202/rodri-guez-galvez23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gammelli2023graph,
  abstract = {Optimization problems over dynamic networks have been extensively studied and widely used in the past decades to formulate numerous real-world problems. However, traditional optimization-based approaches do not scale to large networks, and the design of good heuristics or approximation algorithms often requires significant manual trial-and-error. In this work, we argue that data-driven strategies can automate this process and learn efficient algorithms without compromising optimality. To do so, we present network control problems through the lens of reinforcement learning and propose a graph network-based framework to handle a broad class of problems. Instead of naively computing actions over high-dimensional graph elements, e.g., edges, we propose a bi-level formulation where we specify a desired next state via RL, and solve a convex program to best achieve it, leading to drastically improved scalability and performance.},
  author = {Daniele Gammelli and James Harrison and Kaidi Yang and Marco Pavone and Filipe Rodrigues and Francisco C. Pereira},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gammelli2023graph.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10587--10610},
  pdf = {https://proceedings.mlr.press/v202/gammelli23a/gammelli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Reinforcement Learning for Network Control via Bi-Level Optimization},
  url = {https://proceedings.mlr.press/v202/gammelli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ganesh2023why,
  abstract = {Remarkable improvements in the privacy-utility tradeoff have been widely reported when the model is pretrained on public data. Some gain is expected as these models inherit the benefits of transfer learning, which is the standard motivation in non-private settings. However, the stark contrast in the gain of pretraining between non-private and private machine learning suggests that the gain in the latter is rooted in a fundamentally different cause. We hypothesize that the non-convex loss landscape of model training necessitates the optimization algorithm to go through two phases: selecting a good basin in the loss landscape, and solving an easy optimization within that basin. The former is harder to solve with private data, while the latter is harder to solve with public data due to distribution shift or data scarcity. We provide theoretical constructions that provably demonstrate this separation, and include systematic experiments on CIFAR10 and LibriSpeech that provide supporting evidence for our hypothesis.},
  author = {Arun Ganesh and Mahdi Haghifam and Milad Nasr and Sewoong Oh and Thomas Steinke and Om Thakkar and Abhradeep Guha Thakurta and Lun Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ganesh2023why.pdf:pdf},
  mdate = {2024-07-24},
  pages = {10611--10627},
  pdf = {https://proceedings.mlr.press/v202/ganesh23a/ganesh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why Is Public Pretraining Necessary for Private Model Training?},
  url = {https://proceedings.mlr.press/v202/ganesh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ganz2023perceptually,
  abstract = {Several works have identified Perceptually Aligned Gradients (PAG) as a byproduct of robust training, but none have considered it as a standalone phenomenon nor studied its own implications. This work focuses on this trait and tests whether Perceptually Aligned Gradients imply Robustness. We develop a novel objective to directly promote PAG in training classifiers and examine whether models with such gradients are more robust to adversarial attacks. Extensive experiments on multiple datasets and architectures validate that models with aligned gradients exhibit significant robustness, exposing the surprising bidirectional connection between PAG and robustness.},
  author = {Roy Ganz and Bahjat Kawar and Michael Elad},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ganz2023perceptually.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10628--10648},
  pdf = {https://proceedings.mlr.press/v202/ganz23a/ganz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do Perceptually Aligned Gradients Imply Robustness?},
  url = {https://proceedings.mlr.press/v202/ganz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023gradient,
  abstract = {The main aim of this paper is to conduct the convergence analysis of the gradient descent for two-layer physics-informed neural networks (PINNs). Here, the loss function involves derivatives of neural network outputs with respect to its inputs, so the interaction between the trainable parameters is more complicated compared with simple regression and classification tasks. We first develop the positive definiteness of Gram matrices and prove that the gradient flow finds the global optima of the empirical loss under over-parameterization. Then, we demonstrate that the standard gradient descent converges to the global optima of the loss with proper choices of learning rates. The framework of our analysis works for various categories of PDEs (e.g., linear second-order PDEs) and common types of network initialization (LecunUniform etc.).},
  author = {Yihang Gao and Yiqi Gu and Michael Ng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023gradient.pdf:pdf},
  mdate = {2024-08-04},
  pages = {10676--10707},
  pdf = {https://proceedings.mlr.press/v202/gao23b/gao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient Descent Finds the Global Optima of Two-Layer Physics-Informed Neural Networks},
  url = {https://proceedings.mlr.press/v202/gao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023generalizing,
  abstract = {Recent neural network-based wave functions have achieved state-of-the-art accuracies in modeling ab-initio ground-state potential energy surface. However, these networks can only solve different spatial arrangements of the same set of atoms. To overcome this limitation, we present Graph-learned orbital embeddings (Globe), a neural network-based reparametrization method that can adapt neural wave functions to different molecules. Globe learns representations of local electronic structures that generalize across molecules via spatial message passing by connecting molecular orbitals to covalent bonds. Further, we propose a size-consistent wave function Ansatz, the Molecular orbital network (Moon), tailored to jointly solve Schr{\"o}dinger equations of different molecules. In our experiments, we find Moon converging in 4.5 times fewer steps to similar accuracy as previous methods or to lower energies given the same time.},
  author = {Nicholas Gao and Stephan G{\"u}nnemann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023generalizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10708--10726},
  pdf = {https://proceedings.mlr.press/v202/gao23c/gao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalizing Neural Wave Functions},
  url = {https://proceedings.mlr.press/v202/gao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023solving,
  abstract = {This paper presents fast first-order methods for solving linear programs (LPs) approximately. We adapt online linear programming algorithms to offline LPs and obtain algorithms that avoid any matrix multiplication. We also introduce a variable-duplication technique that copies each variable K times and reduces the optimality gap and constraint violation by a factor of √K. Furthermore, we show how online algorithms can be effectively integrated into sifting, a column generation scheme for large-scale LPs. Numerical experiments demonstrate that our methods can serve as either an approximate direct solver, or an initialization subroutine for exact LP solving.},
  author = {Wenzhi Gao and Dongdong Ge and Chunlin Sun and Yinyu Ye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023solving.pdf:pdf},
  pages = {10649--10675},
  pdf = {https://proceedings.mlr.press/v202/gao23a/gao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Solving Linear Programs with Fast Online Learning Algorithms},
  url = {https://proceedings.mlr.press/v202/gao23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023impact,
  abstract = {Predictive models in real-world applications have made it critical to ensure that individuals negatively impacted by model outcomes are provided with a means for recourse. While there has been growing research on algorithmic recourse in recent years, and recourses can be beneficial to affected individuals, their large-scale implementation can lead to potential data distribution shifts and other unintended consequences. This paper addresses how algorithmic recourse mechanisms, while beneficial at the individual level, can inadvertently contribute to social segregation at the population level. We examine the broader societal implications of recourse systems beyond their immediate individual benefits.},
  author = {Ruijiang Gao and Himabindu Lakkaraju},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023impact.pdf:pdf},
  pages = {10727--10743},
  pdf = {https://proceedings.mlr.press/v202/gao23d/gao23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Impact of Algorithmic Recourse on Social Segregation},
  url = {https://proceedings.mlr.press/v202/gao23d.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023ddgr,
  abstract = {Catastrophic forgetting, where deep learning models forget previously acquired skills when learning new ones, remains a critical challenge in continual learning. Generative replay (GR) methods, which typically consist of a generator and a classifier, offer an efficient way to mitigate catastrophic forgetting. However, conventional GR methods have two main limitations: they only focus on a single instruction relationship (generator-to-classifier) while ignoring ways the classifier can benefit the generator, and they typically reuse generated samples to update the generator, causing samples to deviate from previous task distributions. To overcome these issues, we propose DDGR, which adopts a diffusion model as the generator and calculates an instruction-operator through the classifier to instruct the generation of samples. Extensive experiments in class incremental (CI) and class incremental with repetition (CIR) settings demonstrate the advantages of DDGR.},
  author = {Rui Gao and Weiwei Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023ddgr.pdf:pdf},
  pages = {10744--10763},
  pdf = {https://proceedings.mlr.press/v202/gao23e/gao23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DDGR: Continual Learning with Deep Diffusion-based Generative Replay},
  url = {https://proceedings.mlr.press/v202/gao23e.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023pal,
  abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought", which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM which uses chain-of-thought by absolute 15% top-1.},
  author = {Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023pal.pdf:pdf},
  pages = {10764--10799},
  pdf = {https://proceedings.mlr.press/v202/gao23f/gao23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PAL: Program-aided Language Models},
  url = {https://proceedings.mlr.press/v202/gao23f.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023scaling,
  abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. We use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as optimization occurs against the proxy reward model using either reinforcement learning or best-of-n sampling, finding that this relationship follows a different functional form depending on the method of optimization. For best-of-n sampling the functional form is quadratic, while for reinforcement learning it is logarithmic. The coefficients in these functions scale smoothly with the number of reward model parameters.},
  author = {Leo Gao and John Schulman and Jacob Hilton},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023scaling.pdf:pdf},
  pages = {10835--10866},
  pdf = {https://proceedings.mlr.press/v202/gao23h/gao23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling Laws for Reward Model Overoptimization},
  url = {https://proceedings.mlr.press/v202/gao23h.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023outofdomain,
  abstract = {Models trained on one set of domains often suffer performance drops on unseen domains, such as when wildlife monitoring models are deployed in new camera locations. We study principles for designing data augmentations for out-of-domain (OOD) generalization in real-world scenarios where some domain-dependent features are robust. We design targeted augmentations that selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. On iWildCam2020-WILDS and Camelyon17-WILDS, two domain generalization datasets, targeted augmentations outperformed the previous state-of-the-art by 3.2% and 14.4% points respectively.},
  author = {Irena Gao and Shiori Sagawa and Pang Wei Koh and Tatsunori Hashimoto and Percy Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023outofdomain.pdf:pdf},
  pages = {10800--10834},
  pdf = {https://proceedings.mlr.press/v202/gao23g/gao23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Out-of-Domain Robustness via Targeted Augmentations},
  url = {https://proceedings.mlr.press/v202/gao23g.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gao2023feature,
  abstract = {Long-tailed learning is one of the most challenging problems in visual recognition. Recent work proposes to learn the balanced representation by fixing the linear classifier as Equiangular Tight Frame (ETF), since they argue what matters in classification is the structure of the feature, instead of their directions. We hold a different view, showing that features with fixed directions may be harmful to model generalization, even if completely symmetric. To address this, we propose the Representation-Balanced Learning Framework (RBL), which introduces orthogonal matrices to learn directions while maintaining the geometric structure of ETF. This approach challenges the conventional view and demonstrates that feature directions matter significantly in long-tailed classification scenarios.},
  author = {Peifeng Gao and Qianqian Xu and Peisong Wen and Zhiyong Yang and Huiyang Shao and Qingming Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gao2023feature.pdf:pdf},
  pages = {27542--27563},
  pdf = {https://proceedings.mlr.press/v202/peifeng23a/peifeng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Feature Directions Matter: Long-Tailed Learning via Rotated Balanced Representation},
  url = {https://proceedings.mlr.press/v202/peifeng23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{garcia2023unreasonable,
  abstract = {Few-shot translation systems trained with unpaired language data work for both high and low-resource language pairs. We show that with only 5 examples of high-quality translation data at inference, a transformer decoder-only model trained with self-supervised learning can match specialized supervised state-of-the-art models and commercial translation systems. We outperformed the best system on the WMT'21 English-Chinese news translation task using only five examples of English-Chinese parallel data at inference. The resulting models are two orders of magnitude smaller than state-of-the-art language models, demonstrating significant advances in making machine translation more efficient through few-shot learning approaches.},
  author = {Xavier Garcia and Yamini Bansal and Colin Cherry and George F. Foster and Maxim Krikun and Melvin Johnson and Orhan Firat},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/garcia2023unreasonable.pdf:pdf},
  pages = {10867--10878},
  pdf = {https://proceedings.mlr.press/v202/garcia23a/garcia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Unreasonable Effectiveness of Few-shot Learning for Machine Translation},
  url = {https://proceedings.mlr.press/v202/garcia23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{garg2023rlsbench,
  abstract = {We introduce RLSbench, a large-scale benchmark for relaxed label shift, consisting of >500 distribution shift pairs spanning vision, tabular, and language modalities, with varying label proportions. Unlike existing benchmarks, which primarily focus on shifts in class-conditional p(x|y), our benchmark also focuses on label marginal shifts. We assessed 13 popular domain adaptation methods, demonstrating more widespread failures under label proportion shifts than were previously known. We developed an effective two-step meta-algorithm that is compatible with most domain adaptation heuristics: (i) pseudo-balance the data at each epoch; and (ii) adjust the final classifier with target label distribution estimate. The meta-algorithm improves existing domain adaptation heuristics under large label proportion shifts, often by 2–10% accuracy points, while conferring minimal effect (<0.5%) when label proportions do not shift.},
  author = {Saurabh Garg and Nick Erickson and James Sharpnack and Alex Smola and Sivaraman Balakrishnan and Zachary Chase Lipton},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/garg2023rlsbench.pdf:pdf},
  pages = {10879--10928},
  pdf = {https://proceedings.mlr.press/v202/garg23a/garg23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {RLSbench: Domain Adaptation Under Relaxed Label Shift},
  url = {https://proceedings.mlr.press/v202/garg23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{garrido2023rankme,
  abstract = {Joint-Embedding Self-Supervised Learning (JE-SSL) lacks visual cues for training success, making it difficult to deploy SSL on new datasets without labels to judge representation quality. We introduce RankMe, a simple unsupervised criterion that assesses the quality of learned JE-SSL representations using their effective rank. RankMe allows assessment of performance on different downstream datasets without requiring any labels and has no training or hyperparameters to tune. Through thorough empirical experiments involving hundreds of training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no reduction in final performance compared to current selection methods that involve dataset labels. We focus on 5 JE-SSL methods, using SimCLR as a representative contrastive method, VICReg as a representative covariance based method, VICReg-exp and VICReg-ctr, and DINO as a clustering approach.},
  author = {Quentin Garrido and Randall Balestriero and Laurent Najman and Yann LeCun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/garrido2023rankme.pdf:pdf},
  pages = {10929--10974},
  pdf = {https://proceedings.mlr.press/v202/garrido23a/garrido23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {RankMe: Assessing the Downstream Performance of Pretrained Self-Supervised Representations by Their Rank},
  url = {https://proceedings.mlr.press/v202/garrido23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{garrido2023selfsupervised,
  abstract = {Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios.},
  author = {Quentin Garrido and Laurent Najman and Yann LeCun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/garrido2023selfsupervised.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10975--10996},
  pdf = {https://proceedings.mlr.press/v202/garrido23b/garrido23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-supervised Learning of Split Invariant Equivariant Representations},
  url = {https://proceedings.mlr.press/v202/garrido23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gascn2023federated,
  abstract = {Motivated by real-life deployments of multi-round federated analytics with secure aggregation, we investigate the fundamental communication-accuracy tradeoffs of the heavy hitter discovery and approximate (open-domain) histogram problems under a linear sketching constraint. We propose efficient algorithms based on local subsampling and invertible bloom look-up tables (IBLTs). We also show that our algorithms are information-theoretically optimal for a broad class of interactive schemes. The results show that the linear sketching constraint does increase the communication cost for both tasks by introducing an extra linear dependence on the number of users in a round. Moreover, our results also establish a separation between the communication cost for heavy hitter discovery and approximate histogram in the multi-round setting. The dependence on the number of rounds $R$ is at most logarithmic for heavy hitter discovery whereas that of approximate histogram is $\Theta(\sqrt{R})$. We also empirically demonstrate our findings.},
  author = {Adrià Gascón and Peter Kairouz and Ziteng Sun and Ananda Theertha Suresh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gascn2023federated.pdf:pdf},
  mdate = {2023-08-28},
  pages = {10997--11012},
  pdf = {https://proceedings.mlr.press/v202/gascon23a/gascon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Federated Heavy Hitter Recovery under Linear Sketching},
  url = {https://proceedings.mlr.press/v202/gascon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gaur2023global,
  abstract = {Deep Q-learning based algorithms have been applied successfully in many decision making problems, while their theoretical foundations are not as well understood. In this paper, we study a Fitted Q-Iteration with two-layer ReLU neural network parameterization, and find the sample complexity guarantees for the algorithm. Our approach estimates the Q-function in each iteration using a convex optimization problem. We show that this approach achieves a sample complexity of $\tilde{\mathcal{O}}(1/\epsilon^{2})$, which is order-optimal. This result holds for a countable state-spaces and does not require any assumptions such as a linear or low rank structure on the MDP.},
  author = {Mudit Gaur and Vaneet Aggarwal and Mridul Agarwal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gaur2023global.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11013--11049},
  pdf = {https://proceedings.mlr.press/v202/gaur23a/gaur23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Global Convergence of Fitted Q-Iteration with Two-layer Neural Network Parametrization},
  url = {https://proceedings.mlr.press/v202/gaur23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ge2023reinforcement,
  abstract = {Mediation analysis learns the causal effect transmitted via mediator variables between treatments and outcomes, and receives increasing attention in various scientific domains to elucidate causal relations. Most existing works focus on point-exposure studies where each subject only receives one treatment at a single time point. However, there are a number of applications (e.g., mobile health) where the treatments are sequentially assigned over time and the dynamic mediation effects are of primary interest. Proposing a reinforcement learning (RL) framework, we are the first to evaluate dynamic mediation effects in settings with infinite horizons. We decompose the average treatment effect into an immediate direct effect, an immediate mediation effect, a delayed direct effect, and a delayed mediation effect. Upon the identification of each effect component, we further develop robust and semi-parametrically efficient estimators under the RL framework to infer these causal effects. The superior performance of the proposed method is demonstrated through extensive numerical studies, theoretical results, and an analysis of a mobile health dataset. A Python implementation of the proposed procedure is available at https://github.com/linlinlin97/MediationRL.},
  author = {Lin Ge and Jitao Wang and Chengchun Shi and Zhenke Wu and Rui Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ge2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11050--11097},
  pdf = {https://proceedings.mlr.press/v202/ge23a/ge23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Reinforcement Learning Framework for Dynamic Mediation Analysis},
  url = {https://proceedings.mlr.press/v202/ge23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{geffner2023compositional,
  abstract = {Neural Posterior Estimation methods for simulation-based inference can be ill-suited for dealing with posterior distributions obtained by conditioning on multiple observations, as they tend to require a large number of simulator calls to learn accurate approximations. In contrast, Neural Likelihood Estimation methods can handle multiple observations at inference time after learning from individual observations, but they rely on standard inference methods, such as MCMC or variational inference, which come with certain performance drawbacks. We introduce a new method based on conditional score modeling that enjoys the benefits of both approaches. We model the scores of the (diffused) posterior distributions induced by individual observations, and introduce a way of combining the learned scores to approximately sample from the target posterior distribution.},
  author = {Tomas Geffner and George Papamakarios and Andriy Mnih},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/geffner2023compositional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11098--11116},
  pdf = {https://proceedings.mlr.press/v202/geffner23a/geffner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Compositional Score Modeling for Simulation-Based Inference},
  url = {https://proceedings.mlr.press/v202/geffner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{geiping2023cramming,
  abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single GPU in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting. We provide code to reproduce all experiments at github.com/JonasGeiping/cramming.},
  author = {Jonas Geiping and Tom Goldstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/geiping2023cramming.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11117--11143},
  pdf = {https://proceedings.mlr.press/v202/geiping23a/geiping23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cramming: Training a Language Model on a Single {GPU} in One Day},
  url = {https://proceedings.mlr.press/v202/geiping23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{geisler2023transformers,
  abstract = {Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding.},
  author = {Simon Geisler and Yujia Li and Daniel J. Mankowitz and Ali Taylan Cemgil and Stephan Günnemann and Cosmin Paduraru},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/geisler2023transformers.pdf:pdf},
  mdate = {2024-08-13},
  pages = {11144--11172},
  pdf = {https://proceedings.mlr.press/v202/geisler23a/geisler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformers Meet Directed Graphs},
  url = {https://proceedings.mlr.press/v202/geisler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{genewein2023memorybased,
  abstract = {Memory-based meta-learning is a technique for approximating Bayes-optimal predictors. Under fairly general conditions, minimizing sequential prediction error, measured by the log loss, leads to implicit meta-learning. The goal of this work is to investigate how far this interpretation can be realized by current sequence prediction models and training regimes. The focus is on piecewise stationary sources with unobserved switching-points, which arguably capture an important characteristic of natural language and action-observation sequences in partially observable environments. We show that various types of memory-based neural models, including Transformers, LSTMs, and RNNs can learn to accurately approximate known Bayes-optimal algorithms for this setting.},
  author = {Tim Genewein and Grégoire Deltang and Anian Ruoss and Li Kevin Wenliang and Elliot Catt and Vincent Dutordoir and Jordi Grau-Moya and Laurent Orseau and Marcus Hutter and Joel Veness},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/genewein2023memorybased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11173--11195},
  pdf = {https://proceedings.mlr.press/v202/genewein23a/genewein23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Memory-Based Meta-Learning on Non-Stationary Distributions},
  url = {https://proceedings.mlr.press/v202/genewein23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{geng2023towards,
  abstract = {Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs.},
  author = {Chuqin Geng and Nham Le and Xiaojie Xu and Zhaoyue Wang and Arie Gurfinkel and Xujie Si},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/geng2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11196--11212},
  pdf = {https://proceedings.mlr.press/v202/geng23a/geng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Reliable Neural Specifications},
  url = {https://proceedings.mlr.press/v202/geng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gerstgrasser2023oracles,
  abstract = {Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches.},
  author = {Matthias Gerstgrasser and David C. Parkes},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gerstgrasser2023oracles.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11213--11236},
  pdf = {https://proceedings.mlr.press/v202/gerstgrasser23a/gerstgrasser23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Oracles \& Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/gerstgrasser23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghadiri2023approximately,
  abstract = {This work studies the combinatorial optimization problem of finding an optimal core tensor shape, also called multilinear rank, for a size-constrained Tucker decomposition. We give an algorithm with provable approximation guarantees for its reconstruction error via connections to higher-order singular values. Specifically, we introduce a novel Tucker packing problem, which we prove is NP-hard, and give a polynomial-time approximation scheme based on a reduction to the 2-dimensional knapsack problem with a matroid constraint. We also generalize our techniques to tree tensor network decompositions. We implement our algorithm using an integer programming solver, and show that its solution quality is competitive with (and sometimes better than) the greedy algorithm that uses the true Tucker decomposition loss at each step, while also running up to 1000x faster.},
  author = {Mehrdad Ghadiri and Matthew Fahrbach and Gang Fu and Vahab Mirrokni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghadiri2023approximately.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11237--11254},
  pdf = {https://proceedings.mlr.press/v202/ghadiri23a/ghadiri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Approximately Optimal Core Shapes for Tensor Decompositions},
  url = {https://proceedings.mlr.press/v202/ghadiri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghamizi2023gat,
  abstract = {While leveraging additional training data is well established to improve adversarial robustness, it incurs the unavoidable cost of data collection and the heavy computation to train models. To mitigate the costs, we propose Guided Adversarial Training (GAT), a novel adversarial training technique that exploits auxiliary tasks under a limited set of training data. Our approach extends single-task models into multi-task models during the min-max optimization of adversarial training, and drives the loss optimization with a regularization of the gradient curvature across multiple tasks. GAT leverages two types of auxiliary tasks: self-supervised tasks, where the labels are generated automatically, and domain-knowledge tasks, where human experts provide additional labels. Experimentally, under limited data, GAT increases the robust accuracy on CIFAR-10 up to four times (from 11\% to 42\% robust accuracy) and the robust AUC of CheXpert medical imaging dataset from 50\% to 83\%. On the full CIFAR-10 dataset, GAT outperforms eight state-of-the-art adversarial training strategies. The large study across five datasets and six tasks demonstrates that task augmentation is an efficient alternative to data augmentation, and can be key to achieving both clean and robust performances.},
  author = {Salah Ghamizi and Jingfeng Zhang and Maxime Cordy and Mike Papadakis and Masashi Sugiyama and Yves Le Traon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghamizi2023gat.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11255--11282},
  pdf = {https://proceedings.mlr.press/v202/ghamizi23a/ghamizi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GAT}: Guided Adversarial Training with {P}areto-optimal Auxiliary Tasks},
  url = {https://proceedings.mlr.press/v202/ghamizi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghazi2023userlevel,
  abstract = {We introduce a new mechanism for stochastic convex optimization (SCO) with user-level differential privacy guarantees. The convergence rates of this mechanism are similar to those in the prior work of Levy et al. (2021) and Narayanan et al. (2022), but with two important improvements. Our mechanism does not require any smoothness assumptions on the loss. Furthermore, our bounds are also the first where the minimum number of users needed for user-level privacy has no dependence on the dimension and only a logarithmic dependence on the desired excess error.},
  author = {Badih Ghazi and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Raghu Meka and Chiyuan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghazi2023userlevel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11283--11299},
  pdf = {https://proceedings.mlr.press/v202/ghazi23a/ghazi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On User-Level Private Convex Optimization},
  url = {https://proceedings.mlr.press/v202/ghazi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghosal2023contextual,
  abstract = {Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. The paper proposes and analyzes a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a model to rely exclusively on these features while being invariant to the rest.},
  author = {Gaurav Rohit Ghosal and Amrith Setlur and Daniel S. Brown and Anca D. Dragan and Aditi Raghunathan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghosal2023contextual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11300--11320},
  pdf = {https://proceedings.mlr.press/v202/ghosal23a/ghosal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contextual Reliability: When Different Features Matter in Different Contexts},
  url = {https://proceedings.mlr.press/v202/ghosal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghosh2023reinforcement,
  abstract = {Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data.},
  author = {Dibya Ghosh and Chethan Anand Bhateja and Sergey Levine},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghosh2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11321--11339},
  pdf = {https://proceedings.mlr.press/v202/ghosh23a/ghosh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reinforcement Learning from Passive Data via Latent Intentions},
  url = {https://proceedings.mlr.press/v202/ghosh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghosh2023harmonic,
  abstract = {Harmonic functions are abundant in nature, appearing in limiting cases of Maxwell's, Navier-Stokes equations, the heat and the wave equation. Consequently, there are many applications of harmonic functions from industrial process optimisation to robotic path planning and the calculation of first exit times of random walks. Despite their ubiquity and relevance, there have been few attempts to incorporate inductive biases towards harmonic functions in machine learning contexts. In this work, we demonstrate effective means of representing harmonic functions in neural networks and extend such results also to quantum neural networks to demonstrate the generality of our approach.},
  author = {Atiyo Ghosh and Antonio Andrea Gentile and Mario Dagrada and Chul Lee and Seong-Hyok Sean Kim and Hyukgeun Cha and Yunjun Choi and Dongho Kim and Jeong-Il Kye and Vincent Emanuel Elfving},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghosh2023harmonic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11340--11359},
  pdf = {https://proceedings.mlr.press/v202/ghosh23b/ghosh23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Harmonic Neural Networks},
  url = {https://proceedings.mlr.press/v202/ghosh23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ghosh2023dividing,
  abstract = {ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat approach (1) identifies a diverse set of instance-specific concepts with high concept completeness via MoIE without compromising in performance, (2) identifies the relatively "harder" samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, and (4) fixes the shortcut learned by the original Blackbox.},
  author = {Shantanu Ghosh and Ke Yu and Forough Arabshahi and Kayhan Batmanghelich},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ghosh2023dividing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11360--11397},
  pdf = {https://proceedings.mlr.press/v202/ghosh23c/ghosh23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dividing and Conquering a {B}lack{B}ox to a Mixture of Interpretable Models: Route, Interpret, Repeat},
  url = {https://proceedings.mlr.press/v202/ghosh23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{giannou2023looped,
  abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. The paper demonstrates how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm.},
  author = {Angeliki Giannou and Shashank Rajput and Jy-yong Sohn and Kangwook Lee and Jason D. Lee and Dimitris Papailiopoulos},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/giannou2023looped.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11398--11442},
  pdf = {https://proceedings.mlr.press/v202/giannou23a/giannou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Looped Transformers as Programmable Computers},
  url = {https://proceedings.mlr.press/v202/giannou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{giovanni2023oversquashing,
  abstract = {Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.},
  author = {Francesco Di Giovanni and Lorenzo Giusti and Federico Barbero and Giulia Luise and Pietro Lio and Michael M. Bronstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/giovanni2023oversquashing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7865--7885},
  pdf = {https://proceedings.mlr.press/v202/di-giovanni23a/di-giovanni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology},
  url = {https://proceedings.mlr.press/v202/di-giovanni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{giuliani2023generalized,
  abstract = {We make two contributions in the field of AI fairness over continuous protected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR) indicator (the only one currently available for such a case) is valuable but subject to a few crucial limitations regarding semantics, interpretability, and robustness. Second, we introduce a family of indicators that are: 1) complementary to HGR in terms of semantics; 2) fully interpretable and transparent; 3) robust over finite samples; 4) configurable to suit specific applications.},
  author = {Luca Giuliani and Eleonora Misino and Michele Lombardi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/giuliani2023generalized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11443--11458},
  pdf = {https://proceedings.mlr.press/v202/giuliani23a/giuliani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalized Disparate Impact for Configurable Fairness Solutions in {ML}},
  url = {https://proceedings.mlr.press/v202/giuliani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{globusharris2023multicalibration,
  abstract = {We study the connection between multicalibration and boosting for squared error regression. First we prove a useful characterization of multicalibration in terms of a ``swap regret'' like condition on squared error. Using this characterization, we give an exceedingly simple algorithm that can be analyzed both as a boosting algorithm for regression and as a multicalibration algorithm for a class H that makes use only of a standard squared error regression oracle for H. We give a weak learning assumption on H that ensures convergence to Bayes optimality without the need to make any realizability assumptions -- giving us an agnostic boosting algorithm for regression. We show that our weak learning assumption on H is both necessary and sufficient for multicalibration with respect to H to imply Bayes optimality, answering an open question. We also show that if H satisfies our weak learning condition relative to another class C then multicalibration with respect to H implies multicalibration with respect to C. Finally we investigate the empirical performance of our algorithm experimentally.},
  author = {Ira Globus-Harris and Declan Harrison and Michael Kearns and Aaron Roth 0001 and Jessica Sorrell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/globusharris2023multicalibration.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11459--11492},
  pdf = {https://proceedings.mlr.press/v202/globus-harris23a/globus-harris23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multicalibration as Boosting for Regression},
  url = {https://proceedings.mlr.press/v202/globus-harris23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{glo2023adversarial,
  abstract = {Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.},
  author = {Manuel Gl{\"o}ckler and Michael Deistler and Jakob H. Macke},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/glo2023adversarial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11493--11524},
  pdf = {https://proceedings.mlr.press/v202/gloeckler23a/gloeckler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Robustness of Amortized {B}ayesian Inference},
  url = {https://proceedings.mlr.press/v202/gloeckler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gmelin2023efficient,
  abstract = {Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. In particular, we reconstruct a robot mask from the agent-centric latent and reconstruct the input image from the environment-centric latent. We supervise the input image encoder jointly with an RL loss, a robot mask reconstruction loss, and an image reconstruction loss. Our method, SEAR, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots, and learns representations that are useful for transfer learning.},
  author = {Kevin Gmelin and Shikhar Bahl and Russell Mendonca and Deepak Pathak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gmelin2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11525--11545},
  pdf = {https://proceedings.mlr.press/v202/gmelin23a/gmelin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient {RL} via Disentangled Environment and Agent Representations},
  url = {https://proceedings.mlr.press/v202/gmelin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{go2023aligning,
  abstract = {Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally optimal objective but that different divergences present different alignment and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good balance between these objectives, and frequently outperforms forward KL divergence by a wide margin, leading to significant improvements over prior work.},
  author = {Dongyoung Go and Tomasz Korbak and Germ{\'a}n Kruszewski and Jos Rozen and Nahyeon Ryu and Marc Dymetman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/go2023aligning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11546--11583},
  pdf = {https://proceedings.mlr.press/v202/go23a/go23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Aligning Language Models with Preferences through f-divergence Minimization},
  url = {https://proceedings.mlr.press/v202/go23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{goibert2023robust,
  abstract = {As the issue of robustness in AI systems becomes vital, statistical learning techniques that are reliable even in presence of partly contaminated data have to be developed. Preference data, in the form of (complete) rankings in the simplest situations, are no exception and the demand for appropriate concepts and tools is all the more pressing given that technologies fed by or producing this type of data (e.g. search engines, recommending systems) are now massively deployed. The lack of vector space structure for the set of rankings (i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of statistics considered in ranking data analysis make the formulation of robustness objectives in this domain challenging. In this paper, we introduce notions of robustness, together with dedicated statistical methods, for Consensus Ranking the flagship problem in ranking data analysis, aiming at summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking. Specific extensions of the popular concept of breakdown point, tailored to consensus ranking, are proposed and the related computational issues are addressed. Beyond the theoretical contributions, the relevance of the approach proposed is supported by an experimental study.},
  author = {Morgane Goibert and Cl{\'e}ment Calauze{\`n}es and Ekhine Irurozki and St{\'e}phan Cl{\'e}men{\c{c}}on},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/goibert2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11584--11597},
  pdf = {https://proceedings.mlr.press/v202/goibert23a/goibert23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues},
  url = {https://proceedings.mlr.press/v202/goibert23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gong2023learning,
  abstract = {Shadow tomography for quantum states provides a sample efficient approach for predicting the properties of quantum systems when the properties are restricted to expectation values of 2-outcome POVMs. However, these shadow tomography procedures yield poor bounds if there are more than 2 outcomes per measurement. In this paper, we consider a general problem of learning properties from unknown quantum states: given an unknown d-dimensional quantum state $\rho$ and M unknown quantum measurements $M_1, \ldots, M_M$ with $K \geq 2$ outcomes, estimating the probability distribution for applying $M_i$ on $\rho$ to within total variation distance $\varepsilon$. Compared to the special case when $K=2$, we need to learn unknown distributions instead of values. We develop an online shadow tomography procedure that solves this problem with high success probability requiring $\tilde{O}(K \log^2 M \log d / \varepsilon^4)$ copies of $\rho$. We also provide an information-theoretic lower bound showing that at least $\Omega(\min\{d^2, K + \log M\} / \varepsilon^2)$ copies of $\rho$ are required to solve this problem with high success probability. Our sample complexity has only logarithmic dependence on M and d and is sample-optimal concerning the dependence on K.},
  author = {Weiyuan Gong and Scott Aaronson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gong2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11598--11613},
  pdf = {https://proceedings.mlr.press/v202/gong23a/gong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Distributions over Quantum Measurement Outcomes},
  url = {https://proceedings.mlr.press/v202/gong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gorbunov2023convergence,
  abstract = {Algorithms for min-max optimization and variational inequalities are often studied under monotonicity assumptions. Motivated by non-monotone machine learning applications, we follow the line of works aiming at going beyond monotonicity by considering the weaker negative comonotonicity assumption. In this paper, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity. In particular, we derive the first non-asymptotic convergence rates for PP under negative comonotonicity and star-negative comonotonicity and show their tightness via constructing worst-case examples. We also relax the assumptions for the last-iterate convergence guarantees for EG and OG and prove the tightness of the existing best-iterate guarantees for EG and OG via constructing counter-examples.},
  author = {Eduard Gorbunov and Adrien B. Taylor and Samuel Horv{\'a}th and Gauthier Gidel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gorbunov2023convergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11614--11641},
  pdf = {https://proceedings.mlr.press/v202/gorbunov23a/gorbunov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity},
  url = {https://proceedings.mlr.press/v202/gorbunov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{goshtasbpour2023adaptive,
  abstract = {Annealed Importance Sampling (AIS) synthesizes weighted samples from an intractable distribution given its unnormalized density function. This algorithm relies on a sequence of interpolating distributions bridging the target to an initial tractable distribution such as the well-known geometric mean path of unnormalized distributions which is assumed to be suboptimal in general. In this work, we prove that the geometric annealing corresponds to the distribution path that minimizes the KL divergence between the current particle distribution and the desired target when the feasible change in the particle distribution is constrained. Following this observation, we derive the constant rate discretization schedule for this annealing sequence, which adjusts the schedule to the difficulty of moving samples between the initial and the target distributions. We extend our results to f-divergences and present the respective dynamics of annealing sequences based on which we propose the Constant Rate AIS (CR-AIS) algorithm and its efficient implementation for $\alpha$-divergences. We empirically show that CR-AIS performs well on multiple benchmark distributions while avoiding the computationally expensive tuning loop in existing Adaptive AIS.},
  author = {Shirin Goshtasbpour and Victor Cohen and Fernando P{\'e}rez-Cruz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/goshtasbpour2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11642--11658},
  pdf = {https://proceedings.mlr.press/v202/goshtasbpour23a/goshtasbpour23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Annealed Importance Sampling with Constant Rate Progress},
  url = {https://proceedings.mlr.press/v202/goshtasbpour23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{graham2023formalizing,
  abstract = {When trying to solve a computational problem, we are often faced with a choice between algorithms that are guaranteed to return the right answer but differ in their runtime distributions (e.g., SAT solvers, sorting algorithms). This paper aims to lay theoretical foundations for such choices by formalizing preferences over runtime distributions. It might seem that we should simply prefer the algorithm that minimizes expected runtime. However, such preferences would be driven by exactly how slow our algorithm is on bad inputs, whereas in practice we are typically willing to cut off occasional, sufficiently long runs before they finish. We propose a principled alternative, taking a utility-theoretic approach to characterize the scoring functions that describe preferences over algorithms. These functions depend on the way our value for solving our problem decreases with time and on the distribution from which captimes are drawn. We also study what happens when we must choose a single algorithm to use for solving each instance in a set of instances drawn from different, known distributions.},
  author = {Devon R. Graham and Kevin Leyton-Brown and Tim Roughgarden},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/graham2023formalizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11659--11682},
  pdf = {https://proceedings.mlr.press/v202/graham23a/graham23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Formalizing Preferences Over Runtime Distributions},
  url = {https://proceedings.mlr.press/v202/graham23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{grande2023topological,
  abstract = {We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge Laplacians associated to an appropriately constructed simplicial complex, the method can leverage a far richer set of topological features. We demonstrate the effectiveness of TPCC on both synthetic and real-world data, and show that it outperforms classical spectral clustering and other topological clustering methods in many scenarios.},
  author = {Vincent Peter Grande and Michael T. Schaub},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/grande2023topological.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11683--11697},
  pdf = {https://proceedings.mlr.press/v202/grande23a/grande23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Topological Point Cloud Clustering},
  url = {https://proceedings.mlr.press/v202/grande23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{grenioux2023sampling,
  abstract = {Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target.},
  author = {Louis Grenioux and Alain Oliviero Durmus and Eric Moulines and Marylou Gabri},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/grenioux2023sampling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11698--11733},
  pdf = {https://proceedings.mlr.press/v202/grenioux23a/grenioux23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Sampling with Approximate Transport Maps},
  url = {https://proceedings.mlr.press/v202/grenioux23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{grigsby2023hidden,
  abstract = {The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings $\theta$ can determine the same function $f$. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries.},
  author = {J. Elisenda Grigsby and Kathryn Lindsey and David Rolnick},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/grigsby2023hidden.pdf:pdf},
  mdate = {2024-10-06},
  pages = {11734--11760},
  pdf = {https://proceedings.mlr.press/v202/grigsby23a/grigsby23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hidden Symmetries of ReLU Networks},
  url = {https://proceedings.mlr.press/v202/grigsby23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gruntkowska2023ef21p,
  abstract = {In this work we focus our attention on distributed optimization problems in the context where the communication time between the server and the workers is non-negligible. We obtain novel methods supporting bidirectional compression (both from the server to the workers and vice versa) that enjoy new state-of-the-art theoretical communication complexity for convex and nonconvex problems.},
  author = {Kaja Gruntkowska and Alexander Tyurin and Peter Richtrik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gruntkowska2023ef21p.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11761--11807},
  pdf = {https://proceedings.mlr.press/v202/gruntkowska23a/gruntkowska23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression},
  url = {https://proceedings.mlr.press/v202/gruntkowska23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gu2023nerfdiff,
  abstract = {Novel view synthesis from a single image requires inferring occluded regions of objects and scenes whilst simultaneously maintaining semantic and physical consistency with the input. Existing approaches condition neural radiance fields (NeRF) on local image features, projecting points to the input image plane, and aggregating 2D features to perform volume rendering. However, under severe occlusion, this projection fails to resolve uncertainty, resulting in blurry renderings that lack details. In this work, we propose NerfDiff, which addresses this issue by distilling the knowledge of a 3D-aware conditional diffusion model (CDM) into NeRF through synthesizing and refining a set of virtual views at test-time.},
  author = {Jiatao Gu and Alex Trevithick and Kai-En Lin and Joshua M. Susskind and Christian Theobalt and Lingjie Liu and Ravi Ramamoorthi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gu2023nerfdiff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11808--11826},
  pdf = {https://proceedings.mlr.press/v202/gu23a/gu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion},
  url = {https://proceedings.mlr.press/v202/gu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guan2023decompdiff,
  abstract = {Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space.},
  author = {Jiaqi Guan and Xiangxin Zhou and Yuwei Yang and Yu Bao and Jian Peng 0001 and Jianzhu Ma and Qiang Liu 0006 and Liang Wang 0001 and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guan2023decompdiff.pdf:pdf},
  mdate = {2024-05-30},
  pages = {11827--11846},
  pdf = {https://proceedings.mlr.press/v202/guan23a/guan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design},
  url = {https://proceedings.mlr.press/v202/guan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guha2023excess,
  abstract = {Dirichlet Process mixture models (DPMM) in combination with Gaussian kernels have been an important modeling tool for numerous data domains arising from biological, physical, and social sciences. However, this versatility in applications does not extend to strong theoretical guarantees for the underlying parameter estimates, for which only a logarithmic rate is achieved.},
  author = {Aritra Guha and Nhat Ho and XuanLong Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guha2023excess.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11847--11870},
  pdf = {https://proceedings.mlr.press/v202/guha23a/guha23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Excess Mass Behavior in Gaussian Mixture Models with Orlicz-Wasserstein Distances},
  url = {https://proceedings.mlr.press/v202/guha23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guha2023conformalization,
  abstract = {Given a sequence of observable variables {(x_1, y_1), ..., (x_n, y_n)}, the conformal prediction method estimates a confidence set for y_{n+1} given x_{n+1} that is valid for any finite sample size by merely assuming that the joint distribution of the data is permutation invariant.},
  author = {Etash Kumar Guha and Eugne Ndiaye and Xiaoming Huo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guha2023conformalization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {11871--11887},
  pdf = {https://proceedings.mlr.press/v202/guha23b/guha23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conformalization of Sparse Generalized Linear Models},
  url = {https://proceedings.mlr.press/v202/guha23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023estimating,
  abstract = {Estimating heterogeneous treatment effects (HTE) from observational studies is rising in importance due to the widespread accumulation of data in many fields. Due to the selection bias behind the inaccessibility of counterfactual data, the problem differs fundamentally from supervised learning in a challenging way.},
  author = {Xingzhuo Guo and Yuchen Zhang 0004 and Jianmin Wang 0001 and Mingsheng Long},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023estimating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12108--12121},
  pdf = {https://proceedings.mlr.press/v202/guo23k/guo23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimating Heterogeneous Treatment Effects: Mutual Information Bounds and Learning Algorithms},
  url = {https://proceedings.mlr.press/v202/guo23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023fedbr,
  abstract = {Federated Learning (FL) is a way for machines to learn from data that is kept locally, in order to protect the privacy of clients. This is typically done using local SGD, which helps to improve communication efficiency. However, such a scheme is currently constrained by slow and unstable convergence due to the variety of data on different clients' devices.},
  author = {Yongxin Guo and Xiaoying Tang 0002 and Tao Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023fedbr.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12034--12054},
  pdf = {https://proceedings.mlr.press/v202/guo23g/guo23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FedBR: Improving Federated Learning on Heterogeneous Data via Local Learning Bias Reduction},
  url = {https://proceedings.mlr.press/v202/guo23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023fedxl,
  abstract = {In this paper, we tackle a novel federated learning (FL) problem for optimizing a family of X-risks, to which no existing FL algorithms are applicable. The objective involves computing expectations across two distributed data sets, with a focus on pairwise loss functions.},
  author = {Zhishuai Guo and Rong Jin 0001 and Jiebo Luo 0001 and Tianbao Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023fedxl.pdf:pdf},
  mdate = {2024-12-04},
  pages = {11934--11966},
  pdf = {https://proceedings.mlr.press/v202/guo23c/guo23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FeDXL: Provable Federated Learning for Deep X-Risk Optimization},
  url = {https://proceedings.mlr.press/v202/guo23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023identifying,
  abstract = {The learnware paradigm aims to build a learnware market containing numerous learnwares, each of which is a well-performing machine learning model with a corresponding specification to describe its functionality so that future users can identify useful models for reuse according to their own requirements. With the learnware paradigm, model developers can spontaneously submit models to the market without leaking data privacy, and users can leverage models in the market to accomplish different machine learning tasks without having to build models from scratch. Recent studies have attempted to realize the model specification through Reduced Kernel Mean Embedding (RKME). In this paper, we make an attempt to improve the effectiveness of RKME specification for heterogeneous label spaces, where the learnware market does not contain a model that has the same label space as the user's task, by considering a class-specific model specification explicitly, along with a class-wise learnware identification method. Both theoretical and empirical analyses show that our proposal can quickly and accurately find useful learnwares that satisfy users' requirements. Moreover, we find that for a specific task, reusing a small model identified via the specification performs better than directly reusing a pre-trained generic big model.},
  author = {Lan-Zhe Guo and Zhi Zhou and Yu-Feng Li and Zhi-Hua Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023identifying.pdf:pdf},
  pages = {12122--12131},
  pdf = {https://proceedings.mlr.press/v202/guo23l/guo23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Identifying Useful Learnwares for Heterogeneous Label Spaces},
  url = {https://proceedings.mlr.press/v202/guo23l.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023privacyaware,
  abstract = {In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model. The main challenge in this setting is balancing privacy with both classification accuracy of the learnt model as well as the number of bits communicated between the clients and server. Prior work has achieved a good trade-off by designing a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism, that numerically solves an optimization problem to determine the parameters of the mechanism. This paper builds upon it by introducing a new interpolation procedure in the numerical design process that allows for a far more efficient privacy analysis. The result is the new Interpolated MVU mechanism that is more scalable, has a better privacy-utility trade-off, and provides SOTA results on communication-efficient federated learning.},
  author = {Chuan Guo and Kamalika Chaudhuri and Pierre Stock and Michael G. Rabbat},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023privacyaware.pdf:pdf},
  pages = {11888--11904},
  pdf = {https://proceedings.mlr.press/v202/guo23a/guo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design},
  url = {https://proceedings.mlr.press/v202/guo23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023outofdistribution,
  abstract = {Out-of-distribution generalization is challenging for non-participating clients of federated learning under distribution shifts. A proven strategy is to explore those invariant relationships between input and target variables, working equally well for non-participating clients. However, learning invariant relationships is often in an explicit manner from data, representation, and distribution, which violates the federated principles of privacy-preserving and limited communication. In this paper, we propose FedIIR, which implicitly learns invariant relationships from parameter for out-of-distribution generalization, adhering to the above principles. Specifically, we utilize the prediction disagreement to quantify invariant relationships and implicitly reduce it through inter-client gradient alignment. Theoretically, we demonstrate the range of non-participating clients to which FedIIR is expected to generalize and present the convergence results for FedIIR in the massively distributed with limited communication. Extensive experiments show that FedIIR significantly outperforms relevant baselines in terms of out-of-distribution generalization of federated learning.},
  author = {Yaming Guo and Kai Guo and Xiaofeng Cao and Tieru Wu and Yi Chang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023outofdistribution.pdf:pdf},
  pages = {11905--11933},
  pdf = {https://proceedings.mlr.press/v202/guo23b/guo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Out-of-Distribution Generalization of Federated Learning via Implicit Invariant Relationships},
  url = {https://proceedings.mlr.press/v202/guo23b.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023provably,
  abstract = {We study representation learning in partially observable Markov Decision Processes (POMDPs), where the agent learns a decoder function that maps a series of high-dimensional raw observations to a compact representation and uses it for more efficient exploration and planning. We focus on the sub-classes of γ-observable and decodable POMDPs, for which it has been shown that statistically tractable learning is possible, but there has not been any computationally efficient algorithm. We first present an algorithm for decodable POMDPs that combines maximum likelihood estimation (MLE) and optimism in the face of uncertainty (OFU) to perform representation learning and achieve efficient sample complexity, while only calling supervised learning computational oracles. We then show how to adapt this algorithm to also work in the broader class of γ-observable POMDPs.},
  author = {Jiacheng Guo and Zihao Li and Huazheng Wang and Mengdi Wang and Zhuoran Yang and Xuezhou Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023provably.pdf:pdf},
  pages = {11967--11997},
  pdf = {https://proceedings.mlr.press/v202/guo23d/guo23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Efficient Representation Learning with Tractable Planning in Low-Rank {POMDP}},
  url = {https://proceedings.mlr.press/v202/guo23d.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023analyzing,
  abstract = {Differential privacy (DP) is by far the most widely accepted framework for mitigating privacy risks in machine learning. However, exactly how small the privacy parameter ε needs to be to protect against certain privacy risks in practice is still not well-understood. In this work, we study data reconstruction attacks for discrete data and analyze it under the framework of multiple hypothesis testing. For a learning algorithm satisfying (α, ε)-Renyi DP, we utilize different variants of the celebrated Fano's inequality to upper bound the attack advantage of a data reconstruction adversary. Our bound can be numerically computed to relate the parameter ε to the desired level of privacy protection in practice, and complements the empirical evidence for the effectiveness of DP against data reconstruction attacks even at relatively large values of ε.},
  author = {Chuan Guo and Alexandre Sablayrolles and Maziar Sanjabi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023analyzing.pdf:pdf},
  pages = {11998--12011},
  pdf = {https://proceedings.mlr.press/v202/guo23e/guo23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From {F}ano},
  url = {https://proceedings.mlr.press/v202/guo23e.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023linkless,
  abstract = {Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distribution-based matching strategies that complement each other. Extensive experiments demonstrate that LLP boosts the link prediction performance of MLPs with significant margins and even outperforms the teacher GNNs on 7 out of 8 benchmarks.},
  author = {Zhichun Guo and William Shiao and Shichang Zhang and Yozen Liu and Nitesh V. Chawla and Neil Shah and Tong Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023linkless.pdf:pdf},
  pages = {12012--12033},
  pdf = {https://proceedings.mlr.press/v202/guo23f/guo23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linkless Link Prediction via Relational Distillation},
  url = {https://proceedings.mlr.press/v202/guo23f.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023hierarchical,
  abstract = {The prediction of molecular properties is a crucial task in the field of material and drug discovery. The potential benefits of using deep learning techniques are reflected in the wealth of recent literature. Still, these techniques are faced with a common challenge in practice: Labeled data are limited by the cost of manual extraction from literature and laborious experimentation. In this work, we propose a data-efficient property predictor by utilizing a learnable hierarchical molecular grammar that can generate molecules from grammar production rules. Such a grammar induces an explicit geometry of the space of molecular graphs, which provides an informative prior on molecular structural similarity. The property prediction is performed using graph neural diffusion over the grammar-induced geometry.},
  author = {Minghao Guo and Veronika Thost and Samuel W. Song and Adithya Balachandran and Payel Das and Jie Chen and Wojciech Matusik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023hierarchical.pdf:pdf},
  pages = {12055--12076},
  pdf = {https://proceedings.mlr.press/v202/guo23h/guo23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction},
  url = {https://proceedings.mlr.press/v202/guo23h.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023graph,
  abstract = {Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features? In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases.},
  author = {Yuhe Guo and Zhewei Wei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023graph.pdf:pdf},
  pages = {12077--12097},
  pdf = {https://proceedings.mlr.press/v202/guo23i/guo23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Neural Networks with Learnable and Optimal Polynomial Bases},
  url = {https://proceedings.mlr.press/v202/guo23i.html},
  volume = {202},
  year = {2023}
}

@inproceedings{guo2023longcoder,
  abstract = {In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.},
  author = {Daya Guo and Canwen Xu and Nan Duan and Jian Yin and Julian J. McAuley},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guo2023longcoder.pdf:pdf},
  pages = {12098--12107},
  pdf = {https://proceedings.mlr.press/v202/guo23j/guo23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LongCoder}: A Long-Range Pre-trained Language Model for Code Completion},
  url = {https://proceedings.mlr.press/v202/guo23j.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gupta2023grafenne,
  abstract = {Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeiler-Leman tests, and therefore, the additional inductivity to unseen features does not come at the cost of expressivity. In addition, as demonstrated over four real-world graphs, GRAFENNE empowers the underlying GNN with high empirical efficacy and the ability to learn in continual fashion over streaming feature sets.},
  author = {Shubham Gupta and Sahil Manchanda and Sayan Ranu and Srikanta J. Bedathur},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gupta2023grafenne.pdf:pdf},
  pages = {12165--12181},
  pdf = {https://proceedings.mlr.press/v202/gupta23b/gupta23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GRAFENNE}: Learning on Graphs with Heterogeneous and Dynamic Feature Sets},
  url = {https://proceedings.mlr.press/v202/gupta23b.html},
  volume = {202},
  year = {2023}
}

@inproceedings{gupta2023online,
  abstract = {We present an online post-hoc calibration method, called Online Platt Scaling ({OPS}), which combines the Platt scaling technique with online logistic regression. We demonstrate that {OPS} smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance {OPS} by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting {OPS}+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all {OPS} ideas to the beta scaling method.},
  author = {Chirag Gupta and Aaditya Ramdas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gupta2023online.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12182--12204},
  pdf = {https://proceedings.mlr.press/v202/gupta23c/gupta23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online {P}latt {S}caling with {C}alibeating},
  url = {https://proceedings.mlr.press/v202/gupta23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gurulingan2023multitask,
  abstract = {Multi-task learning has the potential to improve generalization by maximizing positive transfer between tasks while reducing task interference. Fully achieving this potential is hindered by manually designed architectures that remain static throughout training. On the contrary, learning in the brain occurs through structural changes that are in tandem with changes in synaptic strength. Thus, we propose Multi-Task Structural Learning ({MTSL}) that simultaneously learns the multi-task architecture and its parameters. {MTSL} begins with an identical single-task network for each task and alternates between a task-learning phase and a structural-learning phase. In the task learning phase, each network specializes in the corresponding task. In each of the structural learning phases, starting from the earliest layer, locally similar task layers first transfer their knowledge to a newly created group layer before being removed. {MTSL} then uses the group layer in place of the corresponding removed task layers and moves on to the next layers. Our empirical results show that {MTSL} achieves competitive generalization with various baselines and improves robustness to out-of-distribution data.},
  author = {Naresh Kumar Gurulingan and Bahram Zonooz and Elahe Arani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gurulingan2023multitask.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12205--12223},
  pdf = {https://proceedings.mlr.press/v202/gurulingan23a/gurulingan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Task Structural Learning using Local Task Similarity induced Neuron Creation and Removal},
  url = {https://proceedings.mlr.press/v202/gurulingan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guth2023conditionally,
  abstract = {There is a growing gap between the impressive results of deep image generative models and classical algorithms that offer theoretical guarantees. The former suffer from mode collapse or memorization issues, limiting their application to scientific data. The latter require restrictive assumptions such as log-concavity to escape the curse of dimensionality. We partially bridge this gap by introducing conditionally strongly log-concave ({CSLC}) models, which factorize the data distribution into a product of conditional probability distributions that are strongly log-concave. This factorization is obtained with orthogonal projectors adapted to the data distribution. It leads to efficient parameter estimation and sampling algorithms, with theoretical guarantees, although the data distribution is not globally log-concave. We show that several challenging multiscale processes are conditionally log-concave using wavelet packet orthogonal projectors. Numerical results are shown for physical fields such as the $\varphi^4$ model and weak lensing convergence maps with higher resolution than in previous works.},
  author = {Florentin Guth and Etienne Lempereur and Joan Bruna and St{\'e}phane Mallat},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guth2023conditionally.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12224--12251},
  pdf = {https://proceedings.mlr.press/v202/guth23a/guth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conditionally Strongly Log-Concave Generative Models},
  url = {https://proceedings.mlr.press/v202/guth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{gutteridge2023dr,
  abstract = {Message passing neural networks ({MPNNs}) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any {MPNN} architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop {MPNNs}.},
  author = {Benjamin Gutteridge and Xiaowen Dong and Michael M. Bronstein and Francesco Di Giovanni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/gutteridge2023dr.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12252--12267},
  pdf = {https://proceedings.mlr.press/v202/gutteridge23a/gutteridge23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DR}ew: {D}ynamically {R}ewired {M}essage {P}assing with {D}elay},
  url = {https://proceedings.mlr.press/v202/gutteridge23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{guyomard2023kernel,
  author = {Marie Guyomard and Susana Barbosa and Lionel Fillatre},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/guyomard2023kernel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12268--12291},
  pdf = {https://proceedings.mlr.press/v202/guyomard23a/guyomard23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Kernel Logistic Regression Approximation of an Understandable {R}e{LU} Neural Network},
  url = {https://proceedings.mlr.press/v202/guyomard23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ha2023social,
  abstract = {How have individuals of social animals in nature evolved to learn from each other, and what would be the optimal strategy for such learning in a specific environment? Here, we address both problems by employing a deep reinforcement learning model to optimize the social learning strategies ({SLSs}) of agents in a cooperative game in a multi-dimensional landscape. Throughout the training for maximizing the overall payoff, we find that the agent spontaneously learns various concepts of social learning, such as copying, focusing on frequent and well-performing neighbors, self-comparison, and the importance of balancing between individual and social learning, without any explicit guidance or prior knowledge about the system. The {SLS} from a fully trained agent outperforms all of the traditional, baseline {SLSs} in terms of mean payoff. We demonstrate the superior performance of the reinforcement learning agent in various environments, including temporally changing environments and real social networks, which also verifies the adaptability of our framework to different social settings.},
  author = {Seungwoong Ha and Hawoong Jeong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ha2023social.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12319--12338},
  pdf = {https://proceedings.mlr.press/v202/ha23a/ha23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Social learning spontaneously emerges by searching optimal heuristics with deep reinforcement learning},
  url = {https://proceedings.mlr.press/v202/ha23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{haider2023convex,
  abstract = {The paper uses a frame-theoretic setting to study the injectivity of a {ReLU}-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a {ReLU}-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a {ReLU}-layer and a concrete reconstruction algorithm for any input vector on the ball.},
  author = {Daniel Haider and Martin Ehler and P{\'e}ter Bal{\'a}zs},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/haider2023convex.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12339--12350},
  pdf = {https://proceedings.mlr.press/v202/haider23a/haider23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Convex Geometry of {R}e{LU}-layers, Injectivity on the Ball and Local Reconstruction},
  url = {https://proceedings.mlr.press/v202/haider23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{halabi2023fairness,
  author = {Marwa El Halabi and Federico Fusco and Ashkan Norouzi-Fard and Jakab Tardos and Jakub Tarnawski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/halabi2023fairness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9150--9171},
  pdf = {https://proceedings.mlr.press/v202/el-halabi23a/el-halabi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fairness in Streaming Submodular Maximization over a Matroid Constraint},
  url = {https://proceedings.mlr.press/v202/el-halabi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{halabi2023difference,
  abstract = {Minimizing the difference of two submodular ({DS}) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a {DS} problem can be equivalently formulated as the minimization of the difference of two convex ({DC}) functions, existing algorithms do not fully exploit this connection. A classical algorithm for {DC} problems is called the {DC} algorithm ({DCA}). We introduce variants of {DCA} and its complete form ({CDCA}) that we apply to the {DC} program corresponding to {DS} minimization. We extend existing convergence properties of {DCA}, and connect them to convergence properties on the {DS} problem. Our results on {DCA} match the theoretical guarantees satisfied by existing {DS} algorithms, while providing a more complete characterization of convergence properties. In the case of {CDCA}, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus selection and feature selection.},
  author = {Marwa El Halabi and George Orfanides and Tim Hoheisel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/halabi2023difference.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9172--9201},
  pdf = {https://proceedings.mlr.press/v202/el-halabi23b/el-halabi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Difference of submodular minimization via {DC} programming},
  url = {https://proceedings.mlr.press/v202/el-halabi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hamman2023robust,
  abstract = {There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. We introduce a mathematical abstraction termed naturally-occurring model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. We propose a measure called Stability to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution shows that counterfactuals with sufficiently high value of Stability will remain valid after potential naturally-occurring model changes with high probability, leveraging concentration bounds for Lipschitz function of independent Gaussians. Since our quantification depends on the local Lipschitz constant around a data point which is not always available, we examine practical relaxations of our proposed measure and demonstrate experimentally how they can be incorporated to find robust counterfactuals.},
  author = {Faisal Hamman and Erfaun Noorani and Saumitra Mishra and Daniele Magazzeni and Sanghamitra Dutta},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hamman2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12351--12367},
  pdf = {https://proceedings.mlr.press/v202/hamman23a/hamman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees},
  url = {https://proceedings.mlr.press/v202/hamman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{han2023wrapped,
  abstract = {Addressing imbalanced or long-tailed data is a major challenge in visual recognition tasks due to disparities between training and testing distributions and issues with data noise. We propose the Wrapped Cauchy Distributed Angular Softmax (WCDAS), a novel softmax function that incorporates data-wise Gaussian-based kernels into the angular correlation between feature representations and classifier weights, effectively mitigating noise and sparse sampling concerns. The class-wise distribution of angular representation becomes a sum of these kernels. Our theoretical analysis reveals that the wrapped Cauchy distribution excels the Gaussian distribution in approximating mixed distributions. Additionally, WCDAS uses trainable concentration parameters to dynamically adjust the compactness and margin of each class. Empirical results confirm label-aware behavior in these parameters and demonstrate WCDAS's superiority over other state-of-the-art softmax-based methods in handling long-tailed visual recognition across multiple benchmark datasets.},
  author = {Boran Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/han2023wrapped.pdf:pdf},
  pages = {12368--12388},
  pdf = {https://proceedings.mlr.press/v202/han23a/han23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Wrapped {C}auchy {D}istributed {A}ngular {S}oftmax for {L}ong-{T}ailed {V}isual {R}ecognition},
  url = {https://proceedings.mlr.press/v202/han23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{han2023impact,
  abstract = {However, few have researched the other advantages of knowledge distillation in addition to its improving model performance. In this study, we have attempted to show that knowledge distillation enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different knowledge distillation methods, and according to different measures of interpretability. Our research showed that knowledge distillation models by large models could be used more reliably in various fields.},
  author = {Hyeongrok Han and Siwon Kim and Hyun-Soo Choi and Sungroh Yoon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/han2023impact.pdf:pdf},
  pages = {12389--12410},
  pdf = {https://proceedings.mlr.press/v202/han23b/han23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Impact of Knowledge Distillation for Model Interpretability},
  url = {https://proceedings.mlr.press/v202/han23b.html},
  volume = {202},
  year = {2023}
}

@inproceedings{han2023alternately,
  abstract = {Graph Neural Networks (GNNs) have greatly advanced the semi-supervised node classification task on graphs. The majority of existing GNNs are trained in an end-to-end manner that can be viewed as tackling a bi-level optimization problem. This process is often inefficient in computation and memory usage. In this work, we propose a new optimization framework for semi-supervised learning on graphs from a multi-view learning perspective. The proposed framework can be conveniently solved by the alternating optimization algorithms, resulting in significantly improved efficiency. Extensive experiments demonstrate that the proposed method can achieve comparable or better performance with state-of-the-art baselines while it has significantly better computation and memory efficiency.},
  author = {Haoyu Han and Xiaorui Liu and Haitao Mao and MohamadAli Torkamani and Feng Shi and Victor Lee and Jiliang Tang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/han2023alternately.pdf:pdf},
  pages = {12411--12429},
  pdf = {https://proceedings.mlr.press/v202/han23c/han23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Alternately Optimized Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/han23c.html},
  volume = {202},
  year = {2023}
}

@inproceedings{han2023system,
  abstract = {Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? The authors evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. The performance of commonly used identification techniques such as neural regression and centered kernel alignment at system identification is quite variable and depends significantly on factors independent of the ground truth architecture, such as scoring function and dataset. The paper shows the limitations of using functional similarity scores in identifying higher-level architectural motifs.},
  author = {Yena Han and Tomaso A. Poggio and Brian Cheung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/han2023system.pdf:pdf},
  pages = {12430--12444},
  pdf = {https://proceedings.mlr.press/v202/han23d/han23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {System Identification of Neural Systems: If We Got It Right, Would We Know?},
  url = {https://proceedings.mlr.press/v202/han23d.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hansen2023total,
  abstract = {Recently proposed Graph Neural Networks (GNNs) for vertex clustering are trained with an unsupervised minimum cut objective, approximated by a Spectral Clustering (SC) relaxation. However, the SC relaxation is loose and, while it offers a closed-form solution, it also yields overly smooth cluster assignments that poorly separate the vertices. In this paper, we propose a GNN model that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV). The cluster assignments can be used directly to perform vertex clustering or to implement graph pooling in a graph classification framework.},
  author = {Jonas Berg Hansen and Filippo Maria Bianchi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hansen2023total.pdf:pdf},
  pages = {12445--12468},
  pdf = {https://proceedings.mlr.press/v202/hansen23a/hansen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Total Variation Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/hansen23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hansen2023learning,
  abstract = {Recent work in scientific machine learning (SciML) has focused on incorporating partial differential equation (PDE) information into the learning process. Much of this work has focused on relatively ``easy'' PDE operators (e.g., elliptic and parabolic), with less emphasis on relatively ``hard'' PDE operators (e.g., hyperbolic). Within numerical PDEs, the latter problem class requires control of a type of volume element or conservation constraint, which is known to be challenging. Delivering on the promise of SciML requires seamlessly incorporating both types of problems into the learning process. To address this issue, we propose ProbConserv, a framework for incorporating conservation constraints into a generic SciML architecture. To do so, ProbConserv combines the integral form of a conservation law with a Bayesian update.},
  author = {Derek Hansen and Danielle C. Maddix and Shima Alizadeh and Gaurav Gupta and Michael W. Mahoney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hansen2023learning.pdf:pdf},
  pages = {12469--12510},
  pdf = {https://proceedings.mlr.press/v202/hansen23b/hansen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Physical Models that Can Respect Conservation Laws},
  url = {https://proceedings.mlr.press/v202/hansen23b.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hao2023leveraging,
  abstract = {We investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We show that the degree of improvement must depend on the quality of the demonstration data. To generate portable insights, we focus on Thompson sampling (TS) applied to a multi-armed bandit as a prototypical online learning algorithm and model. The demonstration data is generated by an expert with a given competence level, a notion we introduce. We propose an informed TS algorithm that utilizes the demonstration data in a coherent way through Bayes' rule and derive a prior-dependent Bayesian regret bound. This offers insight into how pretraining can greatly improve online performance and how the degree of improvement increases with the expert's competence level.},
  author = {Botao Hao and Rahul Jain and Tor Lattimore and Benjamin Van Roy and Zheng Wen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hao2023leveraging.pdf:pdf},
  pages = {12527--12545},
  pdf = {https://proceedings.mlr.press/v202/hao23a/hao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Leveraging Demonstrations to Improve Online Learning: Quality Matters},
  url = {https://proceedings.mlr.press/v202/hao23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hao2023coupled,
  abstract = {Variational auto-encoders are powerful probabilistic models in generative tasks but suffer from generating low-quality samples which are caused by the holes in the prior. The authors propose the Coupled Variational Auto-Encoder (C-VAE), which formulates the VAE problem as one of Optimal Transport (OT) between the prior and data distributions. The C-VAE allows greater flexibility in priors and natural resolution of the prior hole problem by enforcing coupling between the prior and the data distribution and enables flexible optimization through the primal, dual, and semi-dual formulations of entropic OT. Simulations on synthetic and real data show that the C-VAE outperforms alternatives including VAE, WAE, and InfoVAE in fidelity to the data, quality of the latent representation, and in quality of generated samples.},
  author = {Xiaoran Hao and Patrick Shafto},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hao2023coupled.pdf:pdf},
  pages = {12546--12555},
  pdf = {https://proceedings.mlr.press/v202/hao23b/hao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coupled Variational Autoencoder},
  url = {https://proceedings.mlr.press/v202/hao23b.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hao2023gnot,
  abstract = {Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improvement compared with alternative methods.},
  author = {Zhongkai Hao and Zhengyi Wang and Hang Su and Chengyang Ying and Yinpeng Dong and Songming Liu and Ze Cheng and Jian Song and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hao2023gnot.pdf:pdf},
  pages = {12556--12569},
  pdf = {https://proceedings.mlr.press/v202/hao23c/hao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GNOT}: A General Neural Operator Transformer for Operator Learning},
  url = {https://proceedings.mlr.press/v202/hao23c.html},
  volume = {202},
  year = {2023}
}

@inproceedings{hardt2023algorithmic,
  abstract = {We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: nonparametric optimal learning, parametric risk minimization, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT-like language model, we see a striking correspondence emerge between our empirical observations and the predictions made by our theory. Taken together, our theory and experiments broadly support the conclusion that algorithmic collectives of exceedingly small fractional size can exert significant control over a platform's learning algorithm.},
  author = {Moritz Hardt and Eric Mazumdar and Celestine Mendler-D\"unner and Tijana Zrnic},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hardt2023algorithmic.pdf:pdf},
  pages = {12570--12586},
  pdf = {https://proceedings.mlr.press/v202/hardt23a/hardt23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Algorithmic Collective Action in Machine Learning},
  url = {https://proceedings.mlr.press/v202/hardt23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{ha2023gaussian,
  abstract = {Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equation, and Maxwell's equations, where we improve upon the state of the art in computation time and precision, in some experiments by several orders of magnitude.},
  author = {Marc H{\"{a}}rk{\"{o}}nen and Markus Lange-Hegermann and Bogdan Raita},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ha2023gaussian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12587--12615},
  pdf = {https://proceedings.mlr.press/v202/harkonen23a/harkonen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients},
  url = {https://proceedings.mlr.press/v202/harkonen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hasson2023theoretical,
  abstract = {Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of 'stacked generalization,' namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform 'much worse' than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the ensemble weights are allowed to vary across items, timestamps in the forecast horizon, and quantiles. Experimental results demonstrate the performance gain of the proposed method.},
  author = {Hilaf Hasson and Danielle C. Maddix and Bernie Wang and Gaurav Gupta and Youngsuk Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hasson2023theoretical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12616--12632},
  pdf = {https://proceedings.mlr.press/v202/hasson23a/hasson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting},
  url = {https://proceedings.mlr.press/v202/hasson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hatamizadeh2023global,
  abstract = {We propose global context vision transformer ({GC ViT}), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in {ViT}s, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed {GC ViT} achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On {ImageNet-1K} dataset for classification, {GC ViT} variants with 51M, 90M and 201M parameters achieve 84.3, 85.9 and 85.7 Top-1 accuracy, respectively.},
  author = {Ali Hatamizadeh and Hongxu Yin and Greg Heinrich and Jan Kautz and Pavlo Molchanov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hatamizadeh2023global.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12633--12646},
  pdf = {https://proceedings.mlr.press/v202/hatamizadeh23a/hatamizadeh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Global Context Vision Transformers},
  url = {https://proceedings.mlr.press/v202/hatamizadeh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{haugh2023counterfactual,
  abstract = {We provide an optimization-based framework to perform counterfactual analysis in a dynamic model with hidden states. Our framework is grounded in the ``abduction, action, and prediction'' approach to answer counterfactual queries and handles two key challenges where (1) the states are hidden and (2) the model is dynamic. Recognizing the lack of knowledge on the underlying causal mechanism and the possibility of infinitely many such mechanisms, we optimize over this space and compute upper and lower bounds on the counterfactual quantity of interest. Our work brings together ideas from causality, state-space models, simulation, and optimization, and we apply it on a breast cancer case study. To the best of our knowledge, we are the first to compute lower and upper bounds on a counterfactual query in a dynamic latent-state model.},
  author = {Martin B. Haugh and Raghav Singal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/haugh2023counterfactual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12647--12677},
  pdf = {https://proceedings.mlr.press/v202/haugh23a/haugh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Counterfactual Analysis in Dynamic Latent State Models},
  url = {https://proceedings.mlr.press/v202/haugh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hayakawa2023samplingbased,
  abstract = {We analyze the {N}ystr{\"{o}}m approximation of a positive definite kernel associated with a probability measure. We first prove an improved error bound for the conventional {N}ystr{\"{o}}m approximation with i.i.d. sampling and singular-value decomposition in the continuous regime; the proof techniques are borrowed from statistical learning theory. We further introduce a refined selection of subspaces in {N}ystr{\"{o}}m approximation with theoretical guarantees that is applicable to non-i.i.d. landmark points. Finally, we discuss their application to convex kernel quadrature and give novel theoretical guarantees as well as numerical observations.},
  author = {Satoshi Hayakawa and Harald Oberhauser and Terry J. Lyons},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hayakawa2023samplingbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12678--12699},
  pdf = {https://proceedings.mlr.press/v202/hayakawa23a/hayakawa23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sampling-based {N}ystr{\"{o}}m Approximation and Kernel Quadrature},
  url = {https://proceedings.mlr.press/v202/hayakawa23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hayou2023width,
  abstract = {We show that taking the width and depth to infinity in a deep neural network with skip connections, when branches are scaled by $1/\sqrt{\text{depth}}$, result in the same covariance structure no matter how that limit is taken. This explains why the standard infinite-width-then-depth approach provides practical insights even for networks with depth of the same order as width. We also demonstrate that the pre-activations, in this case, have Gaussian distributions which has direct applications in Bayesian deep learning.},
  author = {Soufiane Hayou and Greg Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hayou2023width.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12700--12723},
  pdf = {https://proceedings.mlr.press/v202/hayou23a/hayou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Width and Depth Limits Commute in Residual Networks},
  url = {https://proceedings.mlr.press/v202/hayou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{he2023generalization,
  abstract = {Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we propose an alternative approach to overcome these structural limitations by leveraging the {ViT}/{MLP}-Mixer architectures introduced in computer vision. We introduce a new class of GNNs, called Graph {ViT}/{MLP}-Mixer, that holds three key properties. First, they capture long-range dependency and mitigate the issue of over-squashing as demonstrated on Long Range Graph Benchmark and TreeNeighbourMatch datasets. Second, they offer better speed and memory efficiency with a complexity linear to the number of nodes and edges, surpassing the related Graph Transformer and expressive GNN models. Third, they show high expressivity in terms of graph isomorphism as they can distinguish at least 3-WL non-isomorphic graphs. We test our architecture on 4 simulated datasets and 7 real-world benchmarks, and show highly competitive results on all of them.},
  author = {Xiaoxin He and Bryan Hooi and Thomas Laurent and Adam Perold and Yann LeCun and Xavier Bresson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/he2023generalization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12724--12745},
  pdf = {https://proceedings.mlr.press/v202/he23a/he23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Generalization of {ViT}/{MLP}-Mixer to Graphs},
  url = {https://proceedings.mlr.press/v202/he23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{he2023domain,
  abstract = {Unsupervised domain adaptation ({UDA}) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present {RAINCOAT}, the first model for both closed-set and universal domain adaptation on complex time series. {RAINCOAT} addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, {RAINCOAT} improves transferability by identifying label shifts in target domains.},
  author = {Huan He and Owen Queen and Teddy Koker and Consuelo Cuevas and Theodoros Tsiligkaridis and Marinka Zitnik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/he2023domain.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12746--12774},
  pdf = {https://proceedings.mlr.press/v202/he23b/he23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Domain Adaptation for Time Series Under Feature and Label Shifts},
  url = {https://proceedings.mlr.press/v202/he23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{he2023contrastive,
  abstract = {We propose {NeCo} (Neighbor Contrastive Representation Learning on Graphs), which combines self-supervised contrastive learning with graph homophily analysis. Unlike conventional approaches that rely on data augmentation, {NeCo} introduces a heterophily discriminator that models heterophilic edges using Bernoulli distribution and removes them to block heterophilic noise propagation. The retained homophilic edges connect neighbor nodes that are sampled as positive pairs for contrastive learning objectives. {NeCo} integrates graph contrastive learning with homophily discrimination, enabling effective representation learning on both homophilic and heterophilic datasets in an unsupervised manner without requiring additional views or data augmentation.},
  author = {Dongxiao He and Jitao Zhao and Rui Guo and Zhiyong Feng and Di Jin and Yuxiao Huang and Zhen Wang and Weixiong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/he2023contrastive.pdf:pdf},
  mdate = {2024-04-11},
  pages = {12775--12789},
  pdf = {https://proceedings.mlr.press/v202/he23c/he23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contrastive Learning Meets Homophily: Two Birds with One Stone},
  url = {https://proceedings.mlr.press/v202/he23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{he2023nearly,
  abstract = {We study reinforcement learning ({RL}) with linear function approximation. For episodic time-inhomogeneous linear Markov decision processes (linear {MDPs}) whose transition probability can be parameterized as a linear function of a given feature mapping, we propose the first computationally efficient algorithm that achieves the nearly minimax optimal regret $\tilde{O}(d\sqrt{H^3K})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $K$ is the number of episodes. Our algorithm is based on a weighted linear regression scheme with carefully designed weights, which depends on the variance of the feature representation.},
  author = {Jiafan He and Heyang Zhao and Dongruo Zhou and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/he2023nearly.pdf:pdf},
  mdate = {2024-12-24},
  pages = {12790--12822},
  pdf = {https://proceedings.mlr.press/v202/he23d/he23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes},
  url = {https://proceedings.mlr.press/v202/he23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hebbar2023crisp,
  abstract = {Polar codes are widely used state-of-the-art codes for reliable communication that have recently been included in the 5th generation wireless standards (5G). However, there still remains room for design of polar decoders that are both efficient and reliable in the short blocklength regime. Introduces {CRISP}: Curriculum based Sequential neural decoder for Polar codes. Demonstrates performance improvements over successive-cancellation ({SC}) decoder. First data-driven decoder for Polarization-Adjusted-Convolutional ({PAC}) codes.},
  author = {S. Ashwin Hebbar and Viraj Vivek Nadkarni and Ashok Vardhan Makkuva and Suma Bhat and Sewoong Oh and Pramod Viswanath},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hebbar2023crisp.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12823--12845},
  pdf = {https://proceedings.mlr.press/v202/hebbar23a/hebbar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CRISP}: {C}urriculum based {S}equential neural decoders for {P}olar code family},
  url = {https://proceedings.mlr.press/v202/hebbar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hehir2023s,
  abstract = {Data sketching is a critical tool for distinct counting, enabling multisets to be represented by compact summaries that admit fast cardinality estimates. Because sketches may be merged to summarize multiset unions, they are a basic building block in data warehouses. Although many practical sketches for cardinality estimation exist, none provide privacy when merging. We propose the first practical cardinality sketches that are simultaneously mergeable, differentially private ({DP}), and have low empirical errors. These introduce a novel randomized algorithm for performing logical operations on noisy bits, a tight privacy analysis, and provably optimal estimation. Our sketches dramatically outperform existing theoretical solutions in simulations and on real-world data.},
  author = {Jonathan Hehir and Daniel Ting and Graham Cormode},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hehir2023s.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12846--12865},
  pdf = {https://proceedings.mlr.press/v202/hehir23a/hehir23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{S}ketch-{F}lip-{M}erge: {M}ergeable {S}ketches for {P}rivate {D}istinct {C}ounting},
  url = {https://proceedings.mlr.press/v202/hehir23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{heinrichs2023f,
  abstract = {It is desirable for statistical models to detect signals of interest independently of their position. If the data is generated by some smooth process, this additional structure should be taken into account. We introduce a new class of neural networks that are shift invariant and preserve smoothness of the data: functional neural networks ({FNN}s). For this, we use methods from functional data analysis ({FDA}) to extend multi-layer perceptrons and convolutional neural networks to functional data. The authors propose different model architectures, show that the models outperform a benchmark model from {FDA} in terms of accuracy and successfully use {FNN}s to classify electroencephalography ({EEG}) data.},
  author = {Florian Heinrichs and Mavin Heim and Corinna Weber},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/heinrichs2023f.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12866--12881},
  pdf = {https://proceedings.mlr.press/v202/heinrichs23a/heinrichs23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{F}unctional {N}eural {N}etworks: {S}hift invariant models for functional data with applications to {EEG} classification},
  url = {https://proceedings.mlr.press/v202/heinrichs23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hejna2023d,
  abstract = {Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning ({IL}) methods based on supervised learning are robust, but require optimal demonstrations, which are hard to collect. Offline goal-conditioned reinforcement learning ({RL}) algorithms promise to learn from sub-optimal data, but face optimization challenges especially with high-dimensional data. To bridge the gap between {IL} and {RL}, we introduce Distance Weighted Supervised Learning or {DWSL}, a supervised method for learning goal-conditioned policies from offline data. {DWSL} models the entire distribution of time-steps between states in offline data with only supervised learning, and uses this distribution to approximate shortest path distances.},
  author = {Joey Hejna and Jensen Gao and Dorsa Sadigh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hejna2023d.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12882--12906},
  pdf = {https://proceedings.mlr.press/v202/hejna23a/hejna23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{D}istance {W}eighted {S}upervised {L}earning for {O}ffline {I}nteraction {D}ata},
  url = {https://proceedings.mlr.press/v202/hejna23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{helwig2023g,
  abstract = {We consider solving partial differential equations ({PDE}s) with {F}ourier neural operators ({FNO}s), which operate in the frequency domain. Since the laws of physics do not depend on the coordinate system used to describe them, it is desirable to encode such symmetries in the neural operator architecture for better performance and easier learning. While encoding symmetries in the physical domain using group theory has been studied extensively, how to capture symmetries in the frequency domain is under-explored. In this work, we extend group convolutions to the frequency domain and design {F}ourier layers that are equivariant to rotations, translations, and reflections by leveraging the equivariance property of the {F}ourier transform. The resulting {G}-{FNO} architecture generalizes well across input resolutions and performs well in settings with varying levels of symmetry.},
  author = {Jacob Helwig and Xuan Zhang and Cong Fu and Jerry Kurtin and Stephan Wojtowytsch and Shuiwang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/helwig2023g.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12907--12930},
  pdf = {https://proceedings.mlr.press/v202/helwig23a/helwig23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{G}roup {E}quivariant {F}ourier {N}eural {O}perators for {P}artial {D}ifferential {E}quations},
  url = {https://proceedings.mlr.press/v202/helwig23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hemachandra2023t,
  abstract = {Existing neural active learning algorithms have aimed to optimize the predictive performance of neural networks ({NN}s) by selecting data for labelling. However, other than a good predictive performance, being robust against random parameter initializations is also a crucial requirement in safety-critical applications. To this end, we introduce our expected variance with {G}aussian processes ({EV}-{GP}) criterion for neural active learning, which is theoretically guaranteed to select data points which lead to trained {NN}s with both (a) good predictive performances and (b) initialization robustness. Importantly, our {EV}-{GP} criterion is training-free, i.e., it does not require any training of the {NN} during data selection, which makes it computationally efficient.},
  author = {Apivich Hemachandra and Zhongxiang Dai and Jasraj Singh and See-Kiong Ng and Bryan Kian Hsiang Low},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hemachandra2023t.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12931--12971},
  pdf = {https://proceedings.mlr.press/v202/hemachandra23a/hemachandra23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{T}raining-{F}ree {N}eural {A}ctive {L}earning with {I}nitialization-{R}obustness {G}uarantees},
  url = {https://proceedings.mlr.press/v202/hemachandra23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{henaff2023a,
  abstract = {Exploration in environments which differ across episodes has received increasing attention in recent years. Current methods use some combination of global novelty bonuses, computed using the agent's entire training experience, and episodic novelty bonuses, computed using only experience from the current episode. However, the use of these two types of bonuses has been ad-hoc and poorly understood. In this work, we shed light on the behavior of these two types of bonuses through controlled experiments on easily interpretable tasks as well as challenging pixel-based settings. We find that the two types of bonuses succeed in different settings, with episodic bonuses being most effective when there is little shared structure across episodes and global bonuses being effective when more structure is shared. We develop a conceptual framework which makes this notion of shared structure precise by considering the variance of the value function across contexts, and which provides a unifying explanation of our empirical results. We furthermore find that combining the two bonuses can lead to more robust performance across different degrees of shared structure, and investigate different algorithmic choices for defining and combining global and episodic bonuses based on function approximation. This results in an algorithm which sets a new state of the art across 16 tasks from the {M}ini{H}ack suite used in prior work, and also performs robustly on {H}abitat and {M}ontezuma's {R}evenge.},
  author = {Mikael Henaff and Minqi Jiang and Roberta Raileanu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/henaff2023a.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12972--12999},
  pdf = {https://proceedings.mlr.press/v202/henaff23a/henaff23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{A} {S}tudy of {G}lobal and {E}pisodic {B}onuses for {E}xploration in {C}ontextual {MDP}s},
  url = {https://proceedings.mlr.press/v202/henaff23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{heo2023r,
  abstract = {Multi-resolution hash encoding has recently been proposed to reduce the computational cost of neural renderings, such as {NeRF}. This method requires accurate camera poses for the neural renderings of given scenes. However, contrary to previous methods jointly optimizing camera poses and 3D scenes, the naive gradient-based camera pose refinement method using multi-resolution hash encoding severely deteriorates performance. We propose a joint optimization algorithm to calibrate the camera pose and learn a geometric representation using efficient multi-resolution hash encoding. Showing that the oscillating gradient flows of hash encoding interfere with the registration of camera poses, our method addresses the issue by utilizing smooth interpolation weighting to stabilize the gradient oscillation for the ray samplings across hash grids.},
  author = {Hwan Heo and Taekyung Kim and Jiyoung Lee and Jaewon Lee and Soohyun Kim and Hyunwoo J. Kim and Jin-Hwa Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/heo2023r.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13000--13016},
  pdf = {https://proceedings.mlr.press/v202/heo23a/heo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{R}obust {C}amera {P}ose {R}efinement for {M}ulti-{R}esolution {H}ash {E}ncoding},
  url = {https://proceedings.mlr.press/v202/heo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hess2023g,
  abstract = {Chaotic dynamical systems ({DS}) are ubiquitous in nature and society. Often we are interested in reconstructing such systems from observed time series for prediction or mechanistic insight, where by reconstruction we mean learning geometrical and invariant temporal properties of the system in question (like attractors). However, training reconstruction algorithms like recurrent neural networks ({RNN}s) on such systems by gradient-descent based techniques faces severe challenges. This is mainly due to exploding gradients caused by the exponential divergence of trajectories in chaotic systems. Moreover, for (scientific) interpretability we wish to have as low dimensional reconstructions as possible, preferably in a model which is mathematically tractable. Here we report that a surprisingly simple modification of teacher forcing leads to provably strictly all-time bounded gradients in training on chaotic systems, and, when paired with a simple architectural rearrangement of a tractable {RNN} design, piecewise-linear {RNN}s ({PLRNN}s), allows for faithful reconstruction in spaces of at most the dimensionality of the observed system. We show on several {DS} that with these amendments we can reconstruct {DS} better than current {SOTA} algorithms, in much lower dimensions. Performance differences were particularly compelling on real world data with which most other methods severely struggled. This work thus led to a simple yet powerful {DS} reconstruction algorithm which is highly interpretable at the same time.},
  author = {Florian Hess and Zahra Monfared and Manuel Brenner and Daniel Durstewitz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hess2023g.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13017--13049},
  pdf = {https://proceedings.mlr.press/v202/hess23a/hess23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{G}eneralized {T}eacher {F}orcing for {L}earning {C}haotic {D}ynamics},
  url = {https://proceedings.mlr.press/v202/hess23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hizli2023c,
  abstract = {A treatment policy defines when and what treatments are applied to affect some outcome of interest. Data-driven decision-making requires the ability to predict what happens if a policy is changed. Existing methods that predict how the outcome evolves under different scenarios assume that the tentative sequences of future treatments are fixed in advance, while in practice the treatments are determined stochastically by a policy and may depend, for example, on the efficiency of previous treatments. Therefore, the current methods are not applicable if the treatment policy is unknown or a counterfactual analysis is needed. To handle these limitations, we model the treatments and outcomes jointly in continuous time, by combining {G}aussian processes and point processes.},
  author = {Caglar Hizli and S. T. John and Anne Tuulikki Juuti and Tuure Tapani Saarinen and Kirsi Hannele Pietilinen and Pekka Marttinen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hizli2023c.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13050--13084},
  pdf = {https://proceedings.mlr.press/v202/hizli23a/hizli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{C}ausal {M}odeling of {P}olicy {I}nterventions {F}rom {T}reatment-{O}utcome {S}equences},
  url = {https://proceedings.mlr.press/v202/hizli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hodgkinson2023monotonicity,
  abstract = {Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures in machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and both should deteriorate with larger input dimensions. However, we prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), performance, as measured by the marginal likelihood, improves monotonically with the input dimension. On the other hand, cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena.},
  author = {Liam Hodgkinson and Chris Van Der Heide and Fred Roosta and Michael W. Mahoney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hodgkinson2023monotonicity.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13085--13117},
  pdf = {https://proceedings.mlr.press/v202/hodgkinson23a/hodgkinson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Monotonicity and Double Descent in Uncertainty Estimation with {G}aussian Processes},
  url = {https://proceedings.mlr.press/v202/hodgkinson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hoeven2023tradingoff,
  abstract = {We investigate online classification with paid stochastic experts. Here, before making their prediction, each expert must be paid. The amount that we pay each expert directly influences the accuracy of their prediction through some unknown Lipschitz 'productivity' function. In each round, the learner must decide how much to pay each expert and then make a prediction. They incur a cost equal to a weighted sum of the prediction error and upfront payments for all experts. We introduce an online learning algorithm whose total cost after {$T$} rounds exceeds that of a predictor which knows the productivity of all experts in advance by at most {$\mathcal{O}(K^2(\log T)\sqrt{T})$} where {$K$} is the number of experts. In order to achieve this result, we combine Lipschitz bandits and online classification with surrogate losses. These tools allow us to improve upon the bound of order {$T^{2/3}$} one would obtain in the standard Lipschitz bandit setting. Our algorithm is empirically evaluated on synthetic data.},
  author = {Dirk van der Hoeven and Ciara Pike-Burke and Hao Qiu and Nicol{\`{o}} Cesa-Bianchi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hoeven2023tradingoff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34809--34830},
  pdf = {https://proceedings.mlr.press/v202/van-der-hoeven23a/van-der-hoeven23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trading-Off Payments and Accuracy in Online Classification with Paid Stochastic Experts},
  url = {https://proceedings.mlr.press/v202/van-der-hoeven23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mo2023adaboost,
  abstract = {{AdaBoost} is a classic boosting algorithm for combining multiple inaccurate classifiers produced by a weak learner, to produce a strong learner with arbitrarily high accuracy when given enough training data. Determining the optimal number of samples necessary to obtain a given accuracy of the strong learner, is a basic learning theoretic question. Larsen and Ritzert ({NeurIPS}'22) recently presented the first provably optimal weak-to-strong learner. However, their algorithm is somewhat complicated and it remains an intriguing question whether the prototypical boosting algorithm {AdaBoost} also makes optimal use of training samples. In this work, we answer this question in the negative. Concretely, we show that the sample complexity of {AdaBoost}, and other classic variations thereof, are sub-optimal by at least one logarithmic factor in the desired accuracy of the strong learner.},
  author = {Mikael M{\o}ller H{\o}gsgaard and Kasper Green Larsen and Martin Ritzert},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mo2023adaboost.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13118--13140},
  pdf = {https://proceedings.mlr.press/v202/hogsgaard23a/hogsgaard23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{AdaBoost} is not an Optimal Weak to Strong Learner},
  url = {https://proceedings.mlr.press/v202/hogsgaard23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kjae2023dual,
  abstract = {Activity difference based learning algorithms---such as contrastive {H}ebbian learning and equilibrium propagation---have been proposed as biologically plausible alternatives to error back-propagation. However, on traditional digital chips these algorithms suffer from having to solve a costly inference problem twice, making these approaches more than two orders of magnitude slower than back-propagation. In the analog realm equilibrium propagation may be promising for fast and energy efficient learning, but states still need to be inferred and stored twice. Inspired by lifted neural networks and compartmental neuron models we propose a simple energy based compartmental neuron model, termed dual propagation, in which each neuron is a dyad with two intrinsic states. At inference time these intrinsic states encode the error/activity duality through their difference and their mean respectively. The advantage of this method is that only a single inference phase is needed and that inference can be solved in layerwise closed-form. Experimentally we show on common computer vision datasets, including {I}magenet32x32, that dual propagation performs equivalently to back-propagation both in terms of accuracy and runtime.},
  author = {Rasmus Kj{\ae}r H{\o}ier and D. Staudt and Christopher Zach},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kjae2023dual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13141--13156},
  pdf = {https://proceedings.mlr.press/v202/hoier23a/hoier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dual Propagation: Accelerating Contrastive {H}ebbian Learning with Dyadic Neurons},
  url = {https://proceedings.mlr.press/v202/hoier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hong2023multitask,
  abstract = {We study learning recommendation policies across multiple similar tasks, such as recommending movies to users with comparable preferences. The key challenge is achieving statistical efficiency in a multi-task setting under distribution shift when only bandit feedback is available. To achieve this, we formulate contextual off-policy optimization in a hierarchical graphical model from logged bandit feedback. We propose a hierarchical off-policy optimization algorithm ({HierOPO}), where we first estimate the parameters of a hierarchical model and then act pessimistically with respect to them. Our theoretical analysis shows a clear improvement over not using the hierarchical model. Empirically, we demonstrate a clear advantage of using the hierarchy over solving each task independently on synthetic and real datasets.},
  author = {Joey Hong and Branislav Kveton and Manzil Zaheer and Sumeet Katariya and Mohammad Ghavamzadeh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hong2023multitask.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13157--13173},
  pdf = {https://proceedings.mlr.press/v202/hong23a/hong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Task Off-Policy Learning from Bandit Feedback},
  url = {https://proceedings.mlr.press/v202/hong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hong2023constrained,
  abstract = {We consider solving equality-constrained nonlinear, nonconvex optimization problems. This class of problems appears widely in a variety of applications in machine learning and engineering, ranging from constrained deep neural networks, to optimal control, to {PDE}-constrained optimization. We develop an adaptive inexact {N}ewton method for this problem class. In each iteration, we solve the {L}agrangian {N}ewton system inexactly via a randomized iterative sketching solver, and select a suitable stepsize by performing line search on an exact augmented {L}agrangian merit function. The randomized solvers have advantages over deterministic linear system solvers by significantly reducing per-iteration flops complexity and storage cost, when equipped with suitable sketching matrices. Our method adaptively controls the accuracy of the randomized solver and the penalty parameters of the exact augmented {L}agrangian, to ensure that the inexact {N}ewton direction is a descent direction of the exact augmented {L}agrangian. This allows us to establish a global almost sure convergence. We also show that a unit stepsize is admissible locally, so that our method exhibits a local linear convergence. Furthermore, we prove that the linear convergence can be strengthened to superlinear convergence if we gradually sharpen the adaptive accuracy condition on the randomized solver. We demonstrate the superior performance of our method on benchmark nonlinear problems in {CUTE}st test set, constrained logistic regression with data from {LIBSVM}, and a {PDE}-constrained problem.},
  author = {Ilgee Hong and Sen Na and Michael W. Mahoney and Mladen Kolar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hong2023constrained.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13174--13198},
  pdf = {https://proceedings.mlr.press/v202/hong23b/hong23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Optimization via Exact Augmented {L}agrangian and Randomized Iterative Sketching},
  url = {https://proceedings.mlr.press/v202/hong23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hong2023revisiting,
  abstract = {Data-free knowledge distillation (KD) helps transfer knowledge from a pre-trained model (known as the teacher model) to a smaller model (known as the student model) without access to the original training data used for training the teacher model. However, the security of the synthetic or out-of-distribution (OOD) data required in data-free {KD} is largely unknown and under-explored. In this work, we make the first effort to uncover the security risk of data-free {KD} w.r.t. untrusted pre-trained models. We then propose Anti-Backdoor Data-Free {KD} ({ABD}), the first plug-in defensive method for data-free {KD} methods to mitigate the chance of potential backdoors being transferred.},
  author = {Junyuan Hong and Yi Zeng and Shuyang Yu and Lingjuan Lyu and Ruoxi Jia and Jiayu Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hong2023revisiting.pdf:pdf},
  mdate = {2024-03-20},
  pages = {13199--13212},
  pdf = {https://proceedings.mlr.press/v202/hong23c/hong23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Data-Free Knowledge Distillation with Poisoned Teachers},
  url = {https://proceedings.mlr.press/v202/hong23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hoogeboom2023simple,
  abstract = {Currently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework. This paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The research question is: How can one train a standard denoising diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches? The four main findings are: (1) the noise schedule should be adjusted for high resolution images, (2) it is sufficient to scale only a particular part of the architecture, (3) dropout should be added at specific locations in the architecture, and (4) downsampling is an effective strategy to avoid high resolution feature maps. By combining these simple yet effective techniques, we achieve state-of-the-art image generation among diffusion models without sampling modifiers on {ImageNet}.},
  author = {Emiel Hoogeboom and Jonathan Heek and Tim Salimans},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hoogeboom2023simple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13213--13232},
  pdf = {https://proceedings.mlr.press/v202/hoogeboom23a/hoogeboom23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {simple diffusion: End-to-end diffusion for high resolution images},
  url = {https://proceedings.mlr.press/v202/hoogeboom23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{horowitz2023causal,
  abstract = {When users can benefit from certain predictive outcomes, they may be prone to act to achieve those outcome, e.g., by strategically modifying their features. The goal in strategic classification is therefore to train predictive models that are robust to such behavior. However, the conventional framework assumes that changing features does not change actual outcomes, which depicts users as 'gaming' the system. Here we remove this assumption, and study learning in a causal strategic setting where true outcomes do change. Focusing on accuracy as our primary objective, we show how strategic behavior and causal effects underlie two complementing forms of distribution shift. We characterize these shifts, and propose a learning algorithm that balances between these two forces and over time, and permits end-to-end training. Experiments on synthetic and semi-synthetic data demonstrate the utility of our approach.},
  author = {Guy Horowitz and Nir Rosenfeld},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/horowitz2023causal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13233--13253},
  pdf = {https://proceedings.mlr.press/v202/horowitz23a/horowitz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Strategic Classification: A Tale of Two Shifts},
  url = {https://proceedings.mlr.press/v202/horowitz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hosseini2023fair,
  abstract = {The integration of machine learning models in various real-world applications is becoming more prevalent to assist humans in their daily decision-making tasks, however, there is a tradeoff between the accuracy and fairness of these decision-making tasks. In some cases, these {AI} systems can be unfair by exhibiting bias or discrimination against certain social groups, which can have severe consequences in real life. To address this issue, inspired by one of the most well-known human learning skills called grouping, we propose a novel {ML} framework where the {ML} model learns to group a diverse set of problems into distinct subgroups to solve each subgroup using its specific sub-model. Through this approach, we can minimize the fairness-accuracy tradeoff and achieve both fair and accurate decision-making. We demonstrate the effectiveness of our approach through comprehensive experiments on multiple datasets and show that our method outperforms baseline methods in terms of both fairness and accuracy.},
  author = {Ramtin Hosseini and Li Zhang and Bhanu Garg and Pengtao Xie},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hosseini2023fair.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13254--13269},
  pdf = {https://proceedings.mlr.press/v202/hosseini23a/hosseini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair and Accurate Decision Making through Group-Aware Learning},
  url = {https://proceedings.mlr.press/v202/hosseini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hotegni2023approximation,
  abstract = {This paper studies the fair range clustering problem in which the data points are from different demographic groups and the goal is to pick k centers with the minimum clustering cost such that each group is at least minimally represented in the centers set and no group dominates the centers set.},
  author = {Sdjro Salomon Hotegni and Sepideh Mahabadi and Ali Vakilian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hotegni2023approximation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13270--13284},
  pdf = {https://proceedings.mlr.press/v202/hotegni23a/hotegni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Approximation Algorithms for Fair Range Clustering},
  url = {https://proceedings.mlr.press/v202/hotegni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hou2023decoding,
  abstract = {In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.},
  author = {Elizabeth Mary Hou and Gregory David Castan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hou2023decoding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13285--13308},
  pdf = {https://proceedings.mlr.press/v202/hou23a/hou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Decoding Layer Saliency in Language Transformers},
  url = {https://proceedings.mlr.press/v202/hou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hou2023promptboosting,
  abstract = {We describe {PromptBoosting}, a query-efficient procedure for building a text classifier from a neural language model ({LM}) without access to the {LM}'s parameters, gradients, or hidden representations. This form of ``black-box'' classifier training has become increasingly important as the cost of training and inference in large-scale {LM}s grows. But existing black-box {LM} classifier learning approaches are themselves computationally inefficient, typically specializing {LM}s to the target task by searching in a large space of (discrete or continuous) prompts using zeroth-order optimization methods. Instead of directly optimizing in prompt space, {PromptBoosting} obtains a small pool of prompts via a gradient-free approach and then constructs a large pool of weak learners by pairing these prompts with different elements of the {LM}'s output distribution. These weak learners are then ensembled using the {AdaBoost} algorithm. The entire learning process requires only a small number of forward passes and no backward pass. Experiments show that {PromptBoosting} achieves state-of-the-art performance in multiple black-box few-shot classification tasks, and matches or outperforms full fine-tuning in both few-shot and standard learning paradigms, while training 10x faster than existing black-box methods.},
  author = {Bairu Hou and Joe O'Connor and Jacob Andreas and Shiyu Chang and Yang Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hou2023promptboosting.pdf:pdf},
  mdate = {2024-02-06},
  pages = {13309--13324},
  pdf = {https://proceedings.mlr.press/v202/hou23b/hou23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PromptBoosting}: Black-Box Text Classification with Ten Forward Passes},
  url = {https://proceedings.mlr.press/v202/hou23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hou2023sparse,
  abstract = {Transfer operators provide a rich framework for representing the dynamics of very general, nonlinear dynamical systems. When interacting with reproducing kernel Hilbert spaces ({RKHS}), descriptions of dynamics often incur prohibitive data storage requirements, motivating dataset sparsification as a precursory step to computation. Further, in practice, data is available in the form of trajectories, introducing correlation between samples. In this work, we present a method for sparse learning of transfer operators from $\beta$-mixing stochastic processes, in both discrete and continuous time, and provide sample complexity analysis extending existing theoretical guarantees for learning from non-sparse, i.i.d. data. In addressing continuous-time settings, we develop precise descriptions using covariance-type operators for the infinitesimal generator that aids in the sample complexity analysis. We empirically illustrate the efficacy of our sparse embedding approach through deterministic and stochastic nonlinear system examples.},
  author = {Boya Hou and Sina Sanjari and Nathan Dahlin and Subhonmesh Bose and Umesh Vaidya},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hou2023sparse.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13325--13352},
  pdf = {https://proceedings.mlr.press/v202/hou23c/hou23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sparse Learning of Dynamical Systems in {RKHS}: An Operator-Theoretic Approach},
  url = {https://proceedings.mlr.press/v202/hou23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hou2023probably,
  abstract = {Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {PASCombUCB} that minimizes the regret over the horizon of time $T$. The paper also develops information-theoretic lower bounds to show the algorithm's performance under both problem-dependent and problem-independent paradigms.},
  author = {Yunlong Hou and Vincent Y. F. Tan and Zixin Zhong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hou2023probably.pdf:pdf},
  mdate = {2023-09-29},
  pages = {13353--13409},
  pdf = {https://proceedings.mlr.press/v202/hou23d/hou23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits},
  url = {https://proceedings.mlr.press/v202/hou23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hounie2023automatic,
  abstract = {Underlying data structures, such as symmetries or invariances to transformations, are often exploited to improve the solution of learning tasks. However, embedding these properties in models or learning algorithms can be challenging and computationally intensive. Data augmentation, on the other hand, induces these symmetries during training by applying multiple transformations to the input data. Despite its ubiquity, its effectiveness depends on the choices of which transformations to apply, when to do so, and how often. In fact, there is both empirical and theoretical evidence that the indiscriminate use of data augmentation can introduce biases that outweigh its benefits. This work tackles these issues by automatically adapting the data augmentation while solving the learning task. To do so, it formulates data augmentation as an invariance-constrained learning problem and leverages Monte Carlo Markov Chain ({MCMC}) sampling to solve it. The result is a practical algorithm that not only does away with a priori searches for augmentation distributions, but also dynamically controls if and when data augmentation is applied. Our experiments illustrate the performance of this method, which achieves state-of-the-art results in automatic data augmentation benchmarks for {CIFAR} datasets. Furthermore, this approach can be used to gather insights on the actual symmetries underlying a learning task.},
  author = {Ignacio Hounie and Luiz F. O. Chamon and Alejandro Ribeiro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hounie2023automatic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13410--13433},
  pdf = {https://proceedings.mlr.press/v202/hounie23a/hounie23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Automatic Data Augmentation via Invariance-Constrained Learning},
  url = {https://proceedings.mlr.press/v202/hounie23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hsieh2023thompson,
  abstract = {In this work, we initiate the idea of using denoising diffusion models to learn priors for online decision making problems. We specifically focus on bandit meta-learning, aiming to learn a policy that performs well across bandit tasks of a same class. To this end, we train a diffusion model that learns the underlying task distribution and combine Thompson sampling with the learned prior to deal with new tasks at test time. Our posterior sampling algorithm carefully balances between the learned prior and the noisy observations that come from the learner's interaction with the environment. To capture realistic bandit scenarios, we propose a novel diffusion model training procedure that trains from incomplete and noisy data, which could be of independent interest.},
  author = {Yu-Guan Hsieh and Shiva Prasad Kasiviswanathan and Branislav Kveton and Patrick Bl{\"o}baum},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hsieh2023thompson.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13434--13468},
  pdf = {https://proceedings.mlr.press/v202/hsieh23a/hsieh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Thompson Sampling with Diffusion Generative Prior},
  url = {https://proceedings.mlr.press/v202/hsieh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023tighter,
  abstract = {In this paper, we provide a tighter analysis for {ProxSkip}, an algorithm that allows fewer proximal operator computations to solve composite optimization problems. We improve the existing decreasing speed of Lyapunov function from $\mathcal{O}(p^2)$ to $\mathcal{O}(p)$, when $p$, the frequency of the proximal operators is small enough. Our theoretical analysis also reveals the drawbacks of using large step sizes for gradient descent in {ProxSkip} when the proximal operator part is the bottleneck. Our main motivation comes from the continuous limit in which the original analysis of {ProxSkip} fails to guarantee convergence when both the step size $\gamma$ and frequency $p$ tend to zero. We construct a counterexample to demonstrate why such counterintuitive behavior occurs for the original analysis and then propose a novel Lyapunov function variant to construct a tighter analysis, avoiding the problem of the old one. Such a new Lyapunov function can be directly extended to many other variants of {ProxSkip}. When applied to stochastic gradient setup, our analysis leads to an improved proximal operator complexity for {SProxSkip} from $\mathcal{O}(\sqrt{\frac{1}{\varepsilon\mu^2}}\log(\frac{1}{\varepsilon}))$ to $\mathcal{O}(\sqrt{\kappa}\log(\frac{1}{\varepsilon}))$.},
  author = {Zhengmian Hu and Heng Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023tighter.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13469--13496},
  pdf = {https://proceedings.mlr.press/v202/hu23a/hu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tighter Analysis for {ProxSkip}},
  url = {https://proceedings.mlr.press/v202/hu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023gflownetem,
  abstract = {Latent variable models ({LVM}s) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization ({EM}), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of {GFlowNets}, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training {GFlowNets} to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, {GFlowNet-EM}, enables the training of expressive {LVM}s with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders ({VAE}s) without conditional independence enforced in the encoder.},
  author = {Edward J. Hu and Nikolay Malkin and Moksh Jain and Katie E. Everett and Alexandros Graikos and Yoshua Bengio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023gflownetem.pdf:pdf},
  mdate = {2023-08-28},
  pages = {13528--13549},
  pdf = {https://proceedings.mlr.press/v202/hu23c/hu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GFlowNet-EM} for Learning Compositional Latent Variable Models},
  url = {https://proceedings.mlr.press/v202/hu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023omnipredictors,
  abstract = {The notion of omnipredictors (Gopalan, Kalai, Reingold, Sharan and Wieder ITCS 2022), suggested a new paradigm for loss minimization. Rather than learning a predictor based on a known loss function, omnipredictors can easily be post-processed to minimize any one of a rich family of loss functions compared with the loss of hypotheses in a class $\mathcal{C}$. It has been shown that such omnipredictors exist and are implied (for all convex and Lipschitz loss functions) by the notion of multicalibration from the algorithmic fairness literature. In this paper, we introduce omnipredictors for constrained optimization and study their complexity and implications. The notion that we introduce allows the learner to be unaware of the loss function that will be later assigned \emph{as well as the constraints that will be later imposed}, as long as the subpopulations that are used to define these constraints are known. We show how to obtain omnipredictors for constrained optimization problems, relying on appropriate variants of multicalibration. We also investigate the implications of this notion when the constraints used are so-called group fairness notions.},
  author = {Lunjia Hu and Inbal Rachel Livni Navon and Omer Reingold and Chutong Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023omnipredictors.pdf:pdf},
  mdate = {2023-09-30},
  pages = {13497--13527},
  pdf = {https://proceedings.mlr.press/v202/hu23b/hu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Omnipredictors for Constrained Optimization},
  url = {https://proceedings.mlr.press/v202/hu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023blockwise,
  abstract = {The paper addresses non-convex multi-block bilevel optimization (MBBO) problems with multiple lower-level problems. The authors aim to develop an algorithm with three key properties: (1) Matching complexity of standard bilevel optimization, (2) Achieving parallel speedup, and (3) Avoiding high-dimensional Hessian matrix inverse computation. Key contributions include proposing two stochastic algorithms using blockwise variance-reduction techniques and proving iteration complexity for finding an ε-stationary point.},
  author = {Quanqi Hu and Zi-Hao Qiu and Zhishuai Guo and Lijun Zhang and Tianbao Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023blockwise.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  openreview = {https://openreview.net/forum?id=JeEnuF0zm1},
  pages = {13550--13583},
  pdf = {https://proceedings.mlr.press/v202/hu23d/hu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization},
  url = {https://proceedings.mlr.press/v202/hu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023language,
  abstract = {One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. The authors propose the "instructRL" framework that uses language models to generate policy based on human instructions and regularizes reinforcement learning objective, demonstrating improved human-AI coordination in Hanabi benchmark.},
  author = {Hengyuan Hu and Dorsa Sadigh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023language.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  openreview = {https://openreview.net/forum?id=CSAAs2QAyW},
  pages = {13584--13598},
  pdf = {https://proceedings.mlr.press/v202/hu23e/hu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Language Instructed Reinforcement Learning for Human-{AI} Coordination},
  url = {https://proceedings.mlr.press/v202/hu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023learning,
  abstract = {Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from pre-trained models without training data. The authors propose a "Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer meta knowledge from black-box APIs" to a single meta model. Key contributions include addressing limitations of existing DFML approaches, recovering training data via zero-order gradient estimator, designing a boundary query set recovery technique, and proposing task memory replay for better generalization.},
  author = {Zixuan Hu and Li Shen and Zhenyi Wang and Baoyuan Wu and Chun Yuan and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023learning.pdf:pdf},
  mdate = {2025-02-10},
  month = {7},
  openreview = {https://openreview.net/forum?id=r6p2HUDoG8},
  pages = {13610--13627},
  pdf = {https://proceedings.mlr.press/v202/hu23g/hu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Learn from {APIs}: Black-Box Data-Free Meta-Learning},
  url = {https://proceedings.mlr.press/v202/hu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023surface,
  abstract = {Reconstructing the 3D shape of objects observed in a single image is a challenging task. Recent approaches rely on visual cues extracted from a given image learned from a deep net. In this work, the authors leverage recent advances in monocular scene understanding to incorporate an additional geometric cue of surface normals. The paper proposes a novel optimization layer that aligns face normals with estimated surface normals, using a computationally efficient conjugate-gradient-based method.},
  author = {Yuan-Ting Hu and Alexander G. Schwing and Raymond A. Yeh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023surface.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  openreview = {https://openreview.net/forum?id=C8ijRC4ZvS},
  pages = {13599--13609},
  pdf = {https://proceedings.mlr.press/v202/hu23f/hu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Surface Snapping Optimization Layer for Single Image Object Shape Reconstruction},
  url = {https://proceedings.mlr.press/v202/hu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023beyond,
  abstract = {Negative and positive curvatures affect optimization in different ways. However, a lot of existing optimization theories are based on the Lipschitz smoothness assumption, which cannot differentiate between the two. In this paper, the authors propose to use two separate assumptions for positive and negative curvatures, so that they can study the different implications of the two. They analyze the Lookahead and Local SGD methods as concrete examples. Both of them require multiple copies of model parameters and communication among them for every certain period of time in order to prevent divergence. The authors show that the minimum communication frequency is inversely proportional to the negative curvature, and when the negative curvature becomes zero, they recover the existing theory results for convex optimization.},
  author = {Zhengmian Hu and Xidong Wu and Heng Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hu2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  openreview = {https://openreview.net/forum?id=ehfsxFKgNu},
  pages = {13652--13678},
  pdf = {https://proceedings.mlr.press/v202/hu23i/hu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond Lipschitz Smoothness: A Tighter Analysis for Nonconvex Optimization},
  url = {https://proceedings.mlr.press/v202/hu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023pretrained,
  abstract = {In recent years, increasing attention has been directed to leveraging pre-trained vision models for motor control. While existing works mainly emphasize the importance of this pre-training phase, the arguably equally important role played by downstream policy learning during control-specific fine-tuning is often neglected. The authors conduct a comprehensive study on 14 pre-trained vision models using 3 distinct classes of policy learning methods: reinforcement learning (RL), imitation learning through behavior cloning (BC), and imitation learning with a visual reward function (VRF). The study reveals that the effectiveness of pre-training depends on the downstream policy learning algorithm, with conventional RL-based evaluations being highly variable and unreliable. The authors advocate for using more robust methods like VRF and BC, and release a benchmark of 21 tasks across 3 different environments.},
  author = {Yingdong Hu and Renhao Wang and Li Erran Li and Yang Gao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  eprint = {2304.04591},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/hu2023pretrained.pdf:pdf},
  mdate = {2025-01-02},
  month = {7},
  pages = {13628--13651},
  pdf = {https://proceedings.mlr.press/v202/hu23h/hu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal},
  url = {https://proceedings.mlr.press/v202/hu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hu2023understanding,
  abstract = {While it has long been empirically observed that adversarial robustness may be at odds with standard accuracy and may have further disparate impacts on different classes, it remains an open question to what extent such observations hold and how the class imbalance plays a role within. In this paper, the authors attempt to understand this question of accuracy disparity by taking a closer look at linear classifiers under a Gaussian mixture model. They decompose the impact of adversarial robustness into two parts: an inherent effect that will degrade the standard accuracy on all classes due to the robustness constraint, and the other caused by the class imbalance ratio, which will increase the accuracy disparity compared to standard training. The research extends beyond the Gaussian mixture model by generalizing to the general family of stable distributions, showing that while the constraint of adversarial robustness consistently degrades the standard accuracy in the balanced class setting, the class imbalance ratio plays a fundamentally different role in accuracy disparity compared to the Gaussian case, due to the heavy tail of the stable distribution.},
  author = {Yuzheng Hu and Fan Wu and Hongyang Zhang and Han Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  eprint = {2211.15762},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/hu2023understanding.pdf:pdf},
  mdate = {2024-03-26},
  month = {7},
  pages = {13679--13709},
  pdf = {https://proceedings.mlr.press/v202/hu23j/hu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding the Impact of Adversarial Robustness on Accuracy Disparity},
  url = {https://proceedings.mlr.press/v202/hu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023theoretical,
  abstract = {The authors study a new connection between a technical measure called μ-conductance that arises in the study of Markov chains for sampling convex bodies and the network community profile that characterizes size-resolved properties of clusters and communities in social and information networks. The idea of μ-conductance is similar to the traditional graph conductance, but disregards sets with small volume. They derive a sequence of optimization problems including a low-rank semi-definite program from which they can derive a lower bound on the optimal μ-conductance value. These ideas give the first theoretically sound bound on the behavior of the network community profile for a wide range of cluster sizes, with an algorithm that scales up to graphs with hundreds of thousands of nodes.},
  author = {Yufan Huang and C. Seshadhri and David F. Gleich},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  eprint = {2303.14550},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/huang2023theoretical.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13976--13992},
  pdf = {https://proceedings.mlr.press/v202/huang23l/huang23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Theoretical Bounds on the Network Community Profile from Low-rank Semi-definite Programming},
  url = {https://proceedings.mlr.press/v202/huang23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023banker,
  abstract = {The authors propose Banker Online Mirror Descent (Banker-OMD), a novel framework generalizing the classical Online Mirror Descent (OMD) technique in the online learning literature. The Banker-OMD framework almost completely decouples feedback delay handling and the task-specific OMD algorithm design, thus facilitating the design of new algorithms capable of efficiently and robustly handling feedback delays. Specifically, it offers a general methodology for achieving regret bounds in online bandit learning tasks with delayed feedback, where T is the number of rounds and D is the total feedback delay. The authors demonstrate the power of Banker-OMD by applications to two important bandit learning scenarios with delayed feedback, including delayed scale-free adversarial Multi-Armed Bandits (MAB) and delayed adversarial linear bandits.},
  author = {Jiatai Huang and Yan Dai and Longbo Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  eprint = {2301.10500},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/huang2023banker.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  openreview = {https://openreview.net/forum?id=n2bG2TExOL},
  pages = {13814--13844},
  pdf = {https://proceedings.mlr.press/v202/huang23e/huang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Banker Online Mirror Descent: A Universal Approach for Delayed Online Bandit Learning},
  url = {https://proceedings.mlr.press/v202/huang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023large,
  abstract = {This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. The authors carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. They obtain the best-ever pure ConvNet under 30M parameters with 83.1% top-1 accuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2 and Swin V2. They also find that beneficial characteristics of large-kernel ConvNets, e.g., larger effective receptive fields, can be seamlessly transferred to students through this large-to-small kernel distillation.},
  author = {Tianjin Huang and Lu Yin and Zhenyu Zhang and Li Shen and Meng Fang and Mykola Pechenizkiy and Zhangyang Wang and Shiwei Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  eprint = {2305.19412},
  eprinttype = {arXiv},
  file = {:/home/b/documents/inproceedings/huang2023large.pdf:pdf},
  mdate = {2024-08-02},
  month = {7},
  pages = {14023--14038},
  pdf = {https://proceedings.mlr.press/v202/huang23o/huang23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Large Kernels Better Teachers than Transformers for {ConvNets}?},
  url = {https://proceedings.mlr.press/v202/huang23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023reinforcement,
  abstract = {MDPs with low-rank transitions---that is, the transition matrix can be factored into the product of two matrices, left and right---is a highly representative structure that enables tractable learning. The left matrix enables expressive function approximation for value-based learning and has been studied extensively. In this work, we instead investigate sample-efficient learning with density features, i.e., the right matrix, which induce powerful models for state-occupancy distributions. This setting not only sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for settings like convex RL. In the offline setting, we propose an algorithm for off-policy estimation of occupancies that can handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that constructs exploratory data distributions in a level-by-level manner. As a central technical challenge, the additive error of occupancy estimation is incompatible with the multiplicative definition of data coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend to the representation learning setting, when the density features are unknown and must be learned from an exponentially large candidate set.},
  author = {Audrey Huang and Jinglin Chen and Nan Jiang 0008},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13710--13752},
  pdf = {https://proceedings.mlr.press/v202/huang23a/huang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reinforcement Learning in Low-rank {MDP}s with Density Features},
  url = {https://proceedings.mlr.press/v202/huang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023composer,
  abstract = {Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available.},
  author = {Lianghua Huang and Di Chen and Yu Liu 0063 and Yujun Shen and Deli Zhao and Jingren Zhou 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023composer.pdf:pdf},
  mdate = {2025-03-19},
  month = {7},
  pages = {13753--13773},
  pdf = {https://proceedings.mlr.press/v202/huang23b/huang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Composer: Creative and Controllable Image Synthesis with Composable Conditions},
  url = {https://proceedings.mlr.press/v202/huang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023highdimensional,
  abstract = {Clustering aims to group unlabelled samples based on their similarities. It has become a significant tool for the analysis of high-dimensional data. However, most of the clustering methods merely generate pseudo labels and thus are unable to simultaneously present the similarities between different clusters and outliers. This paper proposes a new framework called High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above problems. First, HCHC combines global structure with local structure in one objective function for deep clustering, improving the labels as relative probabilities, to mine the similarities between different clusters while keeping the local structure in each cluster. Then, the anchors of different clusters are sorted on the optimal Hamiltonian cycle generated by the cluster similarities and mapped on the circumference of a circle. Finally, a sample with a higher probability of a cluster will be mapped closer to the corresponding anchor. In this way, our framework allows us to appreciate three aspects visually and simultaneously -- clusters (formed by samples with high probabilities), cluster similarities (represented as circular distances), and outliers (recognized as dots far away from all clusters). The experiments illustrate the superiority of HCHC.},
  author = {Tianyi Huang and Shenghui Cheng and Stan Z. Li and Zhengjun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023highdimensional.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13791--13813},
  pdf = {https://proceedings.mlr.press/v202/huang23d/huang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {High-dimensional Clustering onto {H}amiltonian Cycle},
  url = {https://proceedings.mlr.press/v202/huang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023modelaware,
  abstract = {Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE-based methods suffer from some dilemmas, such as uniformity-tolerance dilemma (UTD) and gradient reduction, both of which are related to a $\mathcal{P}_{ij}$ term. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effective with fewer negative samples, and summarily present a gradient reweighting to escape this dilemma. Extensive remarkable empirical results in vision, sentence, and graph modality validate our approach's general improvement for representation learning and downstream tasks.},
  author = {Zizheng Huang and Haoxing Chen and Ziqi Wen and Chao Zhang 0078 and Huaxiong Li and Bo Wang 0027 and Chunlin Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023modelaware.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13774--13790},
  pdf = {https://proceedings.mlr.press/v202/huang23c/huang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-Aware Contrastive Learning: Towards Escaping the Dilemmas},
  url = {https://proceedings.mlr.press/v202/huang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023fast,
  abstract = {In this paper, we study the k-clustering problems with outliers in distributed setting. The current best results for the distributed k-center problem with outliers have quadratic local running time with communication cost dependent on the aspect ratio of the given instance, which may constraint the scalability of the algorithms for handling large-scale datasets. To achieve better communication cost for the problem with faster local running time, we propose an inliers-recalling sampling method, which avoids guessing the optimal radius of the given instance, and can achieve a 4-round bi-criteria approximation with linear local running time in the data size and communication cost independent of the aspect ratio. We also propose a space-narrowing sampling method that automatically adjusts sample size to adapt to different outliers distributions and achieves a 2-round bi-criteria approximation with communication cost independent of the number of outliers. Furthermore, we extend our algorithms to k-median and k-means problems with outliers, achieving good approximation ratios with communication cost independent of the number of outliers when data points are randomly partitioned across machines.},
  author = {Junyu Huang and Qilong Feng and Ziyun Huang and Jinhui Xu 0001 and Jianxin Wang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023fast.pdf:pdf},
  mdate = {2024-05-07},
  month = {7},
  pages = {13845--13868},
  pdf = {https://proceedings.mlr.press/v202/huang23f/huang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Algorithms for Distributed k-Clustering with Outliers},
  url = {https://proceedings.mlr.press/v202/huang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023searching,
  abstract = {Integer Linear Programs (ILPs) are powerful tools for modeling and solving a large number of combinatorial optimization problems. Recently, it has been shown that Large Neighborhood Search (LNS), as a heuristic algorithm, can find high quality solutions to ILPs faster than Branch and Bound. However, how to find the right heuristics to maximize the performance of LNS remains an open problem. In this paper, we propose a novel approach, CL-LNS, that delivers state-of-the-art anytime performance on several ILP benchmarks measured by metrics including the primal gap, the primal integral, survival rates and the best performing rate. Specifically, CL-LNS collects positive and negative solution samples from an expert heuristic that is slow to compute and learns a new one with a contrastive loss. We use graph attention networks and a richer set of features to further improve its performance.},
  author = {Taoan Huang and Aaron M. Ferber and Yuandong Tian and Bistra Dilkina and Benoit Steiner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023searching.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13869--13890},
  pdf = {https://proceedings.mlr.press/v202/huang23g/huang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Searching Large Neighborhoods for Integer Linear Programs with Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/huang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023coresets,
  abstract = {This paper considers constructing small coresets for k-Median in Euclidean spaces, where given a large set of data points P ⊂ ℝᵈ, a coreset is a much smaller set S ⊂ ℝᵈ, so that the k-Median costs of any k centers w.r.t. P and S are close. Existing literature mainly focuses on the high-dimension case and there has been great success in obtaining dimension-independent bounds, whereas the case for small d is largely unexplored. Considering many applications of Euclidean clustering algorithms are in small dimensions and the lack of systematic studies in the current literature, this paper investigates coresets for k-Median in small dimensions. For small d, a natural question is whether existing near-optimal dimension-independent bounds can be significantly improved. We provide affirmative answers to this question for a range of parameters. We also provide new lower bound results, which are the highest for small d. In particular, we completely settle the coreset size bound for 1-d k-Median (up to log factors). Interestingly, our results imply a strong separation between 1-d 1-Median and 1-d 2-Median. As far as we know, this is the first such separation between k=1 and k=2 in any dimension.},
  author = {Lingxiao Huang and Ruiyuan Huang and Zengfeng Huang and Xuan Wu 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023coresets.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13891--13915},
  pdf = {https://proceedings.mlr.press/v202/huang23h/huang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Coresets for Clustering in Small Dimensional {E}uclidean Spaces},
  url = {https://proceedings.mlr.press/v202/huang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023makeanaudio,
  abstract = {Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.},
  author = {Rongjie Huang 0001 and Jiawei Huang 0008 and Dongchao Yang and Yi Ren 0006 and Luping Liu and Mingze Li and Zhenhui Ye and Jinglin Liu and Xiang Yin 0006 and Zhou Zhao 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023makeanaudio.pdf:pdf},
  mdate = {2025-02-25},
  month = {7},
  pages = {13916--13932},
  pdf = {https://proceedings.mlr.press/v202/huang23i/huang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models},
  url = {https://proceedings.mlr.press/v202/huang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023power,
  abstract = {We study the power of uniform sampling for k-Median in various metric spaces. We relate the query complexity for approximating k-Median, to a key parameter of the dataset, called the balancedness β ∈ (0, 1] (with 1 being perfectly balanced). We show that any algorithm must make Ω(1/β) queries to the point set in order to achieve O(1)-approximation for k-Median. This particularly implies existing constructions of coresets, a popular data reduction technique, cannot be query-efficient. On the other hand, we show a simple uniform sample of poly(k ε⁻¹ β⁻¹) points suffices for (1 + ε)-approximation for k-Median for various metric spaces, which nearly matches the lower bound. Experiments verify that in many real datasets, the balancedness parameter is usually well bounded, and that the uniform sampling performs consistently well even for the case with moderately large balancedness, which justifies that uniform sampling is indeed a viable approach for solving k-Median.},
  author = {Lingxiao Huang and Shaofeng H.-C. Jiang and Jianing Lou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023power.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13933--13956},
  pdf = {https://proceedings.mlr.press/v202/huang23j/huang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Power of Uniform Sampling for k-Median},
  url = {https://proceedings.mlr.press/v202/huang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023reparameterized,
  abstract = {We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. We demonstrate the effectiveness of RPG on a suite of multimodal continuous control tasks, showing that RPG outperforms existing methods in terms of sample efficiency and final performance.},
  author = {Zhiao Huang and Litian Liang and Zhan Ling and Xuanlin Li and Chuang Gan and Hao Su 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023reparameterized.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {13957--13975},
  pdf = {https://proceedings.mlr.press/v202/huang23k/huang23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reparameterized Policy Learning for Multimodal Trajectory Optimization},
  url = {https://proceedings.mlr.press/v202/huang23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023neuralstagger,
  abstract = {Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct the original solution. Due to the perfect parallelism between them, the solution is achieved as fast as a coarse-resolution neural solver. In addition, the trained solvers bring the flexibility of simulating with multiple levels of resolution. We demonstrate the successful application of NeuralStagger on 2D and 3D fluid dynamics simulations, which leads to an additional $10\sim100\times$ speed-up. Moreover, the experiment also shows that the learned model could be well used for optimal control.},
  author = {Xinquan Huang and Wenlei Shi and Qi Meng and Yue Wang 0017 and Xiaotian Gao and Jia Zhang 0004 and Tie-Yan Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023neuralstagger.pdf:pdf},
  mdate = {2024-10-01},
  pages = {13993--14006},
  pdf = {https://proceedings.mlr.press/v202/huang23m/huang23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {NeuralStagger: Accelerating Physics-constrained Neural {PDE} Solver with Spatial-temporal Decomposition},
  url = {https://proceedings.mlr.press/v202/huang23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023policy,
  abstract = {Adversarial imitation learning (AIL) is a popular method that has recently achieved much success. However, the performance of AIL is still unsatisfactory on the more challenging tasks. We find that one of the major reasons is due to the low quality of AIL discriminator representation. Since the AIL discriminator is trained via binary classification that does not necessarily discriminate the policy from the expert in a meaningful way, the resulting reward might not be meaningful either. We propose a new method called Policy Contrastive Imitation Learning (PCIL) to resolve this issue. PCIL learns a contrastive representation space by anchoring on different policies and uses a smooth cosine-similarity-based reward to encourage imitation learning. Our proposed representation learning objective can be viewed as a stronger version of the AIL objective and provide a more meaningful comparison between the agent and the policy. From a theoretical perspective, we show the validity of our method using the apprenticeship learning framework. Furthermore, our empirical evaluation on the DeepMind Control suite demonstrates that PCIL can achieve state-of-the-art performance. Finally, qualitative results suggest that PCIL builds a smoother and more meaningful representation space for imitation learning.},
  author = {Jialei Huang and Zhao-Heng Yin and Yingdong Hu and Yang Gao 0029},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023policy.pdf:pdf},
  mdate = {2025-01-02},
  pages = {14007--14022},
  pdf = {https://proceedings.mlr.press/v202/huang23n/huang23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Policy Contrastive Imitation Learning},
  url = {https://proceedings.mlr.press/v202/huang23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023achieving,
  abstract = {Federated bilevel learning has received increasing attention in various emerging machine learning and communication applications. Recently, several Hessian-vector-based algorithms have been proposed to solve the federated bilevel optimization problem. However, several important properties in federated learning such as the partial client participation and the linear speedup for convergence (i.e., the convergence rate and complexity are improved linearly with respect to the number of sampled clients) in the presence of non-i.i.d. datasets, still remain open. In this paper, we fill these gaps by proposing a new federated bilevel algorithm named FedMBO with a novel client sampling scheme in the federated hypergradient estimation. We show that FedMBO achieves a convergence rate of $\mathcal{O}\big(\frac{1}{\sqrt{nK}}+\frac{1}{K}+\frac{\sqrt{n}}{K^{3/2}}\big)$ on non-i.i.d. datasets, where $n$ is the number of participating clients in each round, and $K$ is the total number of iteration. This is the first theoretical linear speedup result for non-i.i.d. federated bilevel optimization. Extensive experiments validate our theoretical results and demonstrate the effectiveness of our proposed method.},
  author = {Minhui Huang and Dewei Zhang and Kaiyi Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023achieving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14039--14059},
  pdf = {https://proceedings.mlr.press/v202/huang23p/huang23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Achieving Linear Speedup in Non-{IID} Federated Bilevel Learning},
  url = {https://proceedings.mlr.press/v202/huang23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huang2023federated,
  abstract = {This paper studies federated linear contextual bandits under the notion of user-level differential privacy (DP). We first introduce a unified federated bandits framework that can accommodate various definitions of DP in the sequential decision-making setting. We then formally introduce user-level central DP (CDP) and local DP (LDP) in the federated bandits framework, and investigate the fundamental trade-offs between the learning regrets and the corresponding DP guarantees in a federated linear contextual bandits model. For CDP, we propose a federated algorithm termed as $\texttt{ROBIN}$ and show that it is near-optimal in terms of the number of clients $M$ and the privacy budget $\varepsilon$ by deriving nearly-matching upper and lower regret bounds when user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret blow-up factor at least $\min\{1/\varepsilon,M\}$ or $\min\{1/\sqrt{\varepsilon},\sqrt{M}\}$ under different conditions.},
  author = {Ruiquan Huang and Huanyu Zhang and Luca Melis and Milan Shen and Meisam Hejazinia and Jing Yang 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huang2023federated.pdf:pdf},
  mdate = {2023-11-02},
  pages = {14060--14095},
  pdf = {https://proceedings.mlr.press/v202/huang23q/huang23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Federated Linear Contextual Bandits with User-level Differential Privacy},
  url = {https://proceedings.mlr.press/v202/huang23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huh2023straightening,
  abstract = {This work examines the challenges of training neural networks using vector quantization using straight-through estimation. We find that the main cause of training instability is the discrepancy between the model embedding and the code-vector distribution. We identify the factors that contribute to this issue, including the codebook gradient sparsity and the asymmetric nature of the commitment loss, which leads to misaligned code-vector assignments. We propose to address this issue via affine re-parameterization of the code vectors. Additionally, we introduce an alternating optimization to reduce the gradient error introduced by the straight-through estimation. Moreover, we propose an improvement to the commitment loss to ensure better alignment between the codebook representation and the model embedding. These optimization methods improve the mathematical approximation of the straight-through estimation and, ultimately, the model performance. We demonstrate the effectiveness of our methods on several common model architectures, such as AlexNet, ResNet, and ViT, across various tasks, including image classification and generative modeling.},
  author = {Minyoung Huh and Brian Cheung and Pulkit Agrawal 0001 and Phillip Isola},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huh2023straightening.pdf:pdf},
  mdate = {2024-06-02},
  pages = {14096--14113},
  pdf = {https://proceedings.mlr.press/v202/huh23a/huh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks},
  url = {https://proceedings.mlr.press/v202/huh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hui2023cut,
  abstract = {Nearly all practical neural models for classification are trained using the cross-entropy loss. Yet this ubiquitous choice is supported by little theoretical or empirical evidence. Recent work (Hui \& Belkin, 2020) suggests that training using the (rescaled) square loss is often superior in terms of the classification accuracy. In this paper we propose the ``squentropy'' loss, which is the sum of two terms: the cross-entropy loss and the average square loss over the incorrect classes. We provide an extensive set of experiment on multi-class classification problems showing that the squentropy loss outperforms both the pure cross-entropy and rescaled square losses in terms of the classification accuracy. We also demonstrate that it provides significantly better model calibration than either of these alternative losses and, furthermore, has less variance with respect to the random initialization. Additionally, in contrast to the square loss, squentropy loss can frequently be trained using exactly the same optimization parameters, including the learning rate, as the standard cross-entropy loss, making it a true ``plug-and-play'' replacement. Finally, unlike the rescaled square loss, multiclass squentropy contains no parameters that need to be adjusted.},
  author = {Like Hui and Mikhail Belkin and Stephen Wright},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hui2023cut.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14114--14131},
  pdf = {https://proceedings.mlr.press/v202/hui23a/hui23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cut your Losses with Squentropy},
  url = {https://proceedings.mlr.press/v202/hui23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{huijben2023somcpc,
  abstract = {Continuous monitoring with an ever-increasing number of sensors has become ubiquitous across many application domains. Acquired data are typically high-dimensional and difficult to interpret, but they are also hypothesized to lie on a lower-dimensional manifold. Many deep learning (DL) models aim to identify this manifold, but do not promote structure nor interpretability. We propose the SOM-CPC model, which jointly optimizes Contrastive Predictive Coding (CPC), and a Self-Organizing Map (SOM) to find such an organized manifold. We address a largely unexplored and challenging set of scenarios comprising high-rate time series, and show on synthetic and real-life medical and audio data that SOM-CPC outperforms strong baseline models that combine DL with SOMs. SOM-CPC has great potential to expose latent patterns in high-rate data streams, and may therefore contribute to a better understanding of many different processes and systems.},
  author = {Iris A. M. Huijben and Arthur Andreas Nijdam and Sebastiaan Overeem and Merel M. van Gilst and Ruud van Sloun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/huijben2023somcpc.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14132--14152},
  pdf = {https://proceedings.mlr.press/v202/huijben23a/huijben23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SOM-CPC}: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series},
  url = {https://proceedings.mlr.press/v202/huijben23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{humbert2023oneshot,
  abstract = {In this paper, we introduce a conformal prediction method to construct prediction sets in a one-shot federated learning setting. More specifically, we define a quantile-of-quantiles estimator and prove that for any distribution, it is possible to output prediction sets with desired coverage in only one round of communication. To mitigate privacy issues, we also describe a locally differentially private version of our estimator. Finally, over a wide range of experiments, we show that our method returns prediction sets with coverage and length very similar to those obtained in a centralized setting.},
  author = {Pierre Humbert and Batiste Le Bars and Aurélien Bellet and Sylvain Arlot},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/humbert2023oneshot.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14153--14177},
  pdf = {https://proceedings.mlr.press/v202/humbert23a/humbert23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-Shot Federated Conformal Prediction},
  url = {https://proceedings.mlr.press/v202/humbert23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hussain2023impact,
  abstract = {Understanding the impact of exploration on the behaviour of multi-agent learning has, so far, benefited from the restriction to potential, or network zero-sum games in which convergence to an equilibrium can be shown. Outside of these classes, learning dynamics rarely converge and little is known about the effect of exploration in the face of non-convergence. To progress this front, we study the smooth Q-Learning dynamics. We show that, in any network game, exploration by agents results in the convergence of Q-Learning to a neighbourhood of an equilibrium. This holds independently of whether the dynamics reach the equilibrium or display complex behaviours. We show that increasing the exploration rate decreases the size of this neighbourhood and also decreases the ability of all agents to improve their payoffs. Furthermore, in a broad class of games, the payoff performance of Q-Learning dynamics, measured by Social Welfare, decreases when the exploration rate increases. Our experiments show this to be a general phenomenon, namely that exploration leads to improved convergence of Q-Learning, at the cost of payoff performance.},
  author = {Aamal Abbas Hussain and Francesco Belardinelli and Dario Paccagnan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hussain2023impact.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14178--14202},
  pdf = {https://proceedings.mlr.press/v202/hussain23a/hussain23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Impact of Exploration on Convergence and Performance of Multi-Agent {Q}-Learning Dynamics},
  url = {https://proceedings.mlr.press/v202/hussain23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hwang2023maganet,
  abstract = {Combinatorial generalization refers to the ability to collect and assemble various attributes from diverse data to generate novel unexperienced data. This ability is considered a necessary passing point for achieving human-level intelligence. To achieve this ability, previous unsupervised approaches mainly focused on learning the disentangled representation, such as the variational autoencoder. However, recent studies discovered that the disentangled representation is insufficient for combinatorial generalization and is not even correlated. In this regard, we propose a novel framework for data generation that can robustly generalize under these distribution shift situations. Instead of representing each data, our model discovers the fundamental transformation between a pair of data by simulating a group action. To test the combinatorial generalizability, we evaluated our model in two settings: Recombination-to-Element and Recombination-to-Range. The experiments demonstrated that our method has quantitatively and qualitatively superior generalizability and generates better images than traditional models.},
  author = {Geonho Hwang and Jaewoong Choi and Hyunsoo Cho and Myungjoo Kang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hwang2023maganet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14237--14248},
  pdf = {https://proceedings.mlr.press/v202/hwang23b/hwang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MAGANet}: Achieving Combinatorial Generalization by Modeling a Group Action},
  url = {https://proceedings.mlr.press/v202/hwang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hwang2023combinatorial,
  abstract = {We consider a contextual combinatorial bandit problem where in each round a learning agent selects a subset of arms and receives feedback on the selected arms according to their scores. The score of an arm is an unknown function of the arm's feature. Approximating this unknown score function with deep neural networks, we propose algorithms: Combinatorial Neural UCB (CN-UCB) and Combinatorial Neural Thompson Sampling (CN-TS). We prove that CN-UCB achieves $\tilde{O}(\tilde{d}\sqrt{T})$ or $\tilde{O}(\sqrt{\tilde{d}TK})$ regret, where $\tilde{d}$ is the effective dimension of a neural tangent kernel matrix, $K$ is the size of a subset of arms, and $T$ is the time horizon. For CN-TS, we adapt an optimistic sampling technique to ensure optimism of the sampled combinatorial action, achieving a worst-case (frequentist) regret of $\tilde{O}(\tilde{d}\sqrt{TK})$. The proposed algorithms are the first combinatorial neural bandit algorithms with regret performance guarantees. In particular, CN-TS is the first Thompson sampling algorithm with worst-case regret guarantees for the general contextual combinatorial bandit problem.},
  author = {Taehyun Hwang and Kyuwook Chai and Min-Hwan Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hwang2023combinatorial.pdf:pdf},
  mdate = {2024-09-30},
  month = {7},
  pages = {14203--14236},
  pdf = {https://proceedings.mlr.press/v202/hwang23a/hwang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Combinatorial Neural Bandits},
  url = {https://proceedings.mlr.press/v202/hwang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{hwang2023informationtheoretic,
  abstract = {Multi-View Reinforcement Learning (MVRL) seeks to find an optimal control for an agent given multi-view observations from various sources. Despite recent advances in multi-view learning that aim to extract the latent representation from multi-view data, it is not straightforward to apply them to control tasks, especially when the observations are temporally dependent on one another. The problem can be even more challenging if the observations are intermittently missing for a subset of views. In this paper, we introduce Fuse2Control (F2C), an information-theoretic approach to capturing the underlying state space model from the sequences of multi-view observations. We conduct an extensive set of experiments in various control tasks showing that our method is highly effective in aggregating task-relevant information across many views, that scales linearly with the number of views while retaining robustness to arbitrary missing view scenarios.},
  author = {Hyeongjoo Hwang and Seokin Seo and Youngsoo Jang and Sungyoon Kim and Geon-Hyeong Kim and Seunghoon Hong and Kee-Eung Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/hwang2023informationtheoretic.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14249--14282},
  pdf = {https://proceedings.mlr.press/v202/hwang23c/hwang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Information-Theoretic State Space Model for Multi-View Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/hwang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ibrahim2023undercounted,
  abstract = {Systematic under-counting effects are observed ubiquitously in data collected across many disciplines, such as epidemiology and ecology. Under-counted tensor completion (UC-TC) is motivated for data analytics tasks like inferring case numbers of infectious diseases at unobserved locations from under-counted case numbers in neighboring regions. This work proposes a low-rank Poisson tensor model with an expressive unknown nonlinear side information extractor for under-counted multi-aspect data. A joint low-rank tensor completion and neural network learning algorithm is designed to recover the model, with theoretical guarantees for both tensor recovery and neural network learning.},
  author = {Shahana Ibrahim and Xiao Fu and Rebecca A. Hutchinson and Eugene Seo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ibrahim2023undercounted.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14283--14315},
  pdf = {https://proceedings.mlr.press/v202/ibrahim23a/ibrahim23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Under-Counted Tensor Completion with Neural Incorporation of Attributes},
  url = {https://proceedings.mlr.press/v202/ibrahim23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{immer2023stochastic,
  abstract = {Selecting hyperparameters in deep learning greatly impacts effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that our estimators can significantly accelerate gradient-based hyperparameter optimization.},
  author = {Alexander Immer and Tycho F. A. van der Ouderaa and Mark van der Wilk and Gunnar R{\"a}tsch and Bernhard Sch{\"o}lkopf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/immer2023stochastic.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14333--14352},
  pdf = {https://proceedings.mlr.press/v202/immer23b/immer23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels},
  url = {https://proceedings.mlr.press/v202/immer23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{immer2023identifiability,
  abstract = {We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task.},
  author = {Alexander Immer and Christoph Schultheiss and Julia E. Vogt and Bernhard Sch{\"o}lkopf and Peter B{\"u}hlmann and Alexander Marx},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/immer2023identifiability.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14316--14332},
  pdf = {https://proceedings.mlr.press/v202/immer23a/immer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Identifiability and Estimation of Causal Location-Scale Noise Models},
  url = {https://proceedings.mlr.press/v202/immer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{imola2023differentially,
  abstract = {Hierarchical clustering is a popular unsupervised machine learning method with decades of history and numerous applications. We initiate the study of differentially-private approximation algorithms for hierarchical clustering under the rigorous framework introduced by Dasgupta (2016). We show strong lower bounds for the problem: that any $\varepsilon$-DP algorithm must exhibit $O(|V|^2/\varepsilon)$-additive error for an input dataset $V$. Then, we exhibit a polynomial-time approximation algorithm with $O(|V|^{2.5}/\varepsilon)$-additive error, and an exponential-time algorithm that meets the lower bound. To overcome the lower bound, we focus on the stochastic block model. We design a private $1+o(1)$ approximation algorithm which also recovers the blocks exactly. Finally, we perform an empirical evaluation of our techniques and show that they can achieve good performance on realistic datasets.},
  author = {Jacob Imola and Alessandro Epasto and Mohammad Mahdian and Vincent Cohen-Addad and Vahab Mirrokni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/imola2023differentially.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14353--14375},
  pdf = {https://proceedings.mlr.press/v202/imola23a/imola23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Hierarchical Clustering with Provable Approximation Guarantees},
  url = {https://proceedings.mlr.press/v202/imola23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{irwin2023neural,
  abstract = {In this paper, we introduce neural network accelerated implicit filtering (NNAIF), a novel family of methods for solving noisy derivative free (i.e. black box, zeroth order) optimization problems. NNAIF intelligently combines the established literature on implicit filtering (IF) optimization methods with a neural network (NN) surrogate model of the objective function, resulting in accelerated derivative free methods for unconstrained optimization problems. The NN surrogate model consists of a fixed number of parameters, which can be as few as $\approx 1.3 \times 10^{4}$, that are updated as NNAIF progresses. We show that NNAIF directly inherits the convergence properties of IF optimization methods, and thus NNAIF is guaranteed to converge towards a critical point of the objective function under appropriate assumptions. Numerical experiments with $31$ noisy problems from the CUTEst optimization benchmark set demonstrate the benefits and costs associated with NNAIF.},
  author = {Brian Irwin and Eldad Haber and Raviv Gal and Avi Ziv},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/irwin2023neural.pdf:pdf},
  mdate = {2023-11-12},
  month = {7},
  pages = {14376--14389},
  pdf = {https://proceedings.mlr.press/v202/irwin23a/irwin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Network Accelerated Implicit Filtering: Integrating Neural Network Surrogates With Provably Convergent Derivative Free Optimization Methods},
  url = {https://proceedings.mlr.press/v202/irwin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{islam2023principled,
  abstract = {Learning to control an agent from offline data collected in a rich pixel-based visual observation space is vital for real-world applications of reinforcement learning (RL). A major challenge in this setting is the presence of input information that is hard to model and irrelevant to controlling the agent. This problem has been approached by the theoretical RL community through the lens of ``exogenous information'', i.e., any control-irrelevant information contained in observations. For example, a robot navigating in busy streets needs to ignore irrelevant information, such as other people walking in the background, textures of objects, or birds in the sky. In this paper, we focus on the setting with visually detailed exogenous information, and introduce new offline RL benchmarks that offer the ability to study this problem. We find that contemporary representation learning techniques can fail on datasets where the noise is a complex, time-dependent process -- which is prevalent in practical applications.},
  author = {Riashat Islam and Manan Tomar and Alex Lamb and Yonathan Efroni and Hongyu Zang and Aniket Rajiv Didolkar and Dipendra Misra and Xin Li and Harm van Seijen and Remi Tachet des Combes and John Langford},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/islam2023principled.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14390--14421},
  pdf = {https://proceedings.mlr.press/v202/islam23a/islam23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Principled Offline RL in the Presence of Rich Exogenous Information},
  url = {https://proceedings.mlr.press/v202/islam23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{issenhuth2023unveiling,
  abstract = {Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improves the performance of GANs.},
  author = {Thibaut Issenhuth and Ugo Tanielian and J{\'e}r{\'e}mie Mary and David Picard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/issenhuth2023unveiling.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {14422--14444},
  pdf = {https://proceedings.mlr.press/v202/issenhuth23a/issenhuth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unveiling the Latent Space Geometry of Push-Forward Generative Models},
  url = {https://proceedings.mlr.press/v202/issenhuth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ivanova2023cobed,
  abstract = {We formalize the problem of contextual optimization through the lens of Bayesian experimental design and propose CO-BED---a general, model-agnostic framework for designing contextual experiments using information-theoretic principles. After formulating a suitable information-based objective, we employ black-box variational methods to simultaneously estimate it and optimize the designs in a single stochastic gradient scheme. In addition, to accommodate discrete actions within our framework, we propose leveraging continuous relaxation schemes, which can naturally be integrated into our variational objective. As a result, CO-BED provides a general and automated solution to a wide range of contextual optimization problems. We illustrate its effectiveness in a number of experiments, where CO-BED demonstrates competitive performance even when compared to bespoke, model-specific alternatives.},
  author = {Desi R. Ivanova and Joel Jennings and Tom Rainforth and Cheng Zhang and Adam Foster},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ivanova2023cobed.pdf:pdf},
  mdate = {2024-02-09},
  month = {7},
  pages = {14445--14464},
  pdf = {https://proceedings.mlr.press/v202/ivanova23a/ivanova23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CO-BED}: Information-Theoretic Contextual Optimization via {Bayesian} Experimental Design},
  url = {https://proceedings.mlr.press/v202/ivanova23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ivgi2023dog,
  abstract = {We propose a tuning-free dynamic SGD step size formula, which we call Distance over Gradients (DoG). The DoG step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no "learning rate" parameter. Theoretically, we show that, for stochastic convex optimization, a slight variation of the DoG formula enjoys strong, high-probability parameter-free convergence guarantees and iterate movement bounds. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that DoG's performance is close to that of SGD with tuned learning rate. We also propose a per-layer variant of DoG that generally outperforms tuned SGD, approaching the performance of tuned Adam. A PyTorch implementation of our algorithms is available at https://github.com/formll/dog.},
  author = {Maor Ivgi and Oliver Hinder and Yair Carmon},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ivgi2023dog.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14465--14499},
  pdf = {https://proceedings.mlr.press/v202/ivgi23a/ivgi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DoG is {SGD}'s Best Friend: A Parameter-Free Dynamic Step Size Schedule},
  url = {https://proceedings.mlr.press/v202/ivgi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{iyer2023maximal,
  abstract = {Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\eta^{\ast}$, we observe that in constant-width fully-connected ReLU networks, $\eta^{\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\eta^{\ast}$ is well predicted as a power of depth $\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We further analyze the relationship between $\eta^{\ast}$ and the sharpness $\lambda_{1}$ of the network at initialization, indicating they are closely though not inversely related. We formally prove bounds for $\lambda_{1}$ in terms of depth $\times$ width that align with our empirical results.},
  author = {Gaurav Iyer and Boris Hanin and David Rolnick},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/iyer2023maximal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14500--14530},
  pdf = {https://proceedings.mlr.press/v202/iyer23a/iyer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Maximal Initial Learning Rates in Deep {ReLU} Networks},
  url = {https://proceedings.mlr.press/v202/iyer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{izzo2023datadriven,
  abstract = {Medical studies frequently require to extract the relationship between each covariate and the outcome with statistical confidence measures. To do this, simple parametric models are frequently used (e.g. coefficients of linear regression) but always fitted on the whole dataset. However, it is common that the covariates may not have a uniform effect over the whole population and thus a unified simple model can miss the heterogeneous signal. For example, a linear model may be able to explain a subset of the data but fail on the rest due to the nonlinearity and heterogeneity in the data. In this paper, we propose DDGroup (data-driven group discovery), a data-driven method to effectively identify subgroups in the data with a uniform linear relationship between the features and the label. DDGroup outputs an interpretable region in which the linear model is expected to hold. It is simple to implement and computationally tractable for use. We show theoretically that, given a large enough sample, DDGroup recovers a region where a single linear model with low variance is well-specified (if one exists), and experiments on real-world medical datasets confirm that it can discover regions where a local linear model has improved performance. Our experiments also show that DDGroup can uncover subgroups with qualitatively different relationships which are missed by simply applying parametric approaches to the whole dataset.},
  author = {Zachary Izzo and Ruishan Liu and James Zou 0001},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/izzo2023datadriven.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14531--14552},
  pdf = {https://proceedings.mlr.press/v202/izzo23a/izzo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data-Driven Subgroup Identification for Linear Regression},
  url = {https://proceedings.mlr.press/v202/izzo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jabri2023scalable,
  abstract = {Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Network (RIN), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to1024×1024 images without cascades or guidance, while being domain-agnostic and up to 10× more efficient than 2D and 3D U-Nets.},
  author = {Allan Jabri and David J. Fleet and Ting Chen},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jabri2023scalable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14569--14589},
  pdf = {https://proceedings.mlr.press/v202/jabri23a/jabri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scalable Adaptive Computation for Iterative Generation},
  url = {https://proceedings.mlr.press/v202/jabri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jacobsen2023unconstrained,
  abstract = {Algorithms for online learning typically require one or more boundedness assumptions: that the domain is bounded, that the losses are Lipschitz, or both. In this paper, we develop a new setting for online learning with unbounded domains and non-Lipschitz losses. For this setting we provide an algorithm which guarantees $R_{T}(u)\le \tilde O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is unimprovable without further assumptions. We leverage this algorithm to develop new saddle-point optimization algorithms that converge in duality gap in unbounded domains, even in the absence of meaningful curvature. Finally, we provide the first algorithm achieving non-trivial dynamic regret in an unbounded domain for non-Lipschitz losses, as well as a matching lower bound. The regret of our dynamic regret algorithm automatically improves to a novel $L^{*}$ bound when the losses are smooth.},
  author = {Andrew Jacobsen and Ashok Cutkosky},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jacobsen2023unconstrained.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14590--14630},
  pdf = {https://proceedings.mlr.press/v202/jacobsen23a/jacobsen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unconstrained Online Learning with Unbounded Losses},
  url = {https://proceedings.mlr.press/v202/jacobsen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jain2023multiobjective,
  abstract = {We study the problem of generating diverse candidates in the context of Multi-Objective Optimization. In many applications of machine learning such as drug discovery and material design, the goal is to generate candidates which simultaneously optimize a set of potentially conflicting objectives. Moreover, these objectives are often imperfect evaluations of some underlying property of interest, making it important to generate diverse candidates to have multiple options for expensive downstream evaluations. We propose Multi-Objective GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC, which models a family of independent sub-problems defined by a scalarization function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a sequence of sub-problems defined by an acquisition function in an active learning loop. Our experiments on wide variety of synthetic and benchmark tasks demonstrate advantages of the proposed methods in terms of the Pareto performance and importantly, improved candidate diversity, which is the main contribution of this work.},
  author = {Moksh Jain and Sharath Chandra Raparthy and Alex Hern{\'a}ndez-Garc{\'i}a and Jarrid Rector-Brooks and Yoshua Bengio and Santiago Miret and Emmanuel Bengio},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jain2023multiobjective.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14631--14653},
  pdf = {https://proceedings.mlr.press/v202/jain23a/jain23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Objective {GFlowNets}},
  url = {https://proceedings.mlr.press/v202/jain23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jaiswal2023graph,
  abstract = {Graphs are omnipresent and GNNs are a powerful family of neural networks for learning over graphs. Despite their popularity, scaling GNNs either by deepening or widening suffers from prevalent issues of unhealthy gradients, over-smoothening, information squashing, which often lead to sub-standard performance. In this work, we are interested in exploring a principled way to scale GNNs capacity without deepening or widening, which can improve its performance across multiple small and large graphs. By dividing giant graph data, we build multiple independently and parallelly trained weaker GNNs (soup ingredient) without any intermediate communication, and combine their strength using a greedy interpolation soup procedure to achieve state-of-the-art performance.},
  author = {Ajay Kumar Jaiswal and Shiwei Liu 0003 and Tianlong Chen 0001 and Ying Ding 0001 and Zhangyang Wang},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jaiswal2023graph.pdf:pdf},
  mdate = {2025-05-12},
  pages = {14679--14690},
  pdf = {https://proceedings.mlr.press/v202/jaiswal23a/jaiswal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Ladling: Shockingly Simple Parallel {GNN} Training without Intermediate Communication},
  url = {https://proceedings.mlr.press/v202/jaiswal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jaiswal2023instant,
  abstract = {Large pre-trained transformers have been receiving explosive attention in the past few years, due to their acculturation for numerous downstream applications via fine-tuning, but their exponentially increasing parameter counts are becoming a primary hurdle to even just fine-tune them without industry-standard hardware. Recently, Lottery Ticket Hypothesis (LTH) and its variants, have been exploited to prune these large pre-trained models generating subnetworks which can achieve similar performance as their dense counterparts, but LTH pragmatism is enormously inhibited by repetitive full training and pruning routine of iterative magnitude pruning (IMP) which worsens with increasing model size. Motivated by the recent observations of model soups, which suggest that fine-tuned weights of multiple models can be merged to a better minima, we propose Instant Soup Pruning (ISP) to generate lottery ticket quality subnetworks, using a fraction of the original IMP cost by replacing the expensive intermediate pruning stages of IMP with computationally efficient weak mask generation and aggregation routine. More specifically, during the mask generation stage, ISP takes a small handful of iterations using varying training protocols and data subsets to generate many weak and noisy subnetworks, and superpose them to average out the noise creating a high-quality denoised subnetwork. Our extensive experiments and ablation on two popular large-scale pre-trained models: CLIP (unexplored in pruning till date) and BERT across multiple benchmark vision and language datasets validate the effectiveness of ISP compared to several state-of-the-art pruning methods.},
  author = {Ajay Kumar Jaiswal and Shiwei Liu 0003 and Tianlong Chen 0001 and Ying Ding 0001 and Zhangyang Wang},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jaiswal2023instant.pdf:pdf},
  mdate = {2025-05-12},
  pages = {14691--14701},
  pdf = {https://proceedings.mlr.press/v202/jaiswal23b/jaiswal23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Instant Soup: Cheap Pruning Ensembles in A Single Pass Can Draw Lottery Tickets from Large Models},
  url = {https://proceedings.mlr.press/v202/jaiswal23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jang2023exploring,
  abstract = {Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20\% and 1.29\%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training a separate expert LM per training task instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together.},
  author = {Joel Jang and Seungone Kim and Seonghyeon Ye and Doyoung Kim 0001 and Lajanugen Logeswaran and Moontae Lee and Kyungjae Lee 0002 and Minjoon Seo},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jang2023exploring.pdf:pdf},
  mdate = {2025-01-28},
  pages = {14702--14729},
  pdf = {https://proceedings.mlr.press/v202/jang23a/jang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exploring the Benefits of Training Expert Language Models over Instruction Tuning},
  url = {https://proceedings.mlr.press/v202/jang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jang2023learning,
  abstract = {Recent complicated problems require large-scale datasets and complex model architectures, however, it is difficult to train such large networks due to high computational issues. Significant efforts have been made to make the training more efficient such as momentum, learning rate scheduling, weight regularization, and meta-learning. Based on our observations on 1) high correlation between past weights and future weights, 2) conditions for beneficial weight prediction, and 3) feasibility of weight prediction, we propose a more general framework by intermittently skipping a handful of epochs by periodically forecasting near future weights, i.e., a Weight Nowcaster Network (WNN). As an add-on module, WNN predicts the future weights to make the learning process faster regardless of tasks and architectures. Experimental results show that WNN can significantly save actual time cost for training with an additional marginal time to train WNN. We validate the generalization capability of WNN under various tasks, and demonstrate that it works well even for unseen tasks.},
  author = {Jinhyeok Jang and Woo-han Yun and Won Hwa Kim and Youngwoo Yoon and Jaehong Kim 0001 and Jaeyeon Lee 0001 and ByungOk Han},
  booktitle = {Fortieth International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jang2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14730--14757},
  pdf = {https://proceedings.mlr.press/v202/jang23b/jang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Boost Training by Periodic Nowcasting Near Future Weights},
  url = {https://proceedings.mlr.press/v202/jang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{janjos2023unscented,
  abstract = {The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior. We empirically show competitive performance in Fréchet Inception Distance scores over closely-related models, in addition to a lower training variance than the VAE.},
  author = {Faris Janjos and Lars Rosenbaum and Maxim Dolgov and J. Marius Zoellner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/janjos2023unscented.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14758--14779},
  pdf = {https://proceedings.mlr.press/v202/janjos23a/janjos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unscented Autoencoder},
  url = {https://proceedings.mlr.press/v202/janjos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jarrett2023curiosity,
  abstract = {Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a 'noisy TV'. In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle 'noise' from 'novelty', yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust to stochasticity. Second, we instantiate this framework for the recently introduced BYOL-Explore algorithm as our prime example, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate its behavior under a variety of different stochasticities in a grid world, and find improvements over BYOL-Explore in hard-exploration Atari games with sticky actions. Notably, we show state-of-the-art results in exploring Montezuma's Revenge with sticky actions, while preserving performance in the non-sticky setting.},
  author = {Daniel Jarrett and Corentin Tallec and Florent Altch\'{e} and Thomas Mesnard and R\'{e}mi Munos and Michal Valko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jarrett2023curiosity.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14780--14816},
  pdf = {https://proceedings.mlr.press/v202/jarrett23a/jarrett23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments},
  url = {https://proceedings.mlr.press/v202/jarrett23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jeeveswaran2023birt,
  abstract = {The ability of deep neural networks to continually learn and adapt to a sequence of tasks has remained challenging due to catastrophic forgetting of previously learned tasks. Humans, on the other hand, have a remarkable ability to acquire, assimilate, and transfer knowledge across tasks throughout their lifetime without catastrophic forgetting. The versatility of the brain can be attributed to the rehearsal of abstract experiences through a complementary learning system. However, representation rehearsal in vision transformers lacks diversity, resulting in overfitting and consequently, performance drops significantly compared to raw image rehearsal. Therefore, we propose {BiRT}, a novel representation rehearsal-based continual learning approach using vision transformers. Specifically, we introduce controllable noises at various stages of the vision transformer and enforce consistency in predictions with respect to an exponential moving average of the working model. Our method provides consistent performance gain over raw image and vanilla representation rehearsal on several challenging CL benchmarks while being memory efficient and robust to natural and adversarial corruptions.},
  author = {Kishaan Jeeveswaran and Prashant Shivaram Bhat and Bahram Zonooz and Elahe Arani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jeeveswaran2023birt.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14817--14835},
  pdf = {https://proceedings.mlr.press/v202/jeeveswaran23a/jeeveswaran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{BiRT}: Bio-inspired Replay in Vision Transformers for Continual Learning},
  url = {https://proceedings.mlr.press/v202/jeeveswaran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jeong2023recovering,
  abstract = {Crowdsourcing has emerged as an effective platform for labeling large amounts of data in a cost- and time-efficient manner. Most previous work has focused on designing an efficient algorithm to recover only the ground-truth labels of the data. In this paper, we consider multi-choice crowdsourcing tasks with the goal of recovering not only the ground truth, but also the most confusing answer and the confusion probability. The most confusing answer provides useful information about the task by revealing the most plausible answer other than the ground truth and how plausible it is. To theoretically analyze such scenarios, we propose a model in which there are the top two plausible answers for each task, distinguished from the rest of the choices. Task difficulty is quantified by the probability of confusion between the top two, and worker reliability is quantified by the probability of giving an answer among the top two. Under this model, we propose a two-stage inference algorithm to infer both the top two answers and the confusion probability. We show that our algorithm achieves the minimax optimal convergence rate. We conduct both synthetic and real data experiments and demonstrate that our algorithm outperforms other recent algorithms. We also show the applicability of our algorithms in inferring the difficulty of tasks and in training neural networks with top-two soft labels.},
  author = {Hyeonsu Jeong and Hye Won Chung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jeong2023recovering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14836--14868},
  pdf = {https://proceedings.mlr.press/v202/jeong23a/jeong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Recovering Top-Two Answers and Confusion Probability in Multi-Choice Crowdsourcing},
  url = {https://proceedings.mlr.press/v202/jeong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ji2023leveraging,
  abstract = {In node classification using graph neural networks (GNNs), a typical model generates logits for different class labels at each node. A softmax layer often outputs a label prediction based on the largest logit. We demonstrate that it is possible to infer hidden graph structural information from the dataset using these logits. We introduce the key notion of label non-uniformity, which is derived from the Wasserstein distance between the softmax distribution of the logits and the uniform distribution. We demonstrate that nodes with small label non-uniformity are harder to classify correctly. We theoretically analyze how the label non-uniformity varies across the graph, which provides insights into boosting the model performance: increasing training samples with high non-uniformity or dropping edges to reduce the maximal cut size of the node set of small non-uniformity. These mechanisms can be easily added to a base GNN model. Experimental results demonstrate that our approach improves the performance of many benchmark base models.},
  author = {Feng Ji and See Hian Lee and Hanyang Meng and Kai Zhao and Jielong Yang and Wee Peng Tay},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ji2023leveraging.pdf:pdf},
  mdate = {2024-12-12},
  pages = {14869--14885},
  pdf = {https://proceedings.mlr.press/v202/ji23a/ji23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/ji23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jia2023bidirectional,
  abstract = {Semi-supervised learning (SSL) suffers from severe performance degradation when labeled and unlabeled data come from inconsistent data distributions. However, there is still a lack of sufficient theoretical guidance on how to alleviate this problem. In this paper, we propose a general theoretical framework that demonstrates how distribution discrepancies caused by pseudo-label predictions and target predictions can lead to severe generalization errors. Through theoretical analysis, we identify three main reasons why previous SSL algorithms cannot perform well with inconsistent distributions: coupling between the pseudo-label predictor and the target predictor, biased pseudo labels, and restricted sample weights. To address these challenges, we introduce a practical framework called Bidirectional Adaptation that can adapt to the distribution of unlabeled data for debiased pseudo-label prediction and to the target distribution for debiased target prediction, thereby mitigating these shortcomings. Extensive experimental results demonstrate the effectiveness of our proposed framework.},
  author = {Lin-Han Jia and Lan-Zhe Guo and Zhi Zhou and Jie-Jing Shao and Yuke Xiang and Yufeng Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jia2023bidirectional.pdf:pdf},
  mdate = {2024-05-13},
  pages = {14886--14901},
  pdf = {https://proceedings.mlr.press/v202/jia23a/jia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bidirectional Adaptation for Robust Semi-Supervised Learning with Inconsistent Data Distributions},
  url = {https://proceedings.mlr.press/v202/jia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jia2023shortlived,
  abstract = {Modern platforms leverage randomized experiments to make informed decisions from a given set of items (``treatments''). As a particularly challenging scenario, these items may (i) arrive in high volume, with thousands of new items being released per hour, and (ii) have short lifetime, say, due to the item's transient nature or underlying non-stationarity that impels the platform to perceive the same item as distinct copies over time. Thus motivated, we study a Bayesian multiple-play bandit problem that encapsulates the key features of the multivariate testing (or ``multi-A/B testing'') problem with a high volume of short-lived arms. In each round, a set of k arms arrive, each available for w rounds. Without knowing the mean reward for each arm, the learner selects a multiset of n arms and immediately observes their realized rewards. We aim to minimize the loss due to not knowing the mean rewards, averaged over instances generated from a given prior distribution. We show that if k = O(n^ρ) for some ρ > 0, our policy achieves O(n^{-min{ρ, 1/2(1+1/w)^{-1}}}) average loss on a sufficiently large class of prior distributions.},
  author = {Su Jia and Nishant Oli and Ian Anderson and Paul Duff and Andrew A. Li and R. Ravi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jia2023shortlived.pdf:pdf},
  mdate = {2023-08-29},
  pages = {14902--14929},
  pdf = {https://proceedings.mlr.press/v202/jia23b/jia23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Short-lived High-volume Bandits},
  url = {https://proceedings.mlr.press/v202/jia23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jia2023smooth,
  abstract = {In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time. However, in practice, environments often change smoothly, so such algorithms may incur higher-than-necessary regret. We study a non-stationary bandits problem where each arm's mean reward sequence can be embedded into a $β$-Hölder function, i.e., a function that is $(β-1)$-times Lipschitz-continuously differentiable. The non-stationarity becomes more smooth as $β$ increases. When $β=1$, this corresponds to the non-smooth regime, where previous work established a minimax regret of $\tilde{\Theta}(T^{2/3})$. We show the first separation between the smooth (i.e., $β \geq 2$) and non-smooth (i.e., $β=1$) regimes by presenting a policy with $\tilde{O}(k^{4/5} T^{3/5})$ regret on any $k$-armed, $2$-Hölder instance. We complement this result by showing that the minimax regret on the $β$-Hölder family of instances is $Ω(T^{(β+1)/(2β+1)})$ for any integer $β \geq 1$. This matches our upper bound for $β=2$ up to logarithmic factors. Furthermore, we validated the effectiveness of our policy through a comprehensive numerical study using real-world click-through rate data.},
  author = {Su Jia and Qian Xie and Nathan Kallus and Peter I. Frazier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jia2023smooth.pdf:pdf},
  mdate = {2025-01-09},
  pages = {14930--14944},
  pdf = {https://proceedings.mlr.press/v202/jia23c/jia23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Smooth Non-stationary Bandits},
  url = {https://proceedings.mlr.press/v202/jia23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023unified,
  abstract = {Spiking Neural Networks (SNNs) have gained significant attention for their energy-efficient and fast-inference capabilities, but training SNNs from scratch can be challenging due to the discrete nature of spikes. One alternative method is to convert an Artificial Neural Network (ANN) into an SNN, known as ANN-SNN conversion. Currently, existing ANN-SNN conversion methods often involve redesigning the ANN with a new activation function, rather than utilizing the traditional ReLU, and converting it to an SNN. However, these methods do not take into account the potential performance loss between the regular ANN with ReLU and the tailored ANN. In this work, we propose a unified optimization framework for ANN-SNN conversion that considers both performance loss and conversion error. To achieve this, we introduce the SlipReLU activation function, which is a weighted sum of the threshold-ReLU and the step function. Theoretical analysis demonstrates that conversion error can be zero on a range of shift values $δ \in [-0.5,0.5]$ rather than a fixed shift term 0.5. We evaluate our SlipReLU method on CIFAR datasets, which shows that SlipReLU outperforms current ANN-SNN conversion methods and supervised training methods in terms of accuracy and latency. To the best of our knowledge, this is the first ANN-SNN conversion method that enables SNN inference using only 1 time step.},
  author = {Haiyan Jiang and Srinivas Anumasa and Giulia De Masi and Huan Xiong and Bin Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023unified.pdf:pdf},
  mdate = {2023-08-28},
  pages = {14945--14974},
  pdf = {https://proceedings.mlr.press/v202/jiang23a/jiang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Unified Optimization Framework of {ANN-SNN} Conversion: Towards Optimal Mapping from Activation Values to Firing Rates},
  url = {https://proceedings.mlr.press/v202/jiang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023vima,
  abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\times$ task success rate given the same training data. With $10\times$ less training data, VIMA still performs $2.7\times$ better than the best competing variant.},
  author = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023vima.pdf:pdf},
  mdate = {2023-08-29},
  pages = {14975--15022},
  pdf = {https://proceedings.mlr.press/v202/jiang23b/jiang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{VIMA}: Robot Manipulation with Multimodal Prompts},
  url = {https://proceedings.mlr.press/v202/jiang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023estimating,
  abstract = {A number of methods have been proposed for causal effect estimation, yet few have demonstrated efficacy in handling data with complex structures, such as images. To fill this gap, we propose Causal Multi-task Deep Ensemble (CMDE), a novel framework that learns both shared and group-specific information from the study population. We provide proofs demonstrating equivalency of CDME to a multi-task Gaussian process (GP) with a coregionalization kernel a priori. Compared to multi-task GP, CMDE efficiently handles high-dimensional and multi-modal covariates and provides pointwise uncertainty estimates of causal effects. We evaluate our method across various types of datasets and tasks and find that CMDE outperforms state-of-the-art methods on a majority of these tasks.},
  author = {Ziyang Jiang and Zhuoran Hou and Yiling Liu and Yiman Ren and Keyu Li and David E. Carlson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023estimating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15023--15040},
  pdf = {https://proceedings.mlr.press/v202/jiang23c/jiang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimating Causal Effects using a Multi-task Deep Ensemble},
  url = {https://proceedings.mlr.press/v202/jiang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023online,
  abstract = {We study the online restless bandit problem, where each arm evolves according to a Markov chain independently, and the reward of pulling an arm depends on both the current state of the corresponding Markov chain and the pulled arm. The agent (decision maker) does not know the transition functions and reward functions, and cannot observe the states of arms even after pulling. The goal is to sequentially choose which arms to pull so as to maximize the expected cumulative rewards collected. In this paper, we propose TSEETC, a learning algorithm based on Thompson Sampling with Episodic Explore-Then-Commit. The algorithm proceeds in episodes of increasing length and each episode is divided into exploration and exploitation phases. During the exploration phase, samples of action-reward pairs are collected in a round-robin fashion and utilized to update the posterior distribution as a mixture of Dirichlet distributions. At the beginning of the exploitation phase, TSEETC generates a sample from the posterior distribution as true parameters. It then follows the optimal policy for the sampled model for the rest of the episode. We establish the Bayesian regret bound $\tilde{\mathcal{O}}(\sqrt{T})$ for TSEETC, where $T$ is the time horizon. We show through simulations that TSEETC outperforms existing algorithms in regret.},
  author = {Bowen Jiang and Bo Jiang and Jian Li and Tao Lin and Xinbing Wang and Chenghu Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023online.pdf:pdf},
  mdate = {2024-07-27},
  pages = {15041--15066},
  pdf = {https://proceedings.mlr.press/v202/jiang23d/jiang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Restless Bandits with Unobserved States},
  url = {https://proceedings.mlr.press/v202/jiang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023detecting,
  abstract = {Given a pre-trained in-distribution (ID) model, the inference-time out-of-distribution (OOD) detection aims to recognize OOD data during the inference stage. However, some representative methods share an unproven assumption that the probability that OOD data belong to every ID class should be the same, i.e., these OOD-to-ID probabilities actually form a uniform distribution. In this paper, we show that this assumption makes the above methods incapable when the ID model is trained with class-imbalanced data. Fortunately, by analyzing the causal relations between ID/OOD classes and features, we identify several common scenarios where the OOD-to-ID probabilities should be the ID-class-prior distribution and propose two strategies to modify existing inference-time detection methods: 1) replace the uniform distribution with the ID-class-prior distribution if they explicitly use the uniform distribution; 2) otherwise, reweight their scores according to the similarity between the ID-class-prior distribution and the softmax outputs of the pre-trained model. Extensive experiments show that both strategies can improve the OOD detection performance when the ID model is pre-trained with imbalanced data, reflecting the importance of ID-class prior in OOD detection.},
  author = {Xue Jiang and Feng Liu and Zhen Fang and Hong Chen and Tongliang Liu and Feng Zheng and Bo Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023detecting.pdf:pdf},
  mdate = {2024-07-18},
  pages = {15067--15088},
  pdf = {https://proceedings.mlr.press/v202/jiang23e/jiang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Detecting Out-of-distribution Data through In-distribution Class Prior},
  url = {https://proceedings.mlr.press/v202/jiang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023towards,
  abstract = {We study the problem of how to stably and efficiently train a deep neural network robust to adversarial perturbations bounded by an $\ell_1$ norm. We demonstrate that achieving robustness against $\ell_1$-bounded perturbations is more challenging than in the $\ell_2$ or $\ell_\infty$ cases, because adversarial training against $\ell_1$-bounded perturbations is more likely to suffer from catastrophic overfitting and yield training instabilities. We link these issues to the coordinate descent strategy used in existing methods and address this by introducing Fast-{EG}-$\ell_1$, an efficient adversarial training algorithm based on Euclidean geometry and free of coordinate descent. Fast-{EG}-$\ell_1$ comes with no additional memory costs and no extra hyper-parameters to tune. Our experimental results on various datasets demonstrate that Fast-{EG}-$\ell_1$ yields the best and most stable robustness against $\ell_1$-bounded adversarial attacks among the methods of comparable computational complexity.},
  author = {Yulun Jiang and Chen Liu and Zhichao Huang and Mathieu Salzmann and Sabine Süsstrunk},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023towards.pdf:pdf},
  mdate = {2025-02-12},
  pages = {15089--15104},
  pdf = {https://proceedings.mlr.press/v202/jiang23f/jiang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Stable and Efficient Adversarial Training against {$\ell_1$} Bounded Adversarial Attacks},
  url = {https://proceedings.mlr.press/v202/jiang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023learning,
  abstract = {Learning unnormalized statistical models (e.g., energy-based models) is computationally challenging due to the complexity of handling the partition function. To eschew this complexity, noise-contrastive estimation (NCE) has been proposed by formulating the objective as the logistic loss of the real data and the artificial noise. However, as found in previous works, NCE may perform poorly in many tasks due to its flat loss landscape and slow convergence. In this paper, we study a direct approach for optimizing the negative log-likelihood of unnormalized models from the perspective of compositional optimization. To tackle the partition function, a noise distribution is introduced such that the log partition function can be written as a compositional function whose inner function can be estimated with stochastic samples. Hence, the objective can be optimized by stochastic compositional optimization algorithms. Despite being a simple method, we demonstrate that it is more favorable than NCE by (1) establishing a fast convergence rate and quantifying its dependence on the noise distribution through the variance of stochastic estimators; (2) developing better results for one-dimensional Gaussian mean estimation by showing our objective has a much favorable loss landscape and hence our method enjoys faster convergence; (3) demonstrating better performance on multiple applications, including density estimation, out-of-distribution detection, and real image generation.},
  author = {Wei Jiang and Jiayu Qin and Lingyu Wu and Changyou Chen and Tianbao Yang and Lijun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023learning.pdf:pdf},
  mdate = {2025-05-01},
  pages = {15105--15124},
  pdf = {https://proceedings.mlr.press/v202/jiang23g/jiang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Unnormalized Statistical Models via Compositional Optimization},
  url = {https://proceedings.mlr.press/v202/jiang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023approximate,
  abstract = {Causal effect estimation has been studied by many researchers when only observational data is available. Sound and complete algorithms have been developed for pointwise estimation of identifiable causal queries. For non-identifiable causal queries, researchers developed polynomial programs to estimate tight bounds on causal effect. However, these are computationally difficult to optimize for variables with large support sizes. In this paper, we analyze the effect of weak confounding on causal estimands. More specifically, under the assumption that the unobserved confounders that render a query non-identifiable have small entropy, we propose an efficient linear program to derive the upper and lower bounds of the causal effect. We show that our bounds are consistent in the sense that as the entropy of unobserved confounders goes to zero, the gap between the upper and lower bound vanishes. Finally, we conduct synthetic and real data simulations to compare our bounds with the bounds obtained by the existing work that cannot incorporate such entropy constraints and show that our bounds are tighter for the setting with weak confounders.},
  author = {Ziwei Jiang and Lai Wei and Murat Kocaoglu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023approximate.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15125--15143},
  pdf = {https://proceedings.mlr.press/v202/jiang23h/jiang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Approximate Causal Effect Identification under Weak Confounding},
  url = {https://proceedings.mlr.press/v202/jiang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023mewl,
  abstract = {Without explicit feedback, humans can rapidly learn the meaning of words. Children can acquire a new word after just a few passive exposures, a process known as fast mapping. This word learning capability is believed to be the most fundamental building block of multimodal understanding and reasoning. Despite recent advancements in multimodal learning, a systematic and rigorous evaluation is still missing for human-like word learning in machines. To fill in this gap, we introduce the MachinE Word Learning (MEWL) benchmark to assess how machines learn word meaning in grounded visual scenes. MEWL covers human's core cognitive toolkits in word learning: cross-situational reasoning, bootstrapping, and pragmatic learning. Specifically, MEWL is a few-shot benchmark suite consisting of nine tasks for probing various word learning capabilities. These tasks are carefully designed to be aligned with the children's core abilities in word learning and echo the theories in the developmental literature. By evaluating multimodal and unimodal agents' performance with a comparative analysis of human performance, we notice a sharp divergence in human and machine word learning. We further discuss these differences between humans and machines and call for human-like few-shot word learning in machines.},
  author = {Guangyuan Jiang and Manjie Xu and Shiji Xin and Wei Liang and Yujia Peng and Chi Zhang and Yixin Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023mewl.pdf:pdf},
  mdate = {2024-07-22},
  pages = {15144--15169},
  pdf = {https://proceedings.mlr.press/v202/jiang23i/jiang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MEWL}: Few-shot multimodal word learning with referential uncertainty},
  url = {https://proceedings.mlr.press/v202/jiang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023neuralslice,
  abstract = {Learning-based high-fidelity reconstruction of 3D shapes with varying topology is a fundamental problem in computer vision and computer graphics. Recent advances in learning 3D shapes using explicit and implicit representations have achieved impressive results in 3D modeling. However, the template-based explicit representation is limited by fixed topology, and the implicit representation, although flexible with arbitrary topology, requires a large number of sampled points to regress the surface, which is computationally expensive. In this work, we propose a novel 3D shape representation named NeuralSlice, which represents a 3D shape as the intersection of a 4D tetrahedral mesh and a 4D hyperplane. A novel network is designed to incorporate the proposed representation flexibly, which learns a deformable 4D template and a parameter for slicing 4D hyperplane to reconstruct the 3D object. To learn the local deformation of the 4D template, we further propose a spatial-aware network to locate the 4D points within the 3D feature volume of input shape via positional encoding, which leverages the local geometrical feature to guide the 4D deformation. By addressing the 3D problem in a higher 4D space, our method supports flexible topology changes while being highly efficient. Our method is guaranteed to produce manifold meshes. NeuralSlice outperforms the state-of-the-art explicit-based approaches in terms of reconstruction quality. Compared with implicit approaches, by avoiding point sampling, our method is 10 times faster than the implicit approaches, and better preserves thin structures. NeuralSlice has the capability of representing various shapes and topologies using a single 4D tetrahedral mesh.},
  author = {Chenbo Jiang and Jie Yang and Shwai He and Yu-Kun Lai and Lin Gao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023neuralslice.pdf:pdf},
  mdate = {2023-09-03},
  pages = {15170--15185},
  pdf = {https://proceedings.mlr.press/v202/jiang23j/jiang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{NeuralSlice}: Neural {3D} Triangle Mesh Reconstruction via Slicing {4D} Tetrahedral Meshes},
  url = {https://proceedings.mlr.press/v202/jiang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jiang2023effective,
  abstract = {Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which constructs label embedding from feature embeddings directly. Combining meta-learning the prompt pool and RepVerb, we propose MetaPrompter for effective structured prompting. MetaPrompter is parameter-efficient as only the pool is required to be tuned. Experimental results demonstrate that MetaPrompter performs better than the recent state-of-the-arts and RepVerb outperforms existing soft verbalizers.},
  author = {Weisen Jiang and Yu Zhang and James T. Kwok},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jiang2023effective.pdf:pdf},
  mdate = {2024-08-01},
  pages = {15186--15199},
  pdf = {https://proceedings.mlr.press/v202/jiang23k/jiang23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Effective Structured Prompting by Meta-Learning and Representative Verbalizer},
  url = {https://proceedings.mlr.press/v202/jiang23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jin2023understanding,
  abstract = {It is believed that Gradient Descent (GD) induces an implicit bias towards good generalization in training machine learning models. This paper provides a fine-grained analysis of the dynamics of GD for the matrix sensing problem, whose goal is to recover a low-rank ground-truth matrix from near-isotropic linear measurements. It is shown that GD with small initialization behaves similarly to the greedy low-rank learning heuristics (Li et al., 2020) and follows an incremental learning procedure (Gissin et al., 2019): GD sequentially learns solutions with increasing ranks until it recovers the ground truth matrix. Compared to existing works which only analyze the first learning phase for rank-1 solutions, our result provides characterizations for the whole learning process. Moreover, besides the over-parameterized regime that many prior works focused on, our analysis of the incremental learning procedure also applies to the under-parameterized regime. Finally, we conduct numerical experiments to confirm our theoretical findings.},
  author = {Jikai Jin and Zhiyuan Li and Kaifeng Lyu and Simon Shaolei Du and Jason D. Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jin2023understanding.pdf:pdf},
  mdate = {2024-10-06},
  pages = {15200--15238},
  pdf = {https://proceedings.mlr.press/v202/jin23a/jin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Incremental Learning of Gradient Descent: A Fine-grained Analysis of Matrix Sensing},
  url = {https://proceedings.mlr.press/v202/jin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jin2023thompson,
  abstract = {We propose $\epsilon$-Exploring Thompson Sampling ($\epsilon$-TS), a modified version of the Thompson Sampling (TS) algorithm for multi-armed bandits. In $\epsilon$-TS, arms are selected greedily based on empirical mean rewards with probability $1-\epsilon$, and based on posterior samples obtained from TS with probability $\epsilon$. Here, $\epsilon\in(0,1)$ is a user-defined constant. By reducing exploration, $\epsilon$-TS improves computational efficiency compared to TS while achieving better regret bounds. We establish that $\epsilon$-TS is both minimax optimal and asymptotically optimal for various popular reward distributions, including Gaussian, Bernoulli, Poisson, and Gamma. A key technical advancement in our analysis is the relaxation of the requirement for a stringent anti-concentration bound of the posterior distribution, which was necessary in recent analyses that achieved similar bounds. As a result, $\epsilon$-TS maintains the posterior update structure of TS while minimizing alterations, such as clipping the sampling distribution or solving the inverse of the Kullback-Leibler (KL) divergence between reward distributions, as done in previous work. Furthermore, our algorithm is as easy to implement as TS, but operates significantly faster due to reduced exploration. Empirical evaluations confirm the efficiency and optimality of $\epsilon$-TS.},
  author = {Tianyuan Jin and Xianglin Yang and Xiaokui Xiao and Pan Xu 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jin2023thompson.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15239--15261},
  pdf = {https://proceedings.mlr.press/v202/jin23b/jin23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Thompson Sampling with Less Exploration is Fast and Optimal},
  url = {https://proceedings.mlr.press/v202/jin23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jones2023automatically,
  abstract = {Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to find a non-toxic input that starts with "Barack Obama" that a model maps to a toxic output. This optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and efficiently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. "Barack Obama is a legalized unborn" –$>$ "child murderer"), produces French inputs that complete to English outputs, and finds inputs that generate a specific name. Our work offers a promising new tool to uncover models' failure-modes before deployment. Content Warning: This paper contains examples that may be offensive in nature.},
  author = {Erik Jones and Anca D. Dragan and Aditi Raghunathan and Jacob Steinhardt},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jones2023automatically.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15307--15329},
  pdf = {https://proceedings.mlr.press/v202/jones23a/jones23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Automatically Auditing Large Language Models via Discrete Optimization},
  url = {https://proceedings.mlr.press/v202/jones23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jong2023precomputed,
  abstract = {Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.},
  author = {Michiel de Jong and Yury Zemlyanskiy and Nicholas FitzGerald and Joshua Ainslie and Sumit Sanghai and Fei Sha and William W. Cohen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jong2023precomputed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7329--7342},
  pdf = {https://proceedings.mlr.press/v202/de-jong23a/de-jong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute},
  url = {https://proceedings.mlr.press/v202/de-jong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{joshi2023expressive,
  abstract = {The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3) Higher order tensors and scalarisation enable maximally powerful geometric GNNs; and (4) GWL's discrimination-based perspective is equivalent to universal approximation.},
  author = {Chaitanya K. Joshi and Cristian Bodnar and Simon V. Mathis and Taco Cohen and Pietro Lio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/joshi2023expressive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15330--15355},
  pdf = {https://proceedings.mlr.press/v202/joshi23a/joshi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Expressive Power of Geometric Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/joshi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023dataefficient,
  abstract = {Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this problem for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of contrastive learning on such subsets. Through extensive experiments, we show that we can safely exclude 20% of examples from CIFAR100 and 40% from STL10 and TinyImageNet, without affecting downstream task performance. In general, subsets selected by our method outperform random subsets by over 3% across these datasets. Interestingly, we also discover the subsets that contribute the most to contrastive learning are those that contribute the least to supervised learning.},
  author = {Siddharth Joshi 0004 and Baharan Mirzasoleiman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023dataefficient.pdf:pdf},
  mdate = {2024-10-18},
  pages = {15356--15370},
  pdf = {https://proceedings.mlr.press/v202/joshi23b/joshi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least},
  url = {https://proceedings.mlr.press/v202/joshi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jothimurugan2023robust,
  abstract = {Compositional reinforcement learning is a promising approach for training policies to perform complex long-horizon tasks. Typically, a high-level task is decomposed into a sequence of subtasks and a separate policy is trained to perform each subtask. In this paper, we focus on the problem of training subtask policies in a way that they can be used to perform any task; here, a task is given by a sequence of subtasks. We aim to maximize the worst-case performance over all tasks as opposed to the average-case performance. We formulate the problem as a two agent zero-sum game in which the adversary picks the sequence of subtasks. The paper proposes two RL algorithms to solve this game: one is an adaptation of existing multi-agent RL algorithms to our setting and the other is an asynchronous version which enables parallel training of subtask policies.},
  author = {Kishor Jothimurugan and Steve Hsu and Osbert Bastani and Rajeev Alur},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jothimurugan2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15371--15387},
  pdf = {https://proceedings.mlr.press/v202/jothimurugan23a/jothimurugan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Subtask Learning for Compositional Generalization},
  url = {https://proceedings.mlr.press/v202/jothimurugan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{joudaki2023bridging,
  abstract = {Mean field theory is widely used in the theoretical studies of neural networks. In this paper, we analyze the role of depth in the concentration of mean-field predictions, specifically for deep multilayer perceptron (MLP) with batch normalization (BN) at initialization. By scaling the network width to infinity, it is postulated that the mean-field predictions suffer from layer-wise errors that amplify with depth. We demonstrate that BN stabilizes the distribution of representations that avoids the error propagation of mean-field predictions.},
  author = {Amir Joudaki and Hadi Daneshmand and Francis R. Bach},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/joudaki2023bridging.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15388--15400},
  pdf = {https://proceedings.mlr.press/v202/joudaki23a/joudaki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Bridging the Gap between Mean Field and Finite Width Deep Random Multilayer Perceptron with Batch Normalization},
  url = {https://proceedings.mlr.press/v202/joudaki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jung2023estimating,
  abstract = {The paper addresses estimating the effects of multi-dimensional treatments (i.e., joint treatment effects), which is critical in many data-intensive domains, including genetics and drug evaluation. The main challenges for studying joint treatment effects include the need for large sample sizes to explore different treatment combinations as well as potentially unsafe treatment interactions. The authors develop machinery for estimating joint treatment effects by combining data from multiple experimental datasets. They develop new identification conditions for determining whether a joint treatment effect can be computed in terms of multiple interventional distributions under various scenarios. Further, they develop estimators with statistically appealing properties, including consistency and robustness to model misspecification and slow convergence.},
  author = {Yonghan Jung and Jin Tian 0001 and Elias Bareinboim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jung2023estimating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15451--15527},
  pdf = {https://proceedings.mlr.press/v202/jung23c/jung23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimating Joint Treatment Effects by Combining Multiple Experiments},
  url = {https://proceedings.mlr.press/v202/jung23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jung2023scaling,
  abstract = {The class-wise training losses often diverge as a result of the various levels of intra-class and inter-class appearance variation, and we find that the diverging class-wise training losses cause the uncalibrated prediction with its reliability. To resolve the issue, we propose a new calibration method to synchronize the class-wise training losses. We design a new training loss to alleviate the variance of class-wise training losses by using multiple class-wise scaling factors. The key innovation is that our framework can compensate the training losses of overfitted classes with those of under-fitted classes, the integrated training loss is preserved, preventing the performance drop even after the model calibration. Furthermore, our method can be easily employed in the post-hoc calibration methods, allowing us to use the pre-trained model as an initial model and reduce the additional computation for model calibration. We validate our approach by showing that employing it in the various post-hoc calibration methods generally improves calibration performance while preserving accuracy, and discover through investigation that our approach performs well with unbalanced datasets and untuned hyperparameters.},
  author = {Seungjin Jung and Seungmo Seo and Yonghyun Jeong and Jongwon Choi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jung2023scaling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15421--15434},
  pdf = {https://proceedings.mlr.press/v202/jung23a/jung23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scaling of Class-wise Training Losses for Post-hoc Calibration},
  url = {https://proceedings.mlr.press/v202/jung23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jung2023fighting,
  abstract = {Deep neural networks (DNNs), despite their impressive ability to generalize over-capacity networks, often rely heavily on malignant bias as shortcuts instead of task-related information for discriminative tasks. To address this problem, recent studies utilize auxiliary information related to the bias, which is rarely obtainable in practice, or sift through a handful of bias-free samples for debiasing. However, the success of these methods is not always guaranteed due to the unfulfilled presumptions. In this paper, we propose a novel method, Contrastive Debiasing via Generative Bias-transformation (CDvG), which works without explicit bias labels or bias-free samples. Motivated by our observation that not only discriminative models but also image translation models tend to focus on the malignant bias, CDvG employs an image translation model to transform the bias to another mode of bias while preserving task-relevant information. Through contrastive learning, the bias-transformed views are set against each other to learn bias-invariant representations.},
  author = {Yeonsung Jung and Hajin Shim and June Yong Yang and Eunho Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jung2023fighting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15435--15450},
  pdf = {https://proceedings.mlr.press/v202/jung23b/jung23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fighting Fire with Fire: Contrastive Debiasing without Bias-free Data via Generative Bias-transformation},
  url = {https://proceedings.mlr.press/v202/jung23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jurewicz2023catalog,
  abstract = {Prediction of a varying number of ordered clusters from sets of any cardinality is a challenging task for neural networks, combining elements of set representation, clustering and learning to order. This task arises in many diverse areas, ranging from medical triage and early discharge, through machine part management and multi-channel signal analysis for petroleum exploration to product catalog structure prediction. This paper focuses on product catalog structure prediction, which exemplifies a number of challenges inherent to adaptive ordered clustering, referred to as the eponymous {``Catalog Problem''}. These challenges include learning variable cluster constraints, exhibiting relational reasoning and managing combinatorial complexity. Despite progress in both neural clustering and set-to-sequence methods, no joint, fully differentiable model existed at the time of publication. We develop a modular architecture called Neural Ordered Clusters ({NOC}), enhanced with a specific mechanism for learning cluster-level cardinality constraints.},
  author = {Mateusz Maria Jurewicz and Graham W. Taylor and Leon Derczynski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jurewicz2023catalog.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15528--15545},
  pdf = {https://proceedings.mlr.press/v202/jurewicz23a/jurewicz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Catalog Problem: Clustering and Ordering Variable-Sized Sets},
  url = {https://proceedings.mlr.press/v202/jurewicz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{se2023equivariance,
  abstract = {Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being faster across the board.},
  author = {S{\'e}kou-Oumar Kaba and Arnab Kumar Mondal and Yan Zhang and Yoshua Bengio and Siamak Ravanbakhsh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/se2023equivariance.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15546--15566},
  pdf = {https://proceedings.mlr.press/v202/kaba23a/kaba23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Equivariance with Learned Canonicalization Functions},
  url = {https://proceedings.mlr.press/v202/kaba23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kajino2023biases,
  abstract = {In molecular optimization, given a sample of molecules and their properties, the goal is not only to train a generator of molecules optimized with respect to a target property but also to evaluate its performance accurately. A common practice is to train a predictor of the target property using the sample and apply it to both training and evaluating the generator. However, little is known about its statistical properties, making it uncertain whether this performance estimate is reliable. In this work, we theoretically investigate this evaluation methodology. We show that this evaluation methodology potentially suffers from two biases: one due to misspecification of the predictor and the other to reusing the same finite sample for training and evaluation. We discuss bias reduction methods for each of the biases comprehensively and empirically investigate their effectiveness.},
  author = {Hiroshi Kajino and Kohei Miyaguchi and Takayuki Osogami},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kajino2023biases.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15567--15585},
  pdf = {https://proceedings.mlr.press/v202/kajino23a/kajino23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Biases in Evaluation of Molecular Optimization Methods and Bias Reduction Strategies},
  url = {https://proceedings.mlr.press/v202/kajino23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kalavasis2023statistical,
  abstract = {When two different parties use the same learning rule on their own data, how can we test whether the distributions of the two outcomes are similar? In this work, we study the similarity of outcomes of learning rules through the lens of the Total Variation ({TV}) distance of distributions. We say that a learning rule is {TV} indistinguishable if the expected {TV} distance between the posterior distributions of its outputs, executed on two training data sets drawn independently from the same distribution, is small. We investigate the learnability of hypothesis classes using {TV} indistinguishable learners. Our main results are information-theoretic equivalences between {TV} indistinguishability and existing algorithmic stability notions such as replicability and approximate differential privacy. We also provide statistical amplification and boosting algorithms for {TV} indistinguishable learners.},
  author = {Alkis Kalavasis and Amin Karbasi and Shay Moran and Grigoris Velegkas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kalavasis2023statistical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15586--15622},
  pdf = {https://proceedings.mlr.press/v202/kalavasis23a/kalavasis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Statistical Indistinguishability of Learning Algorithms},
  url = {https://proceedings.mlr.press/v202/kalavasis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kalibhat2023identifying,
  abstract = {We propose Automatic Feature Explanation using Contrasting Concepts ({FALCON}), an interpretability framework to explain features of image representations. For a target feature, {FALCON} captions its highly activating cropped images using a large captioning dataset (like {LAION}-400m) and a pre-trained vision-language model like {CLIP}. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. {FALCON} also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20\% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through {FALCON}. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.},
  author = {Neha Mukund Kalibhat and Shweta Bhardwaj and C. Bayan Bruss and Hamed Firooz and Maziar Sanjabi and Soheil Feizi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kalibhat2023identifying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15623--15638},
  pdf = {https://proceedings.mlr.press/v202/kalibhat23a/kalibhat23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Identifying Interpretable Subspaces in Image Representations},
  url = {https://proceedings.mlr.press/v202/kalibhat23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kaltenpoth2023nonlinear,
  abstract = {Causal discovery is a challenging problem, with one cornerstone assumption being causal sufficiency: that all common causes of all measured variables have been observed. When this assumption does not hold, causal discovery algorithms return networks with many spurious edges. To address this, we propose a nonlinear causal model involving hidden confounders. We show that this model is identifiable from only the observed data and propose an efficient method for recovering this causal model. At the heart of our approach is a variational autoencoder which parametrizes both the causal interactions between observed variables as well as the influence of the unobserved confounders. Empirically we show that it outperforms other state-of-the-art methods for causal discovery under latent confounding on synthetic and real-world data.},
  author = {David Kaltenpoth and Jilles Vreeken},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kaltenpoth2023nonlinear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15639--15654},
  pdf = {https://proceedings.mlr.press/v202/kaltenpoth23a/kaltenpoth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonlinear Causal Discovery with Latent Confounders},
  url = {https://proceedings.mlr.press/v202/kaltenpoth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kamienny2023deep,
  abstract = {Symbolic regression ({SR}) is the problem of learning a symbolic expression from numerical data. Recently, deep neural models trained on procedurally-generated synthetic datasets showed competitive performance compared to more classical Genetic Programming ({GP}) ones. Unlike their {GP} counterparts, these neural approaches are trained to generate expressions from datasets given as context. This allows them to produce accurate expressions in a single forward pass at test time. However, they usually do not benefit from search abilities, which result in low performance compared to {GP} on out-of-distribution datasets. In this paper, we propose a novel method which provides the best of both worlds, based on a {Monte-Carlo Tree Search} procedure using a context-aware neural mutation model, which is initially pre-trained to learn promising mutations, and further refined from successful experiences in an online fashion. The approach demonstrates state-of-the-art performance on the well-known {SRBench} benchmark.},
  author = {Pierre-Alexandre Kamienny and Guillaume Lample and Sylvain Lamprier and Marco Virgolin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kamienny2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15655--15668},
  pdf = {https://proceedings.mlr.press/v202/kamienny23a/kamienny23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Generative Symbolic Regression with {Monte-Carlo-Tree-Search}},
  url = {https://proceedings.mlr.press/v202/kamienny23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kanai2023onevstherest,
  abstract = {Adversarial training has difficulties, e.g., necessity of high model capacity. Focusing on important data points by weighting cross-entropy loss has attracted much attention. However, they are vulnerable to sophisticated attacks, e.g., {Auto-Attack}. This paper experimentally reveals that the cause of their vulnerability is their small margins between logits for the true label and the other labels. Since neural networks classify the data points based on the logits, logit margins should be large enough to avoid flipping the largest logit by the attacks. Importance-aware methods do not increase logit margins of important samples but decrease those of less-important samples compared with cross-entropy loss. To increase logit margins of important samples, we propose switching one-vs-the-rest loss ({SOVR}), which switches from cross-entropy to one-vs-the-rest loss for important samples that have small logit margins. We prove that one-vs-the-rest loss increases logit margins two times greater than cross-entropy and propose switching between cross-entropy and {OVR} by the criterion of logit margins to improve adversarial robustness.},
  author = {Sekitoshi Kanai and Shin'ya Yamaguchi and Masanori Yamada and Hiroshi Takahashi and Kentaro Ohno and Yasutoshi Ida},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kanai2023onevstherest.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15669--15695},
  pdf = {https://proceedings.mlr.press/v202/kanai23a/kanai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-vs-the-Rest Loss to Focus on Important Samples in Adversarial Training},
  url = {https://proceedings.mlr.press/v202/kanai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kandpal2023large,
  abstract = {The Internet contains a wealth of knowledge---from the birthdays of historical figures to tutorials on how to code---all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this work, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., {TriviaQA}), pre-training corpora (e.g., {ROOTS}), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, today's models must be scaled by many orders of magnitude to reach competitive {QA} performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
  author = {Nikhil Kandpal and Haikang Deng and Adam Roberts and Eric Wallace and Colin Raffel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kandpal2023large.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15696--15707},
  pdf = {https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Large Language Models Struggle to Learn Long-Tail Knowledge},
  url = {https://proceedings.mlr.press/v202/kandpal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kandpal2023gittheta,
  abstract = {Most machine learning models are trained by centralized teams and are rarely updated. In contrast, open-source software development involves the iterative development of a shared artifact through distributed collaboration using a version control system. In the interest of enabling collaborative and continual improvement of machine learning models, we introduce {Git-Theta}, a version control system for machine learning models. {Git-Theta} is an extension to {Git}, the most widely used version control software, that allows fine-grained tracking of changes to model parameters alongside code and other artifacts. Unlike existing version control systems that treat a model checkpoint as a blob of data, {Git-Theta} leverages the structure of checkpoints to support communication-efficient updates, automatic model merges, and meaningful reporting about the difference between two versions of a model. We demonstrate {Git-Theta} through example use-cases of collaborative model development, including continual learning and model patching. We publicly release {Git-Theta} in hopes of kickstarting a new era of collaborative model development.},
  author = {Nikhil Kandpal and Brian Lester and Mohammed Muqeeth and Anisha Mascarenhas and Monty Evans and Vishal Baskaran and Tenghao Huang and Haokun Liu and Colin Raffel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kandpal2023gittheta.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15708--15719},
  pdf = {https://proceedings.mlr.press/v202/kandpal23b/kandpal23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Git-Theta}: A {Git} Extension for Collaborative Development of Machine Learning Models},
  url = {https://proceedings.mlr.press/v202/kandpal23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kaneda2023deep,
  abstract = {We present a novel deep learning approach to approximate the solution of large, sparse, symmetric, positive-definite linear systems of equations. Motivated by the conjugate gradients algorithm that iteratively selects search directions for minimizing the matrix norm of the approximation error, we design an approach that utilizes a deep neural network to accelerate convergence via data-driven improvement of the search direction at each iteration.},
  author = {Ayano Kaneda and Osman Akar and Jingyu Chen and Victoria Alicia Trevino Kala and David Hyde and Joseph Teran},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kaneda2023deep.pdf:pdf},
  mdate = {2024-12-03},
  pages = {15720--15736},
  pdf = {https://proceedings.mlr.press/v202/kaneda23a/kaneda23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Deep Conjugate Direction Method for Iteratively Solving Linear Systems},
  url = {https://proceedings.mlr.press/v202/kaneda23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kang2023leveraging,
  abstract = {We consider test-time adaptation (TTA), the task of adapting a trained model to an arbitrary test domain using unlabeled input data on-the-fly during testing. A common practice of TTA is to disregard data used in training due to large memory demand and privacy leakage. However, the training data are the only source of supervision. This motivates us to investigate a proper way of using them while minimizing the side effects. To this end, we propose two lightweight yet informative proxies of the training data and a TTA method fully exploiting them. One of the proxies is composed of a small number of images synthesized (hence, less privacy-sensitive) by data condensation which minimizes their domain-specificity to capture a general underlying structure over a wide spectrum of domains. Then, in TTA, they are translated into labeled test data by stylizing them to match styles of unlabeled test samples. This enables virtually supervised test-time training. The other proxy is inter-class relations of training data, which are transferred to target model during TTA. On four public benchmarks, our method outperforms the state-of-the-art ones at remarkably less computation and memory.},
  author = {Juwon Kang and Nayeong Kim and Donghyeon Kwon and Jungseul Ok and Suha Kwak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kang2023leveraging.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15737--15752},
  pdf = {https://proceedings.mlr.press/v202/kang23a/kang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Leveraging Proxy of Training Data for Test-Time Adaptation},
  url = {https://proceedings.mlr.press/v202/kang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kang2023beyond,
  abstract = {This study focuses on the topic of offline preference-based reinforcement learning (PbRL), a variant of conventional reinforcement learning that dispenses with the need for online interaction or specification of reward functions. Instead, the agent is provided with fixed offline trajectories and human preferences between pairs of trajectories to extract the dynamics and task information, respectively. Since the dynamics and task information are orthogonal, a naive approach would involve using preference-based reward learning followed by an off-the-shelf offline RL algorithm. However, this requires the separate learning of a scalar reward function, which is assumed to be an information bottleneck of the learning process. To address this issue, the authors propose the offline preference-guided policy optimization (OPPO) paradigm, which models offline trajectories and preferences in a one-step process, eliminating the need for separately learning a reward function. OPPO achieves this by introducing an offline hindsight information matching objective for optimizing a contextual policy and a preference modeling objective for finding the optimal context. OPPO further integrates a well-performing decision policy by optimizing the two objectives iteratively. The empirical results demonstrate that OPPO effectively models offline preferences and outperforms prior competing baselines, including offline RL algorithms performed over either true or pseudo reward function specifications.},
  author = {Yachen Kang and Diyuan Shi and Jinxin Liu and Li He and Donglin Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kang2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15753--15768},
  pdf = {https://proceedings.mlr.press/v202/kang23b/kang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond Reward: Offline Preference-guided Policy Optimization},
  url = {https://proceedings.mlr.press/v202/kang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kang2023poisoning,
  abstract = {Generative models have grown into the workhorse of many state-of-the-art machine learning methods. However, their vulnerability under poisoning attacks has been largely understudied. In this work, we investigate this issue in the context of continual learning, where generative replayers are utilized to tackle catastrophic forgetting. By developing a novel customization of dirty-label input-aware backdoors to the online setting, our attacker manages to stealthily promote forgetting while retaining high accuracy at the current task and sustaining strong defenders. Our approach taps into an intriguing property of generative models, namely that they cannot well capture input-dependent triggers. Experiments on four standard datasets corroborate the poisoner's effectiveness.},
  author = {Siteng Kang and Zhan Shi and Xinhua Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kang2023poisoning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15769--15785},
  pdf = {https://proceedings.mlr.press/v202/kang23c/kang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Poisoning Generative Replay in Continual Learning to Promote Forgetting},
  url = {https://proceedings.mlr.press/v202/kang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kang2023node,
  abstract = {In the graph node embedding problem, embedding spaces can vary significantly for different data types, leading to the need for different GNN model types. In this paper, we model the embedding update of a node feature as a Hamiltonian orbit over time. Since the Hamiltonian orbits generalize the exponential maps, this approach allows us to learn the underlying manifold of the graph in training, in contrast to most of the existing literature that assumes a fixed graph embedding manifold with a closed exponential map solution. Our proposed node embedding strategy can automatically learn, without extensive tuning, the underlying geometry of any given graph dataset even if it has diverse geometries. We test Hamiltonian functions of different forms and verify the performance of our approach on two graph node embedding downstream tasks: node classification and link prediction. Numerical experiments demonstrate that our approach adapts better to different types of graph datasets than popular state-of-the-art graph node embedding GNNs.},
  author = {Qiyu Kang and Kai Zhao and Yang Song and Sijie Wang and Wee Peng Tay},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kang2023node.pdf:pdf},
  mdate = {2024-12-12},
  pages = {15786--15808},
  pdf = {https://proceedings.mlr.press/v202/kang23d/kang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Node Embedding from Neural {H}amiltonian Orbits in Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/kang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{karakida2023understanding,
  abstract = {Gradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. While some studies have reported that GR can improve generalization performance, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve the performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost of GR. Next, we show that the finite-difference computation also works better in the sense of generalization performance. We theoretically analyze a solvable model, a diagonal linear network, and clarify that GR has a desirable implicit bias to so-called rich regime and finite-difference computation strengthens this bias. Furthermore, finite-difference GR is closely related to some other algorithms based on iterative ascent and descent steps for exploring flat minima. In particular, we reveal that the flooding method can perform finite-difference GR in an implicit way. Thus, this work broadens our understanding of GR for both practice and theory.},
  author = {Ryo Karakida and Tomoumi Takase and Tomohiro Hayase and Kazuki Osawa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/karakida2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15809--15827},
  pdf = {https://proceedings.mlr.press/v202/karakida23a/karakida23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias},
  url = {https://proceedings.mlr.press/v202/karakida23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{karbasi2023langevin,
  abstract = {Thompson sampling (TS) is widely used in sequential decision making due to its ease of use and appealing empirical performance. However, many existing analytical and empirical results for TS rely on restrictive assumptions on reward distributions, such as belonging to conjugate families, which limits their applicability in realistic scenarios. Moreover, sequential decision making problems are often carried out in a batched manner, either due to the inherent nature of the problem or to serve the purpose of reducing communication and computation costs. In this work, we jointly study these problems in two popular settings, namely, stochastic multi-armed bandits (MABs) and infinite-horizon reinforcement learning (RL), where TS is used to learn the unknown reward distributions and transition dynamics, respectively. We propose batched Langevin Thompson Sampling algorithms that leverage MCMC methods to sample from approximate posteriors with only logarithmic communication costs in terms of batches. Our algorithms are computationally efficient and maintain the same order-optimal regret guarantees of O(log T) for stochastic MABs, and O(sqrt(T)) for RL. We complement our theoretical findings with experimental results.},
  author = {Amin Karbasi and Nikki Lijing Kuang and Yi-An Ma and Siddharth Mitra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/karbasi2023langevin.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15828--15860},
  pdf = {https://proceedings.mlr.press/v202/karbasi23a/karbasi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Langevin Thompson Sampling with Logarithmic Communication: Bandits and Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/karbasi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{karimi2023relationship,
  abstract = {Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher-performing models -- models that are likely to be deployed. Our work is a promising first step towards providing a quantitative measure of the relationship between E and Y, which could also inform the future development of methods for E with a quantitative metric.},
  author = {Amir-Hossein Karimi and Krikamol Muandet and Simon Kornblith and Bernhard Sch{\"o}lkopf and Been Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/karimi2023relationship.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15861--15883},
  pdf = {https://proceedings.mlr.press/v202/karimi23a/karimi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Relationship Between Explanation and Prediction: A Causal View},
  url = {https://proceedings.mlr.press/v202/karimi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kariyappa2023cocktail,
  abstract = {Federated learning (FL) aims to perform privacy-preserving machine learning on distributed data held by multiple data owners. To this end, FL requires the data owners to perform training locally and share the gradients or weight updates (instead of the private inputs) with the central server, which are then securely aggregated over multiple data owners. Although aggregation by itself does not offer provable privacy protection, prior work suggested that if the batch size is sufficiently large the aggregation may be secure enough. In this paper, we propose the Cocktail Party Attack (CPA) that, contrary to prior belief, is able to recover the private inputs from gradients/weight updates aggregated over as many as 1024 samples. CPA leverages the crucial insight that aggregate gradients from a fully connected (FC) layer is a linear combination of its inputs, which allows us to frame gradient inversion as a blind source separation (BSS) problem. We adapt independent component analysis (ICA)---a classic solution to the BSS problem---to recover private inputs for FC and convolutional networks, and show that CPA significantly outperforms prior gradient inversion attacks, scales to ImageNet-sized inputs, and works on large batch sizes of up to 1024.},
  author = {Sanjay Kariyappa and Chuan Guo and Kiwan Maeng and Wenjie Xiong and G. Edward Suh and Moinuddin K. Qureshi and Hsien-Hsin S. Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kariyappa2023cocktail.pdf:pdf},
  mdate = {2024-10-02},
  pages = {15884--15899},
  pdf = {https://proceedings.mlr.press/v202/kariyappa23a/kariyappa23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cocktail Party Attack: Breaking Aggregation-Based Privacy in Federated Learning Using Independent Component Analysis},
  url = {https://proceedings.mlr.press/v202/kariyappa23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{karuvally2023general,
  abstract = {The state-of-the-art memory model is the General Associative Memory Model, a generalization of the classical Hopfield network. Like its ancestor, the general associative memory has a well-defined state-dependant energy surface, and its memories correlate with its fixed points. This is unlike human memories, which are commonly sequential rather than separated fixed points. In this paper, we introduce a class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit a dynamic energy surface, leading to a series of meta-stable states capable of encoding memory sequences. A multiple-timescale architecture enables the dynamic nature of the energy surface with newly introduced asymmetric synapses and signal propagation delays.},
  author = {Arjun Karuvally and Terrence J. Sejnowski and Hava T. Siegelmann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/karuvally2023general.pdf:pdf},
  mdate = {2023-09-30},
  pages = {15900--15910},
  pdf = {https://proceedings.mlr.press/v202/karuvally23a/karuvally23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {General Sequential Episodic Memory Model},
  url = {https://proceedings.mlr.press/v202/karuvally23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{katsuki2023regression,
  abstract = {This paper addresses a regression problem in which output label values are the results of sensing the magnitude of a phenomenon. A low value of such labels can mean either that the actual magnitude of the phenomenon was low or that the sensor made an incomplete observation. This leads to a bias toward lower values in labels and the resultant learning because labels may have lower values due to incomplete observations, even if the actual magnitude of the phenomenon was high. Moreover, because an incomplete observation does not provide any tags indicating incompleteness, we cannot eliminate or impute them. To address this issue, we propose a learning algorithm that explicitly models incomplete observations corrupted with an asymmetric noise that always has a negative value.},
  author = {Takayuki Katsuki and Takayuki Osogami},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/katsuki2023regression.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15911--15927},
  pdf = {https://proceedings.mlr.press/v202/katsuki23a/katsuki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regression with Sensor Data Containing Incomplete Observations},
  url = {https://proceedings.mlr.press/v202/katsuki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kaufman2023data,
  abstract = {Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. The intrinsic dimension of latent codes is not necessarily indicative of curvature.},
  author = {Ilya Kaufman and Omri Azencot},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kaufman2023data.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15928--15945},
  pdf = {https://proceedings.mlr.press/v202/kaufman23a/kaufman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data Representations' Study of Latent Image Manifolds},
  url = {https://proceedings.mlr.press/v202/kaufman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kaul2023multimodal,
  abstract = {The goal of this paper is open-vocabulary object detection (OVOD) — building a model that can detect objects beyond the set of categories seen at training, thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture, and explore three ways for specifying novel categories: via language descriptions, via image exemplars, or via a combination of the two.},
  author = {Prannay Kaul and Weidi Xie and Andrew Zisserman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kaul2023multimodal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {15946--15969},
  pdf = {https://proceedings.mlr.press/v202/kaul23a/kaul23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Modal Classifiers for Open-Vocabulary Object Detection},
  url = {https://proceedings.mlr.press/v202/kaul23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kausik2023learning,
  abstract = {We present an algorithm for learning mixtures of Markov chains and Markov decision processes (MDPs) from short unlabeled trajectories. Specifically, our method handles mixtures of Markov chains with optional control input by going through a multi-step process, involving (1) a subspace estimation step, (2) spectral clustering of trajectories using pairwise distance estimators, along with refinement using the EM algorithm, (3) a model estimation step, and (4) a classification step for predicting labels of new trajectories. We provide end-to-end performance guarantees, where we only explicitly require the length of trajectories to be linear in the number of states and the number of trajectories to be linear in a mixing time parameter. Experimental results support these guarantees, where we attain 96.6% average accuracy on a mixture of two MDPs in gridworld, outperforming the EM algorithm with random initialization (73.2% average accuracy).},
  author = {Chinmaya Kausik and Kevin Tan and Ambuj Tewari},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kausik2023learning.pdf:pdf},
  mdate = {2024-10-06},
  pages = {15970--16017},
  pdf = {https://proceedings.mlr.press/v202/kausik23a/kausik23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Mixtures of Markov Chains and {MDP}s},
  url = {https://proceedings.mlr.press/v202/kausik23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kauvar2023curious,
  abstract = {Agents must be able to adapt quickly as an environment changes. We find that existing model-based reinforcement learning agents are unable to do this well, in part because of how they use past experiences to train their world model. Here, we present Curious Replay -- a form of prioritized experience replay tailored to model-based agents through use of a curiosity-based priority signal. Agents using Curious Replay exhibit improved performance in an exploration paradigm inspired by animal behavior and on the Crafter benchmark. DreamerV3 with Curious Replay surpasses state-of-the-art performance on Crafter, achieving a mean score of 19.4 that substantially improves on the previous high score of 14.5 by DreamerV3 with uniform replay, while also maintaining similar performance on the Deepmind Control Suite.},
  author = {Isaac Kauvar and Chris Doyle and Linqi Zhou and Nick Haber},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kauvar2023curious.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16018--16048},
  pdf = {https://proceedings.mlr.press/v202/kauvar23a/kauvar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Curious Replay for Model-based Adaptation},
  url = {https://proceedings.mlr.press/v202/kauvar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kawaguchi2023how,
  abstract = {Numerous deep learning algorithms have been inspired by and understood via the notion of information bottleneck, where unnecessary information is (often implicitly) minimized while task-relevant information is maximized. However, a rigorous argument for justifying why it is desirable to control information bottlenecks has been elusive. In this paper, we provide the first rigorous learning theory for justifying the benefit of information bottleneck in deep learning by mathematically relating information bottleneck to generalization errors. Our theory proves that controlling information bottleneck is one way to control generalization errors in deep learning, although it is not the only or necessary way. We investigate the merit of our new mathematical findings with experiments across a range of architectures and learning settings.},
  author = {Kenji Kawaguchi and Zhun Deng and Xu Ji and Jiaoyang Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kawaguchi2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16049--16096},
  pdf = {https://proceedings.mlr.press/v202/kawaguchi23a/kawaguchi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Does Information Bottleneck Help Deep Learning?},
  url = {https://proceedings.mlr.press/v202/kawaguchi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kawakami2023instrumental,
  abstract = {Instrumental variable (IV) analysis is a powerful tool widely used to elucidate causal relationships. We study the problem of estimating the average partial causal effect (APCE) of a continuous treatment in an IV setting. Specifically, we develop new methods for estimating APCE based on a recent identification condition via an integral equation. We develop two families of methods, nonparametric and parametric - the former uses the Picard iteration to solve the integral equation; the latter parameterizes APCE using a linear basis function model. We analyze the statistical and computational properties of the proposed APCE estimators and illustrate them on synthetic and real-world data.},
  author = {Yuta Kawakami and Manabu Kuroki and Jin Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kawakami2023instrumental.pdf:pdf},
  mdate = {2024-10-06},
  pages = {16097--16130},
  pdf = {https://proceedings.mlr.press/v202/kawakami23a/kawakami23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Instrumental Variable Estimation of Average Partial Causal Effects},
  url = {https://proceedings.mlr.press/v202/kawakami23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kazan2023test,
  abstract = {We present a generic framework for creating differentially private versions of any hypothesis test in a black-box way. We analyze the resulting tests analytically and experimentally. Most crucially, we show good practical performance for small data sets, demonstrating that at ε = 1 we only need 5-6 times as much data as in the fully public setting. We compare our work to the one existing framework of this type, as well as to several individually-designed private hypothesis tests.},
  author = {Zeki Kazan and Kaiyan Shi and Adam Groce and Andrew P. Bray},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kazan2023test.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16131--16151},
  pdf = {https://proceedings.mlr.press/v202/kazan23a/kazan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Test of Tests: A Framework for Differentially Private Hypothesis Testing},
  url = {https://proceedings.mlr.press/v202/kazan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ke2023exact,
  abstract = {In this paper, we study the problem of inference in high-order structured prediction tasks. In the context of Markov random fields, the goal of a high-order inference task is to maximize a score function on the space of labels, and the score function can be decomposed into sum of unary and high-order potentials. We apply a generative model approach to study the problem of high-order inference, and provide a two-stage convex optimization algorithm for exact label recovery.},
  author = {Chuyang Ke and Jean Honorio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ke2023exact.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16152--16167},
  pdf = {https://proceedings.mlr.press/v202/ke23a/ke23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exact Inference in High-order Structured Prediction},
  url = {https://proceedings.mlr.press/v202/ke23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{keller2023neural,
  abstract = {Traveling waves have been measured at a diversity of regions and scales in the brain, however a consensus as to their computational purpose has yet to be reached. An intriguing hypothesis is that traveling waves serve to structure neural representations both in space and time, thereby acting as an inductive bias towards natural data. In this work, we investigate this hypothesis by introducing the Neural Wave Machine (NWM) -- a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state. After training on simple dynamic sequences, we show that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structure such as traveling waves to encode observed transformations. To measure the computational implications of this structure, we use a suite of sequence classification and physical dynamics modeling tasks to show that the NWM is both more parameter efficient, and is able to forecast future trajectories of simple physical dynamical systems more accurately than existing state of the art counterparts.},
  author = {T. Anderson Keller and Max Welling},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/keller2023neural.pdf:pdf},
  mdate = {2024-11-14},
  pages = {16168--16189},
  pdf = {https://proceedings.mlr.press/v202/keller23a/keller23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks},
  url = {https://proceedings.mlr.press/v202/keller23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{keurti2023homomorphism,
  abstract = {How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. The method is motivated theoretically, and empirically shown to learn a group representation of the actions, thereby capturing the structure of the set of transformations applied to the environment. We further demonstrate that this allows agents to predict the effect of sequences of future actions with improved accuracy.},
  author = {Hamza Keurti and Hsiao-Ru Pan and Michel Besserve and Benjamin F. Grewe and Bernhard Sch{\"o}lkopf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/keurti2023homomorphism.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16190--16215},
  pdf = {https://proceedings.mlr.press/v202/keurti23a/keurti23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Homomorphism {AutoEncoder} -- Learning Group Structured Representations from Observed Transitions},
  url = {https://proceedings.mlr.press/v202/keurti23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khaddaj2023rethinking,
  abstract = {In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them. In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to detect in a general sense. We then revisit existing defenses against backdoor attacks and characterize the often latent assumptions they make and on which they depend.},
  author = {Alaa Khaddaj and Guillaume Leclerc and Aleksandar Makelov and Kristian Georgiev and Hadi Salman and Andrew Ilyas and Aleksander Madry},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khaddaj2023rethinking.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16216--16236},
  pdf = {https://proceedings.mlr.press/v202/khaddaj23a/khaddaj23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethinking Backdoor Attacks},
  url = {https://proceedings.mlr.press/v202/khaddaj23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khakhar2023pac,
  abstract = {Prediction sets have recently been shown to be a promising strategy for quantifying the uncertainty of deep neural networks in a way that provides theoretical guarantees. However, existing techniques have largely targeted settings where the space of labels is simple, so prediction sets can be arbitrary subsets of labels. For structured prediction problems where the space of labels is exponential in size, even prediction sets containing a small fraction of all labels can be exponentially large. In the context of code generation, we propose a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes. Given a trained code generation model, our algorithm leverages a programming language's abstract syntax tree to generate a set of programs such that the correct program is in the set with high-confidence. Valuable applications of our algorithm include a Codex-style code generator with holes in uncertain parts of the generated code, which provides a partial program with theoretical guarantees. We evaluate our approach on PICARD (a T5 model for SQL semantic parsing) and Codex (a GPT model for over a dozen programming languages, including Python), demonstrating that our approach generates compact PAC prediction sets. This is the first research contribution that generates PAC prediction sets for generative code models.},
  author = {Adam Khakhar and Stephen Mell and Osbert Bastani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khakhar2023pac.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16237--16249},
  pdf = {https://proceedings.mlr.press/v202/khakhar23a/khakhar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PAC} Prediction Sets for Large Language Models of Code},
  url = {https://proceedings.mlr.press/v202/khakhar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khalafi2023accelerated,
  abstract = {We investigate a primal-dual (PD) method for the saddle point problem (SPP) that uses a linear approximation of the primal function instead of the standard proximal step, resulting in a linearized PD (LPD) method. For convex-strongly concave SPP, we observe that the LPD method has a suboptimal dependence on the Lipschitz constant of the primal function. To fix this issue, we combine features of Accelerated Gradient Descent with the LPD method resulting in a single-loop Accelerated Linearized Primal-Dual (ALPD) method. The ALPD method achieves the optimal gradient complexity when the SPP has a semi-linear coupling function. We also present an inexact ALPD method for SPPs with a general nonlinear coupling function that maintains the optimal gradient evaluations of the primal parts and significantly improves the gradient evaluations of the coupling term compared to the ALPD method. We verify our findings with numerical experiments.},
  author = {Mohammad Khalafi and Digvijay Boob},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khalafi2023accelerated.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16250--16270},
  pdf = {https://proceedings.mlr.press/v202/khalafi23a/khalafi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point Problems},
  url = {https://proceedings.mlr.press/v202/khalafi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khalili2023loss,
  abstract = {Supervised learning models used in various domains such as lending, college admission, face recognition, and natural language processing may inherit pre-existing biases from training data and exhibit discrimination against protected social groups. In this paper, we focus on Equalized Loss (EL), a fairness notion that requires the expected loss to be approximately equalized across different groups. However, imposing EL on the learning process leads to a non-convex optimization problem even if the loss function is convex, and existing fair learning algorithms cannot properly be adopted to find the fair predictor under the EL constraint. We propose the ELminimizer algorithm to find fair predictors by converting the non-convex optimization problem into a sequence of convex optimization problems that can be solved efficiently.},
  author = {Mohammad Mahdi Khalili and Xueru Zhang and Mahed Abroshan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khalili2023loss.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16271--16290},
  pdf = {https://proceedings.mlr.press/v202/khalili23a/khalili23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Loss Balancing for Fair Supervised Learning},
  url = {https://proceedings.mlr.press/v202/khalili23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khanduri2023linearly,
  abstract = {This work develops analysis and algorithms for solving a class of bilevel optimization problems where the lower-level (LL) problems have linear constraints. Most of the existing approaches for constrained bilevel problems rely on value function-based approximate reformulations, which suffer from issues such as non-convex and non-differentiable constraints. In contrast, in this work, we develop an implicit gradient-based approach, which is easy to implement, and is suitable for machine learning applications. We first provide an in-depth understanding of the problem, by showing that the implicit objective for such problems is in general non-differentiable. However, if we add some small (linear) perturbation to the LL objective, the resulting implicit objective becomes differentiable almost surely. This key observation opens the door for developing (deterministic and stochastic) gradient-based algorithms similar to the state-of-the-art ones for unconstrained bi-level problems.},
  author = {Prashant Khanduri and Ioannis C. Tsaknakis and Yihua Zhang and Jia Liu and Sijia Liu and Jiawei Zhang and Mingyi Hong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khanduri2023linearly.pdf:pdf},
  mdate = {2024-08-27},
  pages = {16291--16325},
  pdf = {https://proceedings.mlr.press/v202/khanduri23a/khanduri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linearly Constrained Bilevel Optimization: A Smoothed Implicit Gradient Approach},
  url = {https://proceedings.mlr.press/v202/khanduri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khayatkhoei2023emergent,
  abstract = {Precision and Recall are prominent metrics for generative performance that separately measure fidelity and diversity of generative models. In this work, we identify a critical flaw in the common approximation of precision and recall metrics using k-nearest-neighbors, showing that the interpretations of fidelity and diversity assigned to Precision and Recall can fail in high dimensions, resulting in misleading conclusions. We empirically and theoretically demonstrate that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions.},
  author = {Mahyar Khayatkhoei and Wael Abd-Almageed},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khayatkhoei2023emergent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16326--16343},
  pdf = {https://proceedings.mlr.press/v202/khayatkhoei23a/khayatkhoei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions},
  url = {https://proceedings.mlr.press/v202/khayatkhoei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{khodak2023learningaugmented,
  abstract = {When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework--previously applied largely to improve time complexity or competitive ratios--as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. We apply this framework to the important task of releasing multiple quantiles of a dataset, and propose learning-augmented algorithms for this task.},
  author = {Mikhail Khodak and Kareem Amin and Travis Dick and Sergei Vassilvitskii},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/khodak2023learningaugmented.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16344--16376},
  pdf = {https://proceedings.mlr.press/v202/khodak23a/khodak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning-augmented private algorithms for multiple quantile release},
  url = {https://proceedings.mlr.press/v202/khodak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023crosssplit,
  abstract = {We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labeled dataset. CrossSplit combines two main ingredients: cross-split label correction, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; and cross-split semi-supervised training, where a network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios.},
  author = {Jihye Kim and Aristide Baratin and Yan Zhang and Simon Lacoste-Julien},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023crosssplit.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16377--16392},
  pdf = {https://proceedings.mlr.press/v202/kim23a/kim23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CrossSplit}: Mitigating Label Noise Memorization through Data Splitting},
  url = {https://proceedings.mlr.press/v202/kim23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023saal,
  abstract = {While deep neural networks play significant roles in many research areas, they are also prone to overfitting problems under limited data instances. To overcome overfitting, this paper introduces the first active learning method to incorporate the sharpness of loss space into the acquisition function. We propose Sharpness-Aware Active Learning (SAAL), which incorporates sharpness as an uncertainty measure to identify informative unlabeled instances. Based on empirical analysis, we identified that the sharpness of the loss surface serves as a complementary yet distinct measure from conventional active learning acquisition functions.},
  author = {Yoon-Yeong Kim and Youngjae Cho and JoonHo Jang and Byeonghu Na and Yeongmin Kim and Kyungwoo Song and Wanmo Kang and Il-Chul Moon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023saal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16424--16440},
  pdf = {https://proceedings.mlr.press/v202/kim23c/kim23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SAAL}: Sharpness-Aware Active Learning},
  url = {https://proceedings.mlr.press/v202/kim23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023trainability,
  abstract = {Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (n{ODE}s) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend n{ODE}s by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural {ODE}s (gn{ODE}s). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gn{ODE}s to learn (approximate) continuous attractors. We further show how reduced-dimensional gn{ODE}s retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attractors. We introduce a novel measure of expressivity which probes the capacity of a neural network to generate complex trajectories. Using this measure, we explore how the phase-space dimension of the n{ODE}s and the complexity of the function modeling the flow field contribute to expressivity. We see that a more complex function for modeling the flow field allows a lower-dimensional n{ODE} to capture a given target dynamics. Finally, we demonstrate the benefit of gating in n{ODE}s on several real-world tasks.},
  author = {Timothy Doyeon Kim and Tankut Can and Kamesh Krishnamurthy},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023trainability.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16393--16423},
  pdf = {https://proceedings.mlr.press/v202/kim23b/kim23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trainability, Expressivity and Interpretability in Gated Neural {ODEs}},
  url = {https://proceedings.mlr.press/v202/kim23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023demonstrationfree,
  abstract = {While reinforcement learning ({RL}) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous {RL} ({ARL}) methods that are capable of learning from non-episodic interactions. However, existing works on {ARL} are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free {ARL} algorithm via Implicit and Bi-directional Curriculum ({IBC}). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage demonstrations.},
  author = {Jigang Kim and Daesol Cho and H. Jin Kim},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023demonstrationfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16441--16457},
  pdf = {https://proceedings.mlr.press/v202/kim23d/kim23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum},
  url = {https://proceedings.mlr.press/v202/kim23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023improved,
  abstract = {We consider the linear contextual multi-class multi-period packing problem ({LMMP}) where the goal is to pack items such that the total vector of consumption is below a given budget vector and the total value is as large as possible. We consider the setting where the reward and the consumption vector associated with each action is a class-dependent linear function of the context, and the decision-maker receives bandit feedback. {LMMP} includes linear contextual bandits with knapsacks and online revenue management as special cases. We establish a new estimator which guarantees a faster convergence rate, and consequently, a lower regret in such problems. We propose a bandit policy that is a closed-form function of said estimated parameters. When the contexts are non-degenerate, the regret of the proposed policy is sublinear in the context dimension, the number of classes, and the time horizon $T$ when the budget grows at least as $\sqrt{T}$. We also resolve an open problem posed by Agrawal \& Devanur (2016) and extend the result to a multi-class setting. Our numerical experiments clearly demonstrate that the performance of our policy is superior to other benchmarks in the literature.},
  author = {Wonyoung Kim and Garud Iyengar and Assaf Zeevi},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16458--16501},
  pdf = {https://proceedings.mlr.press/v202/kim23e/kim23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Algorithms for Multi-period Multi-class Packing Problems with Bandit Feedback},
  url = {https://proceedings.mlr.press/v202/kim23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023efficient,
  abstract = {Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is {NP}-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by {TensorRT} for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in {MobileNetV2} on the {ImageNet} dataset. Specifically, we achieve $1.41\times$ speed-up with $0.11$\%p accuracy gain in {MobileNetV2}-1.0 on the {ImageNet}.},
  author = {Jinuk Kim and Yeonwoo Jeong and Deokjae Lee and Hyun Oh Song},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16502--16520},
  pdf = {https://proceedings.mlr.press/v202/kim23f/kim23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Latency-Aware {CNN} Depth Compression via Two-Stage Dynamic Programming},
  url = {https://proceedings.mlr.press/v202/kim23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023probabilistic,
  abstract = {Interpretable models are designed to make decisions in a human-interpretable manner. Representatively, Concept Bottleneck Models ({CBM}) follow a two-step process of concept prediction and class prediction based on the predicted concepts. {CBM} provides explanations with high-level concepts derived from concept predictions; thus, reliable concept predictions are important for trustworthiness. In this study, we address the ambiguity issue that can harm reliability. While the existence of a concept can often be ambiguous in the data, {CBM} predicts concepts deterministically without considering this ambiguity. To provide a reliable interpretation against this ambiguity, we propose Probabilistic Concept Bottleneck Models ({ProbCBM}). By leveraging probabilistic concept embeddings, {ProbCBM} models uncertainty in concept prediction and provides explanations based on the concept and its corresponding uncertainty. This uncertainty enhances the reliability of the explanations. Furthermore, as class uncertainty is derived from concept uncertainty in {ProbCBM}, we can explain class uncertainty by means of concept uncertainty.},
  author = {Eunji Kim 0002 and Dahuin Jung and Sangha Park and Siwon Kim and Sungroh Yoon},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023probabilistic.pdf:pdf},
  mdate = {2025-05-26},
  pages = {16521--16540},
  pdf = {https://proceedings.mlr.press/v202/kim23g/kim23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Concept Bottleneck Models},
  url = {https://proceedings.mlr.press/v202/kim23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023devformer,
  abstract = {In this paper, we present {DevFormer}, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply {DevFormer} to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than 30\%. Finally, we show that our approach achieves promising results in other offline contextual learning-based combinatorial optimization tasks.},
  author = {Haeyeon Kim and Minsu Kim 0004 and Federico Berto and Joungho Kim and Jinkyoo Park},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023devformer.pdf:pdf},
  mdate = {2024-11-29},
  pages = {16541--16566},
  pdf = {https://proceedings.mlr.press/v202/kim23h/kim23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DevFormer}: A Symmetric Transformer for Context-Aware Device Placement},
  url = {https://proceedings.mlr.press/v202/kim23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023refining,
  abstract = {The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike {GAN}s, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achieve state-of-the-art results on {ImageNet} 256x256 with {FID} 1.83 and recall 0.64, similar to the validation data's {FID} (1.68) and recall (0.66).},
  author = {Dongjun Kim and Yeongmin Kim and Se Jung Kwon and Wanmo Kang and Il-Chul Moon},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023refining.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16567--16598},
  pdf = {https://proceedings.mlr.press/v202/kim23i/kim23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models},
  url = {https://proceedings.mlr.press/v202/kim23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023robust,
  abstract = {The design of codes for feedback-enabled communications has been a long-standing open problem. Recent research on non-linear, deep learning-based coding schemes have demonstrated significant improvements in communication reliability over linear codes, but are still vulnerable to the presence of forward and feedback noise over the channel. In this paper, we develop a new family of non-linear feedback codes that greatly enhance robustness to channel noise. Our autoencoder-based architecture is designed to learn codes based on consecutive blocks of bits, which obtains de-noising advantages over bit-by-bit processing to help overcome the physical separation between the encoder and decoder over a noisy channel. Moreover, we develop a power control layer at the encoder to explicitly incorporate hardware constraints into the learning optimization, and prove that the resulting average power constraint is satisfied asymptotically. Numerical experiments demonstrate that our scheme outperforms state-of-the-art feedback codes by wide margins over practical forward and feedback noise regimes, and provide information-theoretic insights on the behavior of our non-linear codes. Moreover, we observe that, in a long blocklength regime, canonical error correction codes are still preferable to feedback codes when the feedback noise becomes high.},
  author = {Junghoon Kim and Taejoon Kim and David J. Love and Christopher G. Brinton},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16599--16618},
  pdf = {https://proceedings.mlr.press/v202/kim23j/kim23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Non-Linear Feedback Coding via Power-Constrained Deep Learning},
  url = {https://proceedings.mlr.press/v202/kim23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023lesson,
  abstract = {In this paper, a unified framework for exploration in reinforcement learning ({RL}) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the {MiniGrid} and {Atari} environments.},
  author = {Woojun Kim and Jeonghye Kim and Youngchul Sung},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023lesson.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16619--16638},
  pdf = {https://proceedings.mlr.press/v202/kim23k/kim23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LESSON}: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework},
  url = {https://proceedings.mlr.press/v202/kim23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023bpipe,
  abstract = {Pipeline parallelism is a key technique for training large language models within {GPU} clusters. However, it often leads to a memory imbalance problem, where certain {GPU}s face high memory pressure while others underutilize their capacity. This imbalance results in suboptimal training performance, even when the overall {GPU} memory capacity is sufficient for more efficient setups. To address this problem, we propose {BPipe}, a novel approach for achieving memory balance in pipeline parallelism. {BPipe} employs an activation balancing method to transfer intermediate activations between {GPU}s during training, enabling all {GPU}s to utilize comparable amounts of memory. With balanced memory utilization, {BPipe} enhances the training efficiency of large language models by eliminating redundant recomputations or increasing the micro-batch size. Our evaluation on 48 A100 {GPU}s across six nodes interconnected with {HDR} {InfiniBand} shows that {BPipe} accelerates the training of {GPT}-3 96B and {GPT}-3 134B models by 1.25x--2.17x compared to {Megatron}-{LM}, a state-of-the-art framework for training large language models.},
  author = {Taebum Kim and Hyoungjoo Kim and Gyeong-In Yu and Byung-Gon Chun},
  booktitle = {40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023bpipe.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16639--16653},
  pdf = {https://proceedings.mlr.press/v202/kim23l/kim23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{BPipe}: Memory-Balanced Pipeline Parallelism for Training Large Language Models},
  url = {https://proceedings.mlr.press/v202/kim23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023probabilistic,
  abstract = {Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that naïvely combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.},
  author = {Seunghyun Kim and Hyunsu Kim and Eunggu Yun and Hwangrae Lee and Jaehun Lee and Juho Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023probabilistic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16654--16667},
  pdf = {https://proceedings.mlr.press/v202/kim23m/kim23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Imputation for Time-series Classification with Missing Data},
  url = {https://proceedings.mlr.press/v202/kim23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023variational,
  abstract = {Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.},
  author = {Seongun Kim and Kyowoon Lee and Jaesik Choi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023variational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16668--16695},
  pdf = {https://proceedings.mlr.press/v202/kim23n/kim23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills},
  url = {https://proceedings.mlr.press/v202/kim23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023marginbased,
  abstract = {As Machine Learning as a Service (MLaaS) platforms become prevalent, deep neural network (DNN) watermarking techniques are gaining increasing attention, which enables one to verify the ownership of a target DNN model in a black-box scenario. Unfortunately, previous watermarking methods are vulnerable to functionality stealing attacks, thus allowing an adversary to falsely claim the ownership of a DNN model stolen from its original owner. In this work, we propose a novel margin-based DNN watermarking approach that is robust to the functionality stealing attacks based on model extraction and distillation. Specifically, during training, our method maximizes the margins of watermarked samples by using projected gradient ascent on them so that their predicted labels cannot change without compromising the accuracy of the model that the attacker tries to steal.},
  author = {Byungjoo Kim and Suyoung Lee and Seanie Lee and Sooel Son and Sung Ju Hwang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023marginbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16696--16711},
  pdf = {https://proceedings.mlr.press/v202/kim23o/kim23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Margin-based Neural Network Watermarking},
  url = {https://proceedings.mlr.press/v202/kim23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023regularizing,
  abstract = {Datasets often have their intrinsic symmetries, and particular deep-learning models called equivariant or invariant models have been developed to exploit these symmetries. However, if some or all of these symmetries are only approximate, which frequently happens in practice, these models may be suboptimal due to the architectural restrictions imposed on them. We tackle this issue of approximate symmetries in a setup where symmetries are mixed, i.e., they are symmetries of not single but multiple different types and the degree of approximation varies across these types. Instead of proposing a new architectural restriction as in most of the previous approaches, we present a regularizer-based method for building a model for a dataset with mixed approximate symmetries. The key component of our method is what we call equivariance regularizer for a given type of symmetries, which measures how much a model is equivariant with respect to the symmetries of the type. Our method is trained with these regularizers, one per each symmetry type, and the strength of the regularizers is automatically tuned during training, leading to the discovery of the approximation levels of some candidate symmetry types without explicit supervision. Using synthetic function approximation and motion forecasting tasks, we demonstrate that our method achieves better accuracy than prior approaches while discovering the approximate symmetry levels correctly.},
  author = {Hyunsu Kim and Hyungi Lee and Hongseok Yang and Juho Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023regularizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16712--16727},
  pdf = {https://proceedings.mlr.press/v202/kim23p/kim23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regularizing Towards Soft Equivariance Under Mixed Symmetries},
  url = {https://proceedings.mlr.press/v202/kim23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023modelbased,
  abstract = {In this paper, we present a model-based offline reinforcement learning method that integrates count-based conservatism, named Count-MORL. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that Count-MORL with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets.},
  author = {Byeongchan Kim and Min-hwan Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023modelbased.pdf:pdf},
  mdate = {2024-09-30},
  pages = {16728--16746},
  pdf = {https://proceedings.mlr.press/v202/kim23q/kim23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-based Offline Reinforcement Learning with Count-based Conservatism},
  url = {https://proceedings.mlr.press/v202/kim23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023transformerbased,
  abstract = {Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems.},
  author = {Chanyeong Kim and Jongwoong Park and Hyunglip Bae and Woo Chang Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023transformerbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16747--16770},
  pdf = {https://proceedings.mlr.press/v202/kim23r/kim23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization},
  url = {https://proceedings.mlr.press/v202/kim23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023surprogenes,
  abstract = {Identifying prognostic genes associated with patient survival is an important goal in cancer genomics, as this information could inform treatment approaches and improve patient outcomes. However, the identification of prognostic genes is complicated by the high dimensionality of genetic data, which makes their identification computationally intensive. Furthermore, most cancer genomics studies lack appropriate low-risk groups against which to compare. To address these issues, we present a framework that identifies candidate prognostic genes by integrating representation learning and statistical analysis approaches. Specifically, we propose a collaborative filtering-derived mechanism to represent patients in order of their survival risk, facilitating their dichotomization. We also propose a mechanism that allows embedded gene vectors to be polarized on the extremities of, or centered on, both reference axes to facilitate recommendations. Restricting our analysis to a few representative genes within each cluster allowed for the efficient identification of prognostic genes. Finally, we demonstrate the potential of this proposed framework for identifying prognostic genes.},
  author = {Junetae Kim and Kyoungsuk Park and Hanseok Jeong and Youngwook Kim and Jeongseon Kim and Sun-Young Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023surprogenes.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16771--16786},
  pdf = {https://proceedings.mlr.press/v202/kim23s/kim23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SurProGenes}: Survival Risk-Ordered Representation of Cancer Patients and Genes for the Identification of Prognostic Genes},
  url = {https://proceedings.mlr.press/v202/kim23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023stable,
  abstract = {Learning to predict reliable characteristic orientations of 3D point clouds is an important yet challenging problem, as different point clouds of the same class may have largely varying appearances. In this work, we introduce a novel method to decouple the shape geometry and semantics of the input point cloud to achieve both stability and consistency. The proposed method integrates shape-geometry-based SO(3)-equivariant learning and shape-semantics-based SO(3)-invariant residual learning, where a final characteristic orientation is obtained by calibrating an SO(3)-equivariant orientation hypothesis using an SO(3)-invariant residual rotation. In experiments, the proposed method not only demonstrates superior stability and consistency but also exhibits state-of-the-art performances when applied to point cloud part segmentation, given randomly rotated inputs.},
  author = {Seungwook Kim and Chunghyun Park and Yoonwoo Jeong and Jaesik Park and Minsu Cho},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023stable.pdf:pdf},
  mdate = {2024-06-03},
  pages = {16787--16806},
  pdf = {https://proceedings.mlr.press/v202/kim23t/kim23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stable and Consistent Prediction of {3D} Characteristic Orientation via Invariant Residual Learning},
  url = {https://proceedings.mlr.press/v202/kim23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023adaptive,
  abstract = {In this paper, we propose an adaptive entropy-regularization framework (ADER) for multi-agent reinforcement learning (RL) to learn the adequate amount of exploration of each agent for entropy-based exploration. In order to derive a metric for the proper level of exploration entropy for each agent, we disentangle the soft value function into two types: one for pure return and the other for entropy. By applying multi-agent value factorization to the disentangled value function of pure return, we obtain a metric to determine the relevant level of exploration entropy for each agent, given by the partial derivative of the pure-return value function with respect to (w.r.t.) the policy entropy of each agent. Based on this metric, we propose the ADER algorithm based on maximum entropy RL, which controls the necessary level of exploration across agents over time by learning the proper target entropy levels.},
  author = {Woojun Kim and Youngchul Sung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16829--16852},
  pdf = {https://proceedings.mlr.press/v202/kim23v/kim23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Adaptive Entropy-Regularization Framework for Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/kim23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023prefer,
  abstract = {Recent instruction-following datasets and evaluations have made great strides in improving language model performance on helpfulness. However, collecting new additional input-output pairs is often too costly and challenging, particularly considering their marginal impact on improving the current model accuracy. Instead, additional or complementary annotations on the existing input texts in the benchmarks can be preferable as an efficient way to pay the additional human cost. Motivated by this, we propose P2C (Prefer to Classify), a multi-task learning framework that jointly learns (1) a target task (e.g., sentiment classification) and (2) a preference between two samples for the given task label (e.g., positive or negative). From the pair-wise comparison, P2C captures the finer task information which can't be captured in a sample-wise evaluation. Our experimental results on six text classification datasets show that P2C outperforms the baseline methods and achieves comparable performance with methods using 2x or 5x more annotations.},
  author = {Jaehyung Kim and Jinwoo Shin and Dongyeop Kang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023prefer.pdf:pdf},
  mdate = {2024-08-15},
  pages = {16807--16828},
  pdf = {https://proceedings.mlr.press/v202/kim23u/kim23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning},
  url = {https://proceedings.mlr.press/v202/kim23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023practical,
  abstract = {Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the ABC condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.},
  author = {Kyurae Kim and Kaiwen Wu and Jisu Oh and Jacob R. Gardner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023practical.pdf:pdf},
  mdate = {2024-02-05},
  pages = {16853--16876},
  pdf = {https://proceedings.mlr.press/v202/kim23w/kim23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference},
  url = {https://proceedings.mlr.press/v202/kim23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023learnability,
  abstract = {This paper studies the challenging continual learning (CL) setting of Class Incremental Learning (CIL). CIL learns a sequence of tasks consisting of disjoint sets of concepts or classes. At any time, a single model is built that can be applied to predict/classify test instances of any classes learned thus far without providing any task related information for each test instance. While it was still not known whether CIL is actually learnable, this paper shows that CIL is learnable. Based on the theory, a new CIL algorithm is also proposed.},
  author = {Gyuhak Kim and Changnan Xiao and Tatsuya Konishi and Bing Liu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023learnability.pdf:pdf},
  mdate = {2023-09-30},
  pages = {16877--16896},
  pdf = {https://proceedings.mlr.press/v202/kim23x/kim23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learnability and Algorithm for Continual Learning},
  url = {https://proceedings.mlr.press/v202/kim23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023unifying,
  abstract = {Although Nesterov's accelerated gradient method (AGM) has been studied from various perspectives, it remains unclear why the most popular forms of AGMs must handle convex and strongly convex objective functions separately. To address this inconsistency, we propose a novel unified framework for Lagrangians, ordinary differential equation (ODE) models, and algorithms. As a special case, our new simple momentum algorithm, called the unified AGM, seamlessly bridges the gap between the two most popular forms of Nesterov's AGM and has a superior convergence guarantee compared to existing algorithms for non-strongly convex objective functions.},
  author = {Jungbin Kim and Insoon Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023unifying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16897--16954},
  pdf = {https://proceedings.mlr.press/v202/kim23y/kim23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unifying Nesterov's Accelerated Gradient Methods for Convex and Strongly Convex Objective Functions},
  url = {https://proceedings.mlr.press/v202/kim23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023denoising,
  abstract = {The sampling process of diffusion models can be interpreted as solving the reverse stochastic differential equation (SDE) or the ordinary differential equation (ODE) of the diffusion process, which often requires up to thousands of discretization steps to generate a single image. To accelerate the sampling process, we propose an orthogonal approach to existing methods called Denoising MCMC (DMCMC). DMCMC first uses MCMC to produce initialization points for reverse-SDE/ODE in the product space of data and diffusion time, and then a reverse-SDE/ODE integrator is used to denoise the initialization points.},
  author = {Beomsu Kim and Jong Chul Ye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023denoising.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16955--16977},
  pdf = {https://proceedings.mlr.press/v202/kim23z/kim23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Denoising MCMC for Accelerating Diffusion-Based Generative Models},
  url = {https://proceedings.mlr.press/v202/kim23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023structure,
  abstract = {Despite the widespread application of latent factor analysis, existing methods suffer from weaknesses: requiring the number of factors to be known, lack of theoretical guarantees for learning the model structure, and nonidentifiability of the parameters due to rotation invariance properties of the likelihood. We address these concerns by proposing a fast correlation thresholding (CT) algorithm that simultaneously learns the number of latent factors and a rotationally identifiable model structure. The novel approach translates this structure learning problem into the search for so-called independent maximal cliques in a thresholded correlation graph that can be easily constructed from the observed data.},
  author = {Dale Kim and Qing Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023structure.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16978--16996},
  pdf = {https://proceedings.mlr.press/v202/kim23aa/kim23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Structure Learning of Latent Factors via Clique Search on Correlation Thresholded Graphs},
  url = {https://proceedings.mlr.press/v202/kim23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kim2023fair,
  abstract = {We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. The methods are evaluated in a simulation study and illustrated in a real-world case study.},
  author = {Kwangho Kim and Jose R. Zubizarreta},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kim2023fair.pdf:pdf},
  mdate = {2023-08-28},
  pages = {16997--17014},
  pdf = {https://proceedings.mlr.press/v202/kim23ab/kim23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning},
  url = {https://proceedings.mlr.press/v202/kim23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kimpara2023proper,
  abstract = {We initiate the study of proper losses for evaluating generative models in the discrete setting. A loss is proper if the generative distribution that minimizes expected loss equals the target distribution. We provide a general construction and characterization of black-box proper losses, showing they must take a polynomial form, with the number of draws from model and target distribution exceeding the polynomial's degree. We also establish connections to existing evaluation metrics and provide empirical validation of our theoretical results.},
  author = {Dhamma Kimpara and Rafael M. Frongillo and Bo Waggoner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kimpara2023proper.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17015--17040},
  pdf = {https://proceedings.mlr.press/v202/kimpara23a/kimpara23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Proper Losses for Discrete Generative Models},
  url = {https://proceedings.mlr.press/v202/kimpara23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kinoshita2023controlling,
  abstract = {The problem of posterior collapse occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. We introduce an inverse Lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of VAE models equipped with a concrete theoretical guarantee.},
  author = {Yuri Kinoshita and Kenta Oono and Kenji Fukumizu and Yuichi Yoshida and Shin-ichi Maeda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kinoshita2023controlling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17041--17060},
  pdf = {https://proceedings.mlr.press/v202/kinoshita23a/kinoshita23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network},
  url = {https://proceedings.mlr.press/v202/kinoshita23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kirchenbauer2023watermark,
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters.},
  author = {John Kirchenbauer and Jonas Geiping and Yuxin Wen and Jonathan Katz and Ian Miers and Tom Goldstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kirchenbauer2023watermark.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17061--17084},
  pdf = {https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Watermark for Large Language Models},
  url = {https://proceedings.mlr.press/v202/kirchenbauer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kirchhof2023probabilistic,
  abstract = {Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image. However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty.},
  author = {Michael Kirchhof and Enkelejda Kasneci and Seong Joon Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kirchhof2023probabilistic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17085--17104},
  pdf = {https://proceedings.mlr.press/v202/kirchhof23a/kirchhof23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs},
  url = {https://proceedings.mlr.press/v202/kirchhof23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kirchler2023training,
  abstract = {Normalizing flows are powerful non-parametric statistical models that function as a hybrid between density estimators and generative models. Current learning algorithms for normalizing flows assume that data points are sampled independently, an assumption that is frequently violated in practice, which may lead to erroneous density estimation and data generation.},
  author = {Matthias Kirchler and Christoph Lippert and Marius Kloft},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kirchler2023training.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17105--17121},
  pdf = {https://proceedings.mlr.press/v202/kirchler23a/kirchler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Training Normalizing Flows from Dependent Data},
  url = {https://proceedings.mlr.press/v202/kirchler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kishore2023incdsi,
  abstract = {Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose {IncDSI}, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time.},
  author = {Varsha Kishore and Chao Wan and Justin Lovelace and Yoav Artzi and Kilian Q. Weinberger},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kishore2023incdsi.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17122--17134},
  pdf = {https://proceedings.mlr.press/v202/kishore23a/kishore23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{IncDSI}: Incrementally Updatable Document Retrieval},
  url = {https://proceedings.mlr.press/v202/kishore23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kitamura2023regularization,
  abstract = {Mirror descent value iteration ({MDVI}), an abstraction of Kullback-Leibler ({KL}) and entropy-regularized reinforcement learning ({RL}), has served as the basis for recent high-performing practical {RL} algorithms. However, despite the use of function approximation in practice, the theoretical understanding of {MDVI} has been limited to tabular Markov decision processes ({MDP}s).},
  author = {Toshinori Kitamura and Tadashi Kozuno and Yunhao Tang and Nino Vieillard and Michal Valko and Wenhao Yang and Jincheng Mei and Pierre M{\'e}nard and Mohammad Gheshlaghi Azar and R{\'e}mi Munos and Olivier Pietquin and Matthieu Geist and Csaba Szepesv{\'a}ri and Wataru Kumagai and Yutaka Matsuo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kitamura2023regularization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17135--17175},
  pdf = {https://proceedings.mlr.press/v202/kitamura23a/kitamura23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear {MDP}s: Theory and Practice},
  url = {https://proceedings.mlr.press/v202/kitamura23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{klarner2023drug,
  abstract = {Accelerating the discovery of novel and more effective therapeutics is an important pharmaceutical problem in which deep learning is playing an increasingly significant role. However, real-world drug discovery tasks are often characterized by a scarcity of labeled data and significant covariate shift---a setting that poses a challenge to standard deep learning methods.},
  author = {Leo Klarner and Tim G. J. Rudner and Michael Reutlinger and Torsten Schindler and Garrett M. Morris and Charlotte M. Deane and Yee Whye Teh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/klarner2023drug.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17176--17197},
  pdf = {https://proceedings.mlr.press/v202/klarner23a/klarner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions},
  url = {https://proceedings.mlr.press/v202/klarner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{klissarov2023deep,
  abstract = {Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning ({RL}). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options.},
  author = {Martin Klissarov and Marlos C. Machado},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/klissarov2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17198--17217},
  pdf = {https://proceedings.mlr.press/v202/klissarov23a/klissarov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Laplacian-based Options for Temporally-Extended Exploration},
  url = {https://proceedings.mlr.press/v202/klissarov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{knittel2023generalized,
  abstract = {Clustering is a fundamental building block of modern statistical analysis pipelines. Fair clustering has seen much attention from the machine learning community in recent years. We are some of the first to study fairness in the context of hierarchical clustering, after the results of Ahmadian et al. from {NeurIPS} in 2020.},
  author = {Marina Knittel and Max Springer and John P. Dickerson and MohammadTaghi Hajiaghayi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/knittel2023generalized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17218--17242},
  pdf = {https://proceedings.mlr.press/v202/knittel23a/knittel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost},
  url = {https://proceedings.mlr.press/v202/knittel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{knyazev2023can,
  abstract = {Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality {ImageNet} parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse {ImageNet} models available in {PyTorch}. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.},
  author = {Boris Knyazev and Doha Hwang and Simon Lacoste-Julien},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/knyazev2023can.pdf:pdf},
  mdate = {2024-10-08},
  pages = {17243--17259},
  pdf = {https://proceedings.mlr.press/v202/knyazev23a/knyazev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Can We Scale Transformers to Predict Parameters of Diverse {ImageNet} Models?},
  url = {https://proceedings.mlr.press/v202/knyazev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{a2023online,
  abstract = {Sequential learning with feedback graphs is a natural extension of the multi-armed bandit problem where the problem is equipped with an underlying graph structure that provides additional information---playing an action reveals the losses of all the neighbors of the action.},
  author = {Tom{\' a}{\v s} Koc{\' a}k and Alexandra Carpentier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/a2023online.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17260--17282},
  pdf = {https://proceedings.mlr.press/v202/kocak23a/kocak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Learning with Feedback Graphs: The True Shape of Regret},
  url = {https://proceedings.mlr.press/v202/kocak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{koh2023grounding,
  abstract = {We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images.},
  author = {Jing Yu Koh and Ruslan Salakhutdinov and Daniel Fried},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/koh2023grounding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17283--17300},
  pdf = {https://proceedings.mlr.press/v202/koh23a/koh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Grounding Language Models to Images for Multimodal Inputs and Outputs},
  url = {https://proceedings.mlr.press/v202/koh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kohring2023enabling,
  abstract = {Understanding and analyzing markets is crucial, yet analytical equilibrium solutions remain largely infeasible. Recent breakthroughs in equilibrium computation rely on zeroth-order policy gradient estimation. These approaches commonly suffer from high variance and are computationally expensive.},
  author = {Nils Kohring and Fabian Raoul Pieroth and Martin Bichler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kohring2023enabling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17327--17342},
  pdf = {https://proceedings.mlr.press/v202/kohring23a/kohring23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Enabling First-Order Gradient-Based Learning for Equilibrium Computation in Markets},
  url = {https://proceedings.mlr.press/v202/kohring23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{koloskova2023revisiting,
  abstract = {Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c >0$. It is widely used for example for stabilizing the training of deep learning models or for enforcing differential privacy. We investigate convergence guarantees for gradient clipping, highlighting two key findings: (1) For deterministic gradient descent, the clipping threshold only affects higher-order convergence terms. (2) In stochastic settings, convergence to the true optimum cannot be guaranteed under standard noise assumptions, even under arbitrary small step-sizes.},
  author = {Anastasia Koloskova and Hadrien Hendrikx and Sebastian U. Stich},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/koloskova2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17343--17363},
  pdf = {https://proceedings.mlr.press/v202/koloskova23a/koloskova23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees},
  url = {https://proceedings.mlr.press/v202/koloskova23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{komusiewicz2023computing,
  abstract = {Random forests and, more generally, (decision-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We provide two novel algorithms for computing optimal tree ensembles: (1) A $(6\delta D S)^S \cdot \mathrm{poly}$-time algorithm using a witness-tree technique, and (2) A dynamic programming approach with $\ell^n \cdot \mathrm{poly}$-time complexity. Both problems contain as a special case the computation of minimum-size decision trees, which is NP-hard.},
  author = {Christian Komusiewicz and Pascal Kunz and Frank Sommer and Manuel Sorge},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/komusiewicz2023computing.pdf:pdf},
  mdate = {2025-01-09},
  pages = {17364--17374},
  pdf = {https://proceedings.mlr.press/v202/komusiewicz23a/komusiewicz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Computing Optimal Tree Ensembles},
  url = {https://proceedings.mlr.press/v202/komusiewicz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kong2023endtoend,
  abstract = {Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To solve these issues, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody.},
  author = {Xiangzhe Kong and Wenbing Huang and Yang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kong2023endtoend.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17409--17429},
  pdf = {https://proceedings.mlr.press/v202/kong23c/kong23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {End-to-End Full-Atom Antibody Design},
  url = {https://proceedings.mlr.press/v202/kong23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kong2023goat,
  abstract = {Graph transformers have been competitive on graph classification tasks, but they fail to outperform Graph Neural Networks (GNNs) on node classification. To address these issues, we propose GOAT, a scalable global graph transformer where each node conceptually attends to all the nodes in the graph and homophily/heterophily relationships can be learnt adaptively from the data. GOAT allows nodes to attend to all graph nodes and is designed to work effectively on graphs with millions of nodes.},
  author = {Kezhi Kong and Jiuhai Chen and John Kirchenbauer and Renkun Ni and C. Bayan Bruss and Tom Goldstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kong2023goat.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17375--17390},
  pdf = {https://proceedings.mlr.press/v202/kong23a/kong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GOAT}: A Global Transformer on Large-scale Graphs},
  url = {https://proceedings.mlr.press/v202/kong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kong2023autoregressive,
  abstract = {Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. We propose an autoregressive diffusion model for graph generation that defines a node-absorbing diffusion process in discrete graph space. We employ a diffusion ordering network to learn node absorbing ordering and a denoising network to reconstruct graphs by predicting node types and edges.},
  author = {Lingkai Kong and Jiaming Cui and Haotian Sun and Yuchen Zhuang and B. Aditya Prakash and Chao Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kong2023autoregressive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17391--17408},
  pdf = {https://proceedings.mlr.press/v202/kong23b/kong23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Autoregressive Diffusion Model for Graph Generation},
  url = {https://proceedings.mlr.press/v202/kong23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kong2023covariate,
  abstract = {Weighting methods in causal inference have been widely used to achieve a desirable level of covariate balancing. However, the existing weighting methods have desirable theoretical properties only when a certain model, either the propensity score or outcome regression model, is correctly specified. In addition, the corresponding estimators do not behave well for finite samples due to large variance even when the model is correctly specified. In this paper, we consider to use the integral probability metric (IPM), which is a metric between two probability measures, for covariate balancing. We prove that the corresponding estimator can be consistent without correctly specifying any model and empirically show that our proposed method outperforms existing weighting methods with large margins for finite samples.},
  author = {Insung Kong and Yuha Park and Joonhyuk Jung and Kwonsang Lee and Yongdai Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kong2023covariate.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17430--17461},
  pdf = {https://proceedings.mlr.press/v202/kong23d/kong23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Covariate balancing using the integral probability metric for causal inference},
  url = {https://proceedings.mlr.press/v202/kong23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kong2023masked,
  abstract = {Bayesian approaches for learning deep neural networks (BNN) have been received much attention and successfully applied to various applications. Particularly, BNNs have the merit of having better generalization ability as well as better uncertainty quantification. In this paper, we propose a new node-sparse BNN model which has good theoretical properties and is computationally feasible. We prove that the posterior concentration rate to the true model is near minimax optimal and adaptive to the smoothness of the true model. In particular the adaptiveness is the first of its kind for node-sparse BNNs. In addition, we develop a novel MCMC algorithm which makes the Bayesian inference of the node-sparse BNN model feasible in practice.},
  author = {Insung Kong and Dongyoon Yang and Jongjin Lee and Ilsang Ohn and Gyuseung Baek and Yongdai Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kong2023masked.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17462--17491},
  pdf = {https://proceedings.mlr.press/v202/kong23e/kong23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Masked {B}ayesian Neural Networks: Theoretical Guarantee and its Posterior Inference},
  url = {https://proceedings.mlr.press/v202/kong23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{konishi2023parameterlevel,
  abstract = {Existing research on task incremental learning in continual learning has primarily focused on preventing catastrophic forgetting (CF). Although several techniques have achieved learning with no CF, they attain it by letting each task monopolize a sub-network in a shared network, which seriously limits knowledge transfer (KT) and causes over-consumption of the network capacity. The goal of this paper is threefold: (1) overcoming CF, (2) encouraging KT, and (3) tackling the capacity problem. A novel technique (called SPG) is proposed that soft-masks (partially blocks) parameter updating in training based on the importance of each parameter to old tasks. Each task still uses the full network, enabling maximum KT and reduction in capacity usage.},
  author = {Tatsuya Konishi and Mori Kurokawa and Chihiro Ono and Zixuan Ke and Gyuhak Kim and Bing Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/konishi2023parameterlevel.pdf:pdf},
  mdate = {2023-09-30},
  pages = {17492--17505},
  pdf = {https://proceedings.mlr.press/v202/konishi23a/konishi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Parameter-Level Soft-Masking for Continual Learning},
  url = {https://proceedings.mlr.press/v202/konishi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{korbak2023pretraining,
  abstract = {Language models (LMs) are pretrained to imitate text from large and diverse datasets that contain content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, among others. We explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and find a Pareto-optimal and simple approach: conditional training, or learning distribution over tokens conditional on their human preference scores. Conditional training reduces the rate of undesirable content by up to an order of magnitude and maintains downstream task performance.},
  author = {Tomasz Korbak and Kejian Shi and Angelica Chen and Rasika Vinayak Bhalerao and Christopher L. Buckley and Jason Phang and Samuel R. Bowman and Ethan Perez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/korbak2023pretraining.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17506--17533},
  pdf = {https://proceedings.mlr.press/v202/korbak23a/korbak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pretraining Language Models with Human Preferences},
  url = {https://proceedings.mlr.press/v202/korbak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{korkmaz2023detecting,
  abstract = {Learning in MDPs with highly complex state representations is currently possible due to multiple advancements in reinforcement learning algorithm design. However, this incline in complexity, and furthermore the increase in the dimensions of the observation came at the cost of volatility that can be taken advantage of via adversarial attacks. To solve this policy instability problem we propose a novel method to detect the presence of these non-robust directions via local quadratic approximation of the deep neural policy loss. Our method provides a theoretical basis for the fundamental cut-off between safe observations and adversarial observations and is computationally efficient.},
  author = {Ezgi Korkmaz and Jonah Brown-Cohen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/korkmaz2023detecting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17534--17543},
  pdf = {https://proceedings.mlr.press/v202/korkmaz23a/korkmaz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions},
  url = {https://proceedings.mlr.press/v202/korkmaz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kosmala2023ewaldbased,
  abstract = {Neural architectures that learn potential energy surfaces from molecular data have undergone fast improvement in recent years. A key driver of this success is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling with system size partly relies upon a spatial distance limit on messages. While this focus on locality is a useful inductive bias, it also impedes the learning of long-range interactions such as electrostatics and van der Waals forces. To address this drawback, we propose Ewald message passing: a nonlocal Fourier space scheme which limits interactions via a cutoff on frequency instead of distance, and is theoretically well-founded in the Ewald summation method. It can serve as an augmentation on top of existing MPNN architectures as it is computationally inexpensive and agnostic to architectural details. We test the approach with four baseline models and two datasets containing diverse periodic (OC20) and aperiodic structures (OE62). Across all models and datasets, we observe robust improvements in energy mean absolute errors, averaging 10\% on OC20 and 16\% on OE62. Our analysis shows an outsize impact of these improvements on structures with high long-range contributions to the ground-truth energy.},
  author = {Arthur Kosmala and Johannes Gasteiger and Nicholas Gao and Stephan G{\"u}nnemann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kosmala2023ewaldbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17544--17563},
  pdf = {https://proceedings.mlr.press/v202/kosmala23a/kosmala23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Ewald-based Long-Range Message Passing for Molecular Graphs},
  url = {https://proceedings.mlr.press/v202/kosmala23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kotelnikov2023tabddpm,
  abstract = {Denoising diffusion probabilistic models are currently becoming the leading paradigm of generative modeling for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have also recently gained some attention in other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where datapoints are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling, since the individual features can be of completely different nature, i.e., some of them can be continuous and some of them can be discrete. To address such data types, we introduce TabDDPM -- a diffusion model that can be universally applied to any tabular dataset and handles any type of feature. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields. Additionally, we show that TabDDPM is eligible for privacy-oriented setups, where the original datapoints cannot be publicly shared.},
  author = {Akim Kotelnikov and Dmitry Baranchuk and Ivan Rubachev and Artem Babenko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kotelnikov2023tabddpm.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17564--17579},
  pdf = {https://proceedings.mlr.press/v202/kotelnikov23a/kotelnikov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TabDDPM}: Modelling Tabular Data with Diffusion Models},
  url = {https://proceedings.mlr.press/v202/kotelnikov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kothapalli2023randomized,
  abstract = {We introduce a randomized topological augmentor based on Schur complements for Graph Contrastive Learning (GCL). Given a graph laplacian matrix, the technique generates unbiased approximations of its Schur complements and treats the corresponding graphs as augmented views. We discuss the benefits of our approach, provide theoretical justifications and present connections with graph diffusion. Unlike previous efforts, we study the empirical effectiveness of the augmentor in a controlled fashion by varying the design choices for subsequent GCL phases, such as encoding and contrasting. Extensive experiments on node and graph classification benchmarks demonstrate that our technique consistently outperforms pre-defined and adaptive augmentation approaches to achieve state-of-the-art results.},
  author = {Vignesh Kothapalli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kothapalli2023randomized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17580--17614},
  pdf = {https://proceedings.mlr.press/v202/kothapalli23a/kothapalli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Randomized Schur Complement Views for Graph Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/kothapalli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kou2023benign,
  abstract = {Modern deep learning models with great expressive power can be trained to overfit the training data but still generalize well. This phenomenon is referred to as benign overfitting. Recently, a few studies have attempted to theoretically understand benign overfitting in neural networks. However, these works are either limited to neural networks with smooth activation functions or to the neural tangent kernel regime. How and when benign overfitting can occur in ReLU neural networks remains an open problem. In this work, we seek to answer this question by establishing algorithm-dependent risk bounds for learning two-layer ReLU convolutional neural networks with label-flipping noise. We show that, under mild conditions, the neural network trained by gradient descent can achieve near-zero training loss and Bayes optimal test risk. Our result also reveals a sharp transition between benign and harmful overfitting under different conditions on data distribution in terms of test risk.},
  author = {Yiwen Kou and Zixiang Chen and Yuanzhou Chen and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kou2023benign.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17615--17659},
  pdf = {https://proceedings.mlr.press/v202/kou23a/kou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Benign Overfitting in Two-layer {ReLU} Convolutional Neural Networks},
  url = {https://proceedings.mlr.press/v202/kou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{koyuncu2023variational,
  abstract = {Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VaMoH. VaMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VaMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VaMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VaMoH can effectively learn rich distributions over continuous functions.},
  author = {Batuhan Koyuncu and Pablo S{\'a}nchez-Mart{\'i}n and Ignacio Peis and Pablo M. Olmos and Isabel Valera},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/koyuncu2023variational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17660--17683},
  pdf = {https://proceedings.mlr.press/v202/koyuncu23a/koyuncu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Mixture of {HyperGenerators} for Learning Distributions over Functions},
  url = {https://proceedings.mlr.press/v202/koyuncu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kreisler2023gradient,
  abstract = {Recent research shows that when Gradient Descent (GD) is applied to neural networks, the loss almost never decreases monotonically. Instead, the loss oscillates as gradient descent converges to its ``Edge of Stability'' (EoS). Here, we find a quantity that does decrease monotonically throughout GD training: the sharpness attained by the gradient flow solution (GFS)--the solution that would be obtained if, from now until convergence, we train with an infinitesimal step size. Theoretically, we analyze scalar neural networks with the squared loss, perhaps the simplest setting where the EoS phenomena still occur. In this model, we prove that the GFS sharpness decreases monotonically. Using this result, we characterize settings where GD provably converges to the EoS in scalar networks. Empirically, we show that GD monotonically decreases the GFS sharpness in a squared regression model as well as practical neural network architectures.},
  author = {Itai Kreisler and Mor Shpigel Nacson and Daniel Soudry and Yair Carmon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kreisler2023gradient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17684--17744},
  pdf = {https://proceedings.mlr.press/v202/kreisler23a/kreisler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond},
  url = {https://proceedings.mlr.press/v202/kreisler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kremer2023estimation,
  abstract = {Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\phi$-divergence to an empirical distribution. However, the use of $\phi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment restrictions and show that it is asymptotically first-order optimal for such problems. Finally, we show that our method achieves competitive performance on several conditional moment restriction tasks.},
  author = {Heiner Kremer and Yassine Nemmour and Bernhard Sch{\"o}lkopf and Jia-Jie Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kremer2023estimation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17745--17783},
  pdf = {https://proceedings.mlr.press/v202/kremer23a/kremer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimation Beyond Data Reweighting: Kernel Method of Moments},
  url = {https://proceedings.mlr.press/v202/kremer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{krichene2023multitask,
  abstract = {We study the problem of multi-task learning under user-level differential privacy, in which $n$ users contribute data to $m$ tasks, each involving a subset of users. One important aspect of the problem, that can significantly impact quality, is the distribution skew among tasks. Certain tasks may have much fewer data samples than others, making them more susceptible to the noise added for privacy. It is natural to ask whether algorithms can adapt to this skew to improve the overall utility. We give a systematic analysis of the problem, by studying how to optimally allocate a user's privacy budget among tasks. We propose a generic algorithm, based on an adaptive reweighting of the empirical loss, and show that when there is task distribution skew, this gives a quantifiable improvement of excess empirical risk.},
  author = {Walid Krichene and Prateek Jain and Shuang Song and Mukund Sundararajan and Abhradeep Guha Thakurta and Li Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/krichene2023multitask.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17784--17807},
  pdf = {https://proceedings.mlr.press/v202/krichene23a/krichene23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Task Differential Privacy Under Distribution Skew},
  url = {https://proceedings.mlr.press/v202/krichene23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{krishna2023towards,
  abstract = {The Right to Explanation and the Right to be Forgotten are two important principles outlined to regulate algorithmic decision making and data usage in real-world applications. While the right to explanation allows individuals to request an actionable explanation for an algorithmic decision, the right to be forgotten grants them the right to ask for their data to be deleted from all the databases and models of an organization. Intuitively, enforcing the right to be forgotten may trigger model updates which in turn invalidate previously provided explanations, thus violating the right to explanation. In this work, we investigate the technical implications arising due to the interference between the two aforementioned regulatory principles, and propose the first algorithmic framework to resolve the tension between them.},
  author = {Satyapriya Krishna and Jiaqi Ma and Himabindu Lakkaraju},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/krishna2023towards.pdf:pdf},
  mdate = {2024-10-08},
  pages = {17808--17826},
  pdf = {https://proceedings.mlr.press/v202/krishna23a/krishna23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten},
  url = {https://proceedings.mlr.press/v202/krishna23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{krishnagopal2023graph,
  abstract = {Graph neural networks (GNNs) achieve remarkable performance in graph machine learning tasks but can be hard to train on large-graph data, where their learning dynamics are not well understood. We investigate the training dynamics of large-graph GNNs using graph neural tangent kernels (GNTKs) and graphons. In the limit of large width, optimization of an overparametrized NN is equivalent to kernel regression on the NTK. Here, we investigate how the GNTK evolves as another independent dimension is varied: the graph size. We use graphons to define limit objects -- graphon NNs for GNNs, and graphon NTKs for GNTKs, and prove that, on a sequence of growing graphs, the GNTKs converge to the graphon NTK. We further prove that the eigenspaces of the GNTK, which are related to the problem learning directions and associated learning speeds, converge to the spectrum of the GNTK.},
  author = {Sanjukta Krishnagopal and Luana Ruiz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/krishnagopal2023graph.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17827--17841},
  pdf = {https://proceedings.mlr.press/v202/krishnagopal23a/krishnagopal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Neural Tangent Kernel: Convergence on Large Graphs},
  url = {https://proceedings.mlr.press/v202/krishnagopal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{krishnamoorthy2023diffusion,
  abstract = {The goal of offline black-box optimization (BBO) is to optimize an expensive black-box function using a fixed dataset of function evaluations. Prior works consider forward approaches that learn surrogates to the black-box function and inverse approaches that directly map function values to corresponding points in the input domain of the black-box function. These approaches are limited by the quality of the offline dataset and the difficulty in learning one-to-many mappings in high dimensions, respectively. We propose Denoising Diffusion Optimization Models (DDOM), a new inverse approach for offline black-box optimization based on diffusion models. Given an offline dataset, DDOM learns a conditional generative model over the domain of the black-box function conditioned on the function values. We investigate several design choices in DDOM, such as reweighting the dataset to focus on high function values and the use of classifier-free guidance at test-time to enable generalization to function values that can even exceed the dataset maxima. Empirically, we conduct experiments on the Design-Bench benchmark (Trabucco et al., 2022) and show that DDOM achieves results competitive with state-of-the-art baselines.},
  author = {Siddarth Krishnamoorthy and Satvik Mehul Mashkaria and Aditya Grover},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/krishnamoorthy2023diffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17842--17857},
  pdf = {https://proceedings.mlr.press/v202/krishnamoorthy23a/krishnamoorthy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diffusion Models for Black-Box Optimization},
  url = {https://proceedings.mlr.press/v202/krishnamoorthy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{krylov2023learning,
  abstract = {Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90\% at 5\% error margin, while also improving data efficiency by upward of an order of magnitude.},
  author = {Dmitrii Krylov and Pooya Khajeh and Junhan Ouyang and Thomas Reeves and Tongkai Liu and Hiba Ajmal and Hamidreza Aghasi and Roy Fox},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/krylov2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17858--17873},
  pdf = {https://proceedings.mlr.press/v202/krylov23a/krylov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Design Analog Circuits to Meet Threshold Specifications},
  url = {https://proceedings.mlr.press/v202/krylov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kuang2023variance,
  abstract = {Although distributional reinforcement learning (DRL) has been widely examined in the past few years, very few studies investigate the validity of the obtained Q-function estimator in the distributional setting. To fully understand how the approximation errors of the Q-function affect the whole training process, we do some error analysis and theoretically show how to reduce both the bias and the variance of the error terms. With this new understanding, we construct a new estimator Quantiled Expansion Mean (QEM) and introduce a new DRL algorithm (QEMRL) from the statistical perspective. We extensively evaluate our QEMRL algorithm on a variety of Atari and Mujoco benchmark tasks and demonstrate that QEMRL achieves significant improvement over baseline algorithms in terms of sample efficiency and convergence performance.},
  author = {Qi Kuang and Zhoufan Zhu and Liwen Zhang and Fan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kuang2023variance.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17874--17895},
  pdf = {https://proceedings.mlr.press/v202/kuang23a/kuang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variance Control for Distributional Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/kuang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kujanpa2023hierarchical,
  abstract = {The ability to plan actions on multiple levels of abstraction enables intelligent agents to solve complex tasks effectively. However, learning the models for both low and high-level planning from demonstrations has proven challenging, especially with higher-dimensional inputs. To address this issue, we propose to use reinforcement learning to identify subgoals in expert trajectories by associating the magnitude of the rewards with the predictability of low-level actions given the state and the chosen subgoal. We build a vector-quantized generative model for the identified subgoals to perform subgoal-level planning. In experiments, the algorithm excels at solving complex, long-horizon decision-making problems outperforming state-of-the-art. Because of its ability to plan, our algorithm can find better trajectories than the ones in the training set.},
  author = {Kalle Kujanp{\"a}{\"a} and Joni Pajarinen and Alexander Ilin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kujanpa2023hierarchical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17896--17919},
  pdf = {https://proceedings.mlr.press/v202/kujanpaa23a/kujanpaa23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchical Imitation Learning with Vector Quantized Models},
  url = {https://proceedings.mlr.press/v202/kujanpaa23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kulikov2023s,
  abstract = {Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.},
  author = {Vladimir Kulikov and Shahar Yadin and Matan Kleiner and Tomer Michaeli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kulikov2023s.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17920--17930},
  pdf = {https://proceedings.mlr.press/v202/kulikov23a/kulikov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{S}in{DDM}: A Single Image Denoising Diffusion Model},
  url = {https://proceedings.mlr.press/v202/kulikov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kulinski2023towards,
  abstract = {A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work has focused on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of the optimal transport problem, where the candidate mappings are restricted to a set of interpretable mappings. We then use a wide array of quintessential examples of distribution shift in real-world tabular, text, and image cases to showcase how our explanatory mappings provide a better balance between detail and interpretability than baseline explanations by both visual inspection and our PercentExplained metric.},
  author = {Sean Kulinski and David I. Inouye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kulinski2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17931--17952},
  pdf = {https://proceedings.mlr.press/v202/kulinski23a/kulinski23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Explaining Distribution Shifts},
  url = {https://proceedings.mlr.press/v202/kulinski23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kumar2023featured,
  abstract = {Graph coarsening is a dimensionality reduction technique that aims to learn a smaller-tractable graph while preserving the properties of the original input graph. However, many real-world graphs also have features or contexts associated with each node. The existing graph coarsening methods do not consider the node features and rely solely on a graph matrix (e.g., adjacency and Laplacian) to coarsen graphs. However, some recent deep learning-based graph coarsening methods are designed for specific tasks considering both node features and graph matrix. In this paper, we introduce a novel optimization-based framework for graph coarsening that takes both the graph matrix and the node features as the input and jointly learns the coarsened graph matrix and the coarsened feature matrix while ensuring desired properties. To the best of our knowledge, this is the first work that guarantees that the learned coarsened graph is $\epsilon\in[0,1)$ similar to the original graph. Extensive experiments with both real and synthetic benchmark datasets elucidate the proposed framework's efficacy and applicability for numerous graph-based applications, including graph clustering, node classification, stochastic block model identification, and graph summarization.},
  author = {Manoj Kumar and Anurag Sharma and Shashwat Saxena and Sandeep Kumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kumar2023featured.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17953--17975},
  pdf = {https://proceedings.mlr.press/v202/kumar23a/kumar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Featured Graph Coarsening with Similarity Guarantees},
  url = {https://proceedings.mlr.press/v202/kumar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kurenkov2023modeling,
  abstract = {Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patterns typically seen at homes, and show that NEP can be trained to predict the locations of objects in a variety of environments with diverse object movement dynamics, outperforming baselines both in terms of new scene adaptability and overall accuracy. The codebase and more can be found www.scenegraphmemory.com.},
  author = {Andrey Kurenkov and Michael Lingelbach and Tanmay Agarwal and Emily Jin and Chengshu Li and Ruohan Zhang and Li Fei-Fei and Jiajun Wu and Silvio Savarese and Roberto Mart{\'\i}n-Mart{\'\i}n},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kurenkov2023modeling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {17976--17993},
  pdf = {https://proceedings.mlr.press/v202/kurenkov23a/kurenkov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Modeling Dynamic Environments with Scene Graph Memory},
  url = {https://proceedings.mlr.press/v202/kurenkov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{s2023tiedaugment,
  abstract = {Data augmentation methods have played an important role in the recent advance of deep learning models, and have become an indispensable component of state-of-the-art models in semi-supervised, self-supervised, and supervised training for vision. Despite incurring no additional latency at test time, data augmentation often requires more epochs of training to be effective. For example, even the simple flips-and-crops augmentation requires training for more than 5 epochs to improve performance, whereas RandAugment requires more than 90 epochs. We propose a general framework called Tied-Augment, which improves the efficacy of data augmentation in a wide range of applications by adding a simple term to the loss that can control the similarity of representations under distortions. Tied-Augment can improve state-of-the-art methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM), and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can outperform RandAugment by 2.0\% on ImageNet. Notably, using Tied-Augment, data augmentation can be made to improve generalization even when training for a few epochs and when fine-tuning. We open source our code at https://github.com/ekurtulus/tied-augment/tree/main.},
  author = {Emirhan Kurtulu{\c s} and Zichao Li and Yann N. Dauphin and Ekin Do{\u g}u{\c s} Cubuk},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/s2023tiedaugment.pdf:pdf},
  mdate = {2024-08-08},
  pages = {17994--18007},
  pdf = {https://proceedings.mlr.press/v202/kurtulus23a/kurtulus23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tied-Augment: Controlling Representation Similarity Improves Data Augmentation},
  url = {https://proceedings.mlr.press/v202/kurtulus23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kviman2023cooperation,
  abstract = {In this paper, we show how the mixture components cooperate when they jointly adapt to maximize the ELBO. We build upon recent advances in the multiple and adaptive importance sampling literature. We then model the mixture components using separate encoder networks and show empirically that the ELBO is monotonically non-decreasing as a function of the number of mixture components. These results hold for a range of different VAE architectures on the MNIST, FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that increasing the number of mixture components improves the latent-representation capabilities of the VAE on both image and single-cell datasets. This cooperative behavior motivates that using Mixture VAEs should be considered a standard approach for obtaining more flexible variational approximations. Finally, Mixture VAEs are here, for the first time, compared and combined with normalizing flows, hierarchical models and/or the VampPrior in an extensive ablation study. Multiple of our Mixture VAEs achieve state-of-the-art log-likelihood results for VAE architectures on the MNIST and FashionMNIST datasets. The experiments are reproducible using our code, provided https://github.com/Lagergren-Lab/MixtureVAEs.},
  author = {Oskar Kviman and Ricky Mol{\'e}n and Alexandra Hotti and Semih Kurt and V{\'\i}ctor Elvira and Jens Lagergren},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kviman2023cooperation.pdf:pdf},
  mdate = {2024-10-06},
  pages = {18008--18022},
  pdf = {https://proceedings.mlr.press/v202/kviman23a/kviman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders},
  url = {https://proceedings.mlr.press/v202/kviman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwak2023geconerf,
  abstract = {We present a novel framework to regularize Neural Radiance Field ({NeRF}) in a few-shot setting with a geometry-aware consistency regularization.},
  author = {Min-Seop Kwak and Jiuhn Song and Seungryong Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwak2023geconerf.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18023--18036},
  pdf = {https://proceedings.mlr.press/v202/kwak23a/kwak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GeCoNeRF}: Few-shot Neural Radiance Fields via Geometric Consistency},
  url = {https://proceedings.mlr.press/v202/kwak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwon2023dataoob,
  abstract = {Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models.},
  author = {Yongchan Kwon and James Zou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwon2023dataoob.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18135--18152},
  pdf = {https://proceedings.mlr.press/v202/kwon23e/kwon23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Data-OOB}: Out-of-bag Estimate as a Simple and Efficient Data Value},
  url = {https://proceedings.mlr.press/v202/kwon23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwon2023rotation,
  abstract = {In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation.},
  author = {Sehyun Kwon and Joo Young Choi and Ernest K. Ryu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwon2023rotation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18037--18056},
  pdf = {https://proceedings.mlr.press/v202/kwon23a/kwon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rotation and Translation Invariant Representation Learning with Implicit Neural Representations},
  url = {https://proceedings.mlr.press/v202/kwon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwon2023rewardmixing,
  abstract = {We consider episodic reinforcement learning in reward-mixing Markov decision processes ({RMMDPs}): at the beginning of every episode nature randomly picks a latent reward model among M candidates and an agent interacts with the {MDP} throughout the episode for H time steps.},
  author = {Jeongyeol Kwon and Yonathan Efroni and Constantine Caramanis and Shie Mannor},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwon2023rewardmixing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18057--18082},
  pdf = {https://proceedings.mlr.press/v202/kwon23b/kwon23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reward-Mixing {MDPs} with Few Latent Contexts are Learnable},
  url = {https://proceedings.mlr.press/v202/kwon23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwon2023fully,
  abstract = {We consider stochastic unconstrained bilevel optimization problems when only the first-order gradient oracles are available. While numerous optimization methods have been proposed for tackling bilevel problems, existing methods either tend to require possibly expensive calculations regarding Hessians of lower-level objectives, or lack rigorous finite-time performance guarantees.},
  author = {Jeongyeol Kwon and Dohyun Kwon and Stephen Wright and Robert D. Nowak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwon2023fully.pdf:pdf},
  mdate = {2024-10-14},
  pages = {18083--18113},
  pdf = {https://proceedings.mlr.press/v202/kwon23c/kwon23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Fully First-Order Method for Stochastic Bilevel Optimization},
  url = {https://proceedings.mlr.press/v202/kwon23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{kwon2023complexity,
  abstract = {We consider the block coordinate descent methods of Gauss-Seidel type with proximal regularization ({BCD-PR}), which is a classical method of minimizing general nonconvex objectives under constraints.},
  author = {Dohyun Kwon and Hanbaek Lyu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/kwon2023complexity.pdf:pdf},
  mdate = {2024-10-14},
  pages = {18114--18134},
  pdf = {https://proceedings.mlr.press/v202/kwon23d/kwon23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Complexity of Block Coordinate Descent with Proximal Regularization and Applications to Wasserstein {CP}-dictionary Learning},
  url = {https://proceedings.mlr.press/v202/kwon23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{laan2023causal,
  abstract = {We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. In addition, we introduce a novel data-efficient variant of calibration that avoids the need for hold-out calibration sets, which we refer to as cross-calibration.},
  author = {Lars van der Laan and Ernesto Ulloa-Pérez and Marco Carone and Alex Luedtke},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/laan2023causal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34831--34854},
  pdf = {https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Isotonic Calibration for Heterogeneous Treatment Effects},
  url = {https://proceedings.mlr.press/v202/van-der-laan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{labash2023emergence,
  abstract = {Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the 24-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents.},
  author = {Aqeel Labash and Florian Stelzer and Daniel Majoral and Raul Vicente Zafra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/labash2023emergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18153--18170},
  pdf = {https://proceedings.mlr.press/v202/labash23a/labash23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/labash23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lachapelle2023synergies,
  abstract = {Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse task-specific predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse predictors yield disentangled representations.},
  author = {Sébastien Lachapelle and Tristan Deleu and Divyat Mahajan and Ioannis Mitliagkas and Yoshua Bengio and Simon Lacoste-Julien and Quentin Bertrand},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lachapelle2023synergies.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18171--18206},
  pdf = {https://proceedings.mlr.press/v202/lachapelle23a/lachapelle23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning},
  url = {https://proceedings.mlr.press/v202/lachapelle23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{laenen2023nearlyoptimal,
  abstract = {This paper presents two efficient hierarchical clustering ({HC}) algorithms with respect to Dasgupta's cost function. For any input graph G with a clear cluster-structure, our designed algorithms run in nearly-linear time in the input size of G, and return an O(1)-approximate {HC} tree with respect to Dasgupta's cost function.},
  author = {Steinar Laenen and Bogdan-Adrian Manghiuc and He Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/laenen2023nearlyoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18207--18249},
  pdf = {https://proceedings.mlr.press/v202/laenen23a/laenen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs},
  url = {https://proceedings.mlr.press/v202/laenen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lafon2023hybrid,
  abstract = {Out-of-distribution (OOD) detection is a critical requirement for the deployment of deep neural networks. This paper introduces the HEAT model, a new post-hoc OOD detection method estimating the density of in-distribution (ID) samples using hybrid energy-based models (EBM) in the feature space of a pre-trained backbone. HEAT complements prior density estimators of the ID density, e.g. parametric models like the Gaussian Mixture Model (GMM), to provide an accurate yet robust density estimation. A second contribution is to leverage the EBM framework to provide a unified density estimation and to compose several energy terms. Extensive experiments demonstrate the significance of the two contributions. HEAT sets new state-of-the-art OOD detection results on the CIFAR-10 / CIFAR-100 benchmark as well as on the large-scale Imagenet benchmark.},
  author = {Marc Lafon and Elias Ramzi and Cl{\'e}ment Rambour and Nicolas Thome},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lafon2023hybrid.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18250--18268},
  pdf = {https://proceedings.mlr.press/v202/lafon23a/lafon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection},
  url = {https://proceedings.mlr.press/v202/lafon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lahlou2023theory,
  abstract = {Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks.},
  author = {Salem Lahlou and Tristan Deleu and Pablo Lemos and Dinghuai Zhang and Alexandra Volokhova and Alex Hern{\'a}ndez-Garc{\'i}a and L{\'e}na Nehale Ezzine and Yoshua Bengio and Nikolay Malkin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lahlou2023theory.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18269--18300},
  pdf = {https://proceedings.mlr.press/v202/lahlou23a/lahlou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A theory of continuous generative flow networks},
  url = {https://proceedings.mlr.press/v202/lahlou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lai2023ds1000,
  abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.},
  author = {Yuhang Lai and Chengxi Li and Yiming Wang and Tianyi Zhang and Ruiqi Zhong and Luke Zettlemoyer and Wen-Tau Yih and Daniel Fried and Sida I. Wang and Tao Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lai2023ds1000.pdf:pdf},
  mdate = {2024-01-28},
  pages = {18319--18345},
  pdf = {https://proceedings.mlr.press/v202/lai23b/lai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  url = {https://proceedings.mlr.press/v202/lai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lai2023automatically,
  abstract = {Hamiltonian Monte Carlo (HMC) is a powerful algorithm to sample latent variables from Bayesian models. The advent of probabilistic programming languages (PPLs) frees users from writing inference algorithms and lets users focus on modeling. However, many models are difficult for HMC to solve directly, and often require tricks like model reparameterization. We are motivated by the fact that many of those models could be simplified by marginalization. We propose to use automatic marginalization as part of the sampling process using HMC in a graphical model extracted from a PPL, which substantially improves sampling from real-world hierarchical models.},
  author = {Jinlin Lai and Javier Burroni and Hui Guan and Daniel Sheldon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lai2023automatically.pdf:pdf},
  mdate = {2024-06-25},
  pages = {18301--18318},
  pdf = {https://proceedings.mlr.press/v202/lai23a/lai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Automatically marginalized {MCMC} in probabilistic programming},
  url = {https://proceedings.mlr.press/v202/lai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lai2023chipformer,
  abstract = {Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing the runtime by 10x compared to recent state-of-the-art approaches in both public benchmarks and realistic industrial tasks.},
  author = {Yao Lai and Jinxin Liu and Zhentao Tang and Bin Wang and Jianye Hao and Ping Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lai2023chipformer.pdf:pdf},
  mdate = {2024-08-30},
  pages = {18346--18364},
  pdf = {https://proceedings.mlr.press/v202/lai23c/lai23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ChiPFormer: Transferable Chip Placement via Offline Decision Transformer},
  url = {https://proceedings.mlr.press/v202/lai23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lai2023fpdiffusion,
  abstract = {Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of the score FPE, and we show the effectiveness of this approach across various datasets.},
  author = {Chieh-Hsin Lai and Yuhta Takida and Naoki Murata and Toshimitsu Uesaka and Yuki Mitsufuji and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lai2023fpdiffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18365--18398},
  pdf = {https://proceedings.mlr.press/v202/lai23d/lai23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation},
  url = {https://proceedings.mlr.press/v202/lai23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{cle2023private,
  abstract = {This work studies the estimation of many statistical quantiles under differential privacy. More precisely, given a distribution and access to i.i.d. samples from it, we study the estimation of the inverse of its cumulative distribution function (the quantile function) at specific points. For instance, this task is of key importance in private data generation. We present two different approaches. The first one consists in privately estimating the empirical quantiles of the samples and using this result as an estimator of the quantiles of the distribution.},
  author = {Cl{\'e}ment Lalanne and Aur{\'e}lien Garivier and R{\'e}mi Gribonval},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/cle2023private.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18399--18418},
  pdf = {https://proceedings.mlr.press/v202/lalanne23a/lalanne23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Private Statistical Estimation of Many Quantiles},
  url = {https://proceedings.mlr.press/v202/lalanne23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lam2023bootstrap,
  abstract = {The bootstrap is a popular data-driven method to quantify statistical uncertainty, but for modern high-dimensional problems, it could suffer from huge computational costs due to the need to repeatedly generate resamples and refit models. We study the use of bootstraps in high-dimensional environments with a small number of resamples. In particular, we show that with a recent "cheap" bootstrap perspective, using a number of resamples as small as one could attain valid coverage even when the dimension grows closely with the sample size, thus strongly supporting the implementability of the bootstrap for large-scale problems. We validate our theoretical results and compare the performance of our approach with other benchmarks via a range of experiments.},
  author = {Henry Lam and Zhenyuan Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lam2023bootstrap.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18419--18453},
  pdf = {https://proceedings.mlr.press/v202/lam23a/lam23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bootstrap in High Dimension with Low Computation},
  url = {https://proceedings.mlr.press/v202/lam23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lam2023legendretron,
  abstract = {Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as properness, which asserts that Bayes' rule is optimal. Recent works have sought to learn losses and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps ℝ to [0,1] to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between ℝ^(C-1) and the projected probability simplex Δ̃^(C-1) by using monotonicity of gradients of convex functions. We present LegendreTron as a novel and practical method that jointly learns proper canonical losses and probabilities for multiclass problems.},
  author = {Kevin H. Lam and Christian J. Walder and Spiridon I. Penev and Richard Nock},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lam2023legendretron.pdf:pdf},
  mdate = {2024-06-07},
  pages = {18454--18470},
  pdf = {https://proceedings.mlr.press/v202/lam23b/lam23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LegendreTron: Uprising Proper Multiclass Loss Learning},
  url = {https://proceedings.mlr.press/v202/lam23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lamurias2023metagenomic,
  abstract = {Current state-of-the-art techniques for metagenomic binning only utilize local features for the individual DNA sequences (contigs), neglecting additional information such as the assembly graph, in which the contigs are connected according to overlapping reads, and gene markers identified in the contigs. In this paper, we propose the use of a Variational AutoEncoder (VAE) tailored to leverage auxiliary structural information about contig relations when learning contig representations for subsequent metagenomic binning. Our method, CCVAE, improves on previous work that used VAEs for learning latent representations of the individual contigs, by constraining these representations according to the connectivity information from the assembly graph, and incorporates into the model additional information in the form of marker genes to better differentiate contigs from different genomes.},
  author = {Andre Lamurias and Alessandro Tibo and Katja Hose and Mads Albertsen and Thomas Dyhre Nielsen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lamurias2023metagenomic.pdf:pdf},
  mdate = {2023-09-30},
  pages = {18471--18481},
  pdf = {https://proceedings.mlr.press/v202/lamurias23a/lamurias23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Metagenomic Binning using Connectivity-constrained Variational Autoencoders},
  url = {https://proceedings.mlr.press/v202/lamurias23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lan2023bootstrapped,
  abstract = {In reinforcement learning ({RL}), state representations are key to dealing with large or continuous state spaces. While one of the promises of deep learning algorithms is to automatically construct features well-tuned for the task they try to solve, such a representation might not emerge from end-to-end training of deep {RL} agents. To mitigate this issue, auxiliary objectives are often incorporated into the learning process and help shape the learnt state representation. Bootstrapping methods are today's method of choice to make these additional predictions. Yet, it is unclear which features these algorithms capture and how they relate to those from other auxiliary-task-based approaches. In this paper, the authors address this gap and provide a theoretical characterization of the state representation learnt by temporal difference learning. Surprisingly, they find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting. They describe the efficacy of these representations for policy evaluation, and use their theoretical analysis to design new auxiliary learning rules. The authors complement their theoretical results with an empirical comparison of these learning rules for different cumulant functions on classic domains such as the four-room domain and Mountain Car.},
  author = {Charline Le Lan and Stephen Tu and Mark Rowland and Anna Harutyunyan and Rishabh Agarwal and Marc G. Bellemare and Will Dabney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lan2023bootstrapped.pdf:pdf},
  mdate = {2024-10-02},
  pages = {18686--18713},
  pdf = {https://proceedings.mlr.press/v202/le-lan23a/le-lan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bootstrapped Representations in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/le-lan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lancewicki2023delayadapted,
  abstract = {Policy Optimization ({PO}) is one of the most popular methods in Reinforcement Learning ({RL}). Thus, theoretical guarantees for {PO} algorithms have become especially important to the {RL} community. In this paper, we study {PO} in adversarial {MDP}s with a challenge that arises in almost every real-world application -- delayed bandit feedback. We give the first near-optimal regret bounds for {PO} in tabular {MDP}s, and may even surpass state-of-the-art (which uses less efficient methods). Our novel Delay-Adapted {PO} ({DAPO}) is easy to implement and to generalize, allowing us to extend our algorithm to: (i) infinite state space under the assumption of linear $Q$-function, proving the first regret bounds for delayed feedback with function approximation. (ii) deep {RL}, demonstrating its effectiveness in experiments on {MuJoCo} domains.},
  author = {Tal Lancewicki and Aviv Rosenberg and Dmitry Sotnikov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lancewicki2023delayadapted.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18482--18534},
  pdf = {https://proceedings.mlr.press/v202/lancewicki23a/lancewicki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Delay-Adapted Policy Optimization and Improved Regret for Adversarial MDP with Delayed Bandit Feedback},
  url = {https://proceedings.mlr.press/v202/lancewicki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lange2023lottery,
  abstract = {Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies ({ES}) and characterize qualitative differences compared to gradient descent ({GD})-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to {GD}-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different {ES}, related tasks and even to {GD}-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to {GD}, {ES} explore diverse and flat local optima and do not preserve linear mode connectivity across sparsity levels and independent runs. The results highlight qualitative differences between evolution and gradient-based learning dynamics, which can be uncovered by the study of iterative pruning procedures.},
  author = {Robert Tjarko Lange and Henning Sprekeler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lange2023lottery.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18535--18547},
  pdf = {https://proceedings.mlr.press/v202/lange23a/lange23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability},
  url = {https://proceedings.mlr.press/v202/lange23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{laroche2023occupancy,
  abstract = {The state-action occupancy measure of a policy is the expected (discounted or undiscounted) number of times a state-action couple is visited in a trajectory. For decades, {RL} books have been reporting the occupancy equivalence between Markovian and non-Markovian policies in countable state-action spaces under mild conditions. This equivalence states that the occupancy of any non-Markovian policy can be equivalently obtained by a Markovian policy, i.e. a memoryless probability distribution, conditioned only on its current state. While expected, for technical reasons, the translation of this result to continuous state space has resisted until now. Our main contribution is to fill this gap and to provide a general measure-theoretic treatment of the problem, permitting, in particular, its extension to continuous {MDP}s. Furthermore, we show that when the occupancy is infinite, we may encounter some non-trivial cases where the result does not hold anymore.},
  author = {Romain Laroche and Remi Tachet des Combes},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/laroche2023occupancy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18548--18562},
  pdf = {https://proceedings.mlr.press/v202/laroche23a/laroche23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Occupancy Measure of Non-{M}arkovian Policies in Continuous {MDP}s},
  url = {https://proceedings.mlr.press/v202/laroche23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lassota2023minimalistic,
  abstract = {We consider non-clairvoyant scheduling with online precedence constraints, where an algorithm is oblivious to any job dependencies and learns about a job only if all of its predecessors have been completed. Given strong impossibility results in classical competitive analysis, we investigate the problem in a learning-augmented setting, where an algorithm has access to predictions without any quality guarantee. We discuss different prediction models: novel problem-specific models as well as general ones, which have been proposed in previous works. We present lower bounds and algorithmic upper bounds for different precedence topologies, and thereby give a structured overview on which and how additional (possibly erroneous) information helps for designing better algorithms.},
  author = {Alexandra Anna Lassota and Alexander Lindermayr and Nicole Megow and Jens Schlöter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lassota2023minimalistic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18563--18583},
  pdf = {https://proceedings.mlr.press/v202/lassota23a/lassota23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Minimalistic Predictions to Schedule Jobs with Online Precedence Constraints},
  url = {https://proceedings.mlr.press/v202/lassota23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lattanzi2023speeding,
  abstract = {The Bellman-Ford algorithm is a basic primitive for computing single source shortest paths in graphs with negative weight edges. Its running time is governed by the order the algorithm examines vertices for iterative updates on the value of their shortest path. In this work we study this problem through the lens of ``Algorithms with predictions,'' and show how to leverage auxiliary information from similar instances to improve the running time. We do this by identifying the key problem of Minimum Violation Permutations, and give algorithms with strong approximation guarantees as well as formal lower bounds. We complement the theoretical analysis with an empirical evaluation, showing that this approach can lead to a significant speed up in practice.},
  author = {Silvio Lattanzi and Ola Svensson and Sergei Vassilvitskii},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lattanzi2023speeding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18584--18598},
  pdf = {https://proceedings.mlr.press/v202/lattanzi23a/lattanzi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Speeding Up {B}ellman {F}ord via Minimum Violation Permutations},
  url = {https://proceedings.mlr.press/v202/lattanzi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lauffer2023who,
  abstract = {To optimally coordinate with others in cooperative games, it is often crucial to have information about one's collaborators: successful driving requires understanding which side of the road to drive on. However, not every feature of collaborators is strategically relevant: the fine-grained acceleration of drivers may be ignored while maintaining optimal coordination. We show that there is a well-defined dichotomy between strategically relevant and irrelevant information. Moreover, we show that, in dynamic games, this dichotomy has a compact representation that can be efficiently computed via a Bellman backup operator. We apply this algorithm to analyze the strategically relevant information for tasks in both a standard and a partially observable version of the Overcooked environment. Theoretical and empirical results show that our algorithms are significantly more efficient than baselines.},
  author = {Niklas Lauffer and Ameesh Shah and Micah Carroll and Michael D. Dennis and Stuart Russell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lauffer2023who.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18599--18613},
  pdf = {https://proceedings.mlr.press/v202/lauffer23a/lauffer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Who Needs to Know? Minimal Knowledge for Optimal Coordination},
  url = {https://proceedings.mlr.press/v202/lauffer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lavington2023targetbased,
  abstract = {We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a target space (e.g. the logits output by a linear model for classification) that can be minimized efficiently. This allows for multiple parameter updates to the model, amortizing the cost of gradient computation. In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the {SSO} algorithm, which can be viewed as projected stochastic gradient descent in the target space. This connection enables us to prove theoretical guarantees for {SSO} when minimizing convex functions. Our framework allows the use of standard stochastic optimization algorithms to construct surrogates which can be minimized by any deterministic optimization method. To evaluate our framework, we consider a suite of supervised learning and imitation learning problems. Our experiments indicate the benefits of target optimization and the effectiveness of {SSO}.},
  author = {Jonathan Wilder Lavington and Sharan Vaswani and Reza Babanezhad Harikandeh and Mark Schmidt and Nicolas Le Roux},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lavington2023targetbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18614--18651},
  pdf = {https://proceedings.mlr.press/v202/lavington23a/lavington23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Target-based Surrogates for Stochastic Optimization},
  url = {https://proceedings.mlr.press/v202/lavington23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lawless2023cluster,
  abstract = {Clustering is an unsupervised learning problem that aims to partition unlabelled data points into groups with similar features. Traditional clustering algorithms provide limited insight into the groups they find as their main focus is accuracy and not the interpretability of the group assignments. This has spurred a recent line of work on explainable machine learning for clustering. In this paper we focus on the cluster description problem where, given a dataset and its partition into clusters, the task is to explain the clusters. We introduce a new approach to explain clusters by constructing polyhedra around each cluster while minimizing either the complexity of the resulting polyhedra or the number of features used in the description. We formulate the cluster description problem as an integer program and present a column generation approach to search over an exponential number of candidate half-spaces that can be used to build the polyhedra. To deal with large datasets, we introduce a novel grouping scheme that first forms smaller groups of data points and then builds the polyhedra around the grouped data, a strategy which out-performs simply sub-sampling data. Compared to state of the art cluster description algorithms, our approach is able to achieve competitive interpretability with improved description accuracy.},
  author = {Connor Lawless and Oktay Günlük},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lawless2023cluster.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18652--18666},
  pdf = {https://proceedings.mlr.press/v202/lawless23a/lawless23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cluster Explanation via Polyhedral Descriptions},
  url = {https://proceedings.mlr.press/v202/lawless23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{le2023pretraining,
  abstract = {The gap between speech and text modalities is a major challenge in speech-to-text translation ({ST}). Different methods have been proposed to reduce this gap, but most of them require architectural changes in {ST} training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the {ST} model. First, we show that the connectionist temporal classification ({CTC}) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with {CTC} consistently achieves better final {ST} accuracy. Nevertheless, {CTC} is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining {CTC} and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wasserstein space. Extensive experiments on the standard CoVoST-2 and MuST-C datasets show that our pre-training method applied to the vanilla encoder-decoder Transformer achieves state-of-the-art performance under the no-external-data setting, and performs on par with recent strong multi-task learning systems trained with external data. Finally, our method can also be applied on top of these multi-task systems, leading to further improvements for these models.},
  author = {Phuong-Hang Le and Hongyu Gong and Changhan Wang and Juan Pino and Benjamin Lecouteux and Didier Schwab},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/le2023pretraining.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18667--18685},
  pdf = {https://proceedings.mlr.press/v202/le23a/le23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pre-training for Speech Translation: {CTC} Meets Optimal Transport},
  url = {https://proceedings.mlr.press/v202/le23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lechner2023strategic,
  abstract = {In many human-centric applications for Machine Learning instances will adapt to a classifier after its deployment. The field of strategic classification deals with this issue by aiming for a classifier that balances the trade-off between correctness and robustness to manipulation. This task is made harder if the underlying manipulation structure (i.e., the set of manipulations available at every instance) is unknown to the learner. We propose a novel batch-learning setting in which we use unlabeled data from previous rounds to estimate the manipulation structure. We show that it is possible to learn a close-to-optimal classifier in terms of strategic loss, even without prior knowledge of feasible manipulations. Notably, we do not assume agents will act in a best-response manner, but only require that observed manipulations are feasible.},
  author = {Tosca Lechner and Ruth Urner and Shai Ben-David},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lechner2023strategic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18714--18732},
  pdf = {https://proceedings.mlr.press/v202/lechner23a/lechner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Strategic Classification with Unknown User Manipulations},
  url = {https://proceedings.mlr.press/v202/lechner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023implicit,
  abstract = {The success of deep learning is greatly attributed to stochastic gradient descent (SGD), yet it remains unclear how SGD finds well-generalized models. We demonstrate that SGD has an implicit regularization effect on the logit-weight Jacobian norm of neural networks. This regularization effect is weighted with the impurity of the probability output, and thus it is active in a certain phase of training. Moreover, based on these findings, we propose a novel optimization method that explicitly regularizes the Jacobian norm, which leads to similar performance as other state-of-the-art sharpness-aware optimization methods.},
  author = {Sungyoon Lee and Jinseong Park and Jaewook Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023implicit.pdf:pdf},
  mdate = {2024-02-05},
  pages = {19141--19184},
  pdf = {https://proceedings.mlr.press/v202/lee23q/lee23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Implicit {Jacobian} Regularization Weighted with Impurity of Probability Output},
  url = {https://proceedings.mlr.press/v202/lee23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023codi,
  abstract = {With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.},
  author = {Chaejeong Lee and Jayoung Kim and Noseong Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023codi.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18940--18956},
  pdf = {https://proceedings.mlr.press/v202/lee23i/lee23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CoDi}: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis},
  url = {https://proceedings.mlr.press/v202/lee23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023towards,
  abstract = {Graph neural networks (GNNs) learn the representation of graph-structured data, and their expressiveness can be further enhanced by inferring node relations for propagation. Attention-based GNNs infer neighbor importance to manipulate the weight of its propagation. Despite their popularity, the discussion on deep graph attention and its unique challenges has been limited. In this work, we investigate some problematic phenomena related to deep graph attention, including vulnerability to over-smoothed features and smooth cumulative attention. Through theoretical and empirical analyses, we show that various attention-based GNNs suffer from these problems. Motivated by our findings, we propose AERO-GNN, a novel GNN architecture designed for deep graph attention. AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with its adaptive and less smooth attention functions and higher performance at deep layers (up to 64). On 9 out of 12 node classification benchmarks, AERO-GNN outperforms the baseline GNNs, highlighting the advantages of deep graph attention.},
  author = {Soo Yong Lee and Fanchen Bu and Jaemin Yoo and Kijung Shin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18774--18795},
  pdf = {https://proceedings.mlr.press/v202/lee23b/lee23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Deep Attention in Graph Neural Networks: Problems and Remedies},
  url = {https://proceedings.mlr.press/v202/lee23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023ingram,
  abstract = {Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.},
  author = {Jaejun Lee and Chanyoung Chung and Joyce Jiyoung Whang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023ingram.pdf:pdf},
  mdate = {2024-02-05},
  pages = {18796--18809},
  pdf = {https://proceedings.mlr.press/v202/lee23c/lee23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{InGram}: Inductive Knowledge Graph Embedding via Relation Graphs},
  url = {https://proceedings.mlr.press/v202/lee23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023optimality,
  abstract = {In the stochastic multi-armed bandit problem, a randomized probability matching policy called Thompson sampling (TS) has shown excellent performance in various reward models. In addition to the empirical performance, TS has been shown to achieve asymptotic problem-dependent lower bounds in several models. However, its optimality has been mainly addressed under light-tailed or one-parameter models that belong to exponential families. In this paper, we consider the optimality of TS for the Pareto model that has a heavy tail and is parameterized by two unknown parameters. Specifically, we discuss the optimality of TS with probability matching priors that include the Jeffreys prior and the reference priors. We first prove that TS with certain probability matching priors can achieve the optimal regret bound. Then, we show the suboptimality of TS with other priors, including the Jeffreys and the reference priors. We also show that TS with the Jeffreys and reference priors can achieve the asymptotic lower bound if one uses a truncation procedure.},
  author = {Jongyeong Lee and Junya Honda and Chao-Kai Chiang and Masashi Sugiyama},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023optimality.pdf:pdf},
  mdate = {2024-10-06},
  pages = {18810--18851},
  pdf = {https://proceedings.mlr.press/v202/lee23d/lee23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimality of {Thompson} Sampling with Noninformative Priors for {Pareto} Bandits},
  url = {https://proceedings.mlr.press/v202/lee23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023conditional,
  abstract = {Molecular relational learning, whose goal is to learn the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. Recently, graph neural networks have shown great success in molecular relational learning by modeling a molecule as a graph structure, and considering atom-level interactions between two molecules. Despite their success, existing molecular relational learning methods tend to overlook the nature of chemistry, i.e., a chemical compound is composed of multiple substructures such as functional groups that cause distinctive chemical reactions. In this work, we propose a novel relational learning framework, called CGIB, that predicts the interaction behavior between a pair of graphs by detecting core subgraphs therein. The main idea is, given a pair of graphs, to find a subgraph from a graph that contains the minimal sufficient information regarding the task at hand conditioned on the paired graph based on the principle of conditional graph information bottleneck. We argue that our proposed method mimics the nature of chemical reactions, i.e., the core substructure of a molecule varies depending on which other molecule it interacts with. Extensive experiments on various tasks with real-world datasets demonstrate the superiority of CGIB over state-of-the-art baselines.},
  author = {Namkyeong Lee and Dongmin Hyun and Gyoung S. Na and Sungwon Kim and Junseok Lee and Chanyoung Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023conditional.pdf:pdf},
  mdate = {2025-02-03},
  pages = {18852--18871},
  pdf = {https://proceedings.mlr.press/v202/lee23e/lee23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conditional Graph Information Bottleneck for Molecular Relational Learning},
  url = {https://proceedings.mlr.press/v202/lee23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023exploring,
  abstract = {A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules with completely different structures that may have even better properties than known molecules for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion (MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules rather than generating unseen yet trivial ones. We experimentally validate that MOOD is able to explore the chemical space beyond the training distribution, generating molecules that outscore ones found with existing methods, and even the top 0.01% of the original training pool.},
  author = {Seul Lee and Jaehyeong Jo and Sung Ju Hwang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023exploring.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18872--18892},
  pdf = {https://proceedings.mlr.press/v202/lee23f/lee23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exploring Chemical Space with Score-based Out-of-distribution Generation},
  url = {https://proceedings.mlr.press/v202/lee23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023pix2struct,
  abstract = {Visually-situated language is ubiquitous—sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.},
  author = {Kenton Lee and Mandar Joshi and Iulia Raluca Turc and Hexiang Hu and Fangyu Liu and Julian Martin Eisenschlos and Urvashi Khandelwal and Peter Shaw and Ming-Wei Chang and Kristina Toutanova},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023pix2struct.pdf:pdf},
  mdate = {2024-10-02},
  pages = {18893--18912},
  pdf = {https://proceedings.mlr.press/v202/lee23g/lee23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Pix2Struct}: Screenshot Parsing as Pretraining for Visual Language Understanding},
  url = {https://proceedings.mlr.press/v202/lee23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023flexround,
  abstract = {Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corresponding scales, and thus, flexibly quantize pre-trained weights depending on their magnitudes. To the best of our knowledge, our work is the first to carry out comprehensive experiments on not only image classification and natural language understanding but also natural language generation. Moreover, we demonstrate, for the first time, that large language models can be efficiently quantized, with only a negligible impact on performance compared to half-precision baselines.},
  author = {Jung Hyun Lee and Jeonghoon Kim and Se Jung Kwon and Dongsoo Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lee2023flexround.pdf:pdf},
  mdate = {2023-08-28},
  pages = {18913--18939},
  pdf = {https://proceedings.mlr.press/v202/lee23h/lee23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FlexRound}: Learnable Rounding based on Element-wise Division for Post-Training Quantization},
  url = {https://proceedings.mlr.press/v202/lee23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023minimizing,
  abstract = {Recent {ODE}/{SDE}-based generative models, such as diffusion models, rectified flows, and flow matching, define a generative process as a time reversal of a fixed forward process. Even though these models show impressive performance on large-scale datasets, numerical simulation requires multiple evaluations of a neural network, leading to a slow sampling speed. We attribute the reason to the high curvature of the learned generative trajectories, as it is directly related to the truncation error of a numerical solver. Based on the relationship between the forward process and the curvature, here we present an efficient method of training the forward process to minimize the curvature of generative trajectories without any {ODE}/{SDE} simulation. Experiments show that our method achieves a lower curvature than previous models and, therefore, decreased sampling costs while maintaining competitive performance.},
  author = {Sangyun Lee and Beomsu Kim and Jong Chul Ye},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023minimizing.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {18957--18973},
  pdf = {https://proceedings.mlr.press/v202/lee23j/lee23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Minimizing Trajectory Curvature of {ODE}-based Generative Models},
  url = {https://proceedings.mlr.press/v202/lee23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023hlikelihood,
  abstract = {Deep Neural Networks ({DNN}s) are one of the most powerful tools for prediction, but many of them implicitly assume that the data are statistically independent. However, in the real world, it is common for large-scale data to be clustered with temporal-spatial correlation structures. Variational approaches and integrated likelihood approaches have been proposed to obtain approximate maximum likelihood estimators ({MLE}s) for correlated data. However, due to the large size of data, they cannot provide exact {MLE}s. In this study, we propose a new hierarchical likelihood approach to {DNN}s with correlated random effects for clustered data. By jointly optimizing the negative h-likelihood loss, we can provide exact {MLE}s for both mean and dispersion parameters, as well as the best linear unbiased predictors for the random effects. Moreover, the hierarchical likelihood allows a computable procedure for restricted maximum likelihood estimators of dispersion parameters. The proposed two-step algorithm enables online learning for the neural networks, whereas the integrated likelihood cannot decompose like a widely-used loss function in {DNN}s. The proposed h-likelihood approach offers several advantages, which we demonstrate through numerical studies and real data analyses.},
  author = {Hangbin Lee and Youngjo Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023hlikelihood.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {18974--18987},
  pdf = {https://proceedings.mlr.press/v202/lee23k/lee23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {H-Likelihood Approach to Deep Neural Networks with Temporal-Spatial Random Effects for High-Cardinality Categorical Features},
  url = {https://proceedings.mlr.press/v202/lee23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023importance,
  abstract = {Recently, unsupervised representation learning ({URL}) has improved the sample efficiency of Reinforcement Learning ({RL}) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, we found that the subspace of the latent representations collapses into a low-dimensional manifold, which we call representational collapse. This collapse can result in a significant loss of information, leading to poor downstream task performance. To address this problem, we propose a novel {URL} framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, significantly improving the sample efficiency of state-of-the-art {URL} methods on the Atari 100k benchmark.},
  author = {Hojoon Lee and Koanho Lee and Dongyoon Hwang and Hyunho Lee and Byungkun Lee and Jaegul Choo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023importance.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {18988--19009},
  pdf = {https://proceedings.mlr.press/v202/lee23l/lee23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/lee23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023hetal,
  abstract = {Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present {HETAL}, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the {CKKS} homomorphic encryption scheme. {HETAL} is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known benchmark datasets show total training times of 567--3442 seconds, which is less than an hour.},
  author = {Seewoo Lee and Garam Lee and Jung Woo Kim and Junbum Shin and Mun-Kyu Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023hetal.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {19010--19035},
  pdf = {https://proceedings.mlr.press/v202/lee23m/lee23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{HETAL}: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption},
  url = {https://proceedings.mlr.press/v202/lee23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023qasa,
  abstract = {Reasoning is the crux of intellectual thinking. While question answering ({QA}) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow {QA} without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first propose the {QASA} benchmark that consists of 1798 novel question answering pairs that require full-stack reasoning on scientific articles in {AI} and {ML} fields. Then we propose the {QASA} approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that {QASA}'s full-stack inference outperforms the state-of-the-art {InstructGPT} by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.},
  author = {Yoonjoo Lee and Kyungjae Lee 0002 and Sunghyun Park 0005 and Dasol Hwang and Jaehyeon Kim and Hong-In Lee and Moontae Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023qasa.pdf:pdf},
  mdate = {2024-08-08},
  month = {7},
  pages = {19036--19052},
  pdf = {https://proceedings.mlr.press/v202/lee23n/lee23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{QASA}: Advanced Question Answering on Scientific Articles},
  url = {https://proceedings.mlr.press/v202/lee23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023demystifying,
  abstract = {Evaluating the performance of machine learning models under distribution shifts is challenging, especially when we only have unlabeled data from the shifted (target) domain, along with labeled data from the original (source) domain. Recent work suggests that the notion of disagreement, the degree to which two models trained with different randomness differ on the same input, is a key to tackling this problem. Experimentally, disagreement and prediction error have been shown to be strongly connected, which has been used to estimate model performance. Experiments have led to the discovery of the disagreement-on-the-line phenomenon, whereby the classification error under the target domain is often a linear function of the classification error under the source domain; and whenever this property holds, disagreement under the source and target domain follow the same linear relation. In this work, we develop a theoretical foundation for analyzing disagreement in high-dimensional random features regression; and study under what conditions the disagreement-on-the-line phenomenon occurs in our setting. Experiments on {CIFAR}-10-C, Tiny {ImageNet}-C, and Camelyon17 are consistent with our theory and support the universality of the theoretical findings.},
  author = {Donghwan Lee and Behrad Moniri and Xinmeng Huang and Edgar Dobriban and Hamed Hassani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023demystifying.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {19053--19093},
  pdf = {https://proceedings.mlr.press/v202/lee23o/lee23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Demystifying Disagreement-on-the-Line in High Dimensions},
  url = {https://proceedings.mlr.press/v202/lee23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lee2023unsupervised,
  abstract = {Learning shared structures across changing environments enables an agent to efficiently retain obtained knowledge and transfer it between environments. A skill is a promising concept to represent shared structures. Several recent works proposed unsupervised skill discovery algorithms that can discover useful skills without a reward function. However, they focused on discovering skills in stationary environments or assumed that a skill being trained is fixed within an episode, which is insufficient to learn and represent shared structures. We propose a new unsupervised skill discovery algorithm that discovers a set of skills that can represent shared structures across changing environments. Our algorithm trains incremental skills and encourages a new skill to expand state coverage obtained with compositions of previously learned skills. The experimental results show that our algorithm acquires skills that represent shared structures across changing maze navigation and locomotion environments. Furthermore, we demonstrate that our skills are more useful than baselines on downstream tasks.},
  author = {Sang-Hyun Lee and Seung-Woo Seo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lee2023unsupervised.pdf:pdf},
  mdate = {2023-12-17},
  month = {7},
  pages = {19185--19199},
  pdf = {https://proceedings.mlr.press/v202/lee23r/lee23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unsupervised Skill Discovery for Learning Shared Structures across Changing Environments},
  url = {https://proceedings.mlr.press/v202/lee23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lei2023generalization,
  abstract = {Recently, contrastive learning has found impressive success in advancing the state of the art in solving various machine learning tasks. However, the existing generalization analysis is very limited or even not meaningful. In particular, the existing generalization error bounds depend linearly on the number $k$ of negative examples while it was widely shown in practice that choosing a large $k$ is necessary to guarantee good generalization of contrastive learning in downstream tasks. In this paper, we establish novel generalization bounds for contrastive learning which do not depend on $k$, up to logarithmic terms. Our analysis uses structural results on empirical covering numbers and {Rademacher} complexities to exploit the {Lipschitz} continuity of loss functions. For self-bounding {Lipschitz} loss functions, we further improve our results by developing optimistic bounds which imply fast rates in a low noise condition. We apply our results to learning with both linear representation and nonlinear representation by deep neural networks, for both of which we derive {Rademacher} complexity bounds to get improved generalization bounds.},
  author = {Yunwen Lei and Tianbao Yang and Yiming Ying and Ding-Xuan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lei2023generalization.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {19200--19227},
  pdf = {https://proceedings.mlr.press/v202/lei23a/lei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalization Analysis for Contrastive Representation Learning},
  url = {https://proceedings.mlr.press/v202/lei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{leibovich2023learning,
  abstract = {We propose iterative inversion -- an algorithm for learning an inverse function without input-output pairs, but only with samples from the desired output distribution and access to the forward function. The key challenge is a distribution shift between the desired outputs and the outputs of an initial random guess, and we prove that iterative inversion can steer the learning correctly, under rather strict conditions on the function. We apply iterative inversion to learn control, where the input is a set of demonstrations of desired behavior, given as video embeddings of trajectories (without actions), and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. Our approach does not require rewards, and only employs supervised learning, which can be easily scaled to use state-of-the-art trajectory embedding techniques and policy representations. We demonstrate the effectiveness of our method on several continuous control tasks and show that it can learn successful policies from a few demonstrations.},
  author = {Gal Leibovich and Guy Jacob and Or Avner and Gal Novik and Aviv Tamar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/leibovich2023learning.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {19228--19255},
  pdf = {https://proceedings.mlr.press/v202/leibovich23a/leibovich23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Control by Iterative Inversion},
  url = {https://proceedings.mlr.press/v202/leibovich23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lemos2023samplingbased,
  abstract = {Parameter inference, i.e., inferring the posterior distribution of statistical model parameters given data, is central to many scientific disciplines. Generative models can serve as an alternative to {Markov} chain {Monte} {Carlo} for parameter inference, both in likelihood-based and simulation-based problems. However, assessing the accuracy of posteriors encoded in generative models is not straightforward. In this paper, we introduce Tests of Accuracy with Random Points ({TARP}) for evaluating posterior estimators. Our method differs from previously-existing coverage-based methods, which require posterior evaluations. We prove that our approach is necessary and sufficient to show that a posterior estimator is accurate, and demonstrate the method on synthetic examples. We show that {TARP} can be used to test the results of posterior inference analyses in high-dimensional spaces, where generative models are often needed due to the intractability of standard inference methods.},
  author = {Pablo Lemos and Adam Coogan and Yashar Hezaveh and Laurence Perreault Levasseur},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lemos2023samplingbased.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {19256--19273},
  pdf = {https://proceedings.mlr.press/v202/lemos23a/lemos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sampling-Based Accuracy Testing of Posterior Estimators for General Inference},
  url = {https://proceedings.mlr.press/v202/lemos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{leviathan2023fast,
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel.},
  author = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/leviathan2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19274--19286},
  pdf = {https://proceedings.mlr.press/v202/leviathan23a/leviathan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Inference from Transformers via Speculative Decoding},
  url = {https://proceedings.mlr.press/v202/leviathan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{levy2023efficient,
  abstract = {We present the OMG-CMDP! algorithm for regret minimization in adversarial Contextual MDPs. The algorithm operates under the minimal assumptions of realizable function class and access to online least squares and log loss regression oracles. Our algorithm is efficient (assuming efficient online regression oracles), simple and robust to approximation errors. It enjoys an $\widetilde{O}(H^{2.5} \sqrt{ T|S||A| ( \mathcal{R}_{TH}(\mathcal{O}) + H \log(\delta^{-1}) )})$ regret guarantee, with $T$ being the number of episodes, $S$ the state space, $A$ the action space, $H$ the horizon and $\mathcal{R}_{TH}(\mathcal{O}) = \mathcal{R}_{TH}(\mathcal{O}_{sq}^\mathcal{F}) + \mathcal{R}_{TH}(\mathcal{O}_{log}^\mathcal{P})$ is the sum of the square and log-loss regression oracles' regret, used to approximate the context-dependent rewards and dynamics, respectively. To the best of our knowledge, our algorithm is the first efficient rate optimal regret minimization algorithm for adversarial CMDPs that operates under the minimal standard assumption of online function approximation.},
  author = {Orin Levy and Alon Cohen and Asaf B. Cassel and Yishay Mansour},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/levy2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19287--19314},
  pdf = {https://proceedings.mlr.press/v202/levy23a/levy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Rate Optimal Regret for Adversarial Contextual {MDPs} Using Online Function Approximation},
  url = {https://proceedings.mlr.press/v202/levy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ley2023globece,
  abstract = {Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global \& Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathematical analysis of categorical feature translations, utilising it in our method. Experimental evaluation with publicly available datasets and user studies demonstrate that GLOBE-CE performs significantly better than the current state-of-the-art across multiple metrics (e.g., speed, reliability).},
  author = {Dan Ley and Saumitra Mishra and Daniele Magazzeni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ley2023globece.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19315--19342},
  pdf = {https://proceedings.mlr.press/v202/ley23a/ley23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GLOBE-CE}: A Translation Based Approach for Global Counterfactual Explanations},
  url = {https://proceedings.mlr.press/v202/ley23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023local,
  abstract = {In recent years, there has been a significant amount of research focused on expanding the expressivity of Graph Neural Networks (GNNs) beyond the Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of graphs. In this study, we investigate the expressivity of GNNs from the perspective of graph search. Specifically, we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute graph representations that extend beyond the 1-WL. We show that our colouring scheme inherits useful properties from graph search that can help solve problems like graph biconnectivity. Under certain conditions, the expressivity of GNNs increases hierarchically with the radius of the search neighbourhood. To further investigate our scheme, we develop a new type of GNN based on two search strategies: breadth-first search and depth-first search, highlighting the graph properties they can capture on top of 1-WL.},
  author = {Shouheng Li and Dongwoo Kim and Qing Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023local.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19616--19637},
  pdf = {https://proceedings.mlr.press/v202/li23n/li23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Local Vertex Colouring Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/li23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023structured,
  abstract = {We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose Structured Cooperative Learning (SCooL), in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning methods on an extensive set of benchmarks, on which SCooL always achieves the highest accuracy of personalized models and significantly outperforms other baselines on communication efficiency.},
  author = {Shuangtong Li and Tianyi Zhou and Xinmei Tian and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023structured.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20599--20622},
  pdf = {https://proceedings.mlr.press/v202/li23az/li23az.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Structured Cooperative Learning with Graphical Model Priors},
  url = {https://proceedings.mlr.press/v202/li23az.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023analysis,
  abstract = {In federated learning (FL) systems, e.g., wireless networks, the communication cost between the clients and the central server can often be a bottleneck. To reduce the communication cost, the paradigm of communication compression has become a popular strategy in the literature. In this paper, we focus on biased gradient compression techniques in non-convex FL problems. In the classical setting of distributed learning, the method of error feedback (EF) is a common technique to remedy the downsides of biased gradient compression. In this work, we study a compressed FL scheme equipped with error feedback, named Fed-EF. We further propose two variants: Fed-EF-SGD and Fed-EF-AMS, depending on the choice of the global model optimizer. We provide a generic theoretical analysis, which shows that directly applying biased compression in FL leads to a non-vanishing bias in the convergence rate. The proposed Fed-EF is able to match the convergence rate of the full-precision FL counterparts under data heterogeneity with a linear speedup. Moreover, we develop a new analysis of the EF under partial client participation, which is an important scenario in FL. We prove that under partial participation, the convergence rate of Fed-EF exhibits an extra slow-down factor due to a so-called 'stale error compensation' effect. Finally, we also demonstrate that incorporating the two-way compression in Fed-EF does not change the convergence results.},
  author = {Xiaoyun Li and Ping Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023analysis.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19638--19688},
  pdf = {https://proceedings.mlr.press/v202/li23o/li23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Analysis of Error Feedback in Federated Non-Convex Optimization with Biased Compression: Fast Convergence and Partial Participation},
  url = {https://proceedings.mlr.press/v202/li23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023optimal,
  abstract = {Best Arm Identification (BAI) is a general online pure exploration framework to identify optimal decisions among candidates via sequential interactions. We pioneer the Optimal Arms identification with Knapsacks (OAK) problem, which extends the BAI setting to model the resource consumption. We present a novel OAK algorithm and prove the upper bound of our algorithm by exploring the relationship between selecting optimal actions and the structure of the feasible region. Our analysis introduces a new complexity measure, which builds a bridge between the OAK setting and bandits with knapsacks problem. We establish the instance-dependent lower bound for the OAK problem based on the new complexity measure. Our results show that the proposed algorithm achieves a near-optimal probability bound for the OAK problem. In addition, we demonstrate that our algorithm recovers or improves the state-of-the-art upper bounds for several special cases, including the simple OAK setting and some classical problems.},
  author = {Shaoang Li and Lan Zhang and Yingqi Yu and Xiangyang Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20529--20555},
  pdf = {https://proceedings.mlr.press/v202/li23aw/li23aw.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Arms Identification with Knapsacks},
  url = {https://proceedings.mlr.press/v202/li23aw.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023federated,
  abstract = {Federated learning (FL) is a trending training paradigm to utilize decentralized training data. FL allows clients to update model parameters locally for several epochs, then share them to a global model for aggregation. This training paradigm with multi-local step updating before aggregation exposes unique vulnerabilities to adversarial attacks. Adversarial training is a popular and effective method to improve the robustness of networks against adversaries. In this work, we formulate a general form of federated adversarial learning (FAL) that is adapted from adversarial learning in the centralized setting. On the client side of FL training, FAL has an inner loop to generate adversarial samples for adversarial training and an outer loop to update local model parameters. On the server side, FAL aggregates local model updates and broadcasts the aggregated model. We design a global robust training loss and formulate FAL training as a min-max optimization problem. Unlike classical centralized training convergence analysis, analyzing FAL convergence is significantly harder due to the complexity of min-max optimization, model not updating in the gradient direction due to multi-local updates, and inter-client heterogeneity. We address these challenges using gradient approximation and coupling techniques, presenting convergence analysis in the over-parameterized regime.},
  author = {Xiaoxiao Li and Zhao Song and Jiaming Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023federated.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19932--19959},
  pdf = {https://proceedings.mlr.press/v202/li23z/li23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Federated Adversarial Learning: A Framework with Convergence Analysis},
  url = {https://proceedings.mlr.press/v202/li23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023mahalo,
  abstract = {We study a new paradigm for sequential decision making, called offline policy learning from observations (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the data may not have full coverage. Such imperfection is common in real-world learning scenarios, and offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), offline IL from observations (ILfO), and offline reinforcement learning (RL). In this work, we present a generic approach to offline PLfO, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO). Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient coverage. We implement this idea by adversarially training data-consistent critic and reward functions, which forces the learned policy to be robust to data deficiency. We show that MAHALO consistently outperforms or matches specialized algorithms across a variety of offline PLfO tasks in theory and experiments.},
  author = {Anqi Li and Byron Boots and Ching-An Cheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023mahalo.pdf:pdf},
  mdate = {2025-06-25},
  pages = {19360--19384},
  pdf = {https://proceedings.mlr.press/v202/li23b/li23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MAHALO}: Unifying Offline Reinforcement Learning and Imitation Learning from Observations},
  url = {https://proceedings.mlr.press/v202/li23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023internet,
  abstract = {Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30--40 hours.},
  author = {Alexander Cong Li and Ellis Langham Brown and Alexei A. Efros and Deepak Pathak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023internet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19385--19406},
  pdf = {https://proceedings.mlr.press/v202/li23c/li23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Internet Explorer: Targeted Representation Learning on the Open Web},
  url = {https://proceedings.mlr.press/v202/li23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023tips,
  abstract = {Anytime neural networks (AnytimeNNs) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed AnytimeNNs are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of AnytimeNNs as a discrete-time Markov chain (DTMC) and use it to identify the paths that contribute the most to the training of AnytimeNNs. Based on this new DTMC-based analysis, we further propose TIPS, a framework to automatically design AnytimeNNs under various hardware constraints. Our experimental results show that TIPS can improve the convergence rate and test accuracy of AnytimeNNs. Compared to the existing AnytimeNNs approaches, TIPS improves the accuracy by 2\%--6.6\% on multiple datasets and achieves SOTA accuracy-FLOPs tradeoffs.},
  author = {Guihong Li and Kartikeya Bhardwaj and Yuedong Yang and Radu Marculescu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023tips.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19343--19359},
  pdf = {https://proceedings.mlr.press/v202/li23a/li23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TIPS}: Topologically Important Path Sampling for Anytime Neural Networks},
  url = {https://proceedings.mlr.press/v202/li23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023learning,
  abstract = {Efficient numerical solvers for partial differential equations empower science and engineering. One commonly employed numerical solver is the preconditioned conjugate gradient (PCG) algorithm, whose performance is largely affected by the preconditioner quality. However, designing high-performing preconditioner with traditional numerical methods is highly non-trivial, often requiring problem-specific knowledge and meticulous matrix operations. We present a new method that leverages learning-based approach to obtain an approximate matrix factorization to the system matrix to be used as a preconditioner in the context of PCG solvers. Our high-level intuition comes from the shared property between preconditioners and network-based PDE solvers that excels at obtaining approximate solutions at a low computational cost. Such observation motivates us to represent preconditioners as graph neural networks (GNNs). In addition, we propose a new loss function that rewrites traditional preconditioner metrics to incorporate inductive bias from PDE data distributions, enabling effective training of high-performing preconditioners. We conduct extensive experiments to demonstrate the efficacy and generalizability of our proposed approach on solving various 2D and 3D linear second-order PDEs.},
  author = {Yichen Li 0004 and Peter Yichen Chen and Tao Du 0001 and Wojciech Matusik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023learning.pdf:pdf},
  mdate = {2024-08-27},
  pages = {19425--19439},
  pdf = {https://proceedings.mlr.press/v202/li23e/li23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Preconditioners for Conjugate Gradient {PDE} Solvers},
  url = {https://proceedings.mlr.press/v202/li23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023prototypeoriented,
  abstract = {Unsupervised anomaly detection (UAD) of multivariate time series (MTS) aims to learn robust representations of normal multivariate temporal patterns. Existing UAD methods try to learn a fixed set of mappings for each MTS, entailing expensive computation and limited model adaptation. To address this pivotal issue, we propose a prototype-oriented UAD (PUAD) method under a probabilistic framework. Specifically, instead of learning the mappings for each MTS, the proposed PUAD views multiple MTSs as the distribution over a group of prototypes, which are extracted to represent a diverse set of normal patterns. To learn and regulate the prototypes, PUAD introduces a reconstruction-based unsupervised anomaly detection approach, which incorporates a prototype-oriented optimal transport method into a Transformer-powered probabilistic dynamical generative framework. Leveraging meta-learned transferable prototypes, PUAD can achieve high model adaptation capacity for new MTSs. Experiments on five public MTS datasets all verify the effectiveness of the proposed UAD method.},
  author = {Yuxin Li 0003 and Wenchao Chen and Bo Chen 0001 and Dongsheng Wang 0003 and Long Tian and Mingyuan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023prototypeoriented.pdf:pdf},
  mdate = {2025-01-13},
  pages = {19407--19424},
  pdf = {https://proceedings.mlr.press/v202/li23d/li23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Prototype-oriented unsupervised anomaly detection for multivariate time series},
  url = {https://proceedings.mlr.press/v202/li23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023parallel,
  abstract = {Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel Q-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation.},
  author = {Zechu Li and Tao Chen 0046 and Zhang-Wei Hong and Anurag Ajay and Pulkit Agrawal 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023parallel.pdf:pdf},
  mdate = {2024-06-02},
  pages = {19440--19459},
  pdf = {https://proceedings.mlr.press/v202/li23f/li23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Parallel {Q}-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation},
  url = {https://proceedings.mlr.press/v202/li23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023minimum,
  abstract = {The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, (Cai, 2022) shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $\mathcal{K}$, i.e., the UAP for $L^p(\mathcal{K},\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(\mathcal{K},\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.},
  author = {Li'ang Li and Yifei Duan and Guanghua Ji and Yongqiang Cai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023minimum.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19460--19470},
  pdf = {https://proceedings.mlr.press/v202/li23g/li23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Minimum Width of Leaky-{ReLU} Neural Networks for Uniform Universal Approximation},
  url = {https://proceedings.mlr.press/v202/li23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023fairer,
  abstract = {Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while ignoring intermediate neuron alignment. To address the issue, we formulate the fairness as a new task, i.e., decision rationale alignment that requires DNNs' neurons to have consistent responses on subgroups at both intermediate processes and the final prediction. To make this idea practical during optimization, we relax the naive objective function and propose gradient-guided parity alignment, which encourages gradient-weighted consistency of neurons across subgroups. Extensive experiments on a variety of datasets show that our method can significantly enhance fairness while sustaining a high level of accuracy and outperforming other approaches by a wide margin.},
  author = {Tianlin Li and Qing Guo 0005 and Aishan Liu and Mengnan Du and Zhiming Li and Yang Liu 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023fairer.pdf:pdf},
  mdate = {2024-07-30},
  pages = {19471--19489},
  pdf = {https://proceedings.mlr.press/v202/li23h/li23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FAIRER}: Fairness as Decision Rationale Alignment},
  url = {https://proceedings.mlr.press/v202/li23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023nearoptimal,
  abstract = {This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance.},
  author = {Donghao Li and Ruiquan Huang and Cong Shen 0001 and Jing Yang 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023nearoptimal.pdf:pdf},
  mdate = {2023-11-02},
  pages = {19527--19564},
  pdf = {https://proceedings.mlr.press/v202/li23k/li23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints},
  url = {https://proceedings.mlr.press/v202/li23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023adversarial,
  abstract = {Federated Learning (FL) has been a popular approach to enable collaborative learning on multiple parties without exchanging raw data. However, the model performance of FL may degrade a lot due to non-IID data. While many FL algorithms focus on non-IID labels, FL on non-IID features has largely been overlooked. Different from typical FL approaches, the paper proposes a new learning concept called ADCOL (Adversarial Collaborative Learning) for non-IID features. Instead of adopting the widely used model-averaging scheme, ADCOL conducts training in an adversarial way: the server aims to train a discriminator to distinguish the representations of the parties, while the parties aim to generate a common representation distribution. Our experiments show that ADCOL achieves better performance than state-of-the-art FL algorithms on non-IID features.},
  author = {Qinbin Li and Bingsheng He and Dawn Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023adversarial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19504--19526},
  pdf = {https://proceedings.mlr.press/v202/li23j/li23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Collaborative Learning on Non-{IID} Features},
  url = {https://proceedings.mlr.press/v202/li23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023race,
  abstract = {Multi-Agent Reinforcement Learning (MARL) has demonstrated its effectiveness in learning collaboration, but it often struggles with low-quality reward signals and high non-stationarity. In contrast, Evolutionary Algorithm (EA) has shown better convergence, robustness, and signal quality insensitivity. This paper introduces a hybrid framework, Representation Asymmetry and Collaboration Evolution (RACE), which combines EA and MARL for efficient collaboration. RACE maintains a MARL team and a population of EA teams. To enable efficient knowledge sharing and policy exploration, RACE decomposes the policies of different teams controlling the same agent into a shared nonlinear observation representation encoder and individual linear policy representations. To address the partial observation issue, we introduce Value-Aware Mutual Information Maximization to enhance the shared representation with useful information about superior global states.},
  author = {Pengyi Li and Jianye Hao and Hongyao Tang and Yan Zheng 0002 and Xian Fu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023race.pdf:pdf},
  mdate = {2024-06-03},
  pages = {19490--19503},
  pdf = {https://proceedings.mlr.press/v202/li23i/li23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{RACE}: Improve Multi-Agent Reinforcement Learning with Representation Asymmetry and Collaborative Evolution},
  url = {https://proceedings.mlr.press/v202/li23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023transformers,
  abstract = {In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.},
  author = {Yingcong Li and Muhammed Emrullah Ildiz and Dimitris Papailiopoulos and Samet Oymak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023transformers.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19565--19594},
  pdf = {https://proceedings.mlr.press/v202/li23l/li23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  url = {https://proceedings.mlr.press/v202/li23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023improving,
  abstract = {Approximate inference in {G}aussian process ({GP}) models with non-conjugate likelihoods gets entangled with the learning of the model hyperparameters. We improve hyperparameter learning in {GP} models and focus on the interplay between variational inference ({VI}) and the learning target. While {VI}'s lower bound to the marginal likelihood is a suitable objective for inferring the approximate posterior, we show that a direct approximation of the marginal likelihood as in {E}xpectation {P}ropagation ({EP}) is a better learning objective for hyperparameter optimization. We design a hybrid training procedure to bring the best of both worlds: it leverages conjugate-computation {VI} for inference and uses an {EP}-like marginal likelihood approximation for hyperparameter learning. We compare {VI}, {EP}, {L}aplace approximation, and our proposed training procedure and empirically demonstrate the effectiveness of our proposal across a wide range of data sets.},
  author = {Rui Li and S. T. John and Arno Solin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19595--19615},
  pdf = {https://proceedings.mlr.press/v202/li23m/li23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Hyperparameter Learning under Approximate Inference in {G}aussian Process Models},
  url = {https://proceedings.mlr.press/v202/li23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023nearly,
  abstract = {The trade-off between regret and computational cost is a fundamental problem for online kernel regression, and previous algorithms worked on the trade-off can not keep optimal regret bounds at a sublinear computational complexity. In this paper, we propose two new algorithms, {AOGD-ALD} and {NONS-ALD}, which can keep nearly optimal regret bounds at a sublinear computational complexity, and give sufficient conditions under which our algorithms work. Both algorithms dynamically maintain a group of nearly orthogonal basis used to approximate the kernel mapping, and keep nearly optimal regret bounds by controlling the approximate error. The number of basis depends on the approximate error and the decay rate of eigenvalues of the kernel matrix. If the eigenvalues decay exponentially, then {AOGD-ALD} and {NONS-ALD} separately achieves a regret of {$O(\sqrt{L(f)})$} and {$O(d_{\text{eff}}(\mu)\ln T)$} at a computational complexity in {$O(\ln^2 T)$}. If the eigenvalues decay polynomially with degree {$p \geq 1$}, then our algorithms keep the same regret bounds at a computational complexity in {$o(T)$} in the case of {$p > 4$} and {$p \geq 10$}, respectively. {$L(f)$} is the cumulative losses of {$f$} and {$d_{\text{eff}}(\mu)$} is the effective dimension of the problem. The two regret bounds are nearly optimal and are not comparable.},
  author = {Junfan Li and Shizhong Liao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023nearly.pdf:pdf},
  mdate = {2024-08-04},
  pages = {19743--19766},
  pdf = {https://proceedings.mlr.press/v202/li23r/li23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly Optimal Algorithms with Sublinear Computational Complexity for Online Kernel Regression},
  url = {https://proceedings.mlr.press/v202/li23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023distributiondependent,
  abstract = {The concentration of measure inequalities serves an essential role in statistics and machine learning. This paper gives unbounded analogues of the {M}c{D}iarmid-type exponential inequalities for three popular classes of distributions, namely sub-{G}aussian, sub-exponential and heavy-tailed distributions. The inequalities in the sub-{G}aussian and sub-exponential cases are distribution-dependent compared with the recent results, and the inequalities in the heavy-tailed case are not available in the previous works.},
  author = {Shaojie Li and Yong Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023distributiondependent.pdf:pdf},
  mdate = {2023-10-31},
  pages = {19789--19810},
  pdf = {https://proceedings.mlr.press/v202/li23t/li23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distribution-dependent {M}c{D}iarmid-type Inequalities for Functions of Unbounded Interaction},
  url = {https://proceedings.mlr.press/v202/li23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023reconstructive,
  abstract = {Deep neural networks ({DNN}s) have been found to be vulnerable to backdoor attacks, raising security concerns about their deployment in mission-critical applications. While existing defense methods have demonstrated promising results, it is still not clear how to effectively remove backdoor-associated neurons in backdoored {DNN}s. In this paper, we propose a novel defense called {R}econstructive {N}euron {P}runing ({RNP}) to expose and prune backdoor neurons via an unlearning and then recovering process. Specifically, {RNP} first unlearns the neurons by maximizing the model's error on a small subset of clean samples and then recovers the neurons by minimizing the model's error on the same data. In {RNP}, unlearning is operated at the neuron level while recovering is operated at the filter level, forming an asymmetric reconstructive learning procedure.},
  author = {Yige Li and Xixiang Lyu and Xingjun Ma and Nodens Koren and Lingjuan Lyu and Bo Li and Yu-Gang Jiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023reconstructive.pdf:pdf},
  mdate = {2025-01-13},
  pages = {19837--19854},
  pdf = {https://proceedings.mlr.press/v202/li23v/li23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reconstructive Neuron Pruning for Backdoor Defense},
  url = {https://proceedings.mlr.press/v202/li23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023how,
  abstract = {While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks---but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn ``semantic structure'', understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on {W}ikipedia data and synthetic data modeled by {L}atent {D}irichlet {A}llocation ({LDA}), that the embedding layer and the self-attention layer encode the topical structure. In the former case, this manifests as higher average inner product of embeddings between same-topic words. In the latter, it manifests as higher average pairwise attention between same-topic words. The mathematical results involve several assumptions to make the analysis tractable, which we verify on data, and might be of independent interest as well.},
  author = {Yuchen Li and Yuanzhi Li and Andrej Risteski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19689--19729},
  pdf = {https://proceedings.mlr.press/v202/li23p/li23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding},
  url = {https://proceedings.mlr.press/v202/li23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023revisiting,
  abstract = {In federated learning ({FL}), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of {FL}. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Based on the above insights, we propose an effective method for {F}ederated {L}earning with {L}earnable {A}ggregation {W}eights, named as {FedLAW}. Extensive experiments verify that our method can improve the generalization of the global model by a large margin on different datasets and models.},
  author = {Zexi Li and Tao Lin and Xinyi Shang and Chao Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023revisiting.pdf:pdf},
  mdate = {2024-08-06},
  pages = {19767--19788},
  pdf = {https://proceedings.mlr.press/v202/li23s/li23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Weighted Aggregation in Federated Learning with Neural Networks},
  url = {https://proceedings.mlr.press/v202/li23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023optimal,
  abstract = {{N}ystr\"{o}m low-rank approximation has shown great potential in processing large-scale kernel matrix and neural networks. However, there lacks a unified analysis for {N}ystr\"{o}m approximation, and the asymptotical minimax optimality for {N}ystr\"{o}m methods usually require a strict condition, assuming that the target regression lies exactly in the hypothesis space. To tackle these problems, we provide a refined generalization analysis for {N}ystr\"{o}m approximation in the agnostic setting, where the target regression may be out of the hypothesis space. Specifically, we show {N}ystr\"{o}m approximation can still achieve the capacity-dependent optimal rates in the agnostic setting. We first prove the capacity-dependent optimal guarantees of {N}ystr\"{o}m approximation with the standard uniform sampling, which covers both loss functions and applies to some agnostic settings. Then, using data-dependent sampling, for example, leverage scores sampling, we derive the capacity-dependent optimal rates that apply to the whole range of the agnostic setting. To our best knowledge, the capacity-dependent optimality for the whole range of the agnostic setting is first achieved and novel in {N}ystr\"{o}m approximation.},
  author = {Jian Li and Yong Liu and Weiping Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023optimal.pdf:pdf},
  mdate = {2024-07-27},
  pages = {19811--19836},
  pdf = {https://proceedings.mlr.press/v202/li23u/li23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Convergence Rates for Agnostic {N}ystr\"{o}m Kernel Learning},
  url = {https://proceedings.mlr.press/v202/li23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023meta,
  abstract = {Physics-informed neural networks ({PINN}s) are emerging as popular mesh-free solvers for partial differential equations ({PDE}s). Recent extensions decompose the domain, apply different {PINN}s to solve the problem in each subdomain, and stitch the subdomains at the interface. Thereby, they can further alleviate the problem complexity, reduce the computational cost, and allow parallelization. However, the performance of multi-domain {PINN}s is sensitive to the choice of the interface conditions. While quite a few conditions have been proposed, there is no suggestion about how to select the conditions according to specific problems. To address this gap, we propose {META} Learning of Interface Conditions ({METALIC}), a simple, efficient yet powerful approach to dynamically determine appropriate interface conditions for solving a family of parametric {PDE}s. Specifically, we develop two contextual multi-arm bandit ({MAB}) models.},
  author = {Shibo Li and Michael Penwarden and Yiming Xu and Conor Tillinghast and Akil Narayan and Mike Kirby and Shandian Zhe},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023meta.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19855--19881},
  pdf = {https://proceedings.mlr.press/v202/li23w/li23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Meta Learning of Interface Conditions for Multi-Domain Physics-Informed Neural Networks},
  url = {https://proceedings.mlr.press/v202/li23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023deep,
  abstract = {Selecting informative data points for expert feedback can significantly improve the performance of anomaly detection ({AD}) in various contexts, such as medical diagnostics or fraud detection. In this paper, we determine a set of theoretical conditions under which anomaly scores generalize from labeled queries to unlabeled data. Motivated by these results, we propose a data labeling strategy with optimal data coverage under labeling budget constraints.},
  author = {Aodong Li and Chen Qiu and Marius Kloft and Padhraic Smyth and Stephan Mandt and Maja Rudolph},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023deep.pdf:pdf},
  mdate = {2025-05-26},
  pages = {19882--19910},
  pdf = {https://proceedings.mlr.press/v202/li23x/li23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Anomaly Detection under Labeling Budget Constraints},
  url = {https://proceedings.mlr.press/v202/li23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023how,
  abstract = {We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limited bandwidth may not always be able to perfectly reconstruct the original signal. We corroborate our theoretical results with various simulation studies, and generally, two main take-home messages are offered: (i) Not any distribution for selecting random weights is feasible to build a universal approximator; (ii) A suitable assignment of random weights exists but to some degree is associated with the complexity of the target function.},
  author = {Ming Li and Sho Sonoda and Feilong Cao and Yu Guang Wang and Jiye Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023how.pdf:pdf},
  mdate = {2024-08-01},
  pages = {19960--19981},
  pdf = {https://proceedings.mlr.press/v202/li23aa/li23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Powerful are Shallow Neural Networks with Bandlimited Random Weights?},
  url = {https://proceedings.mlr.press/v202/li23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023initialization,
  abstract = {Graph Neural Networks ({GNN}s) have shown promise in graph representation learning. However, classic initialization methods like Xavier initialization were designed for feedforward and convolutional networks, and disregard the impact of the input graph structure and message passing on variance. We analyze the initialization of {GNN} weight matrices and propose a new initialization method called {V}irgo ({V}ariance Instability Reduction within {GNN} Optimization), which aims to equate forward and backward variances across successive layers. We conducted experiments on 15 datasets across node classification, link prediction, and graph classification tasks, demonstrating that {V}irgo can lead to superior model performance and more stable variance at initialization compared to traditional methods.},
  author = {Jiahang Li and Yakun Song and Xiang Song 0003 and David Wipf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023initialization.pdf:pdf},
  mdate = {2024-08-14},
  pages = {19911--19931},
  pdf = {https://proceedings.mlr.press/v202/li23y/li23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Initialization of Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/li23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023hierarchical,
  abstract = {Offline reinforcement learning typically introduces a hierarchical structure to solve the long-horizon problem so as to address its thorny issue of variance accumulation. Problems of deadly triad, limited data and reward sparsity, however, still remain, rendering the design of effective, hierarchical offline {RL} algorithms for general-purpose policy learning a formidable challenge. In this paper, we first formulate the problem of offline long-horizon decision-making from the perspective of conditional generative modeling by incorporating goals into the control-as-inference graphic models. A Hierarchical trajectory-level Diffusion probabilistic model is then proposed with classifier-free guidance. {HDMI} employs a cascade framework that utilizes the reward-conditional goal diffuser for the subgoal discovery and the goal-conditional trajectory diffuser for generating the corresponding action sequence of subgoals. Planning-based subgoal extraction and transformer-based diffusion are employed to deal with the sub-optimal data pollution and long-range subgoal dependencies in the goal diffusion. Numerical experiments verify the advantages of {HDMI} on long-horizon decision-making compared to {SOTA} offline {RL} methods and conditional generative models.},
  author = {Wenhao Li and Xiangfeng Wang and Bo Jin 0003 and Hongyuan Zha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023hierarchical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20035--20064},
  pdf = {https://proceedings.mlr.press/v202/li23ad/li23ad.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchical Diffusion for Offline Decision Making},
  url = {https://proceedings.mlr.press/v202/li23ad.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023efficient,
  abstract = {In this paper, we present efficient quantum algorithms that are exponentially faster than classical algorithms for solving the quantum optimal control problem. This problem involves finding the control variable that maximizes a physical quantity at time {$T$}, where the system is governed by a time-dependent {S}chrödinger equation. This type of control problem also has an intricate relation with machine learning. Our algorithms are based on a time-dependent {H}amiltonian simulation method and a fast gradient-estimation algorithm. We also provide a comprehensive error analysis to quantify the total error from various steps, such as the finite-dimensional representation of the control function, the discretization of the {S}chrödinger equation, the numerical quadrature, and optimization. Our quantum algorithms require fault-tolerant quantum computers.},
  author = {Xiantao Li and Chunhao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19982--19994},
  pdf = {https://proceedings.mlr.press/v202/li23ab/li23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Quantum Algorithms for Quantum Optimal Control},
  url = {https://proceedings.mlr.press/v202/li23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023lowswitching,
  abstract = {Policy optimization methods are powerful algorithms in Reinforcement Learning ({RL}) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problem have only been successful in tabular and linear settings, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, {LPO}, with general non-linear function approximation. We show that our algorithm obtains an $\varepsilon$-optimal policy with only $\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$ samples, where $\varepsilon$ is the suboptimality gap and $d$ is a complexity measure of the function class approximating the policy. This drastically improves the previously best-known sample bound for policy optimization algorithms, $\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^8})$. Moreover, we empirically test our theory with deep neural nets to show the benefits of the theoretical inspiration.},
  author = {Yunfan Li and Yiran Wang and Yu Cheng and Lin Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023lowswitching.pdf:pdf},
  mdate = {2023-08-28},
  pages = {19995--20034},
  pdf = {https://proceedings.mlr.press/v202/li23ac/li23ac.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling},
  url = {https://proceedings.mlr.press/v202/li23ac.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023divide,
  abstract = {We develop a novel, general and computationally efficient framework, called Divide and Conquer Dynamic Programming ({DCDP}), for localizing change points in time series data with high-dimensional features. {DCDP} deploys a class of greedy algorithms that are applicable to a broad variety of high-dimensional statistical models and can enjoy almost linear computational complexity. We investigate the performance of {DCDP} in three commonly studied change point settings in high dimensions: the mean model, the {G}aussian graphical model, and the linear regression model. In all three cases, we derive non-asymptotic bounds for the accuracy of the {DCDP} change point estimators. We demonstrate that the {DCDP} procedures consistently estimate the change points with sharp, and in some cases, optimal rates while incurring significantly smaller computational costs than the best available algorithms. Our findings are supported by extensive numerical experiments on both synthetic and real data.},
  author = {Wanshan Li and Daren Wang and Alessandro Rinaldo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023divide.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20065--20148},
  pdf = {https://proceedings.mlr.press/v202/li23ae/li23ae.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Divide and Conquer Dynamic Programming: An Almost Linear Time Change Point Detection Methodology in High Dimensions},
  url = {https://proceedings.mlr.press/v202/li23ae.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023architectureagnostic,
  abstract = {Masked image modeling ({MIM}), an emerging self-supervised pre-training method, has shown impressive success across numerous downstream vision tasks with {V}ision transformers ({ViT}s). Its underlying idea is simple: a portion of the input image is randomly masked out and then reconstructed via the pre-text task. However, the working principle behind {MIM} is not well explained, and previous studies insist that {MIM} primarily works for the {T}ransformer family but is incompatible with {CNN}s. In this paper, we first study interactions among patches to understand what knowledge is learned and how it is acquired via the {MIM} task. We observe that {MIM} essentially teaches the model to learn better middle-order interactions among patches and extract more generalized features. Based on this fact, we propose an {A}rchitecture-{A}gnostic {M}asked {I}mage {M}odeling framework ({A$^2$MIM}), which is compatible with both {T}ransformers and {CNN}s in a unified way.},
  author = {Siyuan Li 0002 and Di Wu 0057 and Fang Wu 0002 and Zelin Zang and Stan Z. Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023architectureagnostic.pdf:pdf},
  mdate = {2025-01-28},
  pages = {20149--20167},
  pdf = {https://proceedings.mlr.press/v202/li23af/li23af.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Architecture-Agnostic Masked Image Modeling -- From {ViT} back to {CNN}},
  url = {https://proceedings.mlr.press/v202/li23af.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023graphcleaner,
  abstract = {Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we propose {GraphCleaner}, a post-hoc method to detect and correct mislabelled nodes in graph datasets. {GraphCleaner} combines two novel ideas: 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that {GraphCleaner} outperforms the closest baseline, with an average improvement of 0.14 in {F1} score, and 0.16 in {MCC}. On real-data case studies, {GraphCleaner} detects previously unknown mislabels in popular graph benchmarks like {PubMed}, {Cora}, {CiteSeer} and {OGB}-arxiv. The researchers found that at least 6.91\% of {PubMed} data is mislabelled or ambiguous, and simply removing these mislabelled data can boost evaluation performance from 86.71\% to 89.11\%.},
  author = {Yuwen Li and Miao Xiong and Bryan Hooi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023graphcleaner.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20195--20209},
  pdf = {https://proceedings.mlr.press/v202/li23ai/li23ai.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GraphCleaner}: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks},
  url = {https://proceedings.mlr.press/v202/li23ai.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023learning,
  abstract = {Fairness is essential for machine learning systems deployed in high-stake applications. Among all fairness notions, individual fairness, deriving from a consensus that `similar individuals should be treated similarly,' is a vital notion to describe fair treatment for individual cases. Previous studies typically characterize individual fairness as a prediction-invariant problem when perturbing sensitive attributes on samples, and solve it by {D}istributionally {R}obust {O}ptimization ({DRO}) paradigm. However, such adversarial perturbations along a direction covering sensitive information used in {DRO} do not consider the inherent feature correlations or innate data constraints, therefore could mislead the model to optimize at off-manifold and unrealistic samples. In light of this drawback, in this paper, we propose to learn and generate antidote data that approximately follows the data distribution to remedy individual unfairness. These generated on-manifold antidote data can be used through a generic optimization procedure along with original training data, resulting in a pure pre-processing approach to individual unfairness, or can also fit well with the in-processing {DRO} paradigm. Through extensive experiments on multiple tabular datasets, we demonstrate our method resists individual unfairness at a minimal or zero cost to predictive utility compared to baselines.},
  author = {Peizhao Li and Ethan Xia and Hongfu Liu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023learning.pdf:pdf},
  mdate = {2023-12-01},
  pages = {20168--20181},
  pdf = {https://proceedings.mlr.press/v202/li23ag/li23ag.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Antidote Data to Individual Unfairness},
  url = {https://proceedings.mlr.press/v202/li23ag.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023propensity,
  abstract = {Propensity-based weighting methods have been widely studied and demonstrated competitive performance in debiased recommendations. Nevertheless, there are still many questions to be addressed. How to estimate the propensity more conducive to debiasing performance? Which metric is more reasonable to measure the quality of the learned propensities? Is it better to make the cross-entropy loss as small as possible when learning propensities? In this paper, we first discuss the potential problems of the previously widely adopted metrics for learned propensities, and propose balanced-mean-squared-error ({BMSE}) metric for debiased recommendations. Based on {BMSE}, we propose {IPS}-{V2} and {DR}-{V2} as the estimators of unbiased loss, and theoretically show that {IPS}-{V2} and {DR}-{V2} have greater propensity balancing and smaller variance without sacrificing additional bias. We further propose a co-training method for learning balanced representation and unbiased prediction. Extensive experiments are conducted on three real-world datasets including a large industrial dataset, and the results show that our approach boosts the balancing property and results in enhanced debiasing performance.},
  author = {Haoxuan Li 0001 and Yanghao Xiao and Chunyuan Zheng 0001 and Peng Wu 0012 and Peng Cui 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023propensity.pdf:pdf},
  mdate = {2025-02-10},
  pages = {20182--20194},
  pdf = {https://proceedings.mlr.press/v202/li23ah/li23ah.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Propensity Matters: Measuring and Enhancing Balancing for Recommendation},
  url = {https://proceedings.mlr.press/v202/li23ah.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023smurfthp,
  abstract = {Transformer {H}awkes process models have shown to be successful in modeling event sequence data. However, most of the existing training methods rely on maximizing the likelihood of event sequences, which involves calculating some intractable integral. Moreover, the existing methods fail to provide uncertainty quantification for model predictions, e.g., confidence intervals for the predicted event's arrival time. To address these issues, we propose {SMURF-THP}, a score-based method for learning {T}ransformer {H}awkes process and quantifying prediction uncertainty. Specifically, {SMURF-THP} learns the score function of events' arrival time based on a score-matching objective that avoids the intractable computation. With such a learned score function, we can sample arrival time of events from the predictive distribution. This naturally allows for the quantification of uncertainty by computing confidence intervals over the generated samples. We conduct extensive experiments in both event type prediction and uncertainty quantification on time of arrival. In all the experiments, {SMURF-THP} outperforms existing likelihood-based methods in confidence calibration while exhibiting comparable prediction accuracy.},
  author = {Zichong Li and Yanbo Xu and Simiao Zuo and Haoming Jiang and Chao Zhang and Tuo Zhao and Hongyuan Zha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023smurfthp.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20210--20220},
  pdf = {https://proceedings.mlr.press/v202/li23aj/li23aj.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SMURF-THP}: Score Matching-based Uncertainty Quantification for Transformer {H}awkes Process},
  url = {https://proceedings.mlr.press/v202/li23aj.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023horizonfree,
  abstract = {Horizon dependence is an important difference between reinforcement learning and other machine learning paradigms. Yet, existing results tackling the (exact) horizon dependence either assume that the reward is bounded per step, introducing unfair comparison, or assume strict total boundedness that requires the sum of rewards to be bounded _almost surely_ -- allowing only restricted noise on the reward observation. This paper addresses these limitations by introducing a new relaxation -- _expected boundedness_ on rewards, where we allow the reward to be stochastic with only boundedness on the _expected_ sum -- opening the door to study horizon-dependence with a much broader set of reward functions with noises. We establish a novel generic algorithm that achieves _no-horizon dependence_ in terms of sample complexity for both Markov Decision Processes (MDP) and Games, via reduction to a good-conditioned _auxiliary Markovian environment_, in which only "important" state-action pairs are preserved. The algorithm takes only $\tilde{O}(\frac{S^2A}{\epsilon^2})$ episodes interacting with such an environment to achieve an $\epsilon$-optimal policy/strategy (with high probability), improving (zhang, 2022) (which only applies to MDPs with deterministic rewards). Here $S$ is the number of states and $A$ is the number of actions, and the bound is independent of the horizon $H$.},
  author = {Shengshi Li and Lin Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023horizonfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20221--20252},
  pdf = {https://proceedings.mlr.press/v202/li23ak/li23ak.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Horizon-free Learning for {M}arkov {D}ecision {P}rocesses and {G}ames: {S}tochastically {B}ounded {R}ewards and {I}mproved {B}ounds},
  url = {https://proceedings.mlr.press/v202/li23ak.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023transcendental,
  abstract = {Evaluating the performance of perception modules in autonomous driving is one of the most critical tasks in developing the complex intelligent system. While module-level unit test metrics adopted from traditional computer vision tasks are feasible to some extent, it remains far less explored to measure the impact of perceptual noise on the driving quality of autonomous vehicles in a consistent and holistic manner. In this work, we propose a principled framework that provides a coherent and systematic understanding of the impact an error in the perception module imposes on an autonomous agent's planning that actually controls the vehicle. Specifically, the planning process is formulated as expected utility maximisation, where all input signals from upstream modules jointly provide a world state description, and the planner strives for the optimal action by maximising the expected utility determined by both world states and actions. We show that, under practical conditions, the objective function can be represented as an inner product between the world state description and the utility function in a Hilbert space. This geometric interpretation enables a novel way to analyse the impact of noise in world state estimation on planning and leads to a universal metric for evaluating perception. The whole framework resembles the idea of transcendental idealism in the classical philosophical literature, which gives the name to our approach.},
  author = {Weixin Li and Xiaodong Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023transcendental.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20253--20275},
  pdf = {https://proceedings.mlr.press/v202/li23al/li23al.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transcendental Idealism of Planner: Evaluating Perception from Planning Perspective for Autonomous Driving},
  url = {https://proceedings.mlr.press/v202/li23al.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023nesterov,
  abstract = {We propose a new first-order optimization algorithm -- AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent -- for separable convex-concave minimax optimization. The main idea of our algorithm is to carefully leverage the structure of the minimax problem, performing Nesterov acceleration on the individual component and optimistic gradient on the coupling component. Equipped with proper restarting, we show that AG-OG achieves the optimal convergence rate (up to a constant) for a variety of settings, including bilinearly coupled strongly convex-strongly concave minimax optimization (bi-SC-SC), bilinearly coupled convex-strongly concave minimax optimization (bi-C-SC), and bilinear games. We also extend our algorithm to the stochastic setting and achieve the optimal convergence rate in both bi-SC-SC and bi-C-SC settings.},
  author = {Chris Junchi Li and Huizhuo Yuan and Gauthier Gidel and Quanquan Gu and Michael I. Jordan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023nesterov.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20351--20383},
  pdf = {https://proceedings.mlr.press/v202/li23aq/li23aq.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization},
  url = {https://proceedings.mlr.press/v202/li23aq.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023fedvs,
  abstract = {In a vertical federated learning (VFL) system consisting of a central server and many distributed clients, the training data are vertically partitioned such that different features are privately stored on different clients. The problem of split VFL is to train a model split between the server and the clients. This paper aims to address two major challenges in split VFL: 1) performance degradation due to straggling clients during training; and 2) data and model privacy leakage from clients' uploaded data embeddings. We propose FedVS to simultaneously address these two challenges. The key idea of FedVS is to design secret sharing schemes for the local data and models, such that information-theoretical privacy against colluding clients and curious server is guaranteed, and the aggregation of all clients' embeddings is reconstructed losslessly, via decrypting computation shares from the non-straggling clients. Extensive experiments on various types of VFL datasets (including tabular, CV, and multi-view) demonstrate the universal advantages of FedVS in straggler mitigation and privacy protection over baseline protocols.},
  author = {Songze Li and Duanyi Yao and Jin Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023fedvs.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20296--20311},
  pdf = {https://proceedings.mlr.press/v202/li23an/li23an.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FedVS}: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models},
  url = {https://proceedings.mlr.press/v202/li23an.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023achieving,
  abstract = {This paper develops an approximation scheme for solving bilevel programs with equilibrium constraints, which are generally difficult to solve. Among other things, calculating the first-order derivative in such a problem requires differentiation across the hierarchy, which is computationally intensive, if not prohibitive. To bypass the hierarchy, we propose to bound such bilevel programs, equivalent to multiple-followers Stackelberg games, with two new hierarchy-free problems: a T-step Cournot game and a T-step monopoly model. Since they are standard equilibrium or optimization problems, both can be efficiently solved via first-order methods. Importantly, we show that the bounds provided by these problems -- the upper bound by the T-step Cournot game and the lower bound by the T-step monopoly model -- can be made arbitrarily tight by increasing the step parameter T for a wide range of problems. We prove that a small T usually suffices under appropriate conditions to reach an approximation acceptable for most practical purposes.},
  author = {Jiayang Li and Jing Yu and Boyi Liu and Yu Marco Nie and Zhaoran Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023achieving.pdf:pdf},
  mdate = {2025-05-19},
  pages = {20312--20335},
  pdf = {https://proceedings.mlr.press/v202/li23ao/li23ao.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Achieving Hierarchy-Free Approximation for Bilevel Programs with Equilibrium Constraints},
  url = {https://proceedings.mlr.press/v202/li23ao.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023learning,
  abstract = {Many problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but it lacks robustness and can perform arbitrarily poorly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judicious condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item.},
  author = {Pengfei Li and Jianyi Yang and Shaolei Ren},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023learning.pdf:pdf},
  mdate = {2024-11-25},
  pages = {20276--20295},
  pdf = {https://proceedings.mlr.press/v202/li23am/li23am.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees},
  url = {https://proceedings.mlr.press/v202/li23am.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023losparse,
  abstract = {Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.},
  author = {Yixiao Li and Yifan Yu and Qingru Zhang and Chen Liang and Pengcheng He and Weizhu Chen and Tuo Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023losparse.pdf:pdf},
  mdate = {2025-05-27},
  pages = {20336--20350},
  pdf = {https://proceedings.mlr.press/v202/li23ap/li23ap.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LoSparse}: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
  url = {https://proceedings.mlr.press/v202/li23ap.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023understanding,
  abstract = {Reinforcement learning problems can be challenging without well-shaped rewards, and prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. In this work, we take a different approach and study whether we can reap benefits by reformulating a single challenging task as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. We provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum, and under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem.},
  author = {Qiyang Li and Yuexiang Zhai and Yi Ma and Sergey Levine},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20412--20451},
  pdf = {https://proceedings.mlr.press/v202/li23as/li23as.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding the Complexity Gains of Single-Task {RL} with a Curriculum},
  url = {https://proceedings.mlr.press/v202/li23as.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023internally,
  abstract = {We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting Internally Rewarded Reinforcement Learning (IRRL) as the reward is not provided directly by the environment but internally by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function.},
  author = {Mengdi Li and Xufeng Zhao and Jae Hee Lee and Cornelius Weber and Stefan Wermter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023internally.pdf:pdf},
  mdate = {2025-03-11},
  pages = {20556--20574},
  pdf = {https://proceedings.mlr.press/v202/li23ax/li23ax.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Internally Rewarded Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/li23ax.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023does,
  abstract = {Recently, a series of studies have tried to extract interactions between input variables modeled by a DNN and define such interactions as concepts encoded by the DNN. However, strictly speaking, there still lacks a solid guarantee whether such interactions indeed represent meaningful concepts. Therefore, in this paper, we examine the trustworthiness of interaction concepts from four perspectives. Extensive empirical studies have verified that a well-trained DNN usually encodes sparse, transferable, and discriminative concepts, which is partially aligned with human intuition.},
  author = {Mingjie Li and Quanshi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/li2023does.pdf:pdf},
  mdate = {2023-08-28},
  pages = {20452--20469},
  pdf = {https://proceedings.mlr.press/v202/li23at/li23at.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Does a Neural Network Really Encode Symbolic Concepts?},
  url = {https://proceedings.mlr.press/v202/li23at.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023trustworthy,
  abstract = {Trustworthy policy learning has significant importance in making reliable and harmless treatment decisions for individuals. Previous policy learning approaches aim at the well-being of subgroups by maximizing the utility function (e.g., conditional average causal effects, post-view click-through and conversion rate in recommendations), however, individual-level counterfactual no-harm criterion has rarely been discussed. In this paper, we first formalize the counterfactual no-harm criterion for policy learning from a principal stratification perspective. Next, we propose a novel upper bound for the fraction negatively affected by the policy and show the consistency and asymptotic normality of the estimator. Based on the estimators for the policy utility and harm upper bounds, we further propose a policy learning approach that satisfies the counterfactual no-harm criterion, and prove its consistency to the optimal policy reward for parametric and non-parametric policy classes, respectively. Extensive experiments are conducted to show the effectiveness of the proposed policy learning approach for satisfying the counterfactual no-harm criterion.},
  author = {Haoxuan Li and Chunyuan Zheng and Yixiao Cao and Zhi Geng and Yue Liu and Peng Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2025-02-10},
  month = {7},
  pages = {20575--20598},
  pdf = {https://proceedings.mlr.press/v202/li23ay.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trustworthy Policy Learning under the Counterfactual No-Harm Criterion},
  url = {https://proceedings.mlr.press/v202/li23ay.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023alternating,
  abstract = {Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, greatly reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is reached in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that Ω(2^N) evaluations are typically required in TNLS for reaching the objective reduction in the neighborhood, while ideally O(N²R) evaluations are sufficient in TnALE, where N denotes the tensor order and R reflects the "low-rankness" of the neighborhood. Experimental results verify that TnALE can find practically good TN-ranks and permutations with vastly fewer evaluations than the state-of-the-art algorithms.},
  author = {Chao Li and Junhua Zeng and Chunmei Li and Cesar F. Caiafa and Qibin Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2023-08-28},
  month = {7},
  pages = {20384--20411},
  pdf = {https://proceedings.mlr.press/v202/li23ar.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Alternating Local Enumeration ({TnALE}): Solving Tensor Network Structure Search with Fewer Evaluations},
  url = {https://proceedings.mlr.press/v202/li23ar.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{li2023offline,
  abstract = {Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate both one-step and iterative offline RL algorithms with our novel policy improvement operators and empirically demonstrate their effectiveness over state-of-the-art algorithms on the standard D4RL benchmark.},
  author = {Jiachen Li and Edwin Zhang and Ming Yin and Qinxun Bai and Yu-Xiang Wang and William Yang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2023-11-20},
  month = {7},
  pages = {20485--20528},
  pdf = {https://proceedings.mlr.press/v202/li23av.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Offline Reinforcement Learning with Closed-Form Policy Improvement Operators},
  url = {https://proceedings.mlr.press/v202/li23av.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023low,
  abstract = {There has been growing interest in employing neural networks to directly solve constrained optimization problems with low run-time complexity. However, it is non-trivial to ensure NN solutions strictly satisfy problem constraints due to inherent NN prediction errors, while existing feasibility-ensuring methods are either computationally expensive or lack performance guarantee. We propose Homeomorphic Projection as a low-complexity scheme to guarantee NN solution feasibility for optimization over a general set homeomorphic to a unit ball, covering all compact convex sets and certain non-convex sets. The approach involves learning a minimum distortion homeomorphic mapping between the constraint set and a unit ball using a bi-Lipschitz invertible neural network, and then performing a simple bisection operation concerning the unit ball such that the mapped final solution is feasible with respect to the constraint set with minor distortion-induced optimality loss.},
  author = {Enming Liang and Minghua Chen and Steven H. Low},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2023-08-28},
  month = {7},
  pages = {20623--20649},
  pdf = {https://proceedings.mlr.press/v202/liang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Low Complexity Homeomorphic Projection to Ensure Neural-Network Solution Feasibility for Optimization over (Non-)Convex Set},
  url = {https://proceedings.mlr.press/v202/liang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023distribution,
  abstract = {We present a distribution optimization framework that significantly improves confidence bounds for various risk measures compared to previous methods. Our framework encompasses popular risk measures such as the entropic risk measure, conditional value at risk (CVaR), spectral risk measure, distortion risk measure, equivalent certainty, and rank-dependent expected utility, which are well established in risk-sensitive decision-making literature. To achieve this, we introduce two estimation schemes based on concentration bounds derived from the empirical distribution, specifically using either the Wasserstein distance or the supremum distance. Unlike traditional approaches that add or subtract a confidence radius from the empirical risk measures, our proposed schemes evaluate a specific transformation of the empirical distribution based on the distance. Our confidence bounds consistently yield tighter results compared to previous methods. We further verify the efficacy of the proposed framework by providing tighter problem-dependent regret bound for the CVaR bandit.},
  author = {Hao Liang and Zhi-Quan Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2023-08-28},
  month = {7},
  pages = {20677--20705},
  pdf = {https://proceedings.mlr.press/v202/liang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Distribution Optimization Framework for Confidence Bounds of Risk Measures},
  url = {https://proceedings.mlr.press/v202/liang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023consistency,
  abstract = {Consistency plays an important role in learning theory. However, in multiple kernel clustering (MKC), the consistency of kernel weights has not been sufficiently investigated. In this work, we fill this gap with a non-asymptotic analysis on the consistency of kernel weights of a novel method termed SimpleMKKM. Under the assumptions of the eigenvalue gap, we give an infinity norm bound as $\widetilde{\mathcal{O}}(k/\sqrt{n})$, where $k$ is the number of clusters and $n$ is the number of samples. On this basis, we establish an upper bound for the excess clustering risk. The paper also studies the difference of the kernel weights learned from $n$ samples and $r$ points sampled without replacement, and derive its upper bound as $\widetilde{\mathcal{O}}(k\cdot\sqrt{1/r-1/n})$. Based on the above results, we propose a novel strategy with Nyström method to enable SimpleMKKM to handle large-scale data.},
  author = {Weixuan Liang and Xinwang Liu and Yong Liu and Chuan Ma and Yunping Zhao and Zhe Liu and En Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2024-11-14},
  month = {7},
  pages = {20650--20676},
  pdf = {https://proceedings.mlr.press/v202/liang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Consistency of Multiple Kernel Clustering},
  url = {https://proceedings.mlr.press/v202/liang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023adaptdiffuser,
  abstract = {Diffusion models have demonstrated their powerful generative capability in many tasks, with great potential to serve as a paradigm for offline reinforcement learning. However, the quality of the diffusion model is limited by the insufficient diversity of training data, which hinders the performance of planning and the generalizability to new tasks. This paper introduces AdaptDiffuser, an evolutionary planning method with diffusion that can self-evolve to improve the diffusion model hence a better planner, not only for seen tasks but can also adapt to unseen tasks. AdaptDiffuser enables the generation of rich synthetic expert data for goal-conditioned tasks using guidance from reward gradients. It then selects high-quality data via a discriminator to finetune the diffusion model, which improves the generalization ability to unseen tasks. Empirical experiments on two benchmark environments and two carefully designed unseen tasks in KUKA industrial robot arm and Maze2D environments demonstrate the effectiveness of AdaptDiffuser. For example, AdaptDiffuser not only outperforms the previous art Diffuser by 20.8% on Maze2D and 7.5% on MuJoCo locomotion, but also adapts better to new tasks, e.g., KUKA pick-and-place, by 27.9% without requiring additional expert data.},
  author = {Zhixuan Liang and Yao Mu and Mingyu Ding and Fei Ni and Masayoshi Tomizuka and Ping Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2025-02-10},
  month = {7},
  pages = {20725--20745},
  pdf = {https://proceedings.mlr.press/v202/liang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners},
  url = {https://proceedings.mlr.press/v202/liang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023accuracy,
  abstract = {Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations, and the imbalance between subpopulations. Furthermore, we found that the nonlinearity of this "moon shape" is causally influenced by the degree of spurious correlations in the training data. Our controlled experiments show that stronger spurious correlation in the training data creates more nonlinear performance correlation. We provide complementary experimental and theoretical analyses for this phenomenon, and discuss its implications for ML reliability and fairness.},
  author = {Weixin Liang and Yining Mao and Yongchan Kwon and Xinyu Yang and James Zou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2025-01-15},
  month = {7},
  pages = {20706--20724},
  pdf = {https://proceedings.mlr.press/v202/liang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accuracy on the Curve: On the Nonlinear Correlation of {ML} Performance Between Data Subpopulations},
  url = {https://proceedings.mlr.press/v202/liang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023learning,
  abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains.},
  author = {Youwei Liang and Kevin Stone and Ali Shameli and Chris Cummins and Mostafa Elhoushi and Jiadong Guo and Benoit Steiner and Xiaomeng Yang and Pengtao Xie and Hugh James Leather and Yuandong Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2023-08-28},
  month = {7},
  pages = {20746--20762},
  pdf = {https://proceedings.mlr.press/v202/liang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Compiler Pass Orders using Coreset and Normalized Value Prediction},
  url = {https://proceedings.mlr.press/v202/liang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023adversarial,
  abstract = {Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs. Then, based on this framework, we design a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. Extensive experiments show that the generated adversarial examples can effectively hinder DMs from extracting their features. Therefore, our method can be a powerful tool for human artists to protect their copyright against infringers equipped with DM-based AI-for-Art applications.},
  author = {Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  mdate = {2025-02-12},
  month = {7},
  pages = {20763--20786},
  pdf = {https://proceedings.mlr.press/v202/liang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples},
  url = {https://proceedings.mlr.press/v202/liang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023clustseg,
  abstract = {We present {CLUSTSEG}, a general, transformer-based framework that tackles different image segmentation tasks (i.e., superpixel, semantic, instance, and panoptic) through a unified neural clustering scheme. Regarding queries as cluster centers, {CLUSTSEG} is innovative in two aspects: 1) cluster centers are initialized in heterogeneous ways so as to pointedly address task-specific demands (e.g., instance- or category-level distinctiveness), yet without modifying the architecture; and 2) pixel-cluster assignment, formalized in a cross-attention fashion, is alternated with cluster center update, yet without learning additional parameters. These innovations closely link {CLUSTSEG} to {EM} clustering and make it a transparent and powerful framework that yields superior results across the above segmentation tasks.},
  address = {Honolulu, Hawaii, USA},
  author = {James Chenhao Liang and Tianfei Zhou and Dongfang Liu and Wenguan Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/liang2023clustseg.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20787--20809},
  pdf = {https://proceedings.mlr.press/v202/liang23h/liang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CLUSTSEG}: Clustering for Universal Segmentation},
  url = {https://proceedings.mlr.press/v202/liang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023conformal,
  abstract = {Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.},
  address = {Honolulu, Hawaii, USA},
  author = {Ziyi Liang and Yanfei Zhou and Matteo Sesia},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/liang2023conformal.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20810--20851},
  pdf = {https://proceedings.mlr.press/v202/liang23i/liang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conformal Inference is (almost) Free for Neural Networks Trained with Early Stopping},
  url = {https://proceedings.mlr.press/v202/liang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liang2023less,
  abstract = {Layer-wise distillation is a powerful tool to compress large models (i.e. teacher models) into small ones (i.e., student models). The student distills knowledge from the teacher by mimicking the hidden representations of the teacher at every intermediate layer. However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning. To address these challenges, we propose a novel Task-aware layEr-wise Distillation ({TED}). {TED} designs task-aware filters to align the hidden representations of the student and the teacher at each layer. The filters select the knowledge that is useful for the target task from the hidden representations. As such, {TED} reduces the knowledge gap between the two models and helps the student to fit better on the target task. We evaluate {TED} in two scenarios: continual pre-training and fine-tuning. {TED} demonstrates significant and consistent improvements over existing distillation methods in both scenarios.},
  address = {Honolulu, Hawaii, USA},
  author = {Chen Liang and Simiao Zuo and Qingru Zhang and Pengcheng He and Weizhu Chen and Tuo Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/liang2023less.pdf:pdf},
  mdate = {2023-09-07},
  month = {23--29 Jul},
  pages = {20852--20867},
  pdf = {https://proceedings.mlr.press/v202/liang23j/liang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Less is More: Task-aware Layer-wise Distillation for Language Model Compression},
  url = {https://proceedings.mlr.press/v202/liang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liao2023statistical,
  abstract = {We initiate the study of statistical inference and A/B testing for first-price pacing equilibria ({FPPE}). The {FPPE} model captures the dynamics resulting from large-scale first-price auction markets where buyers use pacing-based budget management. Such markets arise in the context of internet advertising, where budgets are prevalent. We propose a statistical framework for the {FPPE} model, in which a limit {FPPE} with a continuum of items models the long-run steady-state behavior of the auction platform, and an observable {FPPE} consisting of a finite number of items provides the data to estimate primitives of the limit {FPPE}, such as revenue, Nash social welfare (a fair metric of efficiency), and other parameters of interest. We develop central limit theorems and asymptotically valid confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of our estimators. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Numerical simulations verify our central limit theorems, and empirical coverage rates for our confidence intervals agree with our theory.},
  address = {Honolulu, Hawaii, USA},
  author = {Luofeng Liao and Christian Kroer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/liao2023statistical.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20868--20905},
  pdf = {https://proceedings.mlr.press/v202/liao23a/liao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Statistical Inference and {A/B} Testing for First-Price Pacing Equilibria},
  url = {https://proceedings.mlr.press/v202/liao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liao2023supervised,
  abstract = {There is extensive interest in metric learning methods for image retrieval. Many metric learning loss functions focus on learning a correct ranking of training samples, but strongly overfit semantically inconsistent labels and require a large amount of data. To address these shortcomings, we propose a new metric learning method, called contextual loss, which optimizes contextual similarity in addition to cosine similarity. Our contextual loss implicitly enforces semantic consistency among neighbors while converging to the correct ranking. We empirically show that the proposed loss is more robust to label noise, and is less prone to overfitting even when a large portion of train data is withheld. Extensive experiments demonstrate that our method achieves a new state-of-the-art across four image retrieval benchmarks and multiple different evaluation settings.},
  address = {Honolulu, Hawaii, USA},
  author = {Christopher Liao and Theodoros Tsiligkaridis and Brian Kulis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/liao2023supervised.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20906--20938},
  pdf = {https://proceedings.mlr.press/v202/liao23b/liao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Supervised Metric Learning to Rank for Retrieval via Contextual Similarity Optimization},
  url = {https://proceedings.mlr.press/v202/liao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lien2023revisiting,
  abstract = {Domain randomization ({DR}) is widely used in reinforcement learning ({RL}) to bridge the gap between simulation and reality by maximizing its average returns under the perturbation of environmental parameters. However, even the most complex simulators cannot capture all details in reality due to finite domain parameters and simplified physical models. Additionally, the existing methods often assume that the distribution of domain parameters belongs to a specific family of probability functions, such as normal distributions, which may not be correct. To overcome these limitations, we propose a new approach to {DR} by rethinking it from the perspective of adversarial state perturbation, without the need for reconfiguring the simulator or relying on prior knowledge about the environment.},
  address = {Honolulu, Hawaii, USA},
  author = {Yun-Hsuan Lien and Ping-Chun Hsieh and Yu-Shuen Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lien2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20939--20949},
  pdf = {https://proceedings.mlr.press/v202/lien23a/lien23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Domain Randomization via Relaxed State-Adversarial Policy Optimization},
  url = {https://proceedings.mlr.press/v202/lien23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lie2023variational,
  abstract = {Retrieval-augmented models have proven to be effective in natural language processing tasks, yet there remains a lack of research on their optimization using variational inference. We introduce the Variational Open-Domain ({VOD}) framework for end-to-end training and evaluation of retrieval-augmented models, focusing on open-domain question answering and language modelling. The {VOD} objective, a self-normalized estimate of the R{\'e}nyi variational bound, approximates the task marginal likelihood and is evaluated under samples drawn from an auxiliary sampling distribution (cached retriever and/or approximate posterior). It remains tractable, even for retriever distributions defined on large corpora. We demonstrate {VOD}'s versatility by training reader-retriever {BERT}-sized models on multiple-choice medical exam questions.},
  address = {Honolulu, Hawaii, USA},
  author = {Valentin Li{\'e}vin and Andreas Geert Motzfeldt and Ida Riis Jensen and Ole Winther},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lie2023variational.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20950--20977},
  pdf = {https://proceedings.mlr.press/v202/lievin23a/lievin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Open-Domain Question Answering},
  url = {https://proceedings.mlr.press/v202/lievin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023generating,
  abstract = {Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of protein structures that performs discrete-time diffusion using a cloud of oriented reference frames in 3D space.},
  address = {Honolulu, Hawaii, USA},
  author = {Yeqing Lin and Mohammed AlQuraishi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lin2023generating.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {20978--21002},
  pdf = {https://proceedings.mlr.press/v202/lin23a/lin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds},
  url = {https://proceedings.mlr.press/v202/lin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023hyperbolic,
  abstract = {Finding meaningful representations and distances of hierarchical data is important in many fields. This paper presents a new method for hierarchical data embedding and distance. Our method relies on combining diffusion geometry, a central approach to manifold learning, and hyperbolic geometry. Specifically, using diffusion geometry, we build multi-scale densities on the data, aimed to reveal their hierarchical structure, and then embed them into a product of hyperbolic spaces. We show theoretically that our embedding and distance recover the underlying hierarchical structure. In addition, we demonstrate the efficacy of the proposed method and its advantages compared to existing methods on graph embedding benchmarks and hierarchical datasets.},
  address = {Honolulu, Hawaii, USA},
  author = {Ya-Wei Eileen Lin and Ronald R. Coifman and Gal Mishne and Ronen Talmon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lin2023hyperbolic.pdf:pdf},
  mdate = {2023-11-12},
  month = {23--29 Jul},
  pages = {21003--21025},
  pdf = {https://proceedings.mlr.press/v202/lin23b/lin23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyperbolic Diffusion Embedding and Distance for Hierarchical Representation Learning},
  url = {https://proceedings.mlr.press/v202/lin23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023simplifying,
  abstract = {Riemannian submanifold optimization with momentum is computationally challenging because, to ensure that the iterates remain on the submanifold, we often need to solve difficult differential equations. Here, we simplify such difficulties for a class of structured symmetric positive-definite matrices with the affine-invariant metric. We do so by proposing a generalized version of the Riemannian normal coordinates that dynamically orthonormalizes the metric and locally converts the problem into an unconstrained problem in the Euclidean space. We use our approach to simplify existing approaches for structured covariances and develop matrix-inverse-free 2nd-order optimizers for deep learning with low precision by using only matrix multiplications.},
  address = {Honolulu, Hawaii, USA},
  author = {Wu Lin and Valentin Duruisseaux and Melvin Leok and Frank Nielsen and Mohammad Emtiyaz Khan and Mark Schmidt},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  editor = {Andreas Krause and Emma Brunskill and Kyunghyun Cho and Barbara Engelhardt and Sivan Sabato and Jonathan Scarlett},
  file = {:/home/b/documents/inproceedings/lin2023simplifying.pdf:pdf},
  mdate = {2023-08-28},
  month = {23--29 Jul},
  pages = {21026--21050},
  pdf = {https://proceedings.mlr.press/v202/lin23c/lin23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning},
  url = {https://proceedings.mlr.press/v202/lin23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023text,
  abstract = {This paper introduces GENIE, a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks: XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples.},
  author = {Zhenghao Lin and Yeyun Gong and Yelong Shen and Tong Wu and Zhihao Fan and Chen Lin and Nan Duan and Weizhu Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023text.pdf:pdf},
  pages = {21051--21064},
  pdf = {https://proceedings.mlr.press/v202/lin23d/lin23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise},
  url = {https://proceedings.mlr.press/v202/lin23d.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023theory,
  abstract = {Continual learning (CL), which aims to learn a sequence of tasks, has attracted significant recent attention. However, most work has focused on the experimental performance of CL, and theoretical studies of CL are still limited. In particular, there is a lack of understanding on what factors are important and how they affect catastrophic forgetting and generalization performance. To fill this gap, our theoretical analysis, under overparameterized linear models, provides the first-known explicit form of the expected forgetting and generalization error for a general CL setup with an arbitrary number of tasks. Further analysis of such a key result yields a number of theoretical explanations about how overparameterization, task similarity, and task ordering affect both forgetting and generalization error of CL. More interestingly, by conducting experiments on real datasets using deep neural networks (DNNs), we show that some of these insights even go beyond the linear models and can be carried over to practical setups.},
  author = {Sen Lin and Peizhong Ju and Yingbin Liang and Ness B. Shroff},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023theory.pdf:pdf},
  pages = {21078--21100},
  pdf = {https://proceedings.mlr.press/v202/lin23f/lin23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Theory on Forgetting and Generalization of Continual Learning},
  url = {https://proceedings.mlr.press/v202/lin23f.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023accelerated,
  abstract = {Exploiting partial first-order information in a cyclic way is arguably the most natural strategy to obtain scalable first-order methods. However, despite their wide use in practice, cyclic schemes are far less understood from a theoretical perspective than their randomized counterparts. Motivated by a recent success in analyzing an extrapolated cyclic scheme for generalized variational inequalities, we propose an Accelerated Cyclic Coordinate Dual Averaging with Extrapolation (A-CODER) method for composite convex optimization, where the objective function can be expressed as the sum of a smooth convex function accessible via a gradient oracle and a convex, possibly nonsmooth, function accessible via a proximal oracle. We show that A-CODER attains the optimal convergence rate with improved dependence on the number of blocks compared to prior work. Furthermore, for the setting where the smooth component of the objective function is expressible in a finite sum form, we introduce a variance-reduced variant of A-CODER, VR-A-CODER, with state-of-the-art complexity guarantees. Finally, we demonstrate the effectiveness of our algorithms through numerical experiments.},
  author = {Cheuk Yin Lin and Chaobing Song and Jelena Diakonikolas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023accelerated.pdf:pdf},
  pages = {21101--21126},
  pdf = {https://proceedings.mlr.press/v202/lin23g/lin23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accelerated Cyclic Coordinate Dual Averaging with Extrapolation for Composite Convex Optimization},
  url = {https://proceedings.mlr.press/v202/lin23g.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023probabilistic,
  abstract = {Latent Gaussian models have a rich history in statistics and machine learning, with applications ranging from factor analysis to compressed sensing to time series analysis. The classical method for maximizing the likelihood of these models is the expectation-maximization (EM) algorithm. For problems with high-dimensional latent variables and large datasets, EM scales poorly because it needs to invert as many large covariance matrices as the number of data points. We introduce probabilistic unrolling, a method that combines Monte Carlo sampling with iterative linear solvers to circumvent matrix inversion. Our theoretical analyses reveal that unrolling and backpropagation through the iterations of the solver can accelerate gradient estimation for maximum likelihood estimation. In experiments on simulated and real data, we demonstrate that probabilistic unrolling learns latent Gaussian models up to an order of magnitude faster than gradient EM, with minimal losses in model performance.},
  author = {Alexander Lin and Bahareh Tolooshams and Yves F. Atchad and Demba E. Ba},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023probabilistic.pdf:pdf},
  pages = {21153--21181},
  pdf = {https://proceedings.mlr.press/v202/lin23i/lin23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Unrolling: Scalable, Inverse-Free Maximum Likelihood Estimation for Latent Gaussian Models},
  url = {https://proceedings.mlr.press/v202/lin23i.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023unveiling,
  abstract = {Recent studies show that paddings in convolutional neural networks encode absolute position information which can negatively affect the model performance for certain tasks. However, existing metrics for quantifying the strength of positional information remain unreliable and frequently lead to erroneous results. To address this issue, we propose novel metrics for measuring and visualizing the encoded positional information. We formally define the encoded information as PPP (Position-information Pattern from Padding) and conduct a series of experiments to study its properties as well as its formation. The proposed metrics measure the presence of positional information more reliably than the existing metrics based on PosENet and a test in F-Conv. We also demonstrate that for any extant and proposed padding schemes, PPP is primarily a learning artifact and is less dependent on the characteristics of the underlying padding schemes.},
  author = {Chieh Hubert Lin and Hung-Yu Tseng and Hsin-Ying Lee and Maneesh Kumar Singh and Ming-Hsuan Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023unveiling.pdf:pdf},
  pages = {21204--21222},
  pdf = {https://proceedings.mlr.press/v202/lin23k/lin23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features},
  url = {https://proceedings.mlr.press/v202/lin23k.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023safe,
  abstract = {Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that approaches this problem from the perspective of trajectory distribution. TREBI models this problem through diffusion model planning and solves it through iterative denoising over the trajectory space. Experiments on both simulated and real-world datasets verify the effectiveness of TREBI in achieving adaptive safe control with flexible real-time budget constraints.},
  author = {Qian Lin and Bo Tang and Zifan Wu and Chao Yu and Shangqin Mao and Qianlong Xie and Xingxing Wang and Dong Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023safe.pdf:pdf},
  pages = {21127--21152},
  pdf = {https://proceedings.mlr.press/v202/lin23h/lin23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Safe Offline Reinforcement Learning with Real-Time Budget Constraints},
  url = {https://proceedings.mlr.press/v202/lin23h.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023fast,
  abstract = {We study the problem of learning prediction sets in the online setting with the goal of maximizing the conditional expected value of each prediction set subject to a cost constraint. This is motivated by practical applications where different prediction outcomes have different values and costs. We develop an online conformal prediction method that jointly maximizes the expected value and controls the conformal cost, extending the standard conformal prediction framework to handle value maximization with cost constraints. Our algorithm achieves regret bounds that scale with the complexity of the value function class while maintaining the desired cost control guarantee. Empirical evaluation on multiple datasets demonstrates the effectiveness of our approach in improving the value of prediction sets while respecting cost constraints.},
  author = {Zhen Lin and Shubhendu Trivedi and Cao Xiao and Jimeng Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023fast.pdf:pdf},
  pages = {21182--21203},
  pdf = {https://proceedings.mlr.press/v202/lin23j/lin23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control},
  url = {https://proceedings.mlr.press/v202/lin23j.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023fair,
  abstract = {In collaborative learning with streaming data, nodes (e.g., organizations) jointly and continuously learn a machine learning model by sharing the latest model updates computed from their latest streaming data. For the more resourceful nodes to be willing to share their model updates, they need to be fairly incentivized. This paper explores an incentive design that guarantees fairness so that nodes receive rewards commensurate to their contributions. Our approach leverages an explore-then-exploit formulation to estimate the nodes' contributions for realizing our theoretically guaranteed fair incentives. However, we observe a rich get richer phenomenon arising from the existing approaches to guarantee fairness and it discourages the participation of the less resourceful nodes. To remedy this, we additionally preserve asymptotic equality, i.e., less resourceful nodes achieve equal performance eventually to the more resourceful nodes. We propose a novel collaborative learning algorithm that provably achieves both fairness and asymptotic equality simultaneously.},
  author = {Xiaoqiang Lin and Xinyi Xu and See-Kiong Ng and Chuan-Sheng Foo and Bryan Kian Hsiang Low},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023fair.pdf:pdf},
  pages = {21223--21259},
  pdf = {https://proceedings.mlr.press/v202/lin23l/lin23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair yet Asymptotically Equal Collaborative Learning},
  url = {https://proceedings.mlr.press/v202/lin23l.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lin2023efficient,
  abstract = {We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop efficient algorithms to compute the approximations. Finally, we propose to incorporate our computations of complete interatomic potentials into message passing neural networks for representation learning. We perform experiments on the JARVIS and Materials Project benchmarks for evaluation. Results show that the use of interatomic potentials and complete interatomic potentials leads to consistent performance improvements with reasonable computational costs.},
  author = {Yuchao Lin and Keqiang Yan and Youzhi Luo and Yi Liu and Xiaoning Qian and Shuiwang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lin2023efficient.pdf:pdf},
  pages = {21260--21287},
  pdf = {https://proceedings.mlr.press/v202/lin23m/lin23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction},
  url = {https://proceedings.mlr.press/v202/lin23m.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lindermayr2023speedoblivious,
  abstract = {We study online scheduling on related machines, where jobs arrive over time and must be assigned to machines with different speeds. The standard assumption is that the algorithm knows the precise speeds of all machines. However, in practice, machine speeds are often unknown or imprecise. We investigate the question of whether knowing precise speeds is necessary for good performance. We present speed-oblivious algorithms that achieve competitive ratios that match or closely approximate those of speed-aware algorithms, even when the precise machine speeds are unknown. Our algorithms adapt to the speed configuration during runtime by observing job completion times. We provide theoretical analysis and empirical evaluation demonstrating the effectiveness of our approach in scenarios where machine speeds are uncertain or change dynamically. The results show that precise speed knowledge is not always essential for optimal online scheduling performance.},
  author = {Alexander Lindermayr and Nicole Megow and Martin Rapp},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lindermayr2023speedoblivious.pdf:pdf},
  pages = {21312--21334},
  pdf = {https://proceedings.mlr.press/v202/lindermayr23a/lindermayr23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Speed-Oblivious Online Scheduling: Knowing (Precise) Speeds is not Necessary},
  url = {https://proceedings.mlr.press/v202/lindermayr23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{ling2023graph,
  abstract = {We study graph data augmentation by mixup, which has been used successfully on images. A key operation of mixup is to compute a convex combination of a pair of inputs. This operation is straightforward for grid-like data, such as images, but challenging for graph data. The key difficulty lies in the fact that different graphs typically have different numbers of nodes, and thus there lacks a node-level correspondence between graphs. In this work, we propose {S-Mixup}, a simple yet effective mixup method for graph classification by soft alignments. Specifically, given a pair of graphs, we explicitly obtain node-level correspondence via computing a soft assignment matrix to match the nodes between two graphs. Based on the soft assignments, we transform the adjacency and node feature matrices of one graph, so that the transformed graph is aligned with the other graph.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Hongyi Ling and Zhimeng Jiang and Meng Liu and Shuiwang Ji and Na Zou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ling2023graph.pdf:pdf},
  mdate = {2025-02-06},
  month = {7},
  pages = {21335--21349},
  pdf = {https://proceedings.mlr.press/v202/ling23a/ling23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Mixup with Soft Alignments},
  url = {https://proceedings.mlr.press/v202/ling23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ling2023deep,
  abstract = {Influence maximization ({IM}) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based {IM} methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based {IM} methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained {IM} variants. To cope with the above challenges, we design a novel framework {DeepIM} to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information diffusion pattern in a data-driven and end-to-end manner. Finally, we design a novel objective function to infer optimal seed sets under flexible node-centrality-based budget constraints. Extensive analyses are conducted over both synthetic and real-world datasets to demonstrate the overall performance of {DeepIM}.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Chen Ling and Junji Jiang and Junxiang Wang and My T. Thai and Renhao Xue and James Song and Meikang Qiu and Liang Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ling2023deep.pdf:pdf},
  mdate = {2024-07-17},
  month = {7},
  pages = {21350--21361},
  pdf = {https://proceedings.mlr.press/v202/ling23b/ling23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Graph Representation Learning and Optimization for Influence Maximization},
  url = {https://proceedings.mlr.press/v202/ling23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ziyin2023spred,
  abstract = {We propose to minimize a generic differentiable objective with {L₁} constraint using a simple reparametrization and straightforward stochastic gradient descent. Our proposal is a direct generalization of previous ideas that the {L₁} penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that our proposed method, spred, is an exact differentiable solver of {L₁} and that the reparametrization trick is completely ``benign'' for a generic nonconvex function. Practically, we demonstrate the usefulness of our method in training sparse neural networks for gene selection tasks and neural network compression.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Liu Ziyin and Zihao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ziyin2023spred.pdf:pdf},
  mdate = {2025-04-10},
  month = {7},
  pages = {43407--43422},
  pdf = {https://proceedings.mlr.press/v202/ziyin23a/ziyin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {spred: Solving {L1} Penalty with {SGD}},
  url = {https://proceedings.mlr.press/v202/ziyin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023image,
  abstract = {Perturbative availability poisons ({PAPs}) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering {PAPs} do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art {PAP} methods are vulnerable to Image Shortcut Squeezing ({ISS}), which is based on simple compression (i.e., grayscale and {JPEG} compressions). On average, {ISS} restores the {CIFAR-10} model accuracy to 81.73\%, surpassing the previous best preprocessing-based countermeasures by 37.97\% absolute. {ISS} also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of {PAP} perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific {ISS} compression yields the best performance for a specific type of {PAP} perturbation.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Zhuoran Liu and Zhengyu Zhao and Martha A. Larson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023image.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22473--22487},
  pdf = {https://proceedings.mlr.press/v202/liu23bb/liu23bb.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression},
  url = {https://proceedings.mlr.press/v202/liu23bb.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023revisiting,
  abstract = {To deal with the challenge of high cost of annotating all relevant labels for each example in multi-label learning, single-positive multi-label learning ({SPMLL}) has been studied in recent years, where each example is annotated with only one positive label. By adopting pseudo-label generation, i.e., assigning pseudo-label to each example by various strategies, existing methods have empirically validated that {SPMLL} would significantly reduce the amount of supervision with a tolerable damage in classification performance. However, there is no existing method that can provide a theoretical guarantee for learning from pseudo-label on {SPMLL}. In this paper, the conditions of the effectiveness of learning from pseudo-label for {SPMLL} are shown and the learnability of pseudo-label-based methods is proven.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Biao Liu and Ning Xu and Jiaqi Lv and Xin Geng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023revisiting.pdf:pdf},
  mdate = {2023-11-03},
  month = {7},
  pages = {22249--22265},
  pdf = {https://proceedings.mlr.press/v202/liu23ar/liu23ar.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Pseudo-Label for Single-Positive Multi-Label Learning},
  url = {https://proceedings.mlr.press/v202/liu23ar.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023emergent,
  abstract = {Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple {AI} areas. In reinforcement learning ({RL}), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Hao Liu and Pieter Abbeel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023emergent.pdf:pdf},
  mdate = {2024-02-10},
  month = {7},
  pages = {21362--21374},
  pdf = {https://proceedings.mlr.press/v202/liu23a/liu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Emergent Agentic Transformer from Chain of Hindsight Experience},
  url = {https://proceedings.mlr.press/v202/liu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023shapley,
  abstract = {We introduce the idea of decomposing the residuals of regression with respect to the data instances instead of features. This allows us to determine the effects of each individual instance on the model and each other, and in doing so makes for a model-agnostic method of identifying instances of interest. We call this approach Shapley based residual decomposition, and demonstrate its utility on both synthetic and real-world datasets for the purposes of outlier detection, data valuation, and uncertainty quantification.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Tommy Liu and Amanda S. Barnard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023shapley.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21375--21387},
  pdf = {https://proceedings.mlr.press/v202/liu23b/liu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Shapley Based Residual Decomposition for Instance Analysis},
  url = {https://proceedings.mlr.press/v202/liu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023learning,
  abstract = {This paper addresses unsupervised representation learning on tabular data containing multiple views generated by distinct sources of measurement. Traditional methods, which tackle this problem using the multi-view framework, are constrained by predefined assumptions that assume feature sets share the same information and representations should learn globally shared factors. However, this assumption is not always valid for real-world tabular datasets with complex dependencies between feature sets, resulting in localized information that is harder to learn. To overcome this limitation, we propose a data-driven approach that learns feature set dependencies by representing feature sets as graph nodes and their relationships as learnable edges. We introduce {LEGATO}, a novel hierarchical graph autoencoder that learns a smaller, latent graph to aggregate information from multiple views.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Tennison Liu and Jeroen Berrevoets and Zhaozhi Qian and Mihaela van der Schaar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023learning.pdf:pdf},
  mdate = {2024-10-06},
  month = {7},
  pages = {21388--21403},
  pdf = {https://proceedings.mlr.press/v202/liu23c/liu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Representations without Compositional Assumptions},
  url = {https://proceedings.mlr.press/v202/liu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023byzantinerobust,
  abstract = {Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust {AGgregation Rules} ({AGRs}) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust {AGRs} when data is non-{Identically} and {Independently} {Distributed} (non-{IID}). In this paper, we first reveal the root causes of performance degradation of current robust {AGRs} in non-{IID} settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose {GAS}, a {GrAdient} {Splitting} approach that can successfully adapt existing robust {AGRs} to non-{IID} settings. We provide a detailed convergence analysis when the existing robust {AGRs} are combined with {GAS}, and experiments on various real-world datasets verify the efficacy of our proposed {GAS} approach.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Yuchen Liu and Chen Chen and Lingjuan Lyu and Fangzhao Wu and Sai Wu and Gang Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023byzantinerobust.pdf:pdf},
  mdate = {2024-11-28},
  month = {7},
  pages = {21404--21425},
  pdf = {https://proceedings.mlr.press/v202/liu23d/liu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting},
  url = {https://proceedings.mlr.press/v202/liu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023audioldm,
  abstract = {Text-to-audio ({TTA}) systems have recently gained attention for their ability to synthesize general audio based on text descriptions. However, previous studies in {TTA} have limited generation quality with high computational costs. In this study, we propose {AudioLDM}, a {TTA} system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining ({CLAP}) embeddings. The pretrained {CLAP} models enable us to train {LDMs} with audio embeddings while providing text embeddings as the condition during sampling. By learning the latent representations of audio signals without modelling the cross-modal relationship, {AudioLDM} improves both generation quality and computational efficiency. Trained on {AudioCaps} with a single {GPU}, {AudioLDM} achieves state-of-the-art {TTA} performance compared to other open-sourced systems, measured by both objective and subjective metrics. {AudioLDM} is also the first {TTA} system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.},
  address = {Honolulu, Hawaii, {USA}},
  author = {Haohe Liu and Zehua Chen and Yi Yuan and Xinhao Mei and Xubo Liu and Danilo P. Mandic and Wenwu Wang and Mark D. Plumbley},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023audioldm.pdf:pdf},
  mdate = {2025-04-11},
  month = {7},
  pages = {21450--21474},
  pdf = {https://proceedings.mlr.press/v202/liu23f/liu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{AudioLDM}: Text-to-Audio Generation with Latent Diffusion Models},
  url = {https://proceedings.mlr.press/v202/liu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023identifiability,
  abstract = {The noise transition matrix plays a central role in the problem of learning with noisy labels. Among many other reasons, a large number of existing solutions rely on the knowledge of it. Identifying and estimating the transition matrix without ground truth labels is a critical and challenging task. When label noise transition depends on each instance, the problem of identifying the instance-dependent noise transition matrix becomes substantially more challenging.},
  author = {Yang Liu and Hao Cheng and Kun Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023identifiability.pdf:pdf},
  mdate = {2024-08-01},
  pages = {21475--21496},
  pdf = {https://proceedings.mlr.press/v202/liu23g/liu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Identifiability of Label Noise Transition Matrix},
  url = {https://proceedings.mlr.press/v202/liu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023rsc,
  abstract = {Training graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by community hardware. Prior art successfully reduces the computation cost of dense matrix based operations (e.g., convolution and linear) via sampling-based approximation. The paper proposes Randomized Sparse Computation (RSC) to address challenges in sparse matrix operations, achieving up to 11.6X speedup for a single sparse operation and 1.6X end-to-end wall-clock time speedup with almost no accuracy drop.},
  author = {Zirui Liu and Shengyuan Chen and Kaixiong Zhou and Daochen Zha and Xiao Huang and Xia Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023rsc.pdf:pdf},
  mdate = {2024-10-24},
  pages = {21951--21968},
  pdf = {https://proceedings.mlr.press/v202/liu23ad/liu23ad.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{RSC}: Accelerate Graph Neural Networks Training via Randomized Sparse Computations},
  url = {https://proceedings.mlr.press/v202/liu23ad.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023perturbation,
  abstract = {Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distributions have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen transition kernels the proposed approach can lead to substantially higher power than the KSD test.},
  author = {Xing Liu and Andrew B. Duncan and Axel Gandy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023perturbation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21527--21547},
  pdf = {https://proceedings.mlr.press/v202/liu23i/liu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy},
  url = {https://proceedings.mlr.press/v202/liu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023group,
  abstract = {Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pretraining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space.},
  author = {Shengchao Liu and Weitao Du and Zhi-Ming Ma and Hongyu Guo and Jian Tang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023group.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21497--21526},
  pdf = {https://proceedings.mlr.press/v202/liu23h/liu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining},
  url = {https://proceedings.mlr.press/v202/liu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023opponentlimited,
  abstract = {In recent years, online search has been playing an increasingly important role in imperfect information games (IIGs). Previous online search is known as common-knowledge subgame solving, which has to consider all the states in a common-knowledge closure. This is only computationally tolerable for medium size games, such as poker. To handle larger games, order-1 Knowledge-Limited Subgame Solving (1-KLSS) only considers the states in a knowledge-limited closure, which results in a much smaller subgame. However, 1-KLSS is unsafe. In this paper, we first extend 1-KLSS to Safe-1-KLSS and prove its safeness. To make Safe-1-KLSS applicable to even larger games, we propose Opponent-Limited Subgame Solving (OLSS) to limit how the opponent reaches a subgame and how it acts in the subgame.},
  author = {Weiming Liu and Haobo Fu and Qiang Fu and Wei Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023opponentlimited.pdf:pdf},
  mdate = {2024-09-25},
  pages = {21567--21585},
  pdf = {https://proceedings.mlr.press/v202/liu23k/liu23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Opponent-Limited Online Search for Imperfect Information Games},
  url = {https://proceedings.mlr.press/v202/liu23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023cones,
  abstract = {Human brains respond to semantic features of presented stimuli with different neurons. It is then curious whether modern deep neural networks admit a similar behavior pattern. Specifically, this paper finds a small cluster of neurons in a diffusion model corresponding to a particular subject. We call those neurons the concept neurons. They can be identified by statistics of network gradients to a stimulation connected with the given subject. The concept neurons demonstrate magnetic properties in interpreting and manipulating generation results. Shutting them can directly yield the related subject contextualized in different scenes. Concatenating multiple clusters of concept neurons can vividly generate all related concepts in a single image.},
  author = {Zhiheng Liu and Ruili Feng and Kai Zhu and Yifei Zhang and Kecheng Zheng and Yu Liu and Deli Zhao and Jingren Zhou and Yang Cao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023cones.pdf:pdf},
  mdate = {2025-03-19},
  pages = {21548--21566},
  pdf = {https://proceedings.mlr.press/v202/liu23j/liu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Cones}: Concept Neurons in Diffusion Models for Customized Generation},
  url = {https://proceedings.mlr.press/v202/liu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023towards,
  abstract = {Previous work demonstrates that the optimal safe reinforcement learning policy in a noise-free environment is vulnerable and could be unsafe under observational attacks. While adversarial training effectively improves robustness and safety, collecting samples by attacking the behavior agent online could be expensive or prohibitively dangerous in many applications. The paper addresses the challenge of making safe reinforcement learning policies more robust to observational attacks without requiring expensive or dangerous online adversarial sample collection.},
  author = {Zuxin Liu and Zijian Guo and Zhepeng Cen and Huan Zhang and Yihang Yao and Hanjiang Hu and Ding Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023towards.pdf:pdf},
  mdate = {2025-05-15},
  pages = {21586--21610},
  pdf = {https://proceedings.mlr.press/v202/liu23l/liu23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Robust and Safe Reinforcement Learning with Benign Off-policy Data},
  url = {https://proceedings.mlr.press/v202/liu23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023constrained,
  abstract = {Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the ε-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy.},
  author = {Zuxin Liu and Zijian Guo and Yihang Yao and Zhepeng Cen and Wenhao Yu and Tingnan Zhang and Ding Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023constrained.pdf:pdf},
  mdate = {2025-05-15},
  pages = {21611--21630},
  pdf = {https://proceedings.mlr.press/v202/liu23m/liu23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Decision Transformer for Offline Safe Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/liu23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023understanding,
  abstract = {Vision Transformer (ViT) is an attention-based model architecture that has demonstrated superior performance on many computer vision tasks. However, its security properties, in particular, the robustness against adversarial attacks, are yet to be thoroughly studied. Recent works have shown that ViT is vulnerable to attention-based adversarial patch attacks, which cover 1-3\% area of the input image using adversarial patches and degrades the model accuracy to 0\%. This work provides a generic study targeting the attention-based patch attack. First, we experimentally observe that adversarial patches only activate in a few layers and become lazy during attention updating. According to experiments, we study the theory of how a small adversarial patch perturbates the whole model. Based on understanding adversarial patch attacks, we propose a simple but efficient defense that correctly detects more than 95\% of adversarial patches.},
  author = {Liang Liu and Yanan Guo and Youtao Zhang and Jun Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023understanding.pdf:pdf},
  mdate = {2024-04-11},
  pages = {21631--21657},
  pdf = {https://proceedings.mlr.press/v202/liu23n/liu23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding and Defending Patched-based Adversarial Attacks for Vision Transformer},
  url = {https://proceedings.mlr.press/v202/liu23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023hierarchical,
  abstract = {Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, we propose to learn a meta-policy that composes a series of programs sampled from a learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors. The experimental results in the Karel domain show that our proposed framework outperforms baseline approaches while addressing the limitations of existing methods like LEAPS that are limited by program dataset distributions.},
  author = {Guan-Ting Liu and En-Pei Hu and Pu-Jen Cheng and Hung-Yi Lee and Shao-Hua Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023hierarchical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {21672--21697},
  pdf = {https://proceedings.mlr.press/v202/liu23p/liu23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs},
  url = {https://proceedings.mlr.press/v202/liu23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023online,
  abstract = {Based on binary inquiries, we developed an algorithm to estimate population quantiles under Local Differential Privacy (LDP). By self-normalizing, our algorithm provides asymptotically normal estimation with valid inference, resulting in tight confidence intervals without the need for nuisance parameters to be estimated. Our proposed method can be conducted fully online, leading to high computational efficiency and minimal storage requirements with $\mathcal{O}(1)$ space. We also proved an optimality result by an elegant application of one central limit theorem of Gaussian Differential Privacy (GDP) when targeting the frequently encountered median estimation problem. With mathematical proof and extensive numerical testing, we demonstrate the validity of our algorithm both theoretically and experimentally.},
  author = {Yi Liu and Qirui Hu and Lei Ding and Linglong Kong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023online.pdf:pdf},
  mdate = {2024-05-07},
  month = {7},
  pages = {21698--21714},
  pdf = {https://proceedings.mlr.press/v202/liu23q/liu23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Local Differential Private Quantile Inference via Self-normalization},
  url = {https://proceedings.mlr.press/v202/liu23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023nuno,
  abstract = {The neural operator has emerged as a powerful tool in learning mappings between function spaces in PDEs. However, when faced with real-world physical data, which are often highly non-uniformly distributed, it is challenging to use mesh-based techniques such as the FFT. To address this, we introduce the Non-Uniform Neural Operator (NUNO), a comprehensive framework designed for efficient operator learning with non-uniform data. Leveraging a K-D tree-based domain decomposition, we transform non-uniform data into uniform grids while effectively controlling interpolation error, thereby paralleling the speed and accuracy of learning from non-uniform data. We conduct extensive experiments on 2D elasticity, (2+1)D channel flow, and a 3D multi-physics heatsink, which, to our knowledge, marks a novel exploration into 3D PDE problems with complex geometries. Our framework has reduced error rates by up to 60% and enhanced training speeds by 2x to 30x. The code is now available at https://github.com/thu-ml/NUNO.},
  author = {Songming Liu and Zhongkai Hao and Chengyang Ying and Hang Su and Ze Cheng and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023nuno.pdf:pdf},
  mdate = {2023-11-14},
  month = {7},
  pages = {21658--21671},
  pdf = {https://proceedings.mlr.press/v202/liu23o/liu23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{NUNO}: A General Framework for Learning Parametric {PDE}s with Non-Uniform Data},
  url = {https://proceedings.mlr.press/v202/liu23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu20232ds,
  abstract = {Data valuation -- quantifying the contribution of individual data sources to certain predictive behaviors of a model -- is of great importance to enhancing the transparency of machine learning and designing incentive systems for data sharing. Existing work has focused on evaluating data sources with the shared feature or sample space. How to valuate fragmented data sources of which each only contains partial features and samples remains an open question. We start by presenting a method to calculate the counterfactual of removing a fragment from the aggregated data matrix. Based on the counterfactual calculation, we further propose 2D-Shapley, a theoretical framework for fragmented data valuation that uniquely satisfies some appealing axioms in the fragmented data context. 2D-Shapley empowers a range of new use cases, such as selecting useful data fragments, providing interpretation for sample-wise data values, and fine-grained data issue diagnosis.},
  author = {Zhihong Liu and Hoang Anh Just and Xiangyu Chang and Xi Chen and Ruoxi Jia},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu20232ds.pdf:pdf},
  mdate = {2024-03-21},
  month = {7},
  pages = {21730--21755},
  pdf = {https://proceedings.mlr.press/v202/liu23s/liu23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {2D-{S}hapley: A Framework for Fragmented Data Valuation},
  url = {https://proceedings.mlr.press/v202/liu23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023gf,
  abstract = {Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GFlowOut to address these issues. GFlowOut leverages the recently proposed probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks. We empirically demonstrate that GFlowOut results in predictive distributions that generalize better to out-of-distribution data, and provide uncertainty estimates which lead to better performance in downstream tasks.},
  author = {Dianbo Liu and Moksh Jain and Bonaventure F. P. Dossou and Qianli Shen and Salem Lahlou and Anirudh Goyal and Nikolay Malkin and Chris Chinenye Emezue and Dinghuai Zhang and Nadhir Hassen and Xu Ji and Kenji Kawaguchi and Yoshua Bengio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023gf.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21715--21729},
  pdf = {https://proceedings.mlr.press/v202/liu23r/liu23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GF}low{O}ut: Dropout with Generative Flow Networks},
  url = {https://proceedings.mlr.press/v202/liu23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023causal,
  abstract = {This paper addresses causal structure learning from non-stationary data where latent interventions affect the underlying causal mechanisms. The authors develop identifiability conditions and algorithms for discovering causal structures in scenarios where distributional changes over time may be caused by unobserved interventions, extending existing causal discovery methods to handle this challenging but practically important setting.},
  author = {Chenxi Liu and Kun Kuang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023causal.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21756--21777},
  pdf = {https://proceedings.mlr.press/v202/liu23t/liu23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Structure Learning for Latent Intervened Non-stationary Data},
  url = {https://proceedings.mlr.press/v202/liu23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023understanding,
  abstract = {Probabilistic Circuits (PCs) are a general and unified computational framework for tractable probabilistic models that support efficient computation of various inference tasks (e.g., computing marginal probabilities). Towards enabling such reasoning capabilities in complex real-world tasks, Liu et al. (2022) propose to distill knowledge (through latent variable assignments) from less tractable but more expressive deep generative models. However, it is still unclear what factors make this distillation work well. In this paper, we theoretically and empirically discover that the performance of a PC can exceed that of its teacher model. Therefore, instead of performing distillation from the most expressive deep generative model, we study what properties the teacher model and the PC should have in order to achieve good distillation performance. This leads to a generic algorithmic improvement as well as other data-type-specific ones over the existing latent variable distillation pipeline. Empirically, we outperform SoTA TPMs by a large margin on challenging image modeling benchmarks. In particular, on ImageNet32, PCs achieve 4.06 bits-per-dimension, which is only 0.34 behind variational diffusion models (Kingma et al., 2021).},
  author = {Xuejie Liu and Anji Liu and Guy Van den Broeck and Yitao Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21825--21838},
  pdf = {https://proceedings.mlr.press/v202/liu23x/liu23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding the Distillation Process from Deep Generative Models to Tractable Probabilistic Circuits},
  url = {https://proceedings.mlr.press/v202/liu23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023oscillationfree,
  abstract = {Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used de facto setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in query and key of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: 1. Statistical weight quantization (StatsQ) to improve quantization robustness 2. Confidence-guided annealing (CGA) that freezes weights with high confidence and calms oscillating weights 3. Query-key reparameterization (QKR) to resolve query-key intertwined oscillation and mitigate gradient misestimation. Extensive experiments demonstrate that these proposed techniques successfully abate weight oscillation and consistently achieve substantial accuracy improvement on ImageNet. Specifically, our 2-bit DeiT-T/DeiT-S algorithms outperform the previous state-of-the-art by 9.8% and 7.7%, respectively.},
  author = {Shih-Yang Liu and Zechun Liu and Kwang-Ting Cheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023oscillationfree.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21813--21824},
  pdf = {https://proceedings.mlr.press/v202/liu23w/liu23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Oscillation-free Quantization for Low-bit Vision Transformers},
  url = {https://proceedings.mlr.press/v202/liu23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023structural,
  abstract = {In many real-world applications, graph-structured data used for training and testing have differences in distribution, such as in high energy physics (HEP) where simulation data used for training may not match real experiments. Graph domain adaptation (GDA) is a method used to address these differences. However, current GDA primarily works by aligning the distributions of node representations output by a single graph neural network encoder shared across the training and testing domains, which may often yield sub-optimal solutions. This work examines different impacts of distribution shifts caused by either graph structure or node attributes and identifies a new type of shift, named conditional structure shift (CSS), which current GDA approaches are provably sub-optimal to deal with. A novel approach, called structural reweighting (StruRW), is proposed to address this issue and is tested on synthetic graphs, four benchmark datasets, and a new application in HEP. StruRW has shown significant performance improvement over the baselines in the settings with large graph structure shifts, and reasonable performance improvement when node attribute shift dominates.},
  author = {Shikun Liu and Tianchun Li and Yongbin Feng and Nhan Tran and Han Zhao and Qiang Qiu and Pan Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023structural.pdf:pdf},
  mdate = {2024-05-21},
  month = {7},
  pages = {21778--21793},
  pdf = {https://proceedings.mlr.press/v202/liu23u/liu23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Structural Re-weighting Improves Graph Domain Adaptation},
  url = {https://proceedings.mlr.press/v202/liu23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023averaged,
  abstract = {Gradient methods have become mainstream techniques for Bi-Level Optimization (BLO) in learning fields. The validity of existing works heavily rely on either a restrictive Lower-Level Strong Convexity (LLSC) condition or on solving a series of approximation subproblems with high accuracy or both. In this work, by averaging the upper and lower level objectives, we propose a single loop Bi-level Averaged Method of Multipliers (sl-BAMM) for BLO that is simple yet efficient for large-scale BLO and gets rid of the limited LLSC restriction. We further provide non-asymptotic convergence analysis of sl-BAMM towards KKT stationary points, and the comparative advantage of our analysis lies in the absence of strong gradient boundedness assumption, which is always required by others. Thus our theory safely captures a wider variety of applications in deep learning, especially where the upper-level objective is quadratic w.r.t. the lower-level variable. Experimental results demonstrate the superiority of our method.},
  author = {Risheng Liu and Yaohua Liu and Wei Yao and Shangzhi Zeng and Jin Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023averaged.pdf:pdf},
  mdate = {2024-07-27},
  month = {7},
  pages = {21839--21866},
  pdf = {https://proceedings.mlr.press/v202/liu23y/liu23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Averaged Method of Multipliers for Bi-Level Optimization without Lower-Level Strong Convexity},
  url = {https://proceedings.mlr.press/v202/liu23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023graph,
  abstract = {Dynamical systems with complex behaviours, e.g. immune system cells interacting with a pathogen, are commonly modelled by splitting the behaviour in different regimes, or modes, each with simpler dynamics, and then learn the switching behaviour from one mode to another. To achieve this, Switching Dynamical Systems (SDS) are a powerful tool that automatically discovers these modes and mode-switching behaviour from time series data. While effective, these methods focus on independent objects, where the modes of one object are independent of the modes of the other objects. In this paper, we focus on the more general interacting object setting for switching dynamical systems, where the per-object dynamics also depend on an unknown and dynamically changing subset of other objects and their modes. To this end, we propose GRAph Switching dynamical Systems (GRASS), in which we use a dynamic graph to characterize interactions between objects and learn both intra-object and inter-object mode-switching behaviour. We validate our approach on several synthetic datasets and demonstrate its efficacy on real-world datasets.},
  author = {Yongtuo Liu and Sara Magliacane and Miltiadis Kofinas and Efstratios Gavves},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023graph.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {21867--21883},
  pdf = {https://proceedings.mlr.press/v202/liu23z/liu23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Switching Dynamical Systems},
  url = {https://proceedings.mlr.press/v202/liu23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023omsdpm,
  abstract = {Diffusion Probabilistic Models (DPMs) have achieved remarkable performance in various generation tasks, but suffer from slow generation speed due to the large number of neural network evaluations required. This work reveals an overlooked dimension -- model schedule -- for optimizing the trade-off between generation quality and speed. We find that small models, though having worse generation quality when used alone, could outperform large models in certain generation steps. Based on this insight, we introduce OMS-DPM, a predictor-based search algorithm to find the optimal model schedule that maximizes generation quality under a given computational budget. Extensive experiments on CIFAR-10, CelebA, ImageNet, and LSUN demonstrate that OMS-DPM significantly improves both generation quality and speed compared to prior methods. When applied to Stable Diffusion, OMS-DPM achieves 2× acceleration while maintaining generation quality.},
  author = {Enshu Liu and Xuefei Ning and Zinan Lin and Huazhong Yang and Yu Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023omsdpm.pdf:pdf},
  month = {7},
  pages = {21915--21936},
  pdf = {https://proceedings.mlr.press/v202/liu23ab/liu23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{OMS-DPM}: Optimizing the Model Schedule for Diffusion Probabilistic Models},
  url = {https://proceedings.mlr.press/v202/liu23ab.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023high,
  abstract = {We describe a generic approach to show convergence with high probability for both stochastic convex and non-convex optimization with sub-Gaussian noise. In previous works for convex optimization, either the convergence is only in expectation or the bound depends on the diameter of the domain. Instead, we show high probability convergence with bounds depending on the initial distance to the optimal solution. The algorithms use step sizes analogous to the standard settings and are universal to Lipschitz functions, smooth functions, and their linear combinations. The method can be applied to the non-convex case. We demonstrate improved convergence rates: an O((1+σ²log(1/δ))/T+σ/√T) rate when T is known and an O((1+σ²log(T/δ))/√T) rate when T is unknown for SGD, where 1-δ is the desired success probability.},
  author = {Zijian Liu and Ta Duy Nguyen and Thien Hang Nguyen and Alina Ene and Huy Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023high.pdf:pdf},
  month = {7},
  pages = {21884--21914},
  pdf = {https://proceedings.mlr.press/v202/liu23aa/liu23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {High Probability Convergence of Stochastic Gradient Methods},
  url = {https://proceedings.mlr.press/v202/liu23aa.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023lazy,
  abstract = {We address the sparse reward problem in multi-agent reinforcement learning (MARL) from a new perspective of lazy agents. We empirically illustrate how lazy agents damage learning from both exploration and exploitation. We examine the causes and types of lazy agents using causal graph analysis of the interaction between agents and their environment. We mathematically define fully lazy agents and teams by calculating the causal effect of their actions on external states using the do-calculus process. Based on our definitions, we provide two intrinsic rewards to motivate agents: individual diligence intrinsic motivation (IDI) and collaborative diligence intrinsic motivation (CDI). IDI and CDI employ counterfactual reasoning based on the external states transition model (ESTM). Empirical results demonstrate that our method achieves state-of-the-art performance on challenging sparse reward benchmarks including SMAC and Google Research Football, helping both value-based and policy-based baselines avoid lazy agents for improved learning efficiency.},
  author = {Boyin Liu and Zhiqiang Pu and Yi Pan and Jianqiang Yi and Yanyan Liang and Du Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023lazy.pdf:pdf},
  month = {7},
  pages = {21937--21950},
  pdf = {https://proceedings.mlr.press/v202/liu23ac/liu23ac.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lazy Agents: A New Perspective on Solving Sparse Reward Problem in Multi-agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/liu23ac.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023simple,
  abstract = {Language emerges in human children as a byproduct of solving non-language tasks (e.g., acquiring food), in contrast to machine learning models that typically learn language by directly training on language tasks. We ask whether embodied reinforcement learning agents can indirectly learn language from non-language tasks. Learning to associate language with its meaning requires a dynamic environment with varied language. We investigate this in a multi-task environment with language that varies across different tasks. Specifically, we design an office navigation environment where the agent's goal is to find a particular office, and office locations differ in different buildings (i.e., tasks). Each building includes a floor plan with a simple language description of the goal office's location, which can be visually read as an RGB image when visited. We show that meta-reinforcement learning agents can learn to ground language and use it to guide their behavior in this environment.},
  author = {Evan Zheran Liu and Sahaana Suri and Tong Mu and Allan Zhou and Chelsea Finn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023simple.pdf:pdf},
  month = {7},
  pages = {21997--22008},
  pdf = {https://proceedings.mlr.press/v202/liu23af/liu23af.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/liu23af.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023algorithms,
  abstract = {We study histogram estimation under user-level differential privacy, where the goal is to preserve the privacy of all entries of any single user. We consider the heterogeneous scenario where the quantity of data can be different for each user, and in this scenario, the amount of noise injected into the histogram to obtain differential privacy is proportional to the maximum user contribution, which can be amplified by few outliers. We propose algorithms to choose the best user contribution bound for histogram estimation under both bounded and unbounded domain settings. When the size of the domain is bounded, we propose a user contribution bounding strategy that almost achieves a two-approximation with respect to the best contribution bound in hindsight.},
  author = {Yuhan Liu and Ananda Theertha Suresh and Wennan Zhu and Peter Kairouz and Marco Gruteser},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023algorithms.pdf:pdf},
  month = {7},
  pages = {21969--21996},
  pdf = {https://proceedings.mlr.press/v202/liu23ae/liu23ae.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Algorithms for bounding contribution for histogram estimation under user-level privacy},
  url = {https://proceedings.mlr.press/v202/liu23ae.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023generating,
  abstract = {We study the problem of efficiently generating differentially private synthetic data that approximates the statistical properties of an underlying sensitive dataset. Recent work has approached this by framing it as an optimization problem, using first-order methods to minimize a given objective function. However, such techniques are restricted to optimizing differentiable objectives only, severely limiting the types of analyses that can be conducted on the resulting synthetic data. To address this limitation, we propose using genetic algorithms to generate private synthetic data. This approach allows us to optimize non-differentiable objectives, leading to more flexible and useful synthetic data generation. We evaluate our approach on multiple datasets and tasks, demonstrating its effectiveness in generating high-quality synthetic data under differential privacy constraints.},
  author = {Terrance Liu and Jingwu Tang and Giuseppe Vietri and Steven Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023generating.pdf:pdf},
  month = {7},
  pages = {22009--22027},
  pdf = {https://proceedings.mlr.press/v202/liu23ag/liu23ag.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generating Private Synthetic Data with Genetic Algorithms},
  url = {https://proceedings.mlr.press/v202/liu23ag.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023fusionretro,
  abstract = {Retrosynthetic planning aims to devise complete multi-step synthetic routes from starting materials to target molecules. Current strategies use decoupled approaches with single-step retrosynthesis models and search algorithms, taking only the product as input while ignoring valuable context information along the synthetic route. We propose FusionRetro, a novel framework that utilizes context information for improved retrosynthetic planning by viewing synthetic routes as reaction graphs and incorporating context through three principled steps: encode molecules into embeddings, aggregate information over routes, and readout to predict reactants. This is the first attempt to utilize in-context learning for retrosynthesis prediction. The entire framework can be efficiently optimized end-to-end and produces more practical and accurate predictions. Comprehensive experiments demonstrate that by fusing in context information over routes, our model significantly improves retrosynthetic planning performance over baselines that are not context-aware, especially for long synthetic routes.},
  author = {Songtao Liu and Zhengkai Tu and Minkai Xu and Zuobai Zhang and Lu Lin and Rex Ying and Jian Tang and Peilin Zhao and Dinghao Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023fusionretro.pdf:pdf},
  month = {7},
  pages = {22028--22041},
  pdf = {https://proceedings.mlr.press/v202/liu23ah/liu23ah.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FusionRetro}: Molecule Representation Fusion via In-Context Learning for Retrosynthetic Planning},
  url = {https://proceedings.mlr.press/v202/liu23ah.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023tr0n,
  abstract = {We introduce TR0N, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models. The conditioning can be highly arbitrary, and requires only a pre-trained auxiliary model. We demonstrate how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP. TR0N learns a lightweight stochastic mapping which translates between the space of conditions and the latent space of the generative model, such that the generated latent corresponds to a data sample satisfying the desired condition. The translated latent samples are then further improved through Langevin dynamics, enabling higher-quality data samples. TR0N requires no training data nor fine-tuning, yet achieves a zero-shot FID of 10.9 on MS-COCO, outperforming competing alternatives not only on this metric but also in sampling speed while retaining much higher generality.},
  author = {Zhaoyan Liu and Noël Vouitsis and Satya Krishna Gorti and Jimmy Ba and Gabriel Loaiza-Ganem},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023tr0n.pdf:pdf},
  month = {7},
  pages = {22092--22112},
  pdf = {https://proceedings.mlr.press/v202/liu23ak/liu23ak.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TR0N}: Translator Networks for 0-Shot Plug-and-Play Conditional Generation},
  url = {https://proceedings.mlr.press/v202/liu23ak.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023i,
  abstract = {We propose Image-to-Image Schrödinger Bridge (I²SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I²SB belongs to a tractable class of Schrödinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I²SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I²SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256×256 and show that I²SB surpasses standard conditional diffusion models with more interpretable generative processes.},
  author = {Guan-Horng Liu and Arash Vahdat and De-An Huang and Evangelos A. Theodorou and Weili Nie and Anima Anandkumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023i.pdf:pdf},
  month = {7},
  pages = {22042--22062},
  pdf = {https://proceedings.mlr.press/v202/liu23ai/liu23ai.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{I}$^2${SB}: Image-to-Image {Schr{\"o}dinger} Bridge},
  url = {https://proceedings.mlr.press/v202/liu23ai.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023global,
  abstract = {We consider the problem of global optimization with noisy zeroth-order oracles — a well-motivated problem useful for various applications ranging from hyperparameter tuning for deep learning to new material design. Existing work relies on Gaussian processes or other non-parametric families, which suffer from the curse of dimensionality. We propose a new algorithm GO-UCB that leverages a parametric family of functions (e.g., neural networks) instead. Under a realizable assumption and a few other mild geometric conditions, GO-UCB achieves a cumulative regret of Õ(√T) where T is the time horizon. At the core of GO-UCB is a carefully designed uncertainty set over parameters based on gradients that allows optimistic exploration. Numerical simulation illustrates that GO-UCB works better than classical Bayesian optimization approaches in high dimensional cases, even if the model is misspecified.},
  author = {Chong Liu and Yu-Xiang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023global.pdf:pdf},
  month = {7},
  pages = {22113--22136},
  pdf = {https://proceedings.mlr.press/v202/liu23al/liu23al.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Global Optimization with Parametric Function Approximation},
  url = {https://proceedings.mlr.press/v202/liu23al.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023deja,
  abstract = {Large language models ({LLM}s) with hundreds of billions of parameters have sparked a new wave of exciting {AI} applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo {LLM}'s in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and {MLP} parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up {LLM} inference in wall-clock time without compromising {LLM}'s quality or in-context learning ability. Based on these insights, we propose {DejaVu}, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up {LLM} inference. We validate that {DejaVu} can reduce the inference latency of {OPT}-175{B} by over 2× compared to the state-of-the-art {FasterTransformer}, and over 6× compared to the widely used {Hugging Face} implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.},
  author = {Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and Beidi Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023deja.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22137--22176},
  pdf = {https://proceedings.mlr.press/v202/liu23am/liu23am.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Deja Vu}: Contextual Sparsity for Efficient {LLM}s at Inference Time},
  url = {https://proceedings.mlr.press/v202/liu23am.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023trapdoor,
  abstract = {This paper introduces a deep model watermark with an irreversible ownership verification scheme: Trapdoor Normalization ({TdN}), inspired by the trapdoor function in traditional cryptography. To protect intellectual property within deep models, the proposed method is able to embed ownership information into normalization layers during training. We argue and empirically validate that relevant methods are vulnerable to ambiguity attacks, where the forged watermarks can cast ambiguity over the ownership verification. The primary trait that distinguishes this work from previous ones, is its design of a bidirectional connection between watermarks and deep models. Thereby, {TdN} enables an irreversible ownership verification scheme that is difficult for the adversary to compromise. In this way, the proposed {TdN} can effectively defeat ambiguity attacks. Extensive experiments demonstrate that the proposed method is not only superior to previous state-of-the-art methods in robustness, but also has better efficiency.},
  author = {Hanwen Liu and Zhenyu Weng and Yuesheng Zhu and Yadong Mu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023trapdoor.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22177--22187},
  pdf = {https://proceedings.mlr.press/v202/liu23an/liu23an.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Trapdoor Normalization with Irreversible Ownership Verification},
  url = {https://proceedings.mlr.press/v202/liu23an.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023taxonomystructured,
  abstract = {Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation.},
  author = {Tianyi Liu and Zihao Xu and Hao He and Guang-Yuan Hao and Guang-He Lee and Hao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023taxonomystructured.pdf:pdf},
  mdate = {2023-12-19},
  pages = {22215--22232},
  pdf = {https://proceedings.mlr.press/v202/liu23ap/liu23ap.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Taxonomy-Structured Domain Adaptation},
  url = {https://proceedings.mlr.press/v202/liu23ap.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023retrosynthetic,
  abstract = {Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of {ML}-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning ({RL}) to improve the single-step predictor, by using a tree-shaped {MDP} to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks ({PDVN}), which alternates between the planning phase and updating phase. In {PDVN}, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely-used {USPTO} dataset, our {PDVN} algorithm improves the search success rate of existing multi-step planners (e.g., increasing the success rate from 85.79\% to 98.95\% for Retro*, and reducing the number of model calls by half while solving 99.47\% molecules for {RetroGraph}). Additionally, {PDVN} helps find shorter synthesis routes (e.g., reducing the average route length from 5.76 to 4.83 for Retro*, and from 5.63 to 4.78 for {RetroGraph}).},
  author = {Guoqing Liu and Di Xue and Shufang Xie and Yingce Xia and Austin Tripp and Krzysztof Maziarz and Marwin H. S. Segler and Tao Qin and Zongzhang Zhang and Tie-Yan Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023retrosynthetic.pdf:pdf},
  mdate = {2024-01-29},
  pages = {22266--22276},
  pdf = {https://proceedings.mlr.press/v202/liu23as/liu23as.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Retrosynthetic Planning with Dual Value Networks},
  url = {https://proceedings.mlr.press/v202/liu23as.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023same,
  abstract = {Language modeling on large-scale datasets improves performance of various downstream tasks. The validation pre-training loss is often used as the evaluation metric for language models since the pre-training loss tends to be well-correlated with downstream performance (which is itself hard to evaluate comprehensively). Contrary to the conventional wisdom, this paper shows that 1) pre-training loss cannot fully explain downstream performance and 2) flatness of the model is well-correlated with downstream performance where pre-training loss is not. We identify three ways to produce models with the same pre-training loss but different downstream performance: continue pre-training after convergence, increasing the model size, and changing the pre-training algorithms. These experiments demonstrate the existence of implicit bias of pre-training algorithms---among models with the same minimal pre-training loss, they implicitly prefer more transferable ones. Toward understanding this implicit bias, we prove that {SGD} with standard mini-batch noise implicitly prefers flatter minima of pre-training loss in language models, and empirically observe a strong correlation between flatness (measured by the trace of Hessian) and downstream performance among models with the same pre-training loss. We also prove in a synthetic language setting that among models with the minimal pre-training loss, the flattest model transfers to downstream tasks.},
  author = {Hong Liu and Sang Michael Xie and Zhiyuan Li and Tengyu Ma},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023same.pdf:pdf},
  mdate = {2024-10-06},
  note = {Oral presentation},
  pages = {22188--22214},
  pdf = {https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models},
  url = {https://proceedings.mlr.press/v202/liu23ao.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023gradientbased,
  abstract = {The output distribution of a neural network ({NN}) over the entire input space captures the complete input-output mapping relationship, offering insights toward a more comprehensive {NN} understanding. Exhaustive enumeration or traditional {Monte Carlo} methods for the entire input space can exhibit impractical sampling time, especially for high-dimensional inputs. To make such difficult sampling computationally feasible, in this paper, we propose a novel Gradient-based {Wang-Landau} ({GWL}) sampler. We first draw the connection between the output distribution of a {NN} and the density of states ({DOS}) of a physical system. Then, we renovate the classic sampler for the {DOS} problem, {Wang-Landau} algorithm, by replacing its random proposals with gradient-based {Monte Carlo} proposals. This way, our {GWL} sampler investigates the under-explored subsets of the input space much more efficiently. Extensive experiments have verified the accuracy of the output distribution generated by {GWL} and also showcased several interesting findings---for example, in a binary image classification task, both {CNN} and {ResNet} mapped the majority of human unrecognizable images to very negative logit values.},
  author = {Weitang Liu and Yi-Zhuang You and Ying-Wai Li and Jingbo Shang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023gradientbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22338--22351},
  pdf = {https://proceedings.mlr.press/v202/liu23aw/liu23aw.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient-based {Wang-Landau} Algorithm: A Novel Sampler for Output Distribution of Neural Networks over the Input Space},
  url = {https://proceedings.mlr.press/v202/liu23aw.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023active,
  abstract = {Reinforcement learning ({RL}) has made significant strides in various complex domains. However, identifying an effective policy via {RL} often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce {MAPS} and {MAPS-SE}, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, {MAPS} actively selects which of the oracles to imitate and improve their value function estimates, and {MAPS-SE} additionally leverages an active state exploration criterion to determine which states one should explore.},
  author = {Xuefeng Liu and Takuma Yoneda and Chaoqi Wang and Matthew R. Walter and Yuxin Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023active.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22320--22337},
  pdf = {https://proceedings.mlr.press/v202/liu23av/liu23av.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Active Policy Improvement from Multiple Black-box Oracles},
  url = {https://proceedings.mlr.press/v202/liu23av.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023vectormapnet,
  abstract = {Autonomous driving systems require High-Definition ({HD}) semantic maps to navigate around urban roads. Existing solutions approach the semantic mapping problem by offline manual annotation, which suffers from serious scalability issues. Recent learning-based methods produce dense rasterized segmentation predictions to construct maps. However, these predictions do not include instance information of individual map elements and require heuristic post-processing to obtain vectorized maps. To tackle these challenges, we introduce an end-to-end vectorized {HD} map learning pipeline, termed {VectorMapNet}. {VectorMapNet} takes onboard sensor observations and predicts a sparse set of polylines in the bird's-eye view. This pipeline can explicitly model the spatial relation between map elements and generate vectorized maps that are friendly to downstream autonomous driving tasks. Extensive experiments show that {VectorMapNet} achieve strong map learning performance on both {nuScenes} and {Argoverse2} dataset, surpassing previous state-of-the-art methods by 14.2 {mAP} and 14.6{mAP}. Qualitatively, {VectorMapNet} is capable of generating comprehensive maps and capturing fine-grained details of road geometry. To the best of our knowledge, {VectorMapNet} is the first work designed towards end-to-end vectorized map learning from onboard observations.},
  author = {Yicheng Liu and Tianyuan Yuan and Yue Wang and Yilun Wang and Hang Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023vectormapnet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {22352--22369},
  pdf = {https://proceedings.mlr.press/v202/liu23ax/liu23ax.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{VectorMapNet}: End-to-end Vectorized {HD} Map Learning},
  url = {https://proceedings.mlr.press/v202/liu23ax.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023online,
  abstract = {This paper studies online nonstochastic control problems with adversarial and static constraints. We propose online nonstochastic control algorithms that achieve both sublinear regret and sublinear adversarial constraint violation while keeping static constraint violation minimal against the optimal constrained linear control policy in hindsight. To establish the results, we introduce an online convex optimization with memory framework under adversarial and static constraints, which serves as a subroutine for the constrained online nonstochastic control algorithms. This subroutine also achieves the state-of-the-art regret and constraint violation bounds for constrained online convex optimization problems, which is of independent interest. Our experiments demonstrate the proposed control algorithms are adaptive to adversarial constraints and achieve smaller cumulative costs and violations. Moreover, our algorithms are less conservative and achieve significantly smaller cumulative costs than the state-of-the-art algorithm.},
  author = {Xin Liu and Zixian Yang and Lei Ying},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023online.pdf:pdf},
  mdate = {2023-10-02},
  pages = {22277--22288},
  pdf = {https://proceedings.mlr.press/v202/liu23at/liu23at.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Nonstochastic Control with Adversarial and Static Constraints},
  url = {https://proceedings.mlr.press/v202/liu23at.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023optimization,
  abstract = {Incorporating a deep generative model as the prior distribution in inverse problems has established substantial success in reconstructing images from corrupted observations. Notwithstanding, the existing optimization approaches use gradient descent largely without adapting to the non-convex nature of the problem and can be sensitive to initial values, impeding further performance improvement. In this paper, we propose an efficient amortized optimization scheme for inverse problems with a deep generative prior. Specifically, the optimization task with high degrees of difficulty is decomposed into optimizing a sequence of much easier ones. We provide a theoretical guarantee of the proposed algorithm and empirically validate it on different inverse problems.},
  author = {Tianci Liu and Tong Yang and Quan Zhang and Qi Lei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023optimization.pdf:pdf},
  mdate = {2024-02-27},
  pages = {22289--22319},
  pdf = {https://proceedings.mlr.press/v202/liu23au/liu23au.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimization for Amortized Inverse Problems},
  url = {https://proceedings.mlr.press/v202/liu23au.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023which,
  abstract = {Addresses the challenge of machine learning models' unreliability during dataset shifts. Presents a comprehensive minimax analysis from a causal perspective to answer which subset of stable information should the model transfer to achieve optimal generalization ability. Introduces a method to identify the optimal subset of stable information to transfer and demonstrates effectiveness using synthetic data and {Alzheimer's} disease diagnosis. Provides a graphical condition for optimal stable information transfer and develops an efficient algorithm with polynomial-complexity searching strategy.},
  author = {Mingzhou Liu and Xiangyu Zheng and Xinwei Sun and Fang Fang and Yizhou Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023which.pdf:pdf},
  pages = {22488--22527},
  pdf = {https://proceedings.mlr.press/v202/liu23bc/liu23bc.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which Invariance Should We Transfer? {A} Causal Minimax Learning Approach},
  url = {https://proceedings.mlr.press/v202/liu23bc.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023partially,
  abstract = {Studies multi-agent reinforcement learning in partially observable stochastic games. Proposes leveraging information-sharing among agents to address computational challenges. Develops an algorithm that is both statistically and computationally quasi-efficient and explores different information structures for improving partially observable multi-agent reinforcement learning. Establishes computational complexity results and proposes approximating shared common information.},
  author = {Xiangyu Liu and Kaiqing Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023partially.pdf:pdf},
  pages = {22370--22419},
  pdf = {https://proceedings.mlr.press/v202/liu23ay/liu23ay.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Partially Observable Multi-agent {RL} with (Quasi-)Efficiency: The Blessing of Information Sharing},
  url = {https://proceedings.mlr.press/v202/liu23ay.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023n,
  abstract = {Explores an interpretable value decomposition framework for cooperative multi-agent reinforcement learning. The Neural Attention Additive Q-learning method provides inherent intelligibility of collaboration behavior by explicitly factorizing the optimal joint policy using shape functions. Constructs identity semantics to estimate agent credits and uses local semantic masks to diagnose agent task relevance, achieving superior performance compared to state-of-the-art methods while offering human-like interpretability.},
  author = {Zichuan Liu and Yuanyang Zhu and Chunlin Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023n.pdf:pdf},
  pages = {22539--22558},
  pdf = {https://proceedings.mlr.press/v202/liu23be/liu23be.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{N$\text{A}^{\text{2}}$Q}: Neural Attention Additive Model for Interpretable Multi-Agent {Q}-Learning},
  url = {https://proceedings.mlr.press/v202/liu23be.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023prometheus,
  abstract = {Focuses on decentralized bilevel optimization in multi-agent learning, addressing decentralized stochastic bilevel optimization problems with domain constraints. Proposes the Prometheus algorithm (proximal tracked stochastic recursive estimator) achieving the first O(ε^-1) results in sample and communication complexities. Provides theoretical foundation for low sample- and communication-complexity constrained decentralized bilevel learning.},
  author = {Zhuqing Liu and Xin Zhang and Prashant Khanduri and Songtao Lu and Jia Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023prometheus.pdf:pdf},
  pages = {22420--22453},
  pdf = {https://proceedings.mlr.press/v202/liu23az/liu23az.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Prometheus: Taming Sample and Communication Complexities in Constrained Decentralized Stochastic Bilevel Learning},
  url = {https://proceedings.mlr.press/v202/liu23az.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023d2match,
  abstract = {Addresses subgraph matching, which is challenging due to its high-order combinatorial nature. Develops D2Match, which proves subgraph matching can degenerate to subtree matching and reduces matching to finding a perfect matching on a bipartite graph. Achieves linear time complexity using graph neural network mechanisms and can incorporate circle structures and node attributes, demonstrating superior performance compared to existing methods.},
  author = {Xuanzhou Liu and Lin Zhang and Jiaqi Sun and Yujiu Yang and Haiqin Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023d2match.pdf:pdf},
  pages = {22454--22472},
  pdf = {https://proceedings.mlr.press/v202/liu23ba/liu23ba.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{D2Match}: Leveraging Deep Learning and Degeneracy for Subgraph Matching},
  url = {https://proceedings.mlr.press/v202/liu23ba.html},
  volume = {202},
  year = {2023}
}

@inproceedings{liu2023unsupervised,
  abstract = {Introduces Lift, Map, Detect ({LMD}), an unsupervised out-of-distribution detection approach using diffusion models. {LMD} lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For out-of-distribution images, the mapped image would have a large distance from its original manifold, allowing detection. Demonstrates competitive performance across various datasets.},
  author = {Zhenzhen Liu and Jin Peng Zhou and Yufan Wang and Kilian Q. Weinberger},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/liu2023unsupervised.pdf:pdf},
  pages = {22528--22538},
  pdf = {https://proceedings.mlr.press/v202/liu23bd/liu23bd.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unsupervised Out-of-Distribution Detection with Diffusion Inpainting},
  url = {https://proceedings.mlr.press/v202/liu23bd.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lobel2023flipping,
  abstract = {Proposes a new method for count-based exploration in high-dimensional state spaces using coin flips to derive state visitation counts. Sets up a supervised learning objective to estimate visitation counts and demonstrates superior performance compared to existing methods, particularly on challenging exploration tasks like {Montezuma's Revenge}. The method addresses the problem of obtaining reliable pseudocounts for states in complex environments.},
  author = {Sam Lobel and Akhil Bagaria and George Konidaris},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lobel2023flipping.pdf:pdf},
  pages = {22594--22613},
  pdf = {https://proceedings.mlr.press/v202/lobel23a/lobel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Flipping Coins to Estimate Pseudocounts for Exploration in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/lobel23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{loh2023multisymmetry,
  abstract = {Introduces Multi-Symmetry Ensembles ({MSE}), a framework for creating diverse machine learning ensembles by capturing the multiplicity of hypotheses along symmetry axes beyond traditional stochastic initialization. Uses contrastive representation learning to create models that capture opposing hypotheses and proposes a method to combine these for improved performance. Demonstrates improved classification, uncertainty quantification, and generalization.},
  author = {Charlotte Loh and Seungwook Han and Shivchander Sudalairaj and Rumen Dangovski and Kai Xu and Florian Wenzel and Marin Soljacic and Akash Srivastava},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/loh2023multisymmetry.pdf:pdf},
  pages = {22614--22630},
  pdf = {https://proceedings.mlr.press/v202/loh23a/loh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries},
  url = {https://proceedings.mlr.press/v202/loh23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{longpre2023flan,
  abstract = {Studies the design decisions of publicly available instruction tuning methods, and breaks down the development of {Flan} 2022. Through careful ablation studies on the {Flan} Collection of tasks and methods, teases apart the effect of design decisions which enable {Flan-T5} to outperform prior work by 3--17\% across evaluation settings. Finds task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and training with mixed prompt settings (zero-shot, few-shot, chain-of-thought) yields equivalent or stronger (2\%) performance in all settings. Shows {Flan-T5} requires less finetuning to converge higher and faster than {T5} on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks.},
  author = {Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/longpre2023flan.pdf:pdf},
  pages = {22631--22648},
  pdf = {https://proceedings.mlr.press/v202/longpre23a/longpre23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The {Flan} Collection: Designing Data and Methods for Effective Instruction Tuning},
  url = {https://proceedings.mlr.press/v202/longpre23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{loo2023dataset,
  abstract = {Proposes a new dataset distillation algorithm using reparameterization and convexification of implicit gradients ({RCIG}), that substantially improves the state-of-the-art. Formulates dataset distillation as a bi-level optimization problem and shows how implicit gradients can be effectively used to compute meta-gradient updates. Equips the algorithm with a convexified approximation corresponding to learning on top of a frozen finite-width neural tangent kernel. Improves bias in implicit gradients by parameterizing the neural network to enable analytical computation of final-layer parameters. Establishes new state-of-the-art on diverse dataset distillation tasks with 108\% improvement on {ImageNet}, 66\% on {Tiny-ImageNet}, and 37\% on {CIFAR-100}.},
  author = {Noel Loo and Ramin M. Hasani and Mathias Lechner and Daniela Rus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/loo2023dataset.pdf:pdf},
  pages = {22649--22674},
  pdf = {https://proceedings.mlr.press/v202/loo23a/loo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dataset Distillation with Convexified Implicit Gradients},
  url = {https://proceedings.mlr.press/v202/loo23a.html},
  volume = {202},
  year = {2023}
}

@inproceedings{lou2023reflected,
  abstract = {We address limitations in score-based diffusion models by introducing Reflected Diffusion Models. The key innovation is reversing a reflected stochastic differential equation evolving on the support of the data to incorporate data constraints more principally. The approach learns a perturbed score function and extends standard diffusion model components like guidance and {ODE} sampling. Experimental results show competitive performance on image benchmarks without architectural changes. Our method is particularly effective for classifier-free guidance, producing more faithful samples under high guidance weight.},
  author = {Aaron Lou and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lou2023reflected.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22675--22701},
  pdf = {https://proceedings.mlr.press/v202/lou23a/lou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Reflected Diffusion Models}},
  url = {https://proceedings.mlr.press/v202/lou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lovell2023never,
  abstract = {We explore the uncertainty in binary classifier performance metrics. We highlight the tendency to focus narrowly on model performance metrics and demonstrate how uncertainty in metrics can overshadow empirical performance differences. We present a 3D lattice representation of confusion matrices and develop interactive visualizations of performance metric contours to raise awareness about the substantial uncertainty in performance metric estimates.},
  author = {David R. Lovell and Dimity Miller and Jaiden Capra and Andrew P. Bradley},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lovell2023never.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22702--22757},
  pdf = {https://proceedings.mlr.press/v202/lovell23a/lovell23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Never mind the metrics---what about the uncertainty? Visualising binary confusion matrix metric distributions to put performance in perspective}},
  url = {https://proceedings.mlr.press/v202/lovell23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023exploring,
  abstract = {Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular {ML} models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning baselines over a range of datasets and models. Our work highlights the critical role played by the poisoning ratio, and sheds new insights on existing empirical results, attacks and mitigation strategies in data poisoning.},
  author = {Yiwei Lu and Gautam Kamath and Yaoliang Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023exploring.pdf:pdf},
  mdate = {2024-06-02},
  month = {7},
  pages = {22856--22879},
  pdf = {https://proceedings.mlr.press/v202/lu23e/lu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks}},
  url = {https://proceedings.mlr.press/v202/lu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023bilevel,
  abstract = {Bilevel optimization has gained significant popularity in recent years due to its ability to formulate various machine learning problems. The decision variables can impact data features and outcomes, leading to the phenomenon known as performativity. We consider scenarios where the upper-level data distribution depends on the lower-level decision variables in bilevel optimization. Our theoretical analysis is corroborated through a series of numerical experiments, wherein we evaluate the performance of the bilevel performative prediction algorithms alongside non-performative counterparts in the context of meta strategic learning problems.},
  author = {Songtao Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023bilevel.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22758--22789},
  pdf = {https://proceedings.mlr.press/v202/lu23a/lu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Bilevel Optimization with Coupled Decision-Dependent Distributions}},
  url = {https://proceedings.mlr.press/v202/lu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023twoscale,
  abstract = {Finding mixed Nash equilibria of two-player zero-sum continuous games is an important and challenging problem in machine learning. The canonical approach uses noisy gradient descent ascent methods, which in the infinite particle limit creates Mean-Field Gradient Descent Ascent dynamics on probability measures. We study convergence of two-scale Mean-Field {GDA} dynamics for entropy-regularized objectives, showing that for each finite temperature, the method converges exponentially to the unique mixed Nash equilibria without assuming the convexity or concavity of the interaction potential. The proof uses new Lyapunov functions that dissipate exponentially along the Mean-Field {GDA}.},
  author = {Yulong Lu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023twoscale.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22790--22811},
  pdf = {https://proceedings.mlr.press/v202/lu23b/lu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Two-Scale Gradient Descent Ascent Dynamics Finds Mixed Nash Equilibria of Continuous Games: A Mean-Field Perspective}},
  url = {https://proceedings.mlr.press/v202/lu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023step,
  abstract = {Recent innovations on hardware (e.g., {Nvidia} {A100}) have motivated learning {N:M} structured sparsity masks from scratch for fast model inference. However, state-of-the-art learning recipes in this regime (e.g., {SR-STE}) are proposed for non-adaptive optimizers like momentum {SGD}, while incurring non-trivial accuracy drop for Adam-trained models like attention-based {LLMs}. In this paper, we first demonstrate such gap origins from poorly estimated second moment (i.e., variance) in Adam states given by the masked weights. We conjecture that learning {N:M} masks with Adam should take the critical regime of variance estimation into account. In light of this, we propose {STEP}, an Adam-aware recipe that learns {N:M} masks with two phases: first, {STEP} calculates a reliable variance estimate (precondition phase) and subsequently, the variance remains fixed and is used as a precondition to learn {N:M} masks (mask-learning phase). {STEP} automatically identifies the switching point of two phases by dynamically sampling variance changes over the training trajectory and testing the sample concentration. Empirically, we evaluate {STEP} and other baselines such as {ASP} and {SR-STE} on multiple tasks including {CIFAR} classification, machine translation and {LLM} fine-tuning ({BERT-Base}, {GPT-2}). We show {STEP} mitigates the accuracy drop of baseline recipes and is robust to aggressive structured sparsity ratios.},
  author = {Yucheng Lu and Shivani Agrawal and Suvinay Subramanian and Oleg Rybakov and Christopher De Sa and Amir Yazdanbakhsh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023step.pdf:pdf},
  mdate = {2023-08-28},
  month = {7},
  pages = {22812--22824},
  pdf = {https://proceedings.mlr.press/v202/lu23c/lu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition}},
  url = {https://proceedings.mlr.press/v202/lu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023qasbench,
  abstract = {Automatic quantum architecture search has been widely studied across disciplines with different implications. In this paper, beyond a particular domain, we formulate the quantum architecture search problem into two basic (and relatively even ideal) tasks: i) arbitrary quantum circuit regeneration given a target quantum circuit; ii) approximating an arbitrary unitary (oracle). The latter can be connected to the setting of various quantum machine learning tasks and other quantum architecture search applications. Based on these two tasks, we generate a public quantum architecture search benchmark including 900 random quantum circuits and 400 random unitary matrices which is still missing in the literature. We evaluate six baseline algorithms including brute force search, simulated annealing, genetic algorithm, reinforcement learning, hybrid algorithm, and differentiable algorithm as part of our benchmark. One characteristic of our proposed evaluation protocol on the basic tasks is that it deprives the domain-specific designs and techniques as used in existing quantum architecture search literature, making a unified evaluation possible and focusing on the vanilla search methods themselves without coupling with domain prior. In fact, the unitary approximation task could be algorithmically more difficult than the specific problems as it needs to explore the whole matrix space to fit the unitary. While specific tasks often only need to fit a partial observation of the unitary as the objective for search.},
  author = {Xudong Lu and Kaisen Pan and Ge Yan and Jiaming Shan and Wenjie Wu and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023qasbench.pdf:pdf},
  mdate = {2023-08-31},
  month = {7},
  pages = {22880--22898},
  pdf = {https://proceedings.mlr.press/v202/lu23f/lu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{QAS-Bench: Rethinking Quantum Architecture Search and A Benchmark}},
  url = {https://proceedings.mlr.press/v202/lu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023learning,
  abstract = {Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization---critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, {PSC6k}, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based {ConvNet} backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction.},
  author = {Xuanchen Lu and Xiaolong Wang and Judith E. Fan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023learning.pdf:pdf},
  mdate = {2023-10-25},
  month = {7},
  pages = {22899--22916},
  pdf = {https://proceedings.mlr.press/v202/lu23g/lu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Learning Dense Correspondences between Photos and Sketches}},
  url = {https://proceedings.mlr.press/v202/lu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lu2023federated,
  abstract = {Conformal prediction is emerging as a popular paradigm for providing rigorous uncertainty quantification in machine learning since it can be easily applied as a post-processing step to already trained models. In this paper, we extend conformal prediction to the federated learning setting. The main challenge we face is data heterogeneity across the clients---this violates the fundamental tenet of exchangeability required for conformal prediction. We propose a weaker notion of partial exchangeability, better suited to the federated learning setting, and use it to develop the Federated Conformal Prediction framework. We show {FCP} enjoys rigorous theoretical guarantees and excellent empirical performance on several computer vision and medical imaging datasets. Our results demonstrate a practical approach to incorporating meaningful uncertainty quantification in distributed and heterogeneous environments.},
  author = {Charles Lu and Yaodong Yu and Sai Praneeth Karimireddy and Michael I. Jordan and Ramesh Raskar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lu2023federated.pdf:pdf},
  mdate = {2023-11-07},
  month = {7},
  pages = {22942--22964},
  pdf = {https://proceedings.mlr.press/v202/lu23i/lu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Federated Conformal Predictors for Distributed Uncertainty Quantification}},
  url = {https://proceedings.mlr.press/v202/lu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lubana2023mechanistic,
  abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes.},
  author = {Ekdeep Singh Lubana and Eric J. Bigelow and Robert P. Dick and David Scott Krueger and Hidenori Tanaka},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lubana2023mechanistic.pdf:pdf},
  mdate = {2023-11-29},
  month = {7},
  pages = {22965--23004},
  pdf = {https://proceedings.mlr.press/v202/lubana23a/lubana23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Mechanistic Mode Connectivity}},
  url = {https://proceedings.mlr.press/v202/lubana23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lundstrm2023unifying,
  abstract = {Deep learning has revolutionized many areas of machine learning, from computer vision to natural language processing, but these high-performance models are generally "black box." Explaining such models would improve transparency and trust in {AI}-powered decision making and is necessary for understanding other practical needs such as robustness and fairness. A popular means of enhancing model transparency is to quantify how individual inputs contribute to model outputs (called attributions) and the magnitude of interactions between groups of inputs. A growing number of these methods import concepts and results from game theory to produce attributions and interactions. This work presents a unifying framework for game-theory-inspired attribution and $k$th-order interaction methods. We show that, given modest assumptions, a unique full account of interactions between features, called synergies, is possible in the continuous input setting. We identify how various methods are characterized by their policy of distributing synergies. We establish that gradient-based methods are characterized by their actions on monomials, a type of synergy function, and introduce unique gradient-based methods. We show that the combination of various criteria uniquely defines the attribution/interaction methods. Thus, the community needs to identify goals and contexts when developing and employing attribution and interaction methods.},
  author = {Daniel Lundstrm and Meisam Razaviyayn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lundstrm2023unifying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23005--23032},
  pdf = {https://proceedings.mlr.press/v202/lundstrom23a/lundstrom23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Unifying Framework to the Analysis of Interaction Methods using Synergy Functions},
  url = {https://proceedings.mlr.press/v202/lundstrom23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{luo2023segclip,
  abstract = {Recently, the contrastive language-image pre-training, e.g., {CLIP}, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a {CLIP}-based model named {SegCLIP} for the topic of open-vocabulary segmentation in an annotation-free manner. The {SegCLIP} achieves segmentation based on {ViT} and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based {KL} loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves comparable or superior segmentation accuracy on the {PASCAL} {VOC} 2012 (+0.3\% {mIoU}), {PASCAL} Context (+2.3\% {mIoU}), and {COCO} (+2.2\% {mIoU}) compared with baselines.},
  author = {Huaishao Luo and Junwei Bao 0001 and Youzheng Wu and Xiaodong He 0001 and Tianrui Li 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/luo2023segclip.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23033--23044},
  pdf = {https://proceedings.mlr.press/v202/luo23a/luo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation},
  url = {https://proceedings.mlr.press/v202/luo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{luo2023image,
  abstract = {This paper presents a stochastic differential equation ({SDE}) approach for general-purpose image restoration. The key construction consists in a mean-reverting {SDE} that transforms a high-quality image into a degraded counterpart as a mean state with fixed {G}aussian noise. Then, by simulating the corresponding reverse-time {SDE}, we are able to restore the origin of the low-quality image without relying on any task-specific prior knowledge. Crucially, the proposed mean-reverting {SDE} has a closed-form solution, allowing us to compute the ground truth time-dependent score and learn it with a neural network. Moreover, we propose a maximum likelihood objective to learn an optimal reverse trajectory that stabilizes the training and improves the restoration results.},
  author = {Ziwei Luo and Fredrik K. Gustafsson and Zheng Zhao 0004 and Jens Sjlund and Thomas B. Schn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/luo2023image.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23045--23066},
  pdf = {https://proceedings.mlr.press/v202/luo23b/luo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Image Restoration with Mean-Reverting Stochastic Differential Equations},
  url = {https://proceedings.mlr.press/v202/luo23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{luo2023dimensionality,
  abstract = {Finding the mode of a high dimensional probability distribution is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when the distribution is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known. The work significantly generalizes a result of {L}ee et al. (2021) on mode approximation for {G}aussian mixture models, developing randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized {G}aussian kernels. As in {L}ee et al.'s work, their dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy.},
  author = {Xinyu Luo and Christopher Musco and Cas Widdershoven},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/luo2023dimensionality.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23067--23082},
  pdf = {https://proceedings.mlr.press/v202/luo23c/luo23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dimensionality Reduction for General KDE Mode Finding},
  url = {https://proceedings.mlr.press/v202/luo23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{luo2023iterative,
  abstract = {Cross-validation ({CV}) is one of the most popular tools for assessing and selecting predictive models. However, standard {CV} suffers from high computational cost when the number of folds is large. Recently, under the empirical risk minimization ({ERM}) framework, a line of works proposed efficient methods to approximate {CV} based on the solution of the {ERM} problem trained on the full dataset. However, in large-scale problems, it can be hard to obtain the exact solution of the {ERM} problem, either due to limited computational resources or due to early stopping as a way of preventing overfitting. In this paper, we propose a new paradigm to efficiently approximate {CV} when the {ERM} problem is solved via an iterative first-order algorithm, without running until convergence.},
  author = {Yuetian Luo and Zhimei Ren and Rina Barber},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/luo2023iterative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23083--23102},
  pdf = {https://proceedings.mlr.press/v202/luo23d/luo23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Iterative Approximate Cross-Validation},
  url = {https://proceedings.mlr.press/v202/luo23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023closer,
  abstract = {Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions.},
  author = {Xu Luo 0003 and Hao Wu 0070 and Ji Zhang 0012 and Lianli Gao and Jing Xu and Jingkuan Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023closer.pdf:pdf},
  mdate = {2025-05-13},
  pages = {23103--23123},
  pdf = {https://proceedings.mlr.press/v202/luo23e/luo23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Closer Look at Few-shot Classification Again},
  url = {https://proceedings.mlr.press/v202/luo23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023hope,
  abstract = {Leading graph ordinary differential equation ({ODE}) models have offered generalized strategies to model interacting multi-agent dynamical systems in a data-driven approach. They typically consist of a temporal graph encoder to get the initial states and a neural {ODE}-based generative model to model the evolution of dynamical systems. However, existing methods have severe deficiencies in capacity and efficiency due to the failure to model high-order correlations in long-term temporal trends. To tackle this, in this paper, we propose a novel model named High-order graph {ODE} ({HOPE}) for learning from dynamic interaction data, which can be naturally represented as a graph. It first adopts a twin graph encoder to initialize the latent state representations of nodes and edges, which consists of two branches to capture spatio-temporal correlations in complementary manners. More importantly, our {HOPE} utilizes a second-order graph {ODE} function which models the dynamics for both nodes and edges in the latent space respectively, which enables efficient learning of long-term dependencies from complex dynamical systems. Experiment results on a variety of datasets demonstrate both the effectiveness and efficiency of our proposed method.},
  author = {Xiao Luo 0001 and Jingyang Yuan and Zijie Huang 0002 and Huiyu Jiang and Yifang Qin and Wei Ju and Ming Zhang 0004 and Yizhou Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023hope.pdf:pdf},
  mdate = {2024-02-28},
  pages = {23124--23139},
  pdf = {https://proceedings.mlr.press/v202/luo23f/luo23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {HOPE: High-order Graph ODE For Modeling Interacting Dynamics},
  url = {https://proceedings.mlr.press/v202/luo23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{luo2023stabilizing,
  abstract = {The paper examines the stability of {GAN}s from the perspective of control theory and proposes a universal higher-order noise-based controller called {B}rownian Motion Controller ({BMC}). We theoretically prove that the training process of {D}irac{GAN}s-{BMC} is globally exponentially stable. The {B}rownian motion controller offers {GAN}s global exponential stability by modeling the dynamics in the function space and providing effective methods to stabilize {GAN}s' training. We first analyze the training dynamic of a prototypical {D}irac {GAN} and adopt closed-loop control to improve its stability, then extend this approach to stabilize the training dynamic of normal {GAN}s. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.},
  author = {Tianjiao Luo and Ziyu Zhu and Jianfei Chen and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/luo2023stabilizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23140--23156},
  pdf = {https://proceedings.mlr.press/v202/luo23g/luo23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stabilizing GANs' Training with Brownian Motion Controller},
  url = {https://proceedings.mlr.press/v202/luo23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lutati2023ocd,
  abstract = {We present a dynamic model in which the weights are conditioned on an input sample $x$ and are learned to match those that would be obtained by finetuning a base model on $x$ and its label $y$. This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, {3D} reconstruction, tabular data, speech separation, and natural language processing.},
  author = {Shahar Lutati and Lior Wolf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lutati2023ocd.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23157--23169},
  pdf = {https://proceedings.mlr.press/v202/lutati23a/lutati23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {OCD: Learning to Overfit with Conditional Diffusion Models},
  url = {https://proceedings.mlr.press/v202/lutati23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lyle2023discobax,
  abstract = {The discovery of therapeutics to treat genetically-driven pathologies relies on identifying genes involved in the underlying disease mechanism. Existing approaches search over the billions of potential interventions to maximize the expected influence on the target phenotype. However, to reduce the risk of failure in future stages of trials, practical experiment design aims to find a set of interventions that maximally change a target phenotype via diverse mechanisms. We propose {D}isco{BAX} -- a sample-efficient method for maximizing the rate of significant discoveries per experiment while simultaneously probing for a wide range of diverse mechanisms during a genomic experiment campaign. We provide theoretical guarantees of optimality under standard assumptions, and conduct a comprehensive experimental evaluation covering both synthetic as well as real-world experimental design tasks. {D}isco{BAX} outperforms existing state-of-the-art methods for experimental design, selecting effective and diverse perturbations in biological systems.},
  author = {Clare Lyle and Arash Mehrjou and Pascal Notin and Andrew Jesson and Stefan Bauer and Yarin Gal and Patrick Schwab},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lyle2023discobax.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23170--23189},
  pdf = {https://proceedings.mlr.press/v202/lyle23a/lyle23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DiscoBAX: Discovery of optimal intervention sets in genomic experiment design},
  url = {https://proceedings.mlr.press/v202/lyle23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lyle2023understanding,
  abstract = {Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it often occurs in the absence of saturated units. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings on larger-scale RL benchmarks in the Arcade Learning Environment.},
  author = {Clare Lyle and Zeyu Zheng and Evgenii Nikishin and Bernardo {\'A}vila Pires and Razvan Pascanu and Will Dabney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lyle2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23190--23211},
  pdf = {https://proceedings.mlr.press/v202/lyle23b/lyle23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Plasticity in Neural Networks},
  url = {https://proceedings.mlr.press/v202/lyle23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lyu2023bandits,
  abstract = {We consider a non-stationary Bandits with Knapsack problem. The outcome distribution at each time is scaled by a non-stationary quantity that signifies changing demand volumes. Instead of studying settings with limited non-stationarity, we investigate how online predictions on the total demand volume Q allows us to improve our performance guarantees. We show that, without any prediction, any online algorithm incurs a linear-in-T regret. In contrast, with online predictions on Q, we propose an online algorithm that judiciously incorporates the predictions, and achieve regret bounds that depends on the accuracy of the predictions. These bounds are shown to be tight in settings when prediction accuracy improves across time. Our theoretical results are corroborated by our numerical findings.},
  author = {Lixing Lyu and Wang Chi Cheung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lyu2023bandits.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23212--23238},
  pdf = {https://proceedings.mlr.press/v202/lyu23a/lyu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bandits with Knapsacks: Advice on Time-Varying Demands},
  url = {https://proceedings.mlr.press/v202/lyu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lyu2023pairwise,
  abstract = {We study the design of loss functions for click-through rates (CTR) to optimize (social) welfare in advertising auctions. Existing works either only focus on CTR predictions without consideration of business objectives (e.g., welfare) in auctions or assume that the distribution over the participants' expected cost-per-impression (eCPM) is known a priori, then use various additional assumptions on the parametric form of the distribution to derive loss functions for predicting CTRs. In this work, we bring back the welfare objectives of ad auctions into CTR predictions and propose a novel weighted rankloss to train the CTR model. Compared to existing literature, our approach provides a provable guarantee on welfare but without assumptions on the eCPMs' distribution while also avoiding the intractability of naively applying existing learning-to-rank methods. Further, we propose a theoretically justifiable technique for calibrating the losses using labels generated from a teacher network, only assuming that the teacher network has bounded $\ell_2$ generalization error.},
  author = {Boxiang Lyu and Zhe Feng and Zachary Robertson and Sanmi Koyejo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lyu2023pairwise.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23239--23263},
  pdf = {https://proceedings.mlr.press/v202/lyu23b/lyu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pairwise Ranking Losses of Click-Through Rates Prediction for Welfare Maximization in Ad Auctions},
  url = {https://proceedings.mlr.press/v202/lyu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{lyzhin2023which,
  abstract = {Nowadays, state-of-the-art learning-to-rank methods are based on gradient-boosted decision trees (GBDT). The most well-known algorithm is LambdaMART which was proposed more than a decade ago. Recently, several other GBDT-based ranking algorithms were proposed. In this paper, we thoroughly analyze these methods in a unified setup. In particular, we address the following questions. Is direct optimization of a smoothed ranking loss preferable over optimizing a convex surrogate? How to properly construct and smooth surrogate ranking losses? To address these questions, we compare LambdaMART with YetiRank and StochasticRank methods and their modifications. We also propose a simple improvement of the YetiRank approach that allows for optimizing specific ranking loss functions. As a result, we gain insights into learning-to-rank techniques and obtain a new state-of-the-art algorithm.},
  author = {Ivan Lyzhin and Aleksei Ustimenko and Andrey Gulin and Liudmila Prokhorenkova},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/lyzhin2023which.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23264--23278},
  pdf = {https://proceedings.mlr.press/v202/lyzhin23a/lyzhin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which Tricks are Important for Learning to Rank?},
  url = {https://proceedings.mlr.press/v202/lyzhin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023graph,
  abstract = {Transformers for graph data are increasingly widely studied and successful in numerous learning tasks. Graph inductive biases are crucial for Graph Transformers, and previous works incorporate them using message-passing modules and/or positional encodings. However, Graph Transformers that use message-passing inherit known issues of message-passing, and differ significantly from Transformers used in other domains, thus making transfer of research advances more difficult. On the other hand, Graph Transformers without message-passing often perform poorly on smaller datasets, where inductive biases are more crucial. To bridge this gap, we propose the Graph Inductive bias Transformer (GRIT) -- a new Graph Transformer that incorporates graph inductive biases without using message passing. GRIT is based on several architectural changes that are each theoretically and empirically justified, including: learned relative positional encodings initialized with random walk probabilities, a flexible attention mechanism that updates node and node-pair representations, and injection of degree information in each layer. We prove that GRIT is expressive -- it can express shortest path distances and various graph propagation matrices. GRIT achieves state-of-the-art empirical performance across a variety of graph datasets, thus showing the power that Graph Transformers without message-passing can deliver.},
  author = {Liheng Ma and Chen Lin and Derek Lim and Adriana Romero-Soriano and Puneet K. Dokania and Mark Coates and Philip H. S. Torr and Ser-Nam Lim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023graph.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23321--23337},
  pdf = {https://proceedings.mlr.press/v202/ma23c/ma23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Inductive Biases in Transformers without Message Passing},
  url = {https://proceedings.mlr.press/v202/ma23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023liv,
  abstract = {We present Language-Image Value learning (LIV), a unified objective for vision-language representation and reward learning from action-free videos with text annotations. Exploiting a novel connection between dual reinforcement learning and mutual information contrastive learning, the LIV objective trains a multi-modal representation that implicitly encodes a universal value function for tasks specified as language or image goals. We use LIV to pre-train the first control-centric vision-language representation from large human video datasets such as EpicKitchen. Given only a language or image goal, the pre-trained LIV model can assign dense rewards to each frame in videos of unseen robots or humans attempting that task in unseen environments. Further, when some target domain-specific data is available, the same objective can be used to fine-tune and improve LIV and even other pre-trained representations for robotic control and reward specification in that domain. In our experiments on several simulated and real-world robot environments, LIV models consistently outperform the best prior input state representations for imitation learning, as well as reward specification methods for policy synthesis. Our results validate the advantages of joint vision-language representation and reward learning within the unified, compact LIV framework.},
  author = {Yecheng Jason Ma and William Liang and Vaidehi Som and Vikash Kumar and Amy Zhang and Osbert Bastani and Dinesh Jayaraman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023liv.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23301--23320},
  pdf = {https://proceedings.mlr.press/v202/ma23b/ma23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LIV: Language-Image Representations and Rewards for Robotic Control},
  url = {https://proceedings.mlr.press/v202/ma23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023learning,
  abstract = {Learning signed distance functions (SDFs) from 3D point clouds is an important task in 3D computer vision. However, without ground truth signed distances, point normals or clean point clouds, current methods still struggle from learning SDFs from noisy point clouds. To overcome this challenge, we propose to learn SDFs via a noise to noise mapping, which does not require any clean point cloud or ground truth supervision for training. Our novelty lies in the noise to noise mapping which can infer a highly accurate SDF of a single object or scene from its multiple or even single noisy point cloud observations. Our novel learning manner is supported by modern Lidar systems which capture multiple noisy observations per second. We achieve this by a novel loss which enables statistical reasoning on point clouds and maintains geometric consistency although point clouds are irregular, unordered and have no point correspondence. Our evaluation under the widely used benchmarks demonstrates our superiority over the state-of-the-art methods in surface reconstruction, point cloud denoising and upsampling.},
  author = {Baorui Ma and Yu-Shen Liu and Zhizhong Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23338--23357},
  pdf = {https://proceedings.mlr.press/v202/ma23d/ma23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Signed Distance Functions from Noisy 3D Point Clouds via Noise to Noise Mapping},
  url = {https://proceedings.mlr.press/v202/ma23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023learning,
  abstract = {An unaddressed challenge in multi-agent coordination is to enable AI agents to exploit the semantic relationships between the features of actions and the features of observations. Humans take advantage of these relationships in highly intuitive ways. For instance, in the absence of a shared language, we might point to the object we desire or hold up our fingers to indicate how many objects we want. To address this challenge, we investigate the effect of network architecture on the propensity of learning algorithms to exploit these semantic relationships. Across a procedurally generated coordination task, we find that attention-based architectures that jointly process a featurized representation of observations and actions have a better inductive bias for learning intuitive policies.},
  author = {Mingwei Ma and Jizhou Liu and Samuel Sokota and Max Kleiman-Weiner and Jakob Nicolaus Foerster},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23358--23372},
  pdf = {https://proceedings.mlr.press/v202/ma23e/ma23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Intuitive Policies Using Action Features},
  url = {https://proceedings.mlr.press/v202/ma23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023overparametrization,
  abstract = {This paper studies the role of over-parametrization in solving non-convex optimization problems. The focus is on the important class of low-rank matrix sensing, where we propose an infinite hierarchy of non-convex problems via the lifting technique and the Burer-Monteiro factorization. This contrasts with the existing over-parametrization technique where the search rank is limited by the dimension of the matrix and it does not allow a rich over-parametrization of an arbitrary degree. We show that although the spurious solutions of the problem remain stationary points through the hierarchy, they will be transformed into strict saddle points (under some technical conditions) and can be escaped via local search methods. This is the first result in the literature showing that over-parametrization creates a negative curvature for escaping spurious solutions.},
  author = {Ziye Ma and Igor Molybog and Javad Lavaei and Somayeh Sojoudi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023overparametrization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23373--23387},
  pdf = {https://proceedings.mlr.press/v202/ma23f/ma23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Over-parametrization via Lifting for Low-rank Matrix Sensing: Conversion of Spurious Solutions to Strict Saddle Points},
  url = {https://proceedings.mlr.press/v202/ma23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023buying,
  abstract = {Stochastic optimization is one of the central problems in Machine Learning and Theoretical Computer Science. In the standard model, the algorithm is given a fixed distribution known in advance. In practice though, one may acquire at a cost extra information to make better decisions. In this paper, we study how to buy information for stochastic optimization and formulate this question as an online learning problem. Assuming the learner has an oracle for the original optimization problem, we design a $2$-competitive deterministic algorithm and a $e/(e-1)$-competitive randomized algorithm for buying information. We show that this ratio is tight as the problem is equivalent to a robust generalization of the ski-rental problem, which we call super-martingale stopping.},
  author = {Mingchen Ma and Christos Tzamos},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023buying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23388--23411},
  pdf = {https://proceedings.mlr.press/v202/ma23g/ma23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Buying Information for Stochastic Optimization},
  url = {https://proceedings.mlr.press/v202/ma23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023generated,
  abstract = {Graph generative models become increasingly effective for data distribution approximation and data augmentation. While they have aroused public concerns about their malicious misuses or misinformation broadcasts, just as what Deepfake visual and auditory media has been delivering to society. Hence it is essential to regulate the prevalence of generated graphs. To tackle this problem, we pioneer the formulation of the generated graph detection problem to distinguish generated graphs from real ones. We propose the first framework to systematically investigate a set of sophisticated models and their performance in four classification scenarios. Each scenario switches between seen and unseen datasets/generators during testing to get closer to real-world settings and progressively challenge the classifiers.},
  author = {Yihan Ma and Zhikun Zhang and Ning Yu and Xinlei He and Michael Backes and Yun Shen and Yang Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023generated.pdf:pdf},
  mdate = {2025-05-08},
  pages = {23412--23428},
  pdf = {https://proceedings.mlr.press/v202/ma23h/ma23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generated Graph Detection},
  url = {https://proceedings.mlr.press/v202/ma23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ma2023calibrating,
  abstract = {Multimodal machine learning has achieved remarkable progress in a wide range of scenarios. However, the reliability of multimodal learning remains largely unexplored. In this paper, through extensive empirical studies, we identify current multimodal classification methods suffer from unreliable predictive confidence that tend to rely on partial modalities when estimating confidence. Specifically, we find that the confidence estimated by current models could even increase when some modalities are corrupted. To address the issue, we introduce an intuitive principle for multimodal learning, i.e., the confidence should not increase when one modality is removed. Accordingly, we propose a novel regularization technique, i.e., Calibrating Multimodal Learning (CML) regularization, to calibrate the predictive confidence of previous methods.},
  author = {Huan Ma and Qingyang Zhang and Changqing Zhang and Bingzhe Wu and Huazhu Fu and Joey Tianyi Zhou and Qinghua Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ma2023calibrating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23429--23450},
  pdf = {https://proceedings.mlr.press/v202/ma23i/ma23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Calibrating Multimodal Learning},
  url = {https://proceedings.mlr.press/v202/ma23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{maalouf2023autocoreset,
  abstract = {A coreset is a small weighted subset of an input set that approximates its loss function, for a given set of queries. Coresets became prevalent in machine learning as they have shown to be advantageous for many applications. Unfortunately, coresets are constructed in a problem-dependent manner, where for each problem, a new coreset construction algorithm is suggested, taking years to prove its correctness. Even the generic frameworks require additional (problem-dependent) computations or proofs to be done by the user. Besides, many problems do not have (provable) small coresets, limiting their applicability. To this end, we suggest an automatic practical framework for constructing coresets, which requires (only) the input data and the desired cost function from the user, without the need for any other task-related computation to be done by the user.},
  author = {Alaa Maalouf and Murad Tukan and Vladimir Braverman and Daniela Rus},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/maalouf2023autocoreset.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23451--23466},
  pdf = {https://proceedings.mlr.press/v202/maalouf23a/maalouf23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{AutoCoreset}: An Automatic Practical Coreset Construction Framework},
  url = {https://proceedings.mlr.press/v202/maalouf23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{madan2023learning,
  abstract = {Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. We introduce subtrajectory balance or SubTB(λ), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB(λ) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before.},
  author = {Kanika Madan and Jarrid Rector-Brooks and Maksym Korablyov and Emmanuel Bengio and Moksh Jain and Andrei Cristian Nica and Tom Bosc and Yoshua Bengio and Nikolay Malkin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/madan2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23467--23483},
  pdf = {https://proceedings.mlr.press/v202/madan23a/madan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning {GFlowNets} From Partial Episodes For Improved Convergence And Stability},
  url = {https://proceedings.mlr.press/v202/madan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{maghakian2023applied,
  abstract = {For many application domains, the integration of machine learning (ML) models into decision making is hindered by the poor explainability and theoretical guarantees of black box models. Although the emerging area of algorithms with predictions offers a way to leverage ML while enjoying worst-case guarantees, existing work usually assumes access to only one predictor. We demonstrate how to more effectively utilize historical datasets and application domain knowledge by intentionally using predictors of different quantities. By leveraging the heterogeneity in our predictors, we are able to achieve improved performance, explainability and computational efficiency over predictor-agnostic methods.},
  author = {Jessica Maghakian and Russell Lee and Mohammad Hajiesmaili and Jian Li and Ramesh K. Sitaraman and Zhenhua Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/maghakian2023applied.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23484--23497},
  pdf = {https://proceedings.mlr.press/v202/maghakian23a/maghakian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Applied Online Algorithms with Heterogeneous Predictors},
  url = {https://proceedings.mlr.press/v202/maghakian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mai2023csp,
  abstract = {Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged images. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on both iNat2018 and fMoW datasets. Especially, on iNat2018, CSP significantly boosts the model performance with 10-34\% relative improvement with various labeled training data sampling ratios.},
  author = {Gengchen Mai and Ni Lao and Yutong He and Jiaming Song and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mai2023csp.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23498--23515},
  pdf = {https://proceedings.mlr.press/v202/mai23a/mai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CSP}: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations},
  url = {https://proceedings.mlr.press/v202/mai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mai2023vertical,
  abstract = {Conventional recommender systems require training on a centralized database, but due to data privacy concerns, this is often impractical when multi-parties are involved. This study proposes the first vertical federated graph neural network-based recommender system, called VerFedGNN. The framework transmits the summation of neighbor embeddings using random projection and gradients of public parameter perturbed by ternary quantization mechanism. Empirical studies show that VerFedGNN has competitive prediction accuracy with existing privacy-preserving GNN frameworks while providing enhanced privacy protection for users' interaction information.},
  author = {Peihua Mai and Yan Pang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mai2023vertical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23516--23535},
  pdf = {https://proceedings.mlr.press/v202/mai23b/mai23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Vertical Federated Graph Neural Network for Recommender System},
  url = {https://proceedings.mlr.press/v202/mai23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{maini2023can,
  abstract = {Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks memorize ``hard'' examples in the final few layers of the model. Memorization refers to the ability to correctly predict on atypical examples of the training set. However, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. We find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers.},
  author = {Pratyush Maini and Michael Curtis Mozer and Hanie Sedghi and Zachary Chase Lipton and J. Zico Kolter and Chiyuan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/maini2023can.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23536--23557},
  pdf = {https://proceedings.mlr.press/v202/maini23a/maini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Can Neural Network Memorization Be Localized?},
  url = {https://proceedings.mlr.press/v202/maini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{majumdar2023fundamental,
  abstract = {This paper seeks to understand fundamental tradeoffs between the accuracy of prior information that a learner has on a given problem and its learning performance. We introduce the notion of prioritized risk, which differs from traditional notions of minimax and Bayes risk by allowing us to study such fundamental tradeoffs in settings where reality does not necessarily conform to the learner's prior. Our main technical contributions include: (1) a general reduction-based approach for extending classical minimax lower-bound techniques in order to lower bound the prioritized risk for statistical estimation problems; (2) a novel generalization of Fano's inequality for lower bounding the prioritized risk in more general settings involving unbounded losses; and (3) applications demonstrating insights into tradeoffs between prior information and learning performance for problems in estimation, regression, and reinforcement learning.},
  author = {Anirudha Majumdar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/majumdar2023fundamental.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23558--23573},
  pdf = {https://proceedings.mlr.press/v202/majumdar23a/majumdar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fundamental Tradeoffs in Learning with Prior Information},
  url = {https://proceedings.mlr.press/v202/majumdar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{malek2023additive,
  abstract = {We explore algorithms to select actions in the causal bandit setting where the learner can choose to intervene on a set of random variables related by a causal graph, and the learner sequentially chooses interventions and observes a sample from the interventional distribution. The learner's goal is to quickly find the intervention, among all interventions on observable variables, that maximizes the expectation of an outcome variable. We depart from previous literature by assuming no knowledge of the causal graph except that latent confounders between the outcome and its ancestors are not present. We first show that the unknown graph problem can be exponentially hard in the parents of the outcome. We adopt an additive assumption to address this challenge and propose a novel action-elimination algorithm for this setting. We show how to apply this algorithm to the causal bandit problem, provide sample complexity bounds, and empirically validate our findings on a suite of randomly generated causal models.},
  author = {Alan Malek and Virginia Aglietti and Silvia Chiappa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/malek2023additive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23574--23589},
  pdf = {https://proceedings.mlr.press/v202/malek23a/malek23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Additive Causal Bandits with Unknown Graph},
  url = {https://proceedings.mlr.press/v202/malek23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{malik2023weighted,
  abstract = {In recommender system or crowdsourcing applications of online learning, a human's preferences or abilities are often a function of the algorithm's recent actions. Motivated by this, a significant line of work has formalized settings where an action's loss is a function of the number of times that action was recently played in the prior $m$ timesteps, where $m$ corresponds to a bound on human memory capacity. To more faithfully capture decay of human memory with time, we introduce the Weighted Tallying Bandit (WTB), which generalizes this setting by requiring that an action's loss is a function of a weighted summation of the number of times that arm was played in the last $m$ timesteps. This WTB setting is intractable without further assumption. So we study it under Repeated Exposure Optimality (REO), a condition motivated by the literature on human physiology, which requires the existence of an action that when repetitively played will eventually yield smaller loss than any other action sequence.},
  author = {Dhruv Malik and Conor Igoe and Yuanzhi Li and Aarti Singh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/malik2023weighted.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23590--23609},
  pdf = {https://proceedings.mlr.press/v202/malik23a/malik23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Weighted Tallying Bandits: Overcoming Intractability via Repeated Exposure Optimality},
  url = {https://proceedings.mlr.press/v202/malik23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{malladi2023kernelbased,
  abstract = {It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks. We extend the NTK formalism to Adam and use Tensor Programs to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models.},
  author = {Sadhika Malladi and Alexander Wettig and Dingli Yu and Danqi Chen and Sanjeev Arora},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/malladi2023kernelbased.pdf:pdf},
  mdate = {2023-12-14},
  pages = {23610--23641},
  pdf = {https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Kernel-Based View of Language Model Fine-Tuning},
  url = {https://proceedings.mlr.press/v202/malladi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mandal2023performative,
  abstract = {We introduce the framework of performative reinforcement learning where the policy chosen by the learner affects the underlying reward and transition dynamics of the environment. Following the recent literature on performative prediction, we introduce the concept of performatively stable policy. We then consider a regularized version of the reinforcement learning problem and show that repeatedly optimizing this objective converges to a performatively stable policy under reasonable assumptions on the transition dynamics. Our proof utilizes the dual perspective of the reinforcement learning problem and may be of independent interest in analyzing the convergence of other algorithms with decision-dependent environments. We then extend our results for the setting where the learner just performs gradient ascent steps instead of fully optimizing the objective, and for the setting where the learner has access to a finite number of trajectories from the changed environment. For both the settings, we leverage the dual formulation of performative reinforcement learning, and establish convergence to a stable solution. Finally, through extensive experiments on a grid-world environment, we demonstrate the dependence of convergence on various parameters e.g. regularization, smoothness, and the number of samples.},
  author = {Debmalya Mandal and Stelios Triantafyllou and Goran Radanovic},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mandal2023performative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23642--23680},
  pdf = {https://proceedings.mlr.press/v202/mandal23a/mandal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Performative Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/mandal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mangold2023differential,
  abstract = {We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. We use this Lipschitz property to prove a non-asymptotic bound showing that, as the number of samples increases, the fairness level of private models gets closer to the one of their non-private counterparts. This bound also highlights the importance of the confidence margin of a model on the disparate impact of differential privacy.},
  author = {Paul Mangold and Michal Perrot and Aurélien Bellet and Marc Tommasi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mangold2023differential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23681--23705},
  pdf = {https://proceedings.mlr.press/v202/mangold23a/mangold23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differential Privacy has Bounded Impact on Fairness in Classification},
  url = {https://proceedings.mlr.press/v202/mangold23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mansour2023random,
  abstract = {A landmark negative result of Long and Servedio has had a considerable impact on research and development in boosting algorithms, around the now famous tagline that "noise defeats all convex boosters". In this paper, we appeal to the half-century+ founding theory of losses for class probability estimation, an extension of Long and Servedio's results and a new general convex booster to demonstrate that the source of their negative result is in fact the model class, linear separators. Losses or algorithms are neither to blame. This leads us to a discussion on an otherwise praised aspect of ML, parameterisation.},
  author = {Yishay Mansour and Richard Nock and Robert C. Williamson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mansour2023random.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23706--23742},
  pdf = {https://proceedings.mlr.press/v202/mansour23a/mansour23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Classification Noise does not defeat All Convex Potential Boosters Irrespective of Model Choice},
  url = {https://proceedings.mlr.press/v202/mansour23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mao2023hconsistency,
  abstract = {We present a detailed study of H-consistency bounds for score-based ranking. These are upper bounds on the target loss estimation error of a predictor in a hypothesis set H, expressed in terms of the surrogate loss estimation error of that predictor. We will show that both in the general pairwise ranking scenario and in the bipartite ranking scenario, there are no meaningful H-consistency bounds for most hypothesis sets used in practice including the family of linear models and that of the neural networks, which satisfy the equicontinuous property with respect to the input. To come up with ranking surrogate losses with theoretical guarantees, we show that a natural solution consists of resorting to a pairwise abstention loss in the general pairwise ranking scenario, and similarly, a bipartite abstention loss in the bipartite ranking scenario, to abstain from making predictions at some limited cost c. For surrogate losses of these abstention loss functions, we give a series of H-consistency bounds for both the family of linear functions and that of neural networks with one hidden-layer. Our experimental results illustrate the effectiveness of ranking with abstention.},
  author = {Anqi Mao and Mehryar Mohri and Yutao Zhong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mao2023hconsistency.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23743--23802},
  pdf = {https://proceedings.mlr.press/v202/mao23a/mao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {H-Consistency Bounds for Pairwise Misranking Loss Surrogates},
  url = {https://proceedings.mlr.press/v202/mao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mao2023crossentropy,
  abstract = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses.},
  author = {Anqi Mao and Mehryar Mohri and Yutao Zhong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mao2023crossentropy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23803--23828},
  pdf = {https://proceedings.mlr.press/v202/mao23b/mao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cross-Entropy Loss Functions: Theoretical Analysis and Applications},
  url = {https://proceedings.mlr.press/v202/mao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mao2023supported,
  abstract = {Offline reinforcement learning suffers from the out-of-distribution issue and extrapolation error. Most policy constraint methods regularize the density of the trained policy towards the behavior policy, which is too restrictive in most cases. We propose Supported Trust Region optimization (STR) which performs trust region policy optimization with the policy constrained within the support of the behavior policy, enjoying the less restrictive support constraint. We show that, when assuming no approximation and sampling error, STR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Further with both errors incorporated, STR still guarantees safe policy improvement for each step.},
  author = {Yixiu Mao and Hongchang Zhang and Chen Chen and Yi Xu and Xiangyang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mao2023supported.pdf:pdf},
  mdate = {2024-08-26},
  pages = {23829--23851},
  pdf = {https://proceedings.mlr.press/v202/mao23c/mao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Supported Trust Region Optimization for Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/mao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mao2023robust,
  abstract = {Deep networks for computer vision are not reliable when they encounter adversarial examples. In this paper, we introduce a framework that uses the dense intrinsic constraints in natural images to robustify inference. By introducing constraints at inference time, we can shift the burden of robustness from training to testing, thereby allowing the model to dynamically adjust to each individual image's unique and potentially novel characteristics at inference time. Our theoretical results show the importance of having dense constraints at inference time. In contrast to existing single-constraint methods, we propose to use equivariance, which naturally allows dense constraints at a fine-grained level in the feature space. Our empirical experiments show that restoring feature equivariance at inference time defends against worst-case adversarial perturbations.},
  author = {Chengzhi Mao and Lingyu Zhang and Abhishek Vaibhav Joshi and Junfeng Yang and Hao Wang and Carl Vondrick},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mao2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23852--23870},
  pdf = {https://proceedings.mlr.press/v202/mao23d/mao23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Perception through Equivariance},
  url = {https://proceedings.mlr.press/v202/mao23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marbut2023reliable,
  abstract = {Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.},
  author = {Anna C. Marbut and Katy McKinney-Bock and Travis J. Wheeler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marbut2023reliable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23871--23885},
  pdf = {https://proceedings.mlr.press/v202/marbut23a/marbut23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reliable Measures of Spread in High Dimensional Latent Spaces},
  url = {https://proceedings.mlr.press/v202/marbut23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marchand2023sratta,
  abstract = {We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.},
  author = {Tanguy Marchand and Regis Loeb and Ulysse Marteau-Ferey and Jean Ogier du Terrail and Arthur Pignet},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marchand2023sratta.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23886--23914},
  pdf = {https://proceedings.mlr.press/v202/marchand23a/marchand23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SRATTA}: Sample {Re-ATTribution} Attack of Secure Aggregation in Federated Learning},
  url = {https://proceedings.mlr.press/v202/marchand23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marconato2023neurosymbolic,
  abstract = {We introduce Neuro-Symbolic Continual Learning, where a model has to solve a sequence of neuro-symbolic tasks, that is, it has to map sub-symbolic inputs to high-level concepts and compute predictions by reasoning consistently with prior knowledge. Our key observation is that neuro-symbolic tasks, although different, often share concepts whose semantics remains stable over time. Traditional approaches fall short: existing continual strategies ignore knowledge altogether, while stock neuro-symbolic architectures suffer from catastrophic forgetting. We show that leveraging prior knowledge by combining neuro-symbolic architectures with continual strategies does help avoid catastrophic forgetting, but also that doing so can yield models affected by reasoning shortcuts. These undermine the semantics of the acquired concepts, even when detailed prior knowledge is provided upfront and inference is exact, and in turn continual performance. To overcome these issues, we introduce cool, a COncept-level cOntinual Learning strategy tailored for neuro-symbolic continual problems that acquires high-quality concepts and remembers them over time.},
  author = {Emanuele Marconato and Gianpaolo Bontempo and Elisa Ficarra and Simone Calderara and Andrea Passerini and Stefano Teso},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marconato2023neurosymbolic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23915--23936},
  pdf = {https://proceedings.mlr.press/v202/marconato23a/marconato23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neuro-Symbolic Continual Learning: Knowledge, Reasoning Shortcuts and Concept Rehearsal},
  url = {https://proceedings.mlr.press/v202/marconato23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marcosmorales2023evaluating,
  abstract = {Unsupervised denoising is a crucial challenge in real-world imaging applications. Unsupervised deep-learning methods have demonstrated impressive performance on benchmarks based on synthetic noise. However, no metrics exist to evaluate these methods in an unsupervised fashion. This is highly problematic for the many practical applications where ground-truth clean images are not available. In this work, we propose two novel metrics: the unsupervised mean squared error (MSE) and the unsupervised peak signal-to-noise ratio (PSNR), which are computed using only noisy data. We provide a theoretical analysis of these metrics, showing that they are asymptotically consistent estimators of the supervised MSE and PSNR.},
  author = {Adria Marcos-Morales and Matan Leibovich and Sreyas Mohan and Joshua Lawrence Vincent and Piyush Haluai and Mai Tan and Peter A. Crozier and Carlos Fernandez-Granda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marcosmorales2023evaluating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23937--23957},
  pdf = {https://proceedings.mlr.press/v202/marcos-morales23a/marcos-morales23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Evaluating Unsupervised Denoising Requires Unsupervised Metrics},
  url = {https://proceedings.mlr.press/v202/marcos-morales23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{e2023regions,
  abstract = {Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules - functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. We provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the region of reliability of a scoring rule - the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions. The results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as commonly performed in the literature.},
  author = {{\'E}tienne Marcotte and Valentina Zantedeschi and Alexandre Drouin and Nicolas Chapados},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/e2023regions.pdf:pdf},
  mdate = {2023-08-28},
  pages = {23958--24004},
  pdf = {https://proceedings.mlr.press/v202/marcotte23a/marcotte23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts},
  url = {https://proceedings.mlr.press/v202/marcotte23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marjieh2023analyzing,
  abstract = {Diffusion models are a class of generative models that learn to synthesize samples by inverting a diffusion process that gradually maps data into noise. While these models have enjoyed great success recently, a full theoretical understanding of their observed properties is still lacking, in particular, their weak sensitivity to the choice of noise family and the role of adequate scheduling of noise levels for good synthesis. By identifying a correspondence between diffusion models and a well-known paradigm in cognitive science known as serial reproduction, whereby human agents iteratively observe and reproduce stimuli from memory, we show how the aforementioned properties of diffusion models can be explained as a natural consequence of this correspondence.},
  author = {Raja Marjieh and Ilia Sucholutsky and Thomas A. Langlois and Nori Jacoby and Thomas L. Griffiths},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marjieh2023analyzing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24005--24019},
  pdf = {https://proceedings.mlr.press/v202/marjieh23a/marjieh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Analyzing Diffusion as Serial Reproduction},
  url = {https://proceedings.mlr.press/v202/marjieh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{markov2023quantized,
  abstract = {Communication-reduction techniques are a popular way to improve scalability in data-parallel training of deep neural networks. The recent emergence of large language models such as GPT has created the need for new approaches to exploit data-parallelism. Among these, fully-sharded data parallel (FSDP) training is highly popular, yet it still encounters scalability bottlenecks. One reason is that applying compression techniques to FSDP is challenging: as the vast majority of the communication involves the model's weights, direct compression alters convergence and leads to accuracy loss. We present QSDP, a variant of FSDP which supports both gradient and weight quantization with theoretical guarantees, is simple to implement and has essentially no overheads. Experiments show that QSDP preserves model accuracy, while completely removing the communication bottlenecks of FSDP, providing end-to-end speedups of up to 2.2x.},
  author = {Ilia Markov and Adrian Vladu and Qi Guo and Dan Alistarh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/markov2023quantized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24020--24044},
  pdf = {https://proceedings.mlr.press/v202/markov23a/markov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantized Distributed Training of Large Models with Convergence Guarantees},
  url = {https://proceedings.mlr.press/v202/markov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{maron2023efficient,
  abstract = {We introduce the Efficient Transformed Gaussian Process (ETGP), which creates C stochastic processes with three key characteristics: 1) the C processes are non-stationary, 2) the C processes are dependent by construction without needing a mixing matrix, 3) training and making predictions is very efficient since the number of Gaussian Process operations do not depend on the number of processes. This makes the ETGP particularly suited for multi-class problems with a very large number of classes. ETGP exploits the recently proposed Transformed Gaussian Process (TGP), which is a stochastic process specified by transforming a Gaussian Process using an invertible transformation.},
  author = {Juan Maro{\~n}as and Daniel Hern{\'a}ndez-Lobato},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/maron2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24045--24081},
  pdf = {https://proceedings.mlr.press/v202/maronas23a/maronas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Transformed {G}aussian Processes for Non-Stationary Dependent Multi-class Classification},
  url = {https://proceedings.mlr.press/v202/maronas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marro2023computational,
  abstract = {We prove that while attacking ReLU classifiers is NP-hard, ensuring their robustness at training time is Σ²P-hard (even on a single example). This asymmetry provides a rationale for the fact that robust classification approaches are frequently fooled in the literature. We show that inference-time robustness certificates are not affected by this asymmetry, by introducing a proof-of-concept approach named Counter-Attack (CA). CA displays a reversed asymmetry: running the defense is NP-hard, while attacking it is Σ₂P-hard. We argue that adversarial attacks can be used in the context of robustness certification, and provide an empirical evaluation of their effectiveness.},
  author = {Samuele Marro and Michele Lombardi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marro2023computational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24082--24138},
  pdf = {https://proceedings.mlr.press/v202/marro23a/marro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Computational Asymmetries in Robust Classification},
  url = {https://proceedings.mlr.press/v202/marro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{marwah2023neural,
  abstract = {We take a step towards studying the representational power of neural networks for approximating solutions to nonlinear PDEs. We focus on a class of PDEs known as nonlinear elliptic variational PDEs, whose solutions minimize an Euler-Lagrange energy functional. The main contribution is to prove that neural networks can approximate the solutions to such PDEs to arbitrarily high accuracy, with explicit characterization of the approximation error in terms of the network architecture. The results subsume and substantially generalize analogous prior results for linear elliptic PDEs over a unit hypercube.},
  author = {Tanya Marwah and Zachary Chase Lipton and Jianfeng Lu and Andrej Risteski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/marwah2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24139--24172},
  pdf = {https://proceedings.mlr.press/v202/marwah23a/marwah23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Network Approximations of {PDE}s Beyond Linearity: A Representational Perspective},
  url = {https://proceedings.mlr.press/v202/marwah23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mashkaria2023generative,
  abstract = {Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. In the offline model-based optimization (MBO) setting, we assume access to a fixed, offline dataset for pretraining and a small budget for online function evaluations. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel model-based optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples.},
  author = {Satvik Mehul Mashkaria and Siddarth Krishnamoorthy and Aditya Grover},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mashkaria2023generative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24173--24197},
  pdf = {https://proceedings.mlr.press/v202/mashkaria23a/mashkaria23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Pretraining for Black-Box Optimization},
  url = {https://proceedings.mlr.press/v202/mashkaria23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{massague2023inferring,
  abstract = {Systems consisting of interacting agents are prevalent in the world, ranging from dynamical systems in physics to complex biological networks. To build systems which can interact robustly in the real world, it is thus important to be able to infer the precise interactions governing such systems. Existing approaches typically discover such interactions by explicitly modeling the feed-forward dynamics of the trajectories. In this work, we propose Neural Interaction Inference with Potentials ({NIIP}) as an alternative approach to discover such interactions that enables greater flexibility in trajectory modeling: it discovers a set of relational potentials, represented as energy functions, which when minimized reconstruct the original trajectory. {NIIP} assigns low energy to the subset of trajectories which respect the relational constraints observed. We illustrate that with these representations {NIIP} displays unique capabilities in test-time. First, it allows trajectory manipulation, such as interchanging interaction types across separately trained models, as well as trajectory forecasting. Additionally, it allows adding external hand-crafted potentials at test-time. Finally, {NIIP} enables the detection of out-of-distribution samples and anomalies without explicit training.},
  author = {Armand Comas Massague and Yilun Du and Christian Fernandez Lopez and Sandesh Ghimire and Mario Sznaier and Joshua B. Tenenbaum and Octavia I. Camps},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/massague2023inferring.pdf:pdf},
  mdate = {2023-08-28},
  pages = {6364--6383},
  pdf = {https://proceedings.mlr.press/v202/comas23a/comas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Inferring Relational Potentials in Interacting Systems},
  url = {https://proceedings.mlr.press/v202/comas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mate2023improved,
  abstract = {We consider the task of evaluating policies of algorithmic resource allocation through randomized controlled trials ({RCTs}). Such policies are tasked with optimizing the utilization of limited intervention resources, with the goal of maximizing the benefits derived. Evaluation of such allocation policies through {RCTs} proves difficult, notwithstanding the scale of the trial, because the individuals' outcomes are inextricably interlinked through resource constraints controlling the policy decisions. Our key contribution is to present a new estimator leveraging our proposed novel concept, that involves retrospective reshuffling of participants across experimental arms at the end of an {RCT}. We identify conditions under which such reassignments are permissible and can be leveraged to construct counterfactual trials, whose outcomes can be accurately ascertained, for free.},
  author = {Aditya Mate and Bryan Wilder and Aparna Taneja and Milind Tambe},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mate2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24198--24213},
  pdf = {https://proceedings.mlr.press/v202/mate23a/mate23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Policy Evaluation for Randomized Trials of Algorithmic Resource Allocation},
  url = {https://proceedings.mlr.press/v202/mate23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{maurais2023multifidelity,
  abstract = {We introduce a multi-fidelity estimator of covariance matrices that employs the log-Euclidean geometry of the symmetric positive-definite manifold. The estimator fuses samples from a hierarchy of data sources of differing fidelities and costs for variance reduction while guaranteeing definiteness, in contrast with previous approaches. The new estimator makes covariance estimation tractable in applications where simulation or data collection is expensive; to that end, we develop an optimal sample allocation scheme that minimizes the mean-squared error of the estimator given a fixed budget. Guaranteed definiteness is crucial to metric learning, data assimilation, and other downstream tasks. Evaluations of our approach using data from physical applications (heat conduction, fluid dynamics) demonstrate more accurate metric learning and speedups of more than one order of magnitude compared to baseline methods.},
  author = {Aimee Maurais and Terrence Alsup and Benjamin Peherstorfer and Youssef M. Marzouk},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/maurais2023multifidelity.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24214--24235},
  pdf = {https://proceedings.mlr.press/v202/maurais23a/maurais23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry},
  url = {https://proceedings.mlr.press/v202/maurais23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mayekar2023communicationconstrained,
  abstract = {We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $\mathtt{SNR}\coloneqq \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor.},
  author = {Prathamesh Mayekar and Jonathan Scarlett and Vincent Y. F. Tan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mayekar2023communicationconstrained.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24236--24250},
  pdf = {https://proceedings.mlr.press/v202/mayekar23a/mayekar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Communication-Constrained Bandits under Additive Gaussian Noise},
  url = {https://proceedings.mlr.press/v202/mayekar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mazzetto2023nonparametric,
  abstract = {Given a sequence of independent samples taken from a distribution that gradually changes in time, we study the problem of estimating the current distribution. This problem, known as density estimation under distribution drift, is fundamental to many machine learning tasks and has been studied extensively in the discrete case. We prove tight minimax risk bounds for both discrete and continuous smooth densities, where the minimum is over all possible estimates and the maximum is over all possible distributions that satisfy the drift constraints. Our technique handles a broad class of drift models, and generalizes previous results on agnostic learning under drift.},
  author = {Alessio Mazzetto and Eli Upfal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mazzetto2023nonparametric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24251--24270},
  pdf = {https://proceedings.mlr.press/v202/mazzetto23a/mazzetto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonparametric Density Estimation under Distribution Drift},
  url = {https://proceedings.mlr.press/v202/mazzetto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mbacke2023pac,
  abstract = {We extend {PAC}-{B}ayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein {GANs} and Energy-Based {GANs}, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein {GANs} on synthetic datasets.},
  author = {Sokhna Diarra Mbacke and Florence Clerc and Pascal Germain},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mbacke2023pac.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24271--24290},
  pdf = {https://proceedings.mlr.press/v202/mbacke23a/mbacke23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PAC}-{B}ayesian Generalization Bounds for Adversarial Generative Models},
  url = {https://proceedings.mlr.press/v202/mbacke23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mckinzie2023robustness,
  abstract = {Multimodal learning is defined as learning over multiple heterogeneous input modalities such as video, audio, and text. In this work, we are concerned with understanding how models behave as the type of modalities differ between training and deployment, a situation that naturally arises in many applications of multimodal learning to hardware platforms. We present a multimodal robustness framework to provide a systematic analysis of common multimodal representation learning methods. Further, we identify robustness short-comings of these approaches and propose two intervention techniques leading to $1.5\times$--$4\times$ robustness improvements on three datasets, AudioSet, Kinetics-400 and ImageNet-Captions.},
  author = {Brandon McKinzie and Vaishaal Shankar and Joseph Yitan Cheng and Yinfei Yang and Jonathon Shlens and Alexander T. Toshev},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mckinzie2023robustness.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24291--24303},
  pdf = {https://proceedings.mlr.press/v202/mckinzie23a/mckinzie23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robustness in Multimodal Learning under Train-Test Modality Mismatch},
  url = {https://proceedings.mlr.press/v202/mckinzie23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mehmetigo2023nonlinear,
  abstract = {We perform an empirical study of the behaviour of deep networks when fully linearizing some of its feature channels through a sparsity prior on the overall number of nonlinear units in the network. In experiments on image classification and machine translation tasks, we investigate how much we can simplify the network function towards linearity before performance collapses. First, we observe a significant performance gap when reducing nonlinearity in the network function early on as opposed to late in training, in-line with recent observations on the time-evolution of the data-dependent {NTK}. Second, we find that after training, we are able to linearize a significant number of nonlinear units while maintaining a high performance, indicating that much of a network's expressivity remains unused but helps gradient descent in early stages of training.},
  author = {Christian H. X. Ali Mehmeti-G{\"o}pel and Jan Disselhoff},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mehmetigo2023nonlinear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {529--546},
  pdf = {https://proceedings.mlr.press/v202/ali-mehmeti-gopel23a/ali-mehmeti-gopel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think},
  url = {https://proceedings.mlr.press/v202/ali-mehmeti-gopel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mehrabi2023modelfree,
  abstract = {Understanding the effect of a feature vector $x\in \mathbb{R}^d$ on the response value (label) $y\in \mathbb{R}$ is the cornerstone of many statistical learning problems. Ideally, it is desired to understand how a set of collected features combine together and influence the response value, but this problem is notoriously difficult, due to the high-dimensionality of data and limited number of labeled data points, among many others. In this work, we take a new perspective on this problem, and we study the question of assessing the difference of influence that the two given features have on the response value. We first propose a notion of closeness for the influence of features, and show that our definition recovers the familiar notion of the magnitude of coefficients in the parametric model. We then propose a novel method to test for the closeness of influence in general model-free supervised learning problems. Our proposed test can be used with finite number of samples with control on type I error rate, no matter the ground truth conditional law $\mathcal{L}(Y|X)$. We analyze the power of our test for two general learning problems i) linear regression, and ii) binary classification under mixture of Gaussian models, and show that under the proper choice of score function, an internal component of our test, with sufficient number of samples will achieve full statistical power. We evaluate our findings through extensive numerical simulations, specifically we adopt the datamodel framework for {CIFAR}-10 dataset to identify pairs of training samples with different influence on the trained model via optional black box training mechanisms.},
  author = {Mohammad Mehrabi and Ryan A. Rossi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mehrabi2023modelfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24304--24324},
  pdf = {https://proceedings.mlr.press/v202/mehrabi23a/mehrabi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Model-free Closeness-of-influence Test for Features in Supervised Learning},
  url = {https://proceedings.mlr.press/v202/mehrabi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mei2023stochastic,
  abstract = {We show that the stochastic gradient bandit algorithm converges to a globally optimal policy at an $O(1/t)$ rate, even with a constant step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability 1.},
  author = {Jincheng Mei and Zixin Zhong and Bo Dai and Alekh Agarwal and Csaba Szepesv{\'a}ri and Dale Schuurmans},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mei2023stochastic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24325--24360},
  pdf = {https://proceedings.mlr.press/v202/mei23a/mei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stochastic Gradient Succeeds for Bandits},
  url = {https://proceedings.mlr.press/v202/mei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{melnychuk2023normalizing,
  abstract = {Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. To address this challenge, we propose Interventional Normalizing Flows as a deep learning method for interventional density estimation. Our method learns to map outcome distributions from the observational distribution to the interventional distribution, enabling full density estimation of potential outcomes. To the best of our knowledge, our Interventional Normalizing Flows are the first proper fully-parametric, deep learning method for density estimation of potential outcomes.},
  author = {Valentyn Melnychuk and Dennis Frauen and Stefan Feuerriegel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/melnychuk2023normalizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24361--24397},
  pdf = {https://proceedings.mlr.press/v202/melnychuk23a/melnychuk23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Normalizing Flows for Interventional Density Estimation},
  url = {https://proceedings.mlr.press/v202/melnychuk23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{melnyk2023reprogramming,
  abstract = {The paper addresses challenges in computational antibody design, specifically focusing on generating diverse complementarity-determining region (CDR) sequences. The authors introduce ReprogBert, a method that repurposes a pretrained English language model for protein sequence infilling using model reprogramming. The approach leverages cross-language adaptation with limited data and generates highly diverse CDR sequences. The method achieves up to more than a two-fold increase of diversity over the baselines while maintaining structural integrity and naturalness, and demonstrates enhanced antigen binding specificity and virus neutralization ability.},
  author = {Igor Melnyk and Vijil Chenthamarakshan and Pin-Yu Chen and Payel Das and Amit Dhurandhar and Inkit Padhi and Devleena Das},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/melnyk2023reprogramming.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24398--24419},
  pdf = {https://proceedings.mlr.press/v202/melnyk23a/melnyk23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reprogramming Pretrained Language Models for Antibody Sequence Infilling},
  url = {https://proceedings.mlr.press/v202/melnyk23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{memarrast2023superhuman,
  abstract = {The fairness of machine learning-based decisions has become an increasingly important focus in the design of supervised machine learning methods. Most fairness approaches optimize a specified trade-off between performance measure(s) (e.g., accuracy, log loss, or AUC) and fairness metric(s) (e.g., demographic parity, equalized odds). This begs the question: are the right performance-fairness trade-offs being specified? We instead re-cast fair machine learning as an imitation learning task by introducing superhuman fairness, which seeks to simultaneously outperform human decisions on multiple predictive performance and fairness measures. We demonstrate the benefits of this approach given suboptimal decisions.},
  author = {Omid Memarrast and Linh Vu and Brian D. Ziebart},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/memarrast2023superhuman.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24420--24435},
  pdf = {https://proceedings.mlr.press/v202/memarrast23a/memarrast23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Superhuman Fairness},
  url = {https://proceedings.mlr.press/v202/memarrast23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{meng2023modelbased,
  abstract = {We develop a variant of the stochastic prox-linear method for minimizing the Conditional Value-at-Risk (CVaR) objective. CVaR is a risk measure focused on minimizing worst-case performance, defined as the average of the top quantile of the losses. In machine learning, such a risk measure is useful to train more robust models. Although the stochastic subgradient method (SGM) is a natural choice for minimizing the CVaR objective, we show that our stochastic prox-linear (SPL+) algorithm can better exploit the structure of the objective, while still providing a convenient closed form update. Our SPL+ method also adapts to the scaling of the loss function, which allows for easier tuning. We then specialize a general convergence theorem for SPL+ to our setting, and show that it allows for a wider selection of step sizes compared to SGM. We support this theoretical finding experimentally.},
  author = {Si Yi Meng and Robert M. Gower},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/meng2023modelbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24436--24456},
  pdf = {https://proceedings.mlr.press/v202/meng23a/meng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Model-Based Method for Minimizing {CVaR} and Beyond},
  url = {https://proceedings.mlr.press/v202/meng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{meng2023tuning,
  abstract = {Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.},
  author = {Yu Meng and Martin Michalski and Jiaxin Huang and Yu Zhang and Tarek F. Abdelzaher and Jiawei Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/meng2023tuning.pdf:pdf},
  mdate = {2023-08-29},
  pages = {24457--24477},
  pdf = {https://proceedings.mlr.press/v202/meng23b/meng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning},
  url = {https://proceedings.mlr.press/v202/meng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{merlis2023preemption,
  abstract = {We study single-machine scheduling of jobs, each belonging to a job type that determines its duration distribution. We start by analyzing the scenario where the type characteristics are known and then move to two learning scenarios where the types are unknown: non-preemptive problems, where each started job must be completed before moving to another job; and preemptive problems, where job execution can be paused in the favor of moving to a different job. In both cases, we design algorithms that achieve sublinear excess cost, compared to the performance with known types, and prove lower bounds for the non-preemptive case. Notably, we demonstrate, both theoretically and through simulations, how preemptive algorithms can greatly outperform non-preemptive ones when the durations of different job types are far from one another, a phenomenon that does not occur when the type durations are known.},
  author = {Nadav Merlis and Hugo Richard and Flore Sentenac and Corentin Odic and Mathieu Molina and Vianney Perchet},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/merlis2023preemption.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24478--24516},
  pdf = {https://proceedings.mlr.press/v202/merlis23a/merlis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Preemption and Learning in Stochastic Scheduling},
  url = {https://proceedings.mlr.press/v202/merlis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mesnard2023quantile,
  abstract = {In reinforcement learning, the credit assignment problem is to distinguish luck from skill, that is, separate the inherent randomness in the environment from the controllable effects of the agent's actions. This paper proposes two novel algorithms, Quantile Credit Assignment (QCA) and Hindsight QCA (HQCA), which incorporate distributional value estimation to perform credit assignment. QCA uses a network that predicts the quantiles of the return distribution, whereas HQCA additionally incorporates information about the future. Both QCA and HQCA have the appealing interpretation of leveraging an estimate of the quantile level of the return (interpreted as the level of luck) in order to derive a luck-dependent baseline for policy gradient methods.},
  author = {Thomas Mesnard and Wenqi Chen and Alaa Saade and Yunhao Tang and Mark Rowland and Theophane Weber and Clare Lyle and Audrunas Gruslys and Michal Valko and Will Dabney and Georg Ostrovski and Eric Moulines and Rmi Munos},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mesnard2023quantile.pdf:pdf},
  mdate = {2024-10-02},
  pages = {24517--24531},
  pdf = {https://proceedings.mlr.press/v202/mesnard23a/mesnard23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantile Credit Assignment},
  url = {https://proceedings.mlr.press/v202/mesnard23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{metelev2023consensus,
  abstract = {We consider decentralized optimization problems where one aims to minimize a sum of convex smooth objective functions distributed between nodes in the network. The links in the network can change from time to time. For the setting when the amount of changes is arbitrary, lower complexity bounds and corresponding optimal algorithms are known, and the consensus acceleration is not possible. However, in practice the magnitude of network changes may be limited. We derive lower complexity bounds for several regimes of velocity of networks changes. The paper shows how to obtain accelerated communication rates for certain classes of time-varying graphs using specific consensus algorithms.},
  author = {Dmitry Metelev and Alexander Rogozin and Dmitry Kovalev and Alexander V. Gasnikov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/metelev2023consensus.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24532--24554},
  pdf = {https://proceedings.mlr.press/v202/metelev23a/metelev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Is Consensus Acceleration Possible in Decentralized Optimization over Slowly Time-Varying Networks?},
  url = {https://proceedings.mlr.press/v202/metelev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{metelli2023towards,
  abstract = {Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards.},
  author = {Alberto Maria Metelli and Filippo Lazzati and Marcello Restelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/metelli2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24555--24591},
  pdf = {https://proceedings.mlr.press/v202/metelli23a/metelli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Theoretical Understanding of Inverse Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/metelli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{meyer2023training,
  abstract = {The spatiotemporal resolution of Partial Differential Equations (PDEs) plays important roles in the mathematical description of the world's physical phenomena. In general, scientists and engineers solve PDEs numerically by the use of computationally demanding solvers. Recently, deep learning algorithms have emerged as a viable alternative for obtaining fast solutions for PDEs. Models are usually trained on synthetic data generated by solvers, stored on disk and read back for training. This paper advocates that relying on a traditional static dataset to train these models does not allow the full benefit of the solver to be used as a data generator. It proposes an open source online training framework for deep surrogate models. The framework implements several levels of parallelism focused on simultaneously generating numerical simulations and training deep neural networks. This approach suppresses the I/O and storage bottleneck associated with disk-loaded datasets, and opens the way to training on significantly larger datasets. Results indicate that exposing deep surrogate models to more dataset diversity, up to hundreds of GB, can increase model generalization capabilities. Fully connected neural networks, Fourier Neural Operator (FNO), and Message Passing PDE Solver prediction accuracy is improved by 68%, 16% and 7%, respectively.},
  author = {Lucas Thibaut Meyer and Marc Schouler and Robert Alexander Caulk and Alejandro Ribs and Bruno Raffin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/meyer2023training.pdf:pdf},
  mdate = {2023-09-30},
  pages = {24614--24630},
  pdf = {https://proceedings.mlr.press/v202/meyer23b/meyer23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Training Deep Surrogate Models with Large Scale Online Learning},
  url = {https://proceedings.mlr.press/v202/meyer23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{meyer2023quantum,
  abstract = {Quantum machine learning implemented by variational quantum circuits (VQCs) is considered a promising concept for the noisy intermediate-scale quantum computing era. Focusing on applications in quantum reinforcement learning, we propose a specific action decoding procedure for a quantum policy gradient approach. We introduce a novel quality measure that enables us to optimize the classical post-processing required for action selection, inspired by local and global quantum measurements. The resulting algorithm demonstrates a significant performance improvement in several benchmark environments. With this technique, we successfully execute a full training routine on a 5-qubit hardware device. Our method introduces only negligible classical overhead and has the potential to improve VQC-based algorithms beyond the field of quantum reinforcement learning.},
  author = {Nico Meyer and Daniel D. Scherer and Axel Plinge and Christopher Mutschler and Michael J. Hartmann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/meyer2023quantum.pdf:pdf},
  mdate = {2023-09-30},
  pages = {24592--24613},
  pdf = {https://proceedings.mlr.press/v202/meyer23a/meyer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum Policy Gradient Algorithm with Optimized Action Decoding},
  url = {https://proceedings.mlr.press/v202/meyer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mguni2023mansa,
  abstract = {In multi-agent reinforcement learning (MARL), independent learning (IL) often shows remarkable performance and easily scales with the number of agents. Yet, using IL can be inefficient and runs the risk of failing to successfully train, particularly in scenarios that require agents to coordinate their actions. Using centralised learning (CL) enables MARL agents to quickly learn how to coordinate their behaviour but employing CL everywhere is often prohibitively expensive in real-world applications. Besides, using CL in value-based methods often needs strong representational constraints (e.g. individual-global-max condition) that can lead to poor performance if violated. In this paper, we introduce a novel plug & play IL framework named Multi-Agent Network Selection Algorithm (MANSA) which selectively employs CL only at states that require coordination. At its core, MANSA has an additional agent that uses switching controls to quickly learn the best states to activate CL during training, using CL only where necessary and vastly reducing the computational burden of CL. Our theory proves MANSA preserves cooperative MARL convergence properties, boosts IL performance and can optimally make use of a fixed budget on the number CL calls. We show empirically in Level-based Foraging (LBF) and StarCraft Multi-agent Challenge (SMAC) that MANSA achieves fast, superior and more reliable performance while making 40% fewer CL calls in SMAC and using CL at only 1% CL calls in LBF.},
  author = {David Henry Mguni and Haojun Chen and Taher Jafferjee and Jianhong Wang and Longfei Yue and Xidong Feng and Stephen Marcus McAleer and Feifei Tong and Jun Wang 0012 and Yaodong Yang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mguni2023mansa.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24631--24658},
  pdf = {https://proceedings.mlr.press/v202/mguni23a/mguni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MANSA: Learning Fast and Slow in Multi-Agent Systems},
  url = {https://proceedings.mlr.press/v202/mguni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mhammedi2023representation,
  abstract = {We study the design of sample-efficient algorithms for reinforcement learning in the presence of rich, high-dimensional observations, formalized via the Block MDP problem. Existing algorithms suffer from either 1) computational intractability, 2) strong statistical assumptions that are not necessarily satisfied in practice, or 3) suboptimal sample complexity. We address these issues by providing the first computationally efficient algorithm that attains rate-optimal sample complexity with respect to the desired accuracy level, with minimal statistical assumptions. Our algorithm, MusIK, combines systematic exploration with representation learning based on multi-step inverse kinematics, a learning objective in which the aim is to predict the learner's own action from the current observation and observations in the (potentially distant) future. MusIK is simple and flexible, and can efficiently take advantage of general-purpose function approximation. Our analysis leverages several new techniques tailored to non-optimistic exploration algorithms, which we anticipate will find broader use.},
  author = {Zakaria Mhammedi and Dylan J. Foster and Alexander Rakhlin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mhammedi2023representation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24659--24700},
  pdf = {https://proceedings.mlr.press/v202/mhammedi23a/mhammedi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Representation Learning with Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation RL},
  url = {https://proceedings.mlr.press/v202/mhammedi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mhanna2023single,
  abstract = {Zero-order (ZO) optimization is a powerful tool for dealing with realistic constraints. On the other hand, the gradient-tracking (GT) technique proved to be an efficient method for distributed optimization aiming to achieve consensus. However, it is a first-order (FO) method that requires knowledge of the gradient, which is not always possible in practice. In this work, we introduce a zero-order distributed optimization method based on a one-point estimate of the gradient tracking technique. We prove that this new technique converges with a single noisy function query at a time in the non-convex setting. We then establish a convergence rate of O(1/K^{1/3}) after a number of iterations K, which competes with that of O(1/K^{1/4}) of its centralized counterparts. Finally, a numerical example validates our theoretical results.},
  author = {Elissa Mhanna and Mohamad Assaad},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mhanna2023single.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24701--24719},
  pdf = {https://proceedings.mlr.press/v202/mhanna23a/mhanna23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Single Point-Based Distributed Zeroth-Order Optimization with a Non-Convex Stochastic Objective Function},
  url = {https://proceedings.mlr.press/v202/mhanna23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{miao2023learning,
  abstract = {The paper introduces InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.},
  author = {Ning Miao and Tom Rainforth and Emile Mathieu and Yann Dubois and Yee Whye Teh and Adam Foster 0001 and Hyunjik Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/miao2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24720--24736},
  pdf = {https://proceedings.mlr.press/v202/miao23a/miao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Instance-Specific Augmentations by Capturing Local Invariances},
  url = {https://proceedings.mlr.press/v202/miao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{michel2023path,
  abstract = {Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.},
  author = {Gaspard Michel and Giannis Nikolentzos and Johannes F. Lutzeyer and Michalis Vazirgiannis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/michel2023path.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24737--24755},
  pdf = {https://proceedings.mlr.press/v202/michel23a/michel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Path Neural Networks: Expressive and Accurate Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/michel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{miconi2023learning,
  abstract = {A hallmark of intelligence is the ability to autonomously learn new flexible, cognitive behaviors - that is, behaviors where the appropriate action depends not just on immediate stimuli (as in simple reflexive stimulus-response associations), but on contextual information that must be adequately acquired, stored and processed. While many meta-learning algorithms can design agents that autonomously learn new tasks, cognitive tasks adds another level of learning and memory to typical "learning-to-learn" problems. Here we evolve neural networks, endowed with plastic connections and neuromodulation, over a sizable set of simple cognitive tasks adapted from a computational neuroscience framework. The resulting evolved networks can automatically modify their own connectivity to acquire a novel simple cognitive task, never seen during evolution, from stimuli and rewards alone, through the spontaneous operation of their evolved neural organization and plasticity system. Our results emphasize the importance of carefully considering the multiple learning loops involved in the emergence of intelligent behavior.},
  author = {Thomas Miconi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/miconi2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24756--24774},
  pdf = {https://proceedings.mlr.press/v202/miconi23a/miconi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning},
  url = {https://proceedings.mlr.press/v202/miconi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{miliotou2023generative,
  abstract = {The challenging task of reconstructing natural images from fMRI recordings is of great importance in neuroscience. Current architectures are bottlenecked because they fail to effectively capture the hierarchical processing of visual stimuli that takes place in the human brain. To address this limitation, we introduce a novel neural network architecture for neural decoding that uses Hierarchical Variational Autoencoders (HVAEs) to learn meaningful representations of natural images and leverages their latent space hierarchy to learn voxel-to-image mappings. By mapping the early stages of the visual pathway to the first set of latent variables and the higher visual cortex areas to the deeper layers in the latent hierarchy, we construct a latent variable neural decoding model that replicates hierarchical visual information processing. The proposed framework provides a new way of bridging the domain gap between fMRI signals and visual images, addressing the challenges of limited fMRI data instances that suffer from low signal-to-noise ratio and extremely high dimensionality.},
  author = {Eleni Miliotou and Panagiotis Kyriakis and Jason D. Hinman and Andrei Irimia and Paul Bogdan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/miliotou2023generative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24775--24784},
  pdf = {https://proceedings.mlr.press/v202/miliotou23a/miliotou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Decoding of Visual Stimuli},
  url = {https://proceedings.mlr.press/v202/miliotou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{min2023informationtheoretic,
  abstract = {In nonstationary bandit learning problems, the decision-maker must continually gather information and adapt their action selection as the latent state of the environment evolves. In each time period, some latent optimal action maximizes expected reward under the environment state. We view the optimal action sequence as a stochastic process, and take an information-theoretic approach to analyze attainable performance. We bound per-period regret in terms of the entropy rate of the optimal action process. The bound applies to a wide array of problems studied in the literature and reflects the problem's information structure through its information-ratio.},
  author = {Seungki Min and Daniel Russo 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/min2023informationtheoretic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24831--24849},
  pdf = {https://proceedings.mlr.press/v202/min23c/min23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Information-Theoretic Analysis of Nonstationary Bandit Learning},
  url = {https://proceedings.mlr.press/v202/min23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{min2023cooperative,
  abstract = {We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an O(d^{3/2}H^2√K) regret with O(dHM^2) communication complexity, where d is the feature dimension, H is the horizon length, M is the total number of agents, and K is the total number of episodes. We also provide a lower bound showing that a minimal Ω(dM) communication complexity is required to improve the performance through collaboration.},
  author = {Yifei Min and Jiafan He and Tianhao Wang 0002 and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/min2023cooperative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24785--24811},
  pdf = {https://proceedings.mlr.press/v202/min23a/min23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cooperative Multi-Agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation},
  url = {https://proceedings.mlr.press/v202/min23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{min2023directed,
  abstract = {Real-world data can be multimodal distributed, and generating multimodal distributed real-world data has become a challenge to existing generative adversarial networks ({GANs}). Neural stochastic differential equations ({Neural} {SDEs}), treated as infinite-dimensional {GANs}, have demonstrated successful performance mainly in generating unimodal time series data. To tackle this challenge, we propose a novel time series generator, named directed chain {GANs} ({DC-GANs}), which inserts a time series dataset (called a neighborhood process of the directed chain or input) into the drift and diffusion coefficients of the directed chain {SDEs} with distributional constraints. {DC-GANs} can generate new time series of the same distribution as the neighborhood process, and the neighborhood process provides the key step in learning and generating multimodal distributed time series. We examine {DC-GANs} on four datasets, including two stochastic models from social sciences and computational neuroscience, and two real-world datasets on stock prices and energy consumption. To our knowledge, {DC-GANs} are the first work that can generate multimodal time series data and consistently outperforms state-of-the-art benchmarks with respect to measures of distribution, data similarity, and predictive ability.},
  author = {Ming Min and Ruimeng Hu and Tomoyuki Ichiba},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/min2023directed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24812--24830},
  pdf = {https://proceedings.mlr.press/v202/min23b/min23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Directed Chain Generative Adversarial Networks},
  url = {https://proceedings.mlr.press/v202/min23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{min2023convergence,
  abstract = {We analyze the convergence of gradient flow on a multi-layer linear model with a loss function of the form $f(W_1W_2\cdots W_L)$. We show that when $f$ satisfies the gradient dominance property, proper weight initialization leads to exponential convergence of the gradient flow to a global minimum of the loss. The convergence rate depends on two trajectory-specific quantities controlled by weight initialization: the imbalance matrices (which measure the difference between the weights of adjacent layers) and the least singular value of the weight product $W=W_1W_2\cdots W_L$. Our analysis exploits the fact that the gradient of the overparameterized loss can be written as the composition of the non-overparameterized gradient with a time-varying (weight-dependent) linear operator whose smallest eigenvalue controls the convergence rate.},
  author = {Hancheng Min and Ren Vidal and Enrique Mallada},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/min2023convergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24850--24887},
  pdf = {https://proceedings.mlr.press/v202/min23d/min23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Convergence of Gradient Flow on Multi-layer Linear Models},
  url = {https://proceedings.mlr.press/v202/min23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mishkin2023optimal,
  abstract = {We develop an analytical framework to characterize the set of optimal {ReLU} neural networks by reformulating the non-convex training problem as a convex program. We show that the global optima of the convex parameterization are given by a polyhedral set and then extend this characterization to the optimal set of the non-convex training objective. Since all stationary points of the {ReLU} training problem can be represented as optima of sub-sampled convex programs, our work provides a general expression for all critical points of the non-convex objective. Our analysis provides several practical contributions: an optimal pruning algorithm that can be used to compute minimal models -- the smallest-width neural networks which are optimal for a given dataset and regularization parameter. We also show that the regularization path of {ReLU} networks is discontinuous in general and establish sufficient conditions for the path to be closed/continuous.},
  author = {Aaron Mishkin and Mert Pilanci},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mishkin2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24888--24924},
  pdf = {https://proceedings.mlr.press/v202/mishkin23a/mishkin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Sets and Solution Paths of {ReLU} Networks},
  url = {https://proceedings.mlr.press/v202/mishkin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mishne2023numerical,
  abstract = {While hyperbolic space is excellent for representing hierarchical datasets due to its ability to embed trees with small distortion, this comes at the cost of numerical instability that can lead to catastrophic {NaN} problems during training. We analyze limitations of two popular hyperbolic space models -- the {Poincaré} ball and the {Lorentz} model. Under 64-bit arithmetic, we find that the {Poincaré} ball has relatively larger capacity for correctly representing points, while the {Lorentz} model is superior from an optimization perspective. To address these limitations, we identify a {Euclidean} parametrization of hyperbolic space that can alleviate these numerical issues, and extend this parametrization to hyperbolic hyperplanes, demonstrating effectiveness in improving hyperbolic {SVM} performance.},
  author = {Gal Mishne and Zhengchao Wan and Yusu Wang and Sheng Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mishne2023numerical.pdf:pdf},
  mdate = {2023-11-12},
  pages = {24925--24949},
  pdf = {https://proceedings.mlr.press/v202/mishne23a/mishne23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Numerical Stability of Hyperbolic Representation Learning},
  url = {https://proceedings.mlr.press/v202/mishne23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mitchell2023detectgpt,
  abstract = {The proliferation of large language models ({LLMs}) has made machine-generated text nearly indistinguishable from human-written text, posing challenges for detecting misuse. We propose {DetectGPT}, a zero-shot method for determining if a piece of text was sampled from a given {LLM}. {DetectGPT} leverages the tendency of {LLMs} to occupy negative curvature regions of the model's log probability function. We define a new curvature-based criterion for judging if a passage was generated from a given {LLM}. The approach does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. We demonstrate that {DetectGPT} improves detection of fake news articles generated by 20B parameter {GPT-NeoX} from 0.81 {AUROC} for the strongest zero-shot baseline to 0.95 {AUROC}.},
  author = {Eric Mitchell and Yoonho Lee and Alexander Khazatsky and Christopher D. Manning and Chelsea Finn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mitchell2023detectgpt.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24950--24962},
  pdf = {https://proceedings.mlr.press/v202/mitchell23a/mitchell23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DetectGPT}: Zero-Shot Machine-Generated Text Detection using Probability Curvature},
  url = {https://proceedings.mlr.press/v202/mitchell23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mittal2023diffusion,
  abstract = {Diffusion-based methods, represented as stochastic differential equations on a continuous-time domain, have proven successful as non-adversarial generative models. The training of such models relies on denoising score matching, which can be seen as multi-scale denoising autoencoders. We augment the denoising score matching framework to enable representation learning without any supervised signal. Unlike generative adversarial networks and variational autoencoders that learn representations by directly transforming latent codes to data samples, our approach learns representations by utilizing the information needed for denoising. This difference allows for manual control of the level of details encoded in the representation. We propose to learn an infinite-dimensional latent code that achieves improvements over state-of-the-art models on semi-supervised image classification.},
  author = {Sarthak Mittal and Korbinian Abstreiter and Stefan Bauer and Bernhard Sch\"olkopf and Arash Mehrjou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mittal2023diffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {24963--24982},
  pdf = {https://proceedings.mlr.press/v202/mittal23a/mittal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diffusion Based Representation Learning},
  url = {https://proceedings.mlr.press/v202/mittal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mo2023unified,
  abstract = {The ability to accurately recognize, localize and separate sound sources is crucial for audio-visual perception tasks. However, these abilities were typically tackled separately, leading to suboptimal performance due to failure to capture interdependence between tasks. In this work, we propose {OneAVM}, a unified audio-visual learning framework that integrates audio and visual cues for joint localization, separation, and recognition. {OneAVM} comprises a shared audio-visual encoder and task-specific decoders trained with three objectives. Extensive experiments demonstrate that joint training improves performance on all three tasks compared to training them separately, and our approach achieves state-of-the-art results on several benchmarks.},
  author = {Shentong Mo and Pedro Morgado},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mo2023unified.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25006--25017},
  pdf = {https://proceedings.mlr.press/v202/mo23b/mo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition},
  url = {https://proceedings.mlr.press/v202/mo23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mo2023disentangled,
  abstract = {Few works in unsupervised multiplex graph representation learning ({UMGRL}) simultaneously focus on common and private information extraction. We argue that it's essential for effective and robust {UMGRL} to extract complete and clean common information, as well as more-complementarity and less-noise private information. We investigate disentangled representation learning for multiplex graphs to capture complete and clean common information, while designing a contrastive constraint to preserve complementarity and remove noise in private information. We provide theoretical analysis showing that the common and private representations learned by our method are provably disentangled and contain more task-relevant and less task-irrelevant information to benefit downstream tasks.},
  author = {Yujie Mo and Yajie Lei and Jialie Shen and Xiaoshuang Shi and Heng Tao Shen and Xiaofeng Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mo2023disentangled.pdf:pdf},
  mdate = {2024-03-07},
  pages = {24983--25005},
  pdf = {https://proceedings.mlr.press/v202/mo23a/mo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Disentangled Multiplex Graph Representation Learning},
  url = {https://proceedings.mlr.press/v202/mo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mo2023pruning,
  abstract = {Neural pruning involves identifying optimal sparse subnetworks to reduce complexity and improve efficiency of deep neural networks. We investigate the evolution of optimal subnetworks with continuously increasing sparsity to understand how to transform unpruned dense models into optimal subnetworks with desired sparsity levels. We propose Sparsity-indexed {ODE} ({SpODE}), a novel pruning framework that provides explicit guidance on how to best preserve model performance while ensuring infinitesimal increases in model sparsity. We develop Pruning via Sparsity-indexed {ODE} ({PSO}), a pruning algorithm that enables effective pruning by traveling along the {SpODE} path.},
  author = {Zhanfeng Mo and Haosen Shi and Sinno Jialin Pan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mo2023pruning.pdf:pdf},
  mdate = {2024-10-01},
  pages = {25018--25036},
  pdf = {https://proceedings.mlr.press/v202/mo23c/mo23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pruning via Sparsity-indexed {ODE}: a Continuous Sparsity Viewpoint},
  url = {https://proceedings.mlr.press/v202/mo23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{moayeri2023texttoconcept,
  abstract = {We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Based on this finding, we propose text-to-concept, where features from a fixed pretrained model are aligned linearly to the {CLIP} space, so that text embeddings from {CLIP}'s text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of {CLIP}, despite being much smaller models and trained on a small fraction of the data compared to {CLIP}. We demonstrate other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints.},
  author = {Mazda Moayeri and Keivan Rezaei and Maziar Sanjabi and Soheil Feizi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/moayeri2023texttoconcept.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25037--25060},
  pdf = {https://proceedings.mlr.press/v202/moayeri23a/moayeri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Text-To-Concept (and Back) via Cross-Model Alignment},
  url = {https://proceedings.mlr.press/v202/moayeri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mohamadi2023a,
  abstract = {Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size NO × NO, taking O((NO)²) memory and up to O((NO)³) computation. Most existing applications have therefore used one of a handful of approximations yielding N × N kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call "sum of logits", converges to the true eNTK at initialization for any network with a wide final "readout" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings.},
  author = {Mohamad Amin Mohamadi and Wonho Bae and Danica J. Sutherland},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mohamadi2023a.pdf:pdf},
  mdate = {2024-10-06},
  pages = {25061--25081},
  pdf = {https://proceedings.mlr.press/v202/mohamadi23a/mohamadi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{A} {F}ast, {W}ell-{F}ounded {A}pproximation to the {E}mpirical {N}eural {T}angent {K}ernel},
  url = {https://proceedings.mlr.press/v202/mohamadi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mohtashami2023s,
  abstract = {When training neural networks, it has been widely observed that a large step size is essential in stochastic gradient descent (SGD) for obtaining superior models. However, the effect of large step sizes on the success of SGD is not well understood theoretically. Several previous works have attributed this success to the stochastic noise present in SGD. However, we show through a novel set of experiments that the stochastic noise is not sufficient to explain good non-convex training, and that instead the effect of a large learning rate itself is essential for obtaining best performance. We demonstrate the same effects also in the noise-less case, i.e. for full-batch GD. We formally prove that GD with large step size —on certain non-convex function classes — follows a different trajectory than GD with a small step size, which can lead to convergence to a global minimum instead of a local one.},
  author = {Amirkeivan Mohtashami and Martin Jaggi and Sebastian U. Stich},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mohtashami2023s.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25082--25104},
  pdf = {https://proceedings.mlr.press/v202/mohtashami23a/mohtashami23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{S}pecial {P}roperties of {G}radient {D}escent with {L}arge {L}earning {R}ates},
  url = {https://proceedings.mlr.press/v202/mohtashami23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{molinaro2023n,
  abstract = {A large class of inverse problems for PDEs are only well-defined as mappings from operators to functions. Existing operator learning frameworks map functions to functions and need to be modified to learn inverse maps from data. We propose a novel architecture termed Neural Inverse Operators (NIOs) to solve these PDE inverse problems. Motivated by the underlying mathematical structure, NIO is based on a suitable composition of DeepONets and FNOs to approximate mappings from operators to functions. A variety of experiments are presented to demonstrate that NIOs significantly outperform baselines.},
  author = {Roberto Molinaro and Yunan Yang and Bj{\"o}rn Engquist and Siddhartha Mishra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/molinaro2023n.pdf:pdf},
  mdate = {2023-11-12},
  pages = {25105--25139},
  pdf = {https://proceedings.mlr.press/v202/molinaro23a/molinaro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{N}eural {I}nverse {O}perators for {S}olving {PDE} {I}nverse {P}roblems},
  url = {https://proceedings.mlr.press/v202/molinaro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{monchot2023i,
  abstract = {When physical sensors are involved, such as image sensors, the uncertainty over the input data is often a major component of the output uncertainty of machine learning models. In this work, we address the problem of input uncertainty propagation through trained neural networks. We do not rely on a Gaussian distribution assumption of the output or of any intermediate layer. We propagate instead a Gaussian Mixture Model (GMM) that offers much more flexibility, using the Split\&Merge algorithm. This paper's main contribution is the computation of a Wasserstein criterion to control the Gaussian splitting procedure for which theoretical guarantees of convergence on the output distribution estimates are derived.},
  author = {Paul Monchot and Lo{\"i}c Coquelin and S{\'e}bastien Julien Petit and S{\'e}bastien Marmin and Erwan Le Pennec and Nicolas Fischer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/monchot2023i.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25140--25173},
  pdf = {https://proceedings.mlr.press/v202/monchot23a/monchot23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{I}nput uncertainty propagation through trained neural networks},
  url = {https://proceedings.mlr.press/v202/monchot23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{montanari2023c,
  abstract = {The paper addresses compression of tabular data with categorical entries commonly used for analytics and machine learning. The authors introduce a family of lossless compression algorithms that proceed in four steps: (i) Estimate latent variables associated to rows and columns; (ii) Partition the table in blocks according to the row/column latents; (iii) Apply a sequential (e.g. Lempel-Ziv) coder to each of the blocks; (iv) Append a compressed encoding of the latents. They evaluate the approach on several benchmark datasets, and study optimal compression in a probabilistic model for tabular data, whereby latent values are independent and table entries are conditionally independent given the latent values. The authors prove that the model has a well defined entropy rate and satisfies an asymptotic equipartition property.},
  author = {Andrea Montanari and Eric Weiner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/montanari2023c.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25174--25208},
  pdf = {https://proceedings.mlr.press/v202/montanari23a/montanari23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{C}ompressing {T}abular {D}ata via {L}atent {V}ariable {E}stimation},
  url = {https://proceedings.mlr.press/v202/montanari23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{morishita2023l,
  abstract = {We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name FLD (Formal Logic Deduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.},
  author = {Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/morishita2023l.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25254--25274},
  pdf = {https://proceedings.mlr.press/v202/morishita23a/morishita23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{L}earning {D}eductive {R}easoning from {S}ynthetic {C}orpus based on {F}ormal {L}ogic},
  url = {https://proceedings.mlr.press/v202/morishita23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{moskovitz2023r,
  abstract = {In recent years, Reinforcement Learning (RL) has been applied to real-world problems with increasing success. Such applications often require to put constraints on the agent's behavior. Existing algorithms for constrained RL (CRL) rely on gradient descent-ascent, but this approach comes with a caveat. While these algorithms are guaranteed to converge on average, they do not guarantee last-iterate convergence, i.e., the current policy of the agent may never converge to the optimal solution. In practice, it is often observed that the policy alternates between satisfying the constraints and maximizing the reward, rarely accomplishing both objectives simultaneously. Here, we address this problem by introducing Reinforcement Learning with Optimistic Ascent-Descent (ReLOAD), a principled CRL method with guaranteed last-iterate convergence. We demonstrate its empirical effectiveness on a wide variety of CRL problems including discrete MDPs and continuous control.},
  author = {Ted Moskovitz and Brendan O'Donoghue and Vivek Veeriah and Sebastian Flennerhag and Satinder Singh and Tom Zahavy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/moskovitz2023r.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25303--25336},
  pdf = {https://proceedings.mlr.press/v202/moskovitz23a/moskovitz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{R}e{LOAD}: {R}einforcement {L}earning with {O}ptimistic {A}scent-{D}escent for {L}ast-{I}terate {C}onvergence in {C}onstrained {MDPs}},
  url = {https://proceedings.mlr.press/v202/moskovitz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{moulin2023o,
  abstract = {We propose a new method for optimistic planning in infinite-horizon discounted Markov decision processes based on the idea of adding regularization to the updates of an otherwise standard approximate value iteration procedure. This technique allows us to avoid contraction and monotonicity arguments typically required by existing analyses of approximate dynamic programming methods, and in particular to use approximate transition functions estimated via least-squares procedures in MDPs with linear function approximation. We use our method to provide a computationally efficient algorithm for learning near-optimal policies in discounted linear kernel/mixture MDPs from a single stream of experience, and show that it achieves near-optimal statistical guarantees.},
  author = {Antoine Moulin and Gergely Neu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/moulin2023o.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25337--25357},
  pdf = {https://proceedings.mlr.press/v202/moulin23a/moulin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{O}ptimistic {P}lanning by {R}egularized {D}ynamic {P}rogramming},
  url = {https://proceedings.mlr.press/v202/moulin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{muckley2023i,
  abstract = {Lossy image compression aims to represent images in as few bits as possible while maintaining fidelity to the original. Theoretical results indicate that optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a discrepancy in the statistics of original images from those of reconstructions, in particular at low bitrates, often manifested by the blurring of the compressed images. Previous work has leveraged adversarial discriminators to improve statistical fidelity. Yet these binary discriminators adopted from generative modeling tasks may not be ideal for image compression. In this paper, we introduce a non-binary discriminator that is conditioned on quantized local image representations obtained via VQ-VAE autoencoders.},
  author = {Matthew J. Muckley and Alaaeldin El-Nouby and Karen Ullrich and Herv{\'e} J{\'e}gou and Jakob Verbeek},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/muckley2023i.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25426--25443},
  pdf = {https://proceedings.mlr.press/v202/muckley23a/muckley23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{I}mproving {S}tatistical {F}idelity for {N}eural {I}mage {C}ompression with {I}mplicit {L}ocal {L}ikelihood {M}odels},
  url = {https://proceedings.mlr.press/v202/muckley23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mu2023a,
  abstract = {We propose energy natural gradient descent, a natural gradient method with respect to a Hessian-induced Riemannian metric as an optimization algorithm for physics-informed neural networks (PINNs) and the deep Ritz method. As a main motivation we show that the update direction in function space resulting from the energy natural gradient corresponds to the Newton direction modulo an orthogonal projection on the model's tangent space. We demonstrate experimentally that energy natural gradient descent yields highly accurate solutions with errors several orders of magnitude smaller than what is obtained when training PINNs with standard optimizers like gradient descent or Adam, even when those are allowed significantly more computation time.},
  author = {Johannes M{\"u}ller and Marius Zeinhofer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mu2023a.pdf:pdf},
  mdate = {2024-10-06},
  pages = {25471--25485},
  pdf = {https://proceedings.mlr.press/v202/muller23b/muller23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{A}chieving {H}igh {A}ccuracy with {PINNs} via {E}nergy {N}atural {G}radient {D}escent},
  url = {https://proceedings.mlr.press/v202/muller23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{murata2023diff,
  abstract = {Differential private optimization for nonconvex smooth objective is considered. In the previous work, the best known utility bound is $\widetilde O(\sqrt{d}/(n\varepsilon_\mathrm{DP}))$ in terms of the squared full gradient norm, which is achieved by Differential Private Gradient Descent (DP-GD) as an instance, where $n$ is the sample size, $d$ is the problem dimensionality and $\varepsilon_\mathrm{DP}$ is the differential privacy parameter. The authors propose a new framework called DIFF2 that constructs a differential private global gradient estimator using gradient differences. They show DIFF2 achieves a utility of $\widetilde O(d^{2/3}/(n\varepsilon_\mathrm{DP})^{4/3})$, which improves upon previous bounds.},
  author = {Tomoya Murata and Taiji Suzuki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/murata2023diff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25523--25548},
  pdf = {https://proceedings.mlr.press/v202/murata23b/murata23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DIFF}2: Differential Private Optimization via Gradient Differences for Nonconvex Distributed Learning},
  url = {https://proceedings.mlr.press/v202/murata23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{murata2023g,
  abstract = {Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.},
  author = {Naoki Murata and Koichi Saito and Chieh-Hsin Lai and Yuhta Takida and Toshimitsu Uesaka and Yuki Mitsufuji and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/murata2023g.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25501--25522},
  pdf = {https://proceedings.mlr.press/v202/murata23a/murata23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{G}ibbs{DDRM}: A Partially Collapsed {G}ibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration},
  url = {https://proceedings.mlr.press/v202/murata23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{murphy2023efficiently,
  abstract = {Identifying a small molecule from its mass spectrum is the primary open problem in computational metabolomics. This is typically cast as information retrieval: an unknown spectrum is matched against spectra predicted computationally from a large database of chemical structures. However, current approaches to spectrum prediction model the output space in ways that force a tradeoff between capturing high resolution mass information and tractable learning. We resolve this tradeoff by casting spectrum prediction as a mapping from an input molecular graph to a probability distribution over chemical formulas. We further discover that a large corpus of mass spectra can be closely approximated using a fixed vocabulary constituting only 2\% of all observed formulas. This enables efficient spectrum prediction using an architecture similar to graph classification - {G}r{AFF}-{MS} - achieving significantly lower prediction error and orders-of-magnitude faster runtime than state-of-the-art methods.},
  author = {Michael Murphy and Stefanie Jegelka and Ernest Fraenkel and Tobias Kind and David Healey and Thomas Butler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/murphy2023efficiently.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25549--25562},
  pdf = {https://proceedings.mlr.press/v202/murphy23a/murphy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficiently predicting high resolution mass spectra with graph neural networks},
  url = {https://proceedings.mlr.press/v202/murphy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{mussi2023dynamical,
  abstract = {In many real-world sequential decision-making problems, an action does not immediately reflect on the feedback and spreads its effects over a long time frame. For instance, in online advertising, investing in a platform produces an instantaneous increase of awareness, but the actual reward, i.e., a conversion, might occur far in the future. Furthermore, whether a conversion takes place depends on: how fast the awareness grows, its vanishing effects, and the synergy or interference with other advertising platforms. Previous work has investigated the Multi-Armed Bandit framework with the possibility of delayed and aggregated feedback, without a particular structure on how an action propagates in the future, disregarding possible dynamical effects. In this paper, we introduce a novel setting, the Dynamical Linear Bandits (DLB), an extension of the linear bandits characterized by a hidden state that evolves according to a linear dynamical system.},
  author = {Marco Mussi and Alberto Maria Metelli and Marcello Restelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/mussi2023dynamical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25563--25587},
  pdf = {https://proceedings.mlr.press/v202/mussi23a/mussi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dynamical Linear Bandits},
  url = {https://proceedings.mlr.press/v202/mussi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nabati2023representationdriven,
  abstract = {The paper presents a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, they leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows them to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. They demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods.},
  author = {Ofir Nabati and Guy Tennenholtz and Shie Mannor},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nabati2023representationdriven.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25588--25603},
  pdf = {https://proceedings.mlr.press/v202/nabati23a/nabati23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Representation-Driven Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/nabati23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nabli2023dadao,
  abstract = {This work introduces DADAO: the first decentralized, accelerated, asynchronous, primal, first-order algorithm to minimize a sum of L-smooth and μ-strongly convex functions distributed over a given network of size n. Our key insight is based on modeling the local gradient updates and gossip communication procedures with separate independent Poisson Point Processes. This allows us to decouple the computation and communication steps, which can be run in parallel, while making the whole approach completely asynchronous. This leads to communication acceleration compared to synchronous approaches. Our new method employs primal gradients and does not use a multi-consensus inner loop nor other ad-hoc mechanisms such as Error Feedback, Gradient Tracking, or a Proximal operator.},
  author = {Adel Nabli and Edouard Oyallon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nabli2023dadao.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25604--25626},
  pdf = {https://proceedings.mlr.press/v202/nabli23a/nabli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DADAO}: Decoupled Accelerated Decentralized Asynchronous Optimization},
  url = {https://proceedings.mlr.press/v202/nabli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nagaraj2023multiuser,
  abstract = {We consider collaborative multi-user reinforcement learning, where multiple users have the same state-action space and transition probabilities but different rewards. Under the assumption that the reward matrix of the N users has a low-rank structure -- a standard and practically successful assumption in the collaborative filtering setting -- we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with N user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When N is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard non-collaborative approach.},
  author = {Dheeraj Mysore Nagaraj and Suhas S. Kowshik and Naman Agarwal and Praneeth Netrapalli and Prateek Jain},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nagaraj2023multiuser.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25627--25659},
  pdf = {https://proceedings.mlr.press/v202/nagaraj23a/nagaraj23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-User Reinforcement Learning with Low Rank Rewards},
  url = {https://proceedings.mlr.press/v202/nagaraj23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nagler2023statistical,
  abstract = {Prior-data fitted networks (PFNs) were recently proposed as a new paradigm for machine learning. Instead of training the network to an observed training set, a fixed model is pre-trained offline on small, simulated training sets from a variety of tasks. The pre-trained model is then used to infer class probabilities in-context on fresh training sets with arbitrary size and distribution. Empirically, PFNs achieve state-of-the-art performance on tasks with similar size to the ones used in pre-training. Surprisingly, their accuracy further improves when passed larger data sets during inference. This article establishes a theoretical foundation for PFNs and illuminates the statistical mechanisms governing their behavior. While PFNs are motivated by Bayesian ideas, a purely frequentistic interpretation of PFNs as pre-tuned, but untrained predictors explains their behavior.},
  author = {Thomas Nagler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nagler2023statistical.pdf:pdf},
  mdate = {2024-10-06},
  pages = {25660--25676},
  pdf = {https://proceedings.mlr.press/v202/nagler23a/nagler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Statistical Foundations of Prior-Data Fitted Networks},
  url = {https://proceedings.mlr.press/v202/nagler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{naik2023machine,
  abstract = {Machine learning models can make critical errors that are easily hidden within vast amounts of data. Such errors often run counter to rules based on human intuition. However, rules based on human knowledge are challenging to scale or to even formalize. We thereby seek to infer statistical rules from the data and quantify the extent to which a model has learned them. We propose a framework SQRL that integrates logic-based methods with statistical inference to derive these rules from a model's training data without supervision. We further show how to adapt models at test time to reduce rule violations and produce more coherent predictions. SQRL generates up to 300K rules over datasets from vision, tabular, and language settings. We uncover up to 158K violations of those rules by state-of-the-art models for classification, object detection, and data imputation. Test-time adaptation reduces these violations by up to 68.7\% with relative performance improvement up to 32\%.},
  author = {Aaditya Naik and Yinjun Wu and Mayur Naik and Eric Wong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/naik2023machine.pdf:pdf},
  mdate = {2023-09-30},
  pages = {25677--25693},
  pdf = {https://proceedings.mlr.press/v202/naik23a/naik23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do Machine Learning Models Learn Statistical Rules Inferred from Data?},
  url = {https://proceedings.mlr.press/v202/naik23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{naiman2023sample,
  abstract = {Unsupervised disentanglement is a long-standing challenge in representation learning. Recently, self-supervised techniques achieved impressive results in the sequential setting, where data is time-dependent. However, the latter methods employ modality-based data augmentations and random sampling or solve auxiliary tasks. In this work, we propose to avoid that by generating, sampling, and comparing empirical distributions from the underlying variational model. Unlike existing work, we introduce a self-supervised sequential disentanglement framework based on contrastive estimation with no external signals, while using common batch sizes and samples from the latent space itself. In practice, we propose a unified, efficient, and easy-to-code sampling strategy for semantically similar and dissimilar views of the data. The paper was evaluated on video, audio, and time series benchmarks and presented state-of-the-art results in comparison to existing techniques.},
  author = {Ilan Naiman and Nimrod Berman and Omri Azencot},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/naiman2023sample.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25694--25717},
  pdf = {https://proceedings.mlr.press/v202/naiman23a/naiman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sample and Predict Your Latent: Modality-free Sequential Disentanglement via Contrastive Estimation},
  url = {https://proceedings.mlr.press/v202/naiman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nasresfahany2023counterfactual,
  abstract = {We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.},
  author = {Arash Nasr-Esfahany and Mohammad Alizadeh and Devavrat Shah},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nasresfahany2023counterfactual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25733--25754},
  pdf = {https://proceedings.mlr.press/v202/nasr-esfahany23a/nasr-esfahany23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Counterfactual Identifiability of Bijective Causal Models},
  url = {https://proceedings.mlr.press/v202/nasr-esfahany23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nasr2023effectively,
  abstract = {Differentially private (DP) machine learning techniques are notorious for their degradation of model utility (e.g., they degrade classification accuracy). A recent line of work has demonstrated that leveraging public data can improve the trade-off between privacy and utility when training models with DP guaranteed. In this work, we further explore the potential of using public data in DP models, showing that utility gains can in fact be significantly higher than what shown in prior works.},
  author = {Milad Nasr and Saeed Mahloujifar and Xinyu Tang and Prateek Mittal and Amir Houmansadr},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nasr2023effectively.pdf:pdf},
  mdate = {2025-02-21},
  pages = {25718--25732},
  pdf = {https://proceedings.mlr.press/v202/nasr23a/nasr23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Effectively Using Public Data in Privacy Preserving Machine Learning},
  url = {https://proceedings.mlr.press/v202/nasr23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nath2023discovering,
  abstract = {Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent 'question' functions and leveraging the subsequent learn general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learn representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptation.},
  author = {Somjit Nath and Gopeshh Raaj Subbaraj and Khimya Khetarpal and Samira Ebrahimi Kahou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nath2023discovering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25755--25768},
  pdf = {https://proceedings.mlr.press/v202/nath23a/nath23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discovering Object-Centric Generalized Value Functions From Pixels},
  url = {https://proceedings.mlr.press/v202/nath23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nauman2023manyactions,
  abstract = {We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.},
  author = {Michal Nauman and Marek Cygan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nauman2023manyactions.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25769--25789},
  pdf = {https://proceedings.mlr.press/v202/nauman23a/nauman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Many-Actions Policy Gradient},
  url = {https://proceedings.mlr.press/v202/nauman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{navon2023equivariant,
  abstract = {Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show how these layers can be implemented using three basic operations: pooling, broadcasting, and fully connected layers applied to the input in an appropriate manner. We demonstrate the effectiveness of our architecture and its advantages over natural baselines in a variety of learning tasks.},
  author = {Aviv Navon and Aviv Shamsian and Idan Achituve and Ethan Fetaya and Gal Chechik and Haggai Maron},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/navon2023equivariant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25790--25816},
  pdf = {https://proceedings.mlr.press/v202/navon23a/navon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Equivariant Architectures for Learning in Deep Weight Spaces},
  url = {https://proceedings.mlr.press/v202/navon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nayak2023scalable,
  abstract = {We consider the problem of multi-agent navigation and collision avoidance when observations are limited to the local neighborhood of each agent. We propose InforMARL, a novel architecture for multi-agent reinforcement learning (MARL) which uses local information intelligently to compute paths for all the agents in a decentralized manner. Specifically, InforMARL aggregates information about the local neighborhood of agents for both the actor and the critic using a graph neural network and can be used in conjunction with any standard MARL algorithm. We show that (1) in training, InforMARL has better sample efficiency and performance than baseline approaches, despite using less information, and (2) in testing, it scales well to environments with arbitrary numbers of agents and obstacles.},
  author = {Siddharth Nayak and Kenneth Choi and Wenqi Ding and Sydney Dolan and Karthik Gopalakrishnan and Hamsa Balakrishnan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nayak2023scalable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25817--25833},
  pdf = {https://proceedings.mlr.press/v202/nayak23a/nayak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation},
  url = {https://proceedings.mlr.press/v202/nayak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nazari2023geometric,
  abstract = {Visualization is a crucial step in exploratory data analysis. One possible approach is to train an autoencoder with low-dimensional latent space. Large network depth and width can help unfolding the data. However, such expressive networks can achieve low reconstruction error even when the latent representation is distorted. To avoid such misleading visualizations, we propose first a differential geometric perspective on the decoder, leading to insightful diagnostics for an embedding's distortion. We propose the Geometric Autoencoder which avoids stretching the embedding spuriously, so that the visualization captures the data structure more faithfully. Additionally, it flags areas where little distortion could not be achieved, thus guarding against misinterpretation.},
  author = {Philipp Nazari and Sebastian Damrich and Fred A. Hamprecht},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nazari2023geometric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25834--25857},
  pdf = {https://proceedings.mlr.press/v202/nazari23a/nazari23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Geometric Autoencoders - What You See is What You Decode},
  url = {https://proceedings.mlr.press/v202/nazari23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{neklyudov2023action,
  abstract = {Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass.},
  author = {Kirill Neklyudov and Rob Brekelmans and Daniel Severo and Alireza Makhzani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/neklyudov2023action.pdf:pdf},
  mdate = {2024-10-06},
  pages = {25858--25889},
  pdf = {https://proceedings.mlr.press/v202/neklyudov23a/neklyudov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Action Matching: Learning Stochastic Dynamics from Samples},
  url = {https://proceedings.mlr.press/v202/neklyudov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nettasinghe2023extending,
  abstract = {Conformal prediction is a widely used method to quantify the uncertainty of a classifier under the assumption of exchangeability (e.g., IID data). We generalize conformal prediction to the Hidden Markov Model (HMM) framework where the assumption of exchangeability is not valid. The key idea of the proposed method is to partition the non-exchangeable Markovian data from the HMM into exchangeable blocks by exploiting the de Finetti's Theorem for Markov Chains discovered by Diaconis and Freedman (1980). The permutations of the exchangeable blocks are viewed as randomizations of the observed Markovian data from the HMM. The proposed method provably retains all desirable theoretical guarantees offered by the classical conformal prediction framework in both exchangeable and Markovian settings.},
  author = {Buddhika Nettasinghe and Samrat Chatterjee and Ramakrishna Tipireddy and Mahantesh M. Halappanavar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nettasinghe2023extending.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25890--25903},
  pdf = {https://proceedings.mlr.press/v202/nettasinghe23a/nettasinghe23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Extending Conformal Prediction to Hidden {M}arkov Models with Exact Validity via de {F}inetti's Theorem for {M}arkov Chains},
  url = {https://proceedings.mlr.press/v202/nettasinghe23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023climax,
  abstract = {Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatiotemporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute and data while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets.},
  author = {Tung Nguyen and Johannes Brandstetter and Ashish Kapoor and Jayesh K. Gupta and Aditya Grover},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023climax.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25904--25938},
  pdf = {https://proceedings.mlr.press/v202/nguyen23a/nguyen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ClimaX}: A foundation model for weather and climate},
  url = {https://proceedings.mlr.press/v202/nguyen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023provable,
  abstract = {Reinforcement learning (RL) so far has limited real-world applications. One key challenge is that typical RL algorithms heavily rely on a reset mechanism to sample proper initial states; these reset mechanisms, in practice, are expensive to implement due to the need for human intervention or heavily engineered environments. To make learning more practical, we propose a generic no-regret reduction to systematically design reset-free RL algorithms. Our reduction turns the reset-free RL problem into a two-player game. We show that achieving sublinear regret in this two-player game would imply learning a policy that has both sublinear performance regret and sublinear total number of resets in the original RL problem. This means that the agent eventually learns to perform optimally and avoid resets. To demonstrate the effectiveness of this reduction, we design an instantiation for linear Markov decision processes, which is the first provably correct reset-free RL algorithm.},
  author = {Hoai-An Nguyen and Ching-An Cheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023provable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25939--25955},
  pdf = {https://proceedings.mlr.press/v202/nguyen23b/nguyen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provable Reset-free Reinforcement Learning by No-Regret Reduction},
  url = {https://proceedings.mlr.press/v202/nguyen23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023revisiting,
  abstract = {Graph Neural Networks (GNNs) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of GNNs to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that over-smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.},
  author = {Khang Nguyen and Nong Minh Hieu and Vinh Duc Nguyen and Nhat Ho and Stanley J. Osher and Tan Minh Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023revisiting.pdf:pdf},
  mdate = {2023-09-05},
  pages = {25956--25979},
  pdf = {https://proceedings.mlr.press/v202/nguyen23c/nguyen23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Over-smoothing and Over-squashing Using {Ollivier-Ricci} Curvature},
  url = {https://proceedings.mlr.press/v202/nguyen23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023deep,
  abstract = {The recent integration of deep learning and pairwise similarity annotation-based constrained clustering -- i.e., deep constrained clustering (DCC) -- has proven effective for incorporating weak supervision into massive data clustering: Less than 1\% of pair similarity annotations can often substantially enhance the clustering accuracy. However, beyond empirical successes, there is a lack of understanding of DCC. In addition, many DCC paradigms are sensitive to annotation noise, but performance-guaranteed noisy DCC methods have been largely elusive. This work first takes a deep look into a recently emerged logistic loss function of DCC, and characterizes its theoretical properties. Our result shows that the logistic DCC loss ensures the identifiability of data membership under reasonable conditions, which may shed light on its effectiveness in practice. Building upon this understanding, a new loss function based on geometric factor analysis is proposed to fend against noisy annotations. It is shown that even under unknown annotation confusions, the data membership can still be provably identified under our proposed learning criterion. The proposed approach is tested over multiple datasets to validate our claims.},
  author = {Tri Nguyen and Shahana Ibrahim and Xiao Fu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {25980--26007},
  pdf = {https://proceedings.mlr.press/v202/nguyen23d/nguyen23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Clustering with Incomplete Noisy Pairwise Annotations: {A} Geometric Regularization Approach},
  url = {https://proceedings.mlr.press/v202/nguyen23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023selfattention,
  abstract = {Max sliced Wasserstein (Max-SW) distance has been widely known as a solution for less discriminative projections of sliced Wasserstein (SW) distance. In applications that have various independent pairs of probability measures, amortized projection optimization is utilized to predict the ``max'' projecting directions given two input measures instead of using projected gradient ascent multiple times. Despite being efficient, Max-SW and its amortized version cannot guarantee metricity property due to the sub-optimality of the projected gradient ascent and the amortization gap. Therefore, we propose to replace Max-SW with distributional sliced Wasserstein distance with von Mises-Fisher (vMF) projecting distribution (v-DSW). Since v-DSW is a metric with any non-degenerate vMF distribution, its amortized version can guarantee the metricity when performing amortization. Furthermore, current amortized models are not permutation invariant and symmetric. To address the issue, we design amortized models based on self-attention architecture. In particular, we adopt efficient self-attention architectures to make the computation linear in the number of supports. With the two improvements, we derive self-attention amortized distributional projection optimization and show its appealing performance in point-cloud reconstruction and its downstream applications.},
  author = {Khai Nguyen and Dang Nguyen and Nhat Ho},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023selfattention.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26008--26030},
  pdf = {https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-Attention Amortized Distributional Projection Optimization for Sliced {Wasserstein} Point-Cloud Reconstruction},
  url = {https://proceedings.mlr.press/v202/nguyen23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nguyen2023building,
  abstract = {Matrix manifolds, such as manifolds of Symmetric Positive Definite (SPD) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for SPD and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.},
  author = {Xuan Son Nguyen and Shuo Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nguyen2023building.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26031--26062},
  pdf = {https://proceedings.mlr.press/v202/nguyen23f/nguyen23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Building Neural Networks on Matrix Manifolds: {A} Gyrovector Space Approach},
  url = {https://proceedings.mlr.press/v202/nguyen23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ngweta2023simple,
  abstract = {Learning visual representations with interpretable features, i.e., disentangled representations, remains a challenging problem. Existing methods demonstrate some success but are hard to apply to large-scale vision datasets like ImageNet. In this work, we propose a simple post-processing framework to disentangle content and style in learned representations from pre-trained vision models. We model the pre-trained features probabilistically as linearly entangled combinations of the latent content and style factors and develop a simple disentanglement algorithm based on the probabilistic model. We show that the method provably disentangles content and style features and verify its efficacy empirically. Our post-processed features yield significant domain generalization performance improvements when the distribution shift occurs due to style changes or style-related spurious correlations.},
  author = {Lilian Ngweta and Subha Maity and Alex Gittens and Yuekai Sun and Mikhail Yurochkin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ngweta2023simple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26063--26086},
  pdf = {https://proceedings.mlr.press/v202/ngweta23a/ngweta23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simple Disentanglement of Style and Content in Visual Representations},
  url = {https://proceedings.mlr.press/v202/ngweta23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ni2023lever,
  abstract = {The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs (4.6\% to 10.9\% with code-davinci-002) and achieves new state-of-the-art results on all of them.},
  author = {Ansong Ni and Srini Iyer and Dragomir Radev and Veselin Stoyanov and Wen-Tau Yih and Sida I. Wang and Xi Victoria Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ni2023lever.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26106--26128},
  pdf = {https://proceedings.mlr.press/v202/ni23b/ni23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LEVER}: Learning to Verify Language-to-Code Generation with Execution},
  url = {https://proceedings.mlr.press/v202/ni23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ni2023metadiffuser,
  abstract = {Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning (RL). However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL (MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.},
  author = {Fei Ni and Jianye Hao and Yao Mu and Yifu Yuan and Yan Zheng and Bin Wang and Zhixuan Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ni2023metadiffuser.pdf:pdf},
  mdate = {2025-02-10},
  pages = {26087--26105},
  pdf = {https://proceedings.mlr.press/v202/ni23a/ni23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MetaDiffuser}: Diffusion Model as Conditional Planner for Offline Meta-{RL}},
  url = {https://proceedings.mlr.press/v202/ni23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ni2023continual,
  abstract = {Large-scale multi-modal contrastive learning frameworks like CLIP typically require a large amount of image-text samples for training. However, these samples are always collected continuously in real scenarios. This paper discusses the feasibility of continual CLIP training using streaming data. Unlike continual learning based on self-supervised learning methods for pure images, which is empirically robust against catastrophic forgetting, CLIP's performance degeneration in the continual setting is significant and non-neglectable. By analyzing the changes in the model's representation space during continual CLIP training from a spatial geometry perspective, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we empirically and theoretically demonstrate how SD leads to a performance decline for CLIP on cross-modal retrieval tasks. To alleviate SD, we propose a new continual vision-language representation learning framework Mod-X: Maintain off-diagonal information-matriX. By selectively aligning the off-diagonal information distribution of contrastive matrices, the Mod-X improves the capability of the multi-modal model by maintaining the multi-modal representation space alignment on the old data domain during continuously fitting the new training data domain. Experiments on commonly used datasets with different scales and scopes have demonstrated the effectiveness of our method.},
  author = {Zixuan Ni and Longhui Wei and Siliang Tang and Yueting Zhuang and Qi Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ni2023continual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26129--26149},
  pdf = {https://proceedings.mlr.press/v202/ni23c/ni23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continual Vision-Language Representation Learning with Off-Diagonal Information},
  url = {https://proceedings.mlr.press/v202/ni23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nie2023attributing,
  abstract = {Generative models have enabled the creation of contents that are indistinguishable from those taken from nature. Open-source development of such models raised concerns about the risks of their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit a significant tradeoff between robust attribution accuracy and generation quality while lacking design principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models.},
  author = {Guangyu Nie and Changhoon Kim and Yezhou Yang and Yi Ren},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nie2023attributing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26150--26165},
  pdf = {https://proceedings.mlr.press/v202/nie23a/nie23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Attributing Image Generative Models using Latent Fingerprints},
  url = {https://proceedings.mlr.press/v202/nie23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nie2023framework,
  abstract = {We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear α-regret methods that only require bandit feedback, achieving O(T^{2/3}log(T)^{1/3}) expected cumulative α-regret dependence on the horizon T.},
  author = {Guanyu Nie and Yididiya Y. Nadew and Yanhui Zhu and Vaneet Aggarwal and Christopher John Quinn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nie2023framework.pdf:pdf},
  pages = {26166--26198},
  pdf = {https://proceedings.mlr.press/v202/nie23b/nie23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback},
  url = {https://proceedings.mlr.press/v202/nie23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nikankin2023sinfusion,
  abstract = {Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling.},
  author = {Yaniv Nikankin and Niv Haim and Michal Irani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nikankin2023sinfusion.pdf:pdf},
  pages = {26199--26214},
  pdf = {https://proceedings.mlr.press/v202/nikankin23a/nikankin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SinFusion}: Training Diffusion Models on a Single Image or Video},
  url = {https://proceedings.mlr.press/v202/nikankin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nikdan2023sparseprop,
  abstract = {This paper presents an efficient implementation of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are sparse. The algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Our results provide the first support for sparse training on commodity hardware.},
  author = {Mahdi Nikdan and Tommaso Pegolotti and Eugenia Iofinova and Eldar Kurtic and Dan Alistarh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nikdan2023sparseprop.pdf:pdf},
  pages = {26215--26227},
  pdf = {https://proceedings.mlr.press/v202/nikdan23a/nikdan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SparseProp}: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge},
  url = {https://proceedings.mlr.press/v202/nikdan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nikulin2023antiexploration,
  abstract = {Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.},
  author = {Alexander Nikulin and Vladislav Kurenkov and Denis Tarasov and Sergey Kolesnikov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nikulin2023antiexploration.pdf:pdf},
  pages = {26228--26244},
  pdf = {https://proceedings.mlr.press/v202/nikulin23a/nikulin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Anti-Exploration by Random Network Distillation},
  url = {https://proceedings.mlr.press/v202/nikulin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ning2023input,
  abstract = {Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that the proposed input perturbation leads to a significant improvement of the sample quality while reducing both the training and the inference times.},
  author = {Mang Ning and Enver Sangineto and Angelo Porrello and Simone Calderara and Rita Cucchiara},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ning2023input.pdf:pdf},
  pages = {26245--26265},
  pdf = {https://proceedings.mlr.press/v202/ning23a/ning23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Input Perturbation Reduces Exposure Bias in Diffusion Models},
  url = {https://proceedings.mlr.press/v202/ning23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nitanda2023primal,
  abstract = {The entropic fictitious play (EFP) is a recently proposed algorithm that minimizes the sum of a convex functional and entropy in the space of measures — such an objective naturally arises in the optimization of a two-layer neural network in the mean-field regime. In this work, we provide a concise primal-dual analysis of EFP in the setting where the learning problem exhibits a finite-sum structure. We establish quantitative global convergence guarantees for both the continuous-time and discrete-time dynamics based on properties of a proximal Gibbs measure introduced in Nitanda et al. (2022). Furthermore, our primal-dual framework entails a memory-efficient particle-based implementation of the EFP update, and also suggests a connection to gradient boosting methods.},
  author = {Atsushi Nitanda and Kazusato Oko and Denny Wu and Nobuhito Takenouchi and Taiji Suzuki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nitanda2023primal.pdf:pdf},
  pages = {26266--26282},
  pdf = {https://proceedings.mlr.press/v202/nitanda23a/nitanda23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Primal and Dual Analysis of Entropic Fictitious Play for Finite-sum Problems},
  url = {https://proceedings.mlr.press/v202/nitanda23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{noarov2023statistical,
  abstract = {We make a connection between multicalibration and property elicitation and show that (under mild technical conditions) it is possible to produce a multicalibrated predictor for a continuous scalar property Γ if and only if Γ is elicitable. On the negative side, we show that for non-elicitable continuous properties there exist simple data distributions on which even the true distributional predictor is not calibrated. On the positive side, for elicitable Γ, we give simple canonical algorithms for the batch and the online adversarial setting, that learn a Γ-multicalibrated predictor. This generalizes past work on multicalibrated means and quantiles, and in fact strengthens existing online quantile multicalibration results. To further counter-weigh our negative result, we show that if a property Γ¹ is not elicitable by itself, but is elicitable conditionally on another elicitable property Γ⁰, then there is a canonical algorithm that jointly multicalibrates Γ¹ and Γ⁰; this generalizes past work on mean-moment multicalibration. Finally, as applications of our theory, we provide novel algorithmic and impossibility results for fair (multicalibrated) risk assessment.},
  author = {Georgy Noarov and Aaron Roth},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/noarov2023statistical.pdf:pdf},
  pages = {26283--26310},
  pdf = {https://proceedings.mlr.press/v202/noarov23a/noarov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Statistical Scope of Multicalibration},
  url = {https://proceedings.mlr.press/v202/noarov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nottingham2023embodied,
  abstract = {Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. In both phases, the agent's policy can condition on language instructions and historical context.},
  author = {Kolby Nottingham and Prithviraj Ammanabrolu and Alane Suhr and Yejin Choi and Hannaneh Hajishirzi and Sameer Singh and Roy Fox},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nottingham2023embodied.pdf:pdf},
  pages = {26311--26325},
  pdf = {https://proceedings.mlr.press/v202/nottingham23a/nottingham23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling},
  url = {https://proceedings.mlr.press/v202/nottingham23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{nova2023gradientfree,
  abstract = {Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT_{BASE} and DistilBERT illustrates the effectiveness of the proposed approach.},
  author = {Azade Nova and Hanjun Dai and Dale Schuurmans},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/nova2023gradientfree.pdf:pdf},
  pages = {26326--26341},
  pdf = {https://proceedings.mlr.press/v202/nova23a/nova23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient-Free Structured Pruning with Unlabeled Data},
  url = {https://proceedings.mlr.press/v202/nova23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{novack2023chils,
  abstract = {Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost.},
  author = {Zachary Novack and Julian J. McAuley and Zachary Chase Lipton and Saurabh Garg},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/novack2023chils.pdf:pdf},
  pages = {26342--26362},
  pdf = {https://proceedings.mlr.press/v202/novack23a/novack23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CHiLS}: Zero-Shot Image Classification with Hierarchical Label Sets},
  url = {https://proceedings.mlr.press/v202/novack23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{novikov2023fewbit,
  abstract = {Memory footprint is one of the main limiting factors for large neural network training. In backpropagation, one needs to store the input to each operation in the computational graph. Every modern neural network model has quite a few pointwise nonlinearities in its architecture, and such operations induce additional memory costs that, as we show, can be significantly reduced by quantization of the gradients. The authors propose a systematic approach to quantize gradients of pointwise nonlinear functions using few bits per element. They demonstrate this through an optimal piecewise-constant approximation of activation function derivatives using dynamic programming.},
  author = {Georgii Sergeevich Novikov and Daniel Bershatsky and Julia Gusak and Alex Shonenkov and Denis Valerievich Dimitrov and Ivan V. Oseledets},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26363--26381},
  pdf = {https://proceedings.mlr.press/v202/novikov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Few-bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction},
  url = {https://proceedings.mlr.press/v202/novikov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{odonoghue2023efficient,
  abstract = {Exploration remains a key challenge in deep reinforcement learning (RL). Optimism in the face of uncertainty is a well-known heuristic with theoretical guarantees in the tabular setting, but how best to translate the principle to deep reinforcement learning, which involves online stochastic gradients and deep network function approximators, is not fully understood. The paper proposes a new differentiable optimistic objective for efficient exploration in deep RL, introducing an epistemic-risk-seeking actor-critic (ERSAC) algorithm. The authors develop a zero-sum two-player game objective that converts uncertainty into value to encourage exploration and show performance improvements on DeepSea and Atari environments.},
  author = {Brendan O'Donoghue},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/odonoghue2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26382--26402},
  pdf = {https://proceedings.mlr.press/v202/o-donoghue23a/o-donoghue23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Exploration via Epistemic-Risk-Seeking Policy Optimization},
  url = {https://proceedings.mlr.press/v202/o-donoghue23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{oh2023provable,
  abstract = {We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. The authors introduce a curse of separability phenomenon, showing that as data distribution becomes more separable, vanilla training's sample complexity increases exponentially. They demonstrate that Mixup can reduce sample complexity by developing new concentration results for pair-wise augmented data. The paper also warns that some masking-based Mixup techniques can lead to suboptimal classifiers.},
  author = {Junsoo Oh and Chulhee Yun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/oh2023provable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26403--26450},
  pdf = {https://proceedings.mlr.press/v202/oh23a/oh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provable Benefit of Mixup for Finding Optimal Decision Boundaries},
  url = {https://proceedings.mlr.press/v202/oh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ohana2023shedding,
  abstract = {The Sliced-Wasserstein distance (SW) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties -- or, more accurately, its generalization properties -- with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the PAC-Bayesian theory and a central observation that SW may be interpreted as an average risk, the quantity PAC-Bayesian bounds have been designed to characterize. We provide three types of results: (i) PAC-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. SW defined with respect to arbitrary distributions of slices (among which data-dependent distributions), (ii) a principled procedure to learn the distribution of slices that yields maximally discriminative SW, by optimizing our theoretical bounds, and (iii) empirical illustrations of our theoretical findings.},
  author = {Ruben Ohana and Kimia Nadjahi and Alain Rakotomamonjy and Liva Ralaivola},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26451--26473},
  pdf = {https://proceedings.mlr.press/v202/ohana23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Shedding a {PAC}-{B}ayesian Light on Adaptive Sliced-{W}asserstein Distances},
  url = {https://proceedings.mlr.press/v202/ohana23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ohayon2023reasons,
  abstract = {Stochastic restoration algorithms allow to explore the space of solutions that correspond to the degraded input. In this paper we reveal additional fundamental advantages of stochastic methods over deterministic ones, which further motivate their use. First, we prove that any restoration algorithm that attains perfect perceptual quality and whose outputs are consistent with the input must be a posterior sampler, and is thus required to be stochastic. Second, we illustrate that while deterministic restoration algorithms may attain high perceptual quality, this can be achieved only by filling up the space of all possible source images using an extremely sensitive mapping, which makes them highly vulnerable to adversarial attacks. Indeed, we show that enforcing deterministic models to be robust to such attacks profoundly hinders their perceptual quality, while robustifying stochastic models hardly influences their perceptual quality, and improves their output variability. These findings provide a motivation to foster progress in stochastic restoration methods, paving the way to better recovery algorithms.},
  author = {Guy Ohayon and Theo Joseph Adrai and Michael Elad and Tomer Michaeli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26474--26494},
  pdf = {https://proceedings.mlr.press/v202/ohayon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality},
  url = {https://proceedings.mlr.press/v202/ohayon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{okati2023withingroup,
  abstract = {Screening classifiers are increasingly used to identify qualified candidates in a variety of selection processes. In this context, it has been recently shown that if a classifier is calibrated, one can identify the smallest set of candidates which contains, in expectation, a desired number of qualified candidates using a threshold decision rule. This lends support to focusing on calibration as the only requirement for screening classifiers. In this paper, we argue that screening policies that use calibrated classifiers may suffer from an understudied type of within-group unfairness—they may unfairly treat qualified members within demographic groups of interest. Further, we argue that this type of unfairness can be avoided if classifiers satisfy within-group monotonicity, a natural monotonicity property within each group.},
  author = {Nastaran Okati and Stratis Tsirtsis and Manuel Gomez Rodriguez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26495--26516},
  pdf = {https://proceedings.mlr.press/v202/okati23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Within-Group Fairness of Screening Classifiers},
  url = {https://proceedings.mlr.press/v202/okati23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{oko2023diffusion,
  abstract = {While efficient distribution learning is no doubt behind the groundbreaking success of diffusion modeling, its theoretical guarantees are quite limited. In this paper, we provide the first rigorous analysis on approximation and generalization abilities of diffusion modeling for well-known function spaces. The highlight of this paper is that when the true density function belongs to the Besov space and the empirical score matching loss is properly minimized, the generated data distribution achieves the nearly minimax optimal estimation rates in the total variation distance and in the Wasserstein distance of order one. Furthermore, we extend our theory to demonstrate how diffusion models adapt to low-dimensional data distributions. We expect these results advance theoretical understandings of diffusion modeling and its ability to generate verisimilar outputs.},
  author = {Kazusato Oko and Shunta Akiyama and Taiji Suzuki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/oko2023diffusion.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26517--26582},
  pdf = {https://proceedings.mlr.press/v202/oko23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diffusion Models are Minimax Optimal Distribution Estimators},
  url = {https://proceedings.mlr.press/v202/oko23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{raphae2023how,
  abstract = {Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness that do not affect accuracy: we show for example that data augmentation can by itself increase adversarial robustness, without using adversarial training.},
  author = {Rapha{\"e}l Olivier and Bhiksha Raj},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26583--26598},
  pdf = {https://proceedings.mlr.press/v202/olivier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Many Perturbations Break This Model? {E}valuating Robustness Beyond Adversarial Accuracy},
  url = {https://proceedings.mlr.press/v202/olivier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{oprescu2023b,
  abstract = {Estimating heterogeneous treatment effects from observational data is a crucial task across many fields, helping policy and decision-makers take better actions. There has been recent progress on robust and efficient methods for estimating the conditional average treatment effect (CATE) function, but these methods often do not take into account the risk of hidden confounding, which could arbitrarily and unknowingly bias any causal estimate based on observational data. We propose a meta-learner called the B-Learner, which can efficiently learn sharp bounds on the CATE function under limits on the level of hidden confounding. The B-Learner can use any function estimator such as random forests and deep neural networks, and we prove its estimates are valid, sharp, efficient, and have a quasi-oracle property with respect to the constituent estimators under more general conditions than existing methods. Semi-synthetic experimental comparisons validate the theoretical findings, and we use real-world data demonstrate how the method might be used in practice.},
  author = {Miruna Oprescu and Jacob Dorn and Marah Ghoummaid and Andrew Jesson and Nathan Kallus and Uri Shalit},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26599--26618},
  pdf = {https://proceedings.mlr.press/v202/oprescu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{B}-Learner: Quasi-Oracle Bounds on Heterogeneous Causal Effects Under Hidden Confounding},
  url = {https://proceedings.mlr.press/v202/oprescu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{orlanski2023measuring,
  abstract = {Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles benchmark that involves translating expert-level python functions to any language. The research investigates if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages.},
  author = {Gabriel Orlanski and Kefan Xiao and Xavier Garcia and Jeffrey Hui and Joshua Howland and Jonathan Malmaud and Jacob Austin and Rishabh Singh and Michele Catasta},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  mdate = {2023-08-28},
  pages = {26619--26645},
  pdf = {https://proceedings.mlr.press/v202/orlanski23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Measuring the Impact of Programming Language Distribution},
  url = {https://proceedings.mlr.press/v202/orlanski23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ortizjime2023when,
  abstract = {Leveraging privileged information (PI), or features available during training but not at test time, has recently been shown to be an effective method for addressing label noise. However, the reasons for its effectiveness are not well understood. In this study, we investigate the role played by different properties of the PI in explaining away label noise. Through experiments on multiple datasets with real PI (CIFAR-N/H) and a new large-scale benchmark ImageNet-PI, we find that PI is most helpful when it allows networks to easily distinguish clean from noisy data, while enabling a learning shortcut to memorize the noisy examples. Interestingly, when PI becomes too predictive of the target label, PI methods often perform worse than their no-PI baselines.},
  author = {Guillermo Ortiz-Jim{\'e}nez and Mark Collier and Anant Nawalgaria and Alexander Nicholas D'Amour and Jesse Berent and Rodolphe Jenatton and Efi Kokiopoulou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ortizjime2023when.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26646--26669},
  pdf = {https://proceedings.mlr.press/v202/ortiz-jimenez23a/ortiz-jimenez23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When does Privileged Information Explain Away Label Noise?},
  url = {https://proceedings.mlr.press/v202/ortiz-jimenez23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{orvieto2023resurrecting,
  abstract = {Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed.},
  author = {Antonio Orvieto and Samuel L. Smith and Albert Gu and Anushan Fernando and {\c{C}}a{\u{g}}lar G{\"u}lçehre and Razvan Pascanu and Soham De},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/orvieto2023resurrecting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26670--26698},
  pdf = {https://proceedings.mlr.press/v202/orvieto23a/orvieto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Resurrecting Recurrent Neural Networks for Long Sequences},
  url = {https://proceedings.mlr.press/v202/orvieto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{oswald2023transformers,
  abstract = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks.},
  author = {Johannes von Oswald and Eyvind Niklasson and Ettore Randazzo and Jo{\~a}o Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/oswald2023transformers.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35151--35174},
  pdf = {https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Transformers Learn In-Context by Gradient Descent},
  url = {https://proceedings.mlr.press/v202/von-oswald23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ouyang2023improving,
  abstract = {Synthetic data generation has become an emerging tool to help improve the adversarial robustness in classification tasks, since robust learning requires a significantly larger amount of training samples compared with standard classification. Among various deep generative models, the diffusion model has been shown to produce high-quality synthetic images and has achieved good performance in improving the adversarial robustness. However, diffusion-type methods are generally slower in data generation as compared with other generative models. Although different acceleration techniques have been proposed recently, it is also of great importance to study how to improve the sample efficiency of synthetic data for the downstream task. In this paper, we first analyze the optimality condition of synthetic distribution for achieving improved robust accuracy. We show that enhancing the distinguishability among the generated data is critical for improving adversarial robustness. Thus, we propose the Contrastive-Guided Diffusion Process (Contrastive-DP), which incorporates the contrastive loss to guide the diffusion model in data generation. We validate our theoretical results using simulations and demonstrate the good performance of Contrastive-DP on image datasets.},
  author = {Yidong Ouyang and Liyan Xie and Guang Cheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ouyang2023improving.pdf:pdf},
  mdate = {2025-03-04},
  pages = {26699--26723},
  pdf = {https://proceedings.mlr.press/v202/ouyang23a/ouyang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Adversarial Robustness Through the Contrastive-Guided Diffusion Process},
  url = {https://proceedings.mlr.press/v202/ouyang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{oymak2023role,
  abstract = {Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model.},
  author = {Samet Oymak and Ankit Singh Rawat and Mahdi Soltanolkotabi and Christos Thrampoulidis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/oymak2023role.pdf:pdf},
  mdate = {2023-11-12},
  pages = {26724--26768},
  pdf = {https://proceedings.mlr.press/v202/oymak23a/oymak23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Role of Attention in Prompt-tuning},
  url = {https://proceedings.mlr.press/v202/oymak23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ozdaglar2023revisiting,
  abstract = {Offline reinforcement learning (RL) aims to find an optimal policy for sequential decision-making using a pre-collected dataset, without further interaction with the environment. Recent theoretical progress has focused on developing sample-efficient offline RL algorithms with various relaxed assumptions on data coverage and function approximators, especially to handle the case with excessively large state-action spaces. Among them, the framework based on the linear-programming (LP) reformulation of Markov decision processes has shown promise: it enables sample-efficient offline RL with function approximation, under only partial data coverage and realizability assumptions on the function classes, with favorable computational tractability. In this work, we revisit the LP framework for offline RL, and provide a new reformulation that advances the existing results in several aspects, relaxing certain assumptions and achieving optimal statistical rates in terms of sample size. Our key enabler is to introduce proper constraints in the reformulation, instead of using any regularization as in the literature, also with careful choices of the function classes and initial state distributions. We hope our insights bring into light the use of LP formulations and the induced primal-dual minimax optimization, in offline RL.},
  author = {Asuman E. Ozdaglar and Sarath Pattathil and Jiawei Zhang and Kaiqing Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ozdaglar2023revisiting.pdf:pdf},
  mdate = {2024-08-15},
  pages = {26769--26791},
  pdf = {https://proceedings.mlr.press/v202/ozdaglar23a/ozdaglar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting the Linear-Programming Framework for Offline {RL} with General Function Approximation},
  url = {https://proceedings.mlr.press/v202/ozdaglar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{padmakumar2023extrapolative,
  abstract = {We study the problem of extrapolative controlled generation, generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are better (e.g., more stable) than existing sequences. By definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approaches despite its simplicity.},
  author = {Vishakh Padmakumar and Richard Yuanzhe Pang and He He and Ankur P. Parikh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/padmakumar2023extrapolative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26792--26808},
  pdf = {https://proceedings.mlr.press/v202/padmakumar23a/padmakumar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Extrapolative Controlled Sequence Generation via Iterative Refinement},
  url = {https://proceedings.mlr.press/v202/padmakumar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pal2023locally,
  abstract = {Neural Differential Equations have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. Controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time-points to guide the training towards learning a dynamical system that is easier to integrate.},
  author = {Avik Pal and Alan Edelman and Christopher Vincent Rackauckas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pal2023locally.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26809--26819},
  pdf = {https://proceedings.mlr.press/v202/pal23a/pal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Locally Regularized Neural Differential Equations: Some Black Boxes were meant to remain closed!},
  url = {https://proceedings.mlr.press/v202/pal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pal2023controlled,
  abstract = {Neural Controlled Differential equations (NCDE) are a powerful mechanism to model the dynamics in temporal sequences, e.g., applications involving physiological measures, where apart from the initial condition, the dynamics also depend on subsequent measures or even a different ``control'' sequence. But NCDEs do not scale well to longer sequences. Existing strategies adapt rough path theory, and instead model the dynamics over summaries known as log signatures. While rigorous and elegant, invertibility of these summaries is difficult, and limits the scope of problems where these ideas can offer strong benefits (reconstruction, generative modeling). For tasks where it is sensible to assume that the (long) sequences in the training data are a fixed length of temporal measurements -- this assumption holds in most experiments tackled in the literature -- we describe an efficient simplification. First, we recast the regression/classification task as an integral transform. We then show how restricting the class of operators (permissible in the integral transform), allows the use of a known algorithm that leverages non-standard Wavelets to decompose the operator. Thereby, our task (learning the operator) radically simplifies. A neural variant of this idea yields consistent improvements across a wide gamut of use cases tackled in existing works. We also describe a novel application on modeling tasks involving coupled differential equations.},
  author = {Sourav Pal and Zhanpeng Zeng and Sathya N. Ravi and Vikas Singh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pal2023controlled.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26820--26836},
  pdf = {https://proceedings.mlr.press/v202/pal23b/pal23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controlled Differential Equations on Long Sequences via Non-standard Wavelets},
  url = {https://proceedings.mlr.press/v202/pal23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pan2023beyond,
  abstract = {Graph neural networks (GNNs) based methods have achieved impressive performance on node clustering task. However, they are designed on the homophilic assumption of graph and clustering on heterophilic graph is overlooked. Due to the lack of labels, it is impossible to first identify a graph as homophilic or heterophilic before a suitable GNN model can be found. Hence, clustering on real-world graph with various levels of homophily poses a new challenge to the graph research community. To fill this gap, we propose a novel graph clustering method, which contains three key components: graph reconstruction, a mixed filter, and dual graph clustering network. To be graph-agnostic, we empirically construct two graphs which are high homophily and heterophily from each data.},
  author = {Erlin Pan and Zhao Kang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pan2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26868--26877},
  pdf = {https://proceedings.mlr.press/v202/pan23b/pan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering},
  url = {https://proceedings.mlr.press/v202/pan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pan2023rewards,
  abstract = {Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models ({LMs}) may incentivize toxicity. So do agents naturally learn to be {Machiavellian}? And how do we measure these behaviors in general-purpose models such as {GPT-4}? Towards answering these questions, we introduce {MACHIAVELLI}, a benchmark of 134 {Choose-Your-Own-Adventure} games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with {LMs}, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate {LM}-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics---designing agents that are {Pareto} improvements in both safety and capabilities.},
  author = {Alexander Pan and Jun Shern Chan and Andy Zou and Nathaniel Li and Steven Basart and Thomas Woodside and Hanlin Zhang 0002 and Scott Emmons and Dan Hendrycks},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pan2023rewards.pdf:pdf},
  mdate = {2025-06-17},
  pages = {26837--26867},
  pdf = {https://proceedings.mlr.press/v202/pan23a/pan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the {MACHIAVELLI} Benchmark},
  url = {https://proceedings.mlr.press/v202/pan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pan2023better,
  abstract = {Generative Flow Networks or {GFlowNets} are related to {Monte-Carlo Markov} chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object $x$ through a sequence of steps with probability proportional to some reward function $R(x)$ (or $\exp(-\mathcal{E}(x))$ with $\mathcal{E}(x)$ denoting the energy function), given at the end of the generative trajectory. Like for other {RL} settings where the reward is only given at the end, the efficiency of training and credit assignment may suffer when those trajectories are longer. With previous {GFlowNet} work, no learning was possible from incomplete trajectories (lacking a terminal state and the computation of the associated reward). In this paper, we consider the case where the energy function can be applied not just to terminal states but also to intermediate states. This is for example achieved when the energy function is additive, with terms available along the trajectory. We show how to reparameterize the {GFlowNet} state flow function to take advantage of the partial reward already accrued at each state. This enables a training objective that can be applied to update parameters even with incomplete trajectories. Even when complete trajectories are available, being able to obtain more localized credit and gradients is found to speed up training convergence, as demonstrated across many simulations.},
  author = {Ling Pan and Nikolay Malkin and Dinghuai Zhang and Yoshua Bengio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pan2023better.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26878--26890},
  pdf = {https://proceedings.mlr.press/v202/pan23c/pan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Better Training of {GFlowNets} with Local Credit and Incomplete Trajectories},
  url = {https://proceedings.mlr.press/v202/pan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pan2023hybrid,
  abstract = {In this paper, we propose a novel {Hadamard} Transform ({HT})-based neural network layer for hybrid quantum-classical computing. It implements the regular convolutional layers in the {Hadamard} transform domain. The idea is based on the {HT} convolution theorem which states that the dyadic convolution between two vectors is equivalent to the element-wise multiplication of their {HT} representation. Computing the {HT} is simply the application of a {Hadamard} gate to each qubit individually, so the {HT} computations of our proposed layer can be implemented on a quantum computer. Compared to the regular {Conv2D} layer, the proposed {HT}-perceptron layer is computationally more efficient. Compared to a {CNN} with the same number of trainable parameters and 99.26\% test accuracy, our {HT} network reaches 99.31\% test accuracy with 57.1\% {MACs} reduced in the {MNIST} dataset; and in our {ImageNet-1K} experiments, our {HT}-based {ResNet-50} exceeds the accuracy of the baseline {ResNet-50} by 0.59\% center-crop top-1 accuracy using 11.5\% fewer parameters with 12.6\% fewer {MACs}.},
  author = {Hongyi Pan and Xin Zhu and Salih Furkan Atici and Ahmet Enis \c{C}etin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pan2023hybrid.pdf:pdf},
  mdate = {2024-01-30},
  pages = {26891--26903},
  pdf = {https://proceedings.mlr.press/v202/pan23d/pan23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Hybrid Quantum-Classical Approach based on the {Hadamard} Transform for the Convolutional Layer},
  url = {https://proceedings.mlr.press/v202/pan23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{panageas2023semi,
  abstract = {In this work, we introduce a new variant of online gradient descent, which provably converges to {Nash} Equilibria and simultaneously attains sublinear regret for the class of congestion games in the semi-bandit feedback setting. Our proposed method admits convergence rates depending only polynomially on the number of players and the number of facilities, but not on the size of the action set, which can be exponentially large in terms of the number of facilities. Moreover, the running time of our method has polynomial-time dependence on the implicit description of the game. Our analysis exploits techniques from convex geometry, in particular {Caratheodory}'s theorem and recent advances in non-convex stochastic optimization.},
  author = {Ioannis Panageas and Stratis Skoulakis and Luca Viano and Xiao Wang 0036 and Volkan Cevher},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/panageas2023semi.pdf:pdf},
  mdate = {2024-02-05},
  pages = {26904--26930},
  pdf = {https://proceedings.mlr.press/v202/panageas23a/panageas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi Bandit Dynamics in Congestion Games: Convergence to {Nash} Equilibrium and No-Regret Guarantees},
  url = {https://proceedings.mlr.press/v202/panageas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{panchal2023flash,
  abstract = {In Federated Learning ({FL}), adaptive optimization is an effective approach to addressing the statistical heterogeneity issue but cannot adapt quickly to concept drifts. In this work, we propose a novel adaptive optimizer called Flash that simultaneously addresses both statistical heterogeneity and the concept drift issues. The fundamental insight is that a concept drift can be detected based on the magnitude of parameter updates that are required to fit the global model to each participating client's local data distribution. Flash uses a two-pronged approach that synergizes client-side early-stopping training to facilitate detection of concept drifts and the server-side drift-aware adaptive optimization to effectively adjust effective learning rate. The authors theoretically prove that Flash matches the convergence rate of state-of-the-art adaptive optimizers and further empirically evaluate the efficacy of Flash on a variety of {FL} benchmarks using different concept drift settings.},
  author = {Kunjal Panchal and Sunav Choudhary and Subrata Mitra and Koyel Mukherjee and Somdeb Sarkhel and Saayan Mitra and Hui Guan 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/panchal2023flash.pdf:pdf},
  mdate = {2024-06-25},
  pages = {26931--26962},
  pdf = {https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Flash: Concept Drift Adaptation in Federated Learning},
  url = {https://proceedings.mlr.press/v202/panchal23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pandey2023learn,
  abstract = {Evidential deep learning offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware by building upon belief theory and subjective logic. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. However, this constraint often leads to inferior predictive performance compared to standard softmax models, and evidential models are only able to achieve predictive performance on par with standard deep architectures in relatively simple learning problems. By leveraging the learned evidence, evidential models are capable of quantifying fine-grained uncertainty that helps to identify the sources of `unknowns'. Furthermore, since only lightweight modifications are introduced to existing {DL} architectures, additional computational costs remain minimum.},
  author = {Deep Shankar Pandey and Qi Yu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pandey2023learn.pdf:pdf},
  mdate = {2023-08-28},
  pages = {26963--26989},
  pdf = {https://proceedings.mlr.press/v202/pandey23a/pandey23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learn to Accumulate Evidence from All Training Samples: Theory and Practice},
  url = {https://proceedings.mlr.press/v202/pandey23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pang2023secure,
  abstract = {The paper proposes the first federated correlation test framework compatible with secure aggregation, namely {FED-χ²}. In their protocol, the statistical computations are recast as frequency moment estimation problems, where the clients collaboratively generate a shared projection matrix and then use stable projection to encode the local information in a compact vector. Since such encodings can be linearly aggregated, secure aggregation can be applied to conceal the individual updates. The authors formally establish the security guarantee of {FED-χ²} by proving that only the minimum necessary information (i.e., the correlation statistics) is revealed to the server. The paper also shows that their protocol can be naturally extended to estimate other statistics that can be recast as frequency moment estimations.},
  author = {Qi Pang and Lun Wang 0001 and Shuai Wang 0011 and Wenting Zheng and Dawn Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pang2023secure.pdf:pdf},
  mdate = {2024-08-08},
  pages = {26990--27010},
  pdf = {https://proceedings.mlr.press/v202/pang23a/pang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Secure Federated Correlation Test and Entropy Estimation},
  url = {https://proceedings.mlr.press/v202/pang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{panigrahi2023taskspecific,
  abstract = {Pre-trained language models can be fine-tuned to solve diverse {NLP} tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (~0.01\% of model parameters) responsible for (>95\%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution (40-90\% error reduction) as well as the quality of predictions out-of-distribution ({OOD}).},
  author = {Abhishek Panigrahi and Nikunj Saunshi and Haoyu Zhao and Sanjeev Arora},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/panigrahi2023taskspecific.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27011--27033},
  pdf = {https://proceedings.mlr.press/v202/panigrahi23a/panigrahi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Task-Specific Skill Localization in Fine-tuned Language Models},
  url = {https://proceedings.mlr.press/v202/panigrahi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023kernel,
  abstract = {The paper addresses the challenge of analyzing compositional data with a large number of components and an abundance of zeros. Most existing approaches lack interpretability or cannot handle zeros properly, as they rely on a log-ratio transformation. The authors approach this problem with sufficient dimension reduction ({SDR}), one of the most studied dimension reduction frameworks in statistics, characterized by the conditional independence of the data to the response on the found subspace. This work proposes a compositional {SDR} that can handle zeros naturally while incorporating the nonlinear nature and spurious negative correlations among components rigorously. The paper also discusses a critical consideration of sub-composition versus amalgamation for compositional variable selection and shows that the proposed compositional {SDR} is statistically consistent in constructing a sub-simplex consisting of true signal variables.},
  author = {Junyoung Park and Jeongyoun Ahn and Cheolwoo Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023kernel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27034--27047},
  pdf = {https://proceedings.mlr.press/v202/park23a/park23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Kernel Sufficient Dimension Reduction and Variable Selection for Compositional Data via Amalgamation},
  url = {https://proceedings.mlr.press/v202/park23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023learning,
  abstract = {Recent approaches to representation learning have successfully demonstrated the benefits in hyperbolic space, driven by an excellent ability to make hierarchical relationships. In this work, we demonstrate that the properties of hyperbolic geometry serve as a valuable alternative to learning hierarchical affinity for spatial propagation tasks. We propose a Hyperbolic Affinity learning Module ({HAM}) to learn spatial affinity by considering geodesic distance on the hyperbolic space. By simply incorporating our {HAM} into conventional spatial propagation tasks, we validate its effectiveness, capturing the pixel hierarchy of affinity maps in hyperbolic space. The proposed methodology can lead to performance improvements in explicit propagation processes such as depth completion and semantic segmentation.},
  author = {Jin-Hwi Park and Jaesung Choe and Inhwan Bae and Hae-Gon Jeon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27048--27073},
  pdf = {https://proceedings.mlr.press/v202/park23b/park23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Affinity with Hyperbolic Representation for Spatial Propagation},
  url = {https://proceedings.mlr.press/v202/park23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023trak,
  abstract = {The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. In this work, we introduce {TRAK} (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, {TRAK} can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of {TRAK} across various modalities and scales: image classifiers trained on {ImageNet}, vision-language models ({CLIP}), and language models ({BERT} and m{T}5).},
  author = {Sung Min Park and Kristian Georgiev and Andrew Ilyas and Guillaume Leclerc and Aleksander Madry},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023trak.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27074-27113},
  pdf = {https://proceedings.mlr.press/v202/park23c/park23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {TRAK: Attributing Model Behavior at Scale},
  url = {https://proceedings.mlr.press/v202/park23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023towards,
  abstract = {Federated Learning ({FL}) is a collaborative machine learning paradigm for data privacy preservation. Recently, a knowledge distillation ({KD}) based information sharing approach in {FL}, which conducts ensemble distillation on an unlabeled public dataset, has been proposed. However, despite its experimental success and usefulness, the theoretical analysis of the {KD} based approach has not been satisfactorily conducted. In this work, we build a theoretical foundation of the ensemble distillation framework in federated learning from the perspective of kernel ridge regression ({KRR}). In this end, we propose a {KD} based {FL} algorithm for {KRR} models which is related with some existing {KD} based {FL} algorithms, and analyze our algorithm theoretically. We show that our algorithm makes local prediction models as much powerful as the centralized {KRR} model (which is a {KRR} model trained by all of local datasets) in terms of the convergence rate of the generalization error if the unlabeled public dataset is sufficiently large. We also provide experimental results to verify our theoretical results on ensemble distillation in federated learning.},
  author = {Sejun Park and Kihun Hong and Ganguk Hwang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27132-27187},
  pdf = {https://proceedings.mlr.press/v202/park23e/park23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Understanding Ensemble Distillation in Federated Learning},
  url = {https://proceedings.mlr.press/v202/park23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023testtime,
  abstract = {In domain generalization ({DG}), the target domain is unknown when the model is being trained, and the trained model should successfully work on an arbitrary (and possibly unseen) target domain during inference. This is a difficult problem, and despite active studies in recent years, it remains a great challenge. In this paper, we take a simple yet effective approach to tackle this issue. We propose test-time style shifting, which shifts the style of the test sample (that has a large style gap with the source domains) to the nearest source domain that the model is already familiar with, before making the prediction. This strategy enables the model to handle any target domains with arbitrary style statistics, without additional model update at test-time. Additionally, we propose style balancing, which provides a great platform for maximizing the advantage of test-time style shifting by handling the {DG}-specific imbalance issues. The proposed ideas are easy to implement and successfully work in conjunction with various other {DG} schemes. Experimental results on different datasets show the effectiveness of our methods.},
  author = {Jungwuk Park and Dong-Jun Han and Soyeong Kim and Jaekyun Moon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023testtime.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27114-27131},
  pdf = {https://proceedings.mlr.press/v202/park23d/park23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization},
  url = {https://proceedings.mlr.press/v202/park23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023learning,
  abstract = {Recent deep-learning-based super-resolution ({SR}) methods have been successful in recovering high-resolution ({HR}) images from their low-resolution ({LR}) counterparts, albeit on the synthetic and simple degradation setting: bicubic downscaling. On the other hand, super-resolution on real-world images demands the capability to handle complex downscaling mechanism which produces different artifacts (e.g., noise, blur, color distortion) upon downscaling factors. To account for complex downscaling mechanism in real-world {LR} images, there have been a few efforts in constructing datasets consisting of {LR} images with real-world downsampling degradation. However, making such datasets entails a tremendous amount of time and effort, thereby resorting to very few number of downscaling factors (e.g., $\times2, \times3, \times4$). To remedy the issue, we propose to generate realistic {SR} datasets for unseen degradation levels by exploring the latent space of real {LR} images and thereby producing more diverse yet realistic {LR} images with complex real-world artifacts. Our quantitative and qualitative experiments demonstrate the accuracy of the generated {LR} images, and we show that the various conventional {SR} networks trained with our newly generated {SR} datasets can produce much better {HR} images.},
  author = {Seobin Park and Dongjin Kim 0004 and Sungyong Baik and Tae Hyun Kim 0006},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023learning.pdf:pdf},
  mdate = {2024-11-14},
  pages = {27188-27203},
  pdf = {https://proceedings.mlr.press/v202/park23f/park23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Controllable Degradation for Real-World Super-Resolution via Constrained Flows},
  url = {https://proceedings.mlr.press/v202/park23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023predictable,
  abstract = {A key component of model-based reinforcement learning ({RL}) is a dynamics model that predicts the outcomes of actions. Errors in this predictive model can degrade the performance of model-based controllers, and complex {M}arkov decision processes ({MDP}s) can present exceptionally difficult prediction problems. To mitigate this issue, we propose predictable {MDP} abstraction ({PMA}): instead of training a predictive model on the original {MDP}, we train a model on a transformed {MDP} with a learned action space that only permits predictable, easy-to-model actions, while covering the original state-action space as much as possible. As a result, model learning becomes easier and more accurate, which allows robust, stable model-based planning or model-based {RL}. This transformation is learned in an unsupervised manner, before any task is specified by the user. Downstream tasks can then be solved with model-based control in a zero-shot fashion, without additional environment interactions. We theoretically analyze {PMA} and empirically demonstrate that {PMA} leads to significant improvements over prior unsupervised model-based {RL} approaches in a range of benchmark environments.},
  author = {Seohong Park and Sergey Levine},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023predictable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27246-27268},
  pdf = {https://proceedings.mlr.press/v202/park23i/park23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Predictable MDP Abstraction for Unsupervised Model-Based RL},
  url = {https://proceedings.mlr.press/v202/park23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023controllabilityaware,
  abstract = {One of the key capabilities of intelligent agents is the ability to discover useful skills without external supervision. However, the current unsupervised skill discovery methods are often limited to acquiring simple, easy-to-learn skills due to the lack of incentives to discover more complex, challenging behaviors. We introduce a novel unsupervised skill discovery method, Controllability-aware Skill Discovery ({CSD}), which actively seeks complex, hard-to-control skills without supervision. The key component of {CSD} is a controllability-aware distance function, which assigns larger values to state transitions that are harder to achieve with the current skills. Combined with distance-maximizing skill discovery, {CSD} progressively learns more challenging skills over the course of training as our jointly trained distance function reduces rewards for easy-to-achieve skills. Our experimental results in six robotic manipulation and locomotion environments demonstrate that {CSD} can discover diverse complex skills including object manipulation and locomotion skills with no supervision, significantly outperforming prior unsupervised skill discovery methods.},
  author = {Seohong Park and Kimin Lee and Youngwoon Lee and Pieter Abbeel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023controllabilityaware.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27225-27245},
  pdf = {https://proceedings.mlr.press/v202/park23h/park23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controllability-Aware Unsupervised Skill Discovery},
  url = {https://proceedings.mlr.press/v202/park23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{park2023neural,
  abstract = {Modeling spatiotemporal dynamics with neural differential equations has become a major line of research that opens new ways to handle various real-world scenarios (e.g., missing observations, irregular times, etc.), but most existing methods still face challenges in providing a general framework for analyzing time series. To tackle this issue, we adopt stochastic differential games to suggest a new philosophy of utilizing interacting collective intelligence in time series analysis. For implementation, we develop a novel gradient descent-based algorithm called deep neural fictitious play to approximate the {N}ash equilibrium. We theoretically analyze the convergence result of the proposed algorithm and discuss the advantage of cooperative games in handling noninformative observation.},
  author = {Sungwoo Park and Byoungwoo Park and Moontae Lee and Changhee Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/park2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27269-27293},
  pdf = {https://proceedings.mlr.press/v202/park23j/park23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Stochastic Differential Games for Time-series Analysis},
  url = {https://proceedings.mlr.press/v202/park23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023accelerated,
  abstract = {As first-order optimization methods become the method of choice for solving large-scale optimization problems, optimization solvers based on first-order algorithms are being built. Such general-purpose solvers must robustly detect infeasible or misspecified problem instances, but the computational complexity of first-order methods for doing so has yet to be formally studied. In this work, we characterize the optimal accelerated rate of infeasibility detection. We show that the standard fixed-point iteration achieves a $\mathcal{O}(1/k^2)$ and $\mathcal{O}(1/k)$ rates, respectively, on the normalized iterates and the fixed-point residual converging to the infimal displacement vector, while the accelerated fixed-point iteration achieves $\mathcal{O}(1/k^2)$ and $\tilde{\mathcal{O}}(1/k^2)$ rates. We then provide a matching complexity lower bound to establish that $\Theta(1/k^2)$ is indeed the optimal accelerated rate.},
  author = {Jisun Park 0003 and Ernest K. Ryu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023accelerated.pdf:pdf},
  mdate = {2025-03-10},
  pages = {27294-27345},
  pdf = {https://proceedings.mlr.press/v202/park23k/park23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accelerated Infeasibility Detection of Constrained Optimization and Fixed-Point Iterations},
  url = {https://proceedings.mlr.press/v202/park23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{parmas2023modelbased,
  abstract = {In model-based reinforcement learning ({MBRL}), policy gradients can be estimated either by derivative-free {RL} methods, such as likelihood ratio gradients ({LR}), or by backpropagating through a differentiable model via reparameterization gradients ({RP}). Instead of using one or the other, the Total Propagation ({TP}) algorithm in prior work showed that a combination of {LR} and {RP} estimators averaged using inverse variance weighting ({IVW}) can achieve orders of magnitude improvement over either method. However, {IVW}-based composite estimators have not yet been applied in modern {RL} tasks, as it is unclear if they can be implemented scalably. We propose a scalable method, Total Propagation X ({TPX}) that improves over {TP} by changing the node used for {IVW}, and employing coordinate wise weighting. We demonstrate the scalability of {TPX} by applying it to the state of the art visual {MBRL} algorithm Dreamer. The experiments showed that Dreamer fails with long simulation horizons, while our {TPX} works reliably for only a fraction of additional computation.},
  author = {Paavo Parmas and Takuma Seno and Yuma Aoki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/parmas2023modelbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27346-27377},
  pdf = {https://proceedings.mlr.press/v202/parmas23a/parmas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-based Reinforcement Learning with Scalable Composite Policy Gradient Estimators},
  url = {https://proceedings.mlr.press/v202/parmas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{parulekar2023pac,
  abstract = {One method for obtaining generalizable solutions to machine learning tasks when presented with diverse training environments is to find invariant representations of the data. These are representations of the covariates such that the best model on top of the representation is invariant across training environments. In the context of linear Structural Equation Models ({SEM}s), invariant representations might allow us to learn models with out-of-distribution guarantees, i.e., models that are robust to interventions in the {SEM}. To address the invariant representation problem in a finite sample setting, we consider the notion of $\epsilon$-approximate invariance. We study the following question: If a representation is approximately invariant with respect to a given number of training interventions, will it continue to be approximately invariant on a larger collection of unseen {SEM}s? This larger collection of {SEM}s is generated through a parameterized family of interventions. Inspired by {PAC} learning, we obtain finite-sample out-of-distribution generalization guarantees for approximate invariance that holds probabilistically over a family of linear {SEM}s without faithfulness assumptions. Our results show bounds that do not scale in ambient dimension when intervention sites are restricted to lie in a constant size subset of in-degree bounded nodes. We also show how to extend our results to a linear indirect observation model that incorporates latent variables.},
  author = {Advait U. Parulekar and Karthikeyan Shanmugam and Sanjay Shakkottai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/parulekar2023pac.pdf:pdf},
  mdate = {2023-09-30},
  pages = {27378-27400},
  pdf = {https://proceedings.mlr.press/v202/parulekar23a/parulekar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PAC Generalization via Invariant Representations},
  url = {https://proceedings.mlr.press/v202/parulekar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pashakhanloo2023stochastic,
  abstract = {Representational drift refers to over-time changes in neural activation accompanied by a stable task performance. Despite being observed in the brain and in artificial networks, the mechanisms of drift and its implications are not fully understood. Motivated by recent experimental findings of stimulus-dependent drift in the piriform cortex, we use theory and simulations to study this phenomenon in a two-layer linear feedforward network. Specifically, in a continual online learning scenario, we study the drift induced by the noise inherent in the Stochastic Gradient Descent (SGD). By decomposing the learning dynamics into the normal and tangent spaces of the minimum-loss manifold, we show the former corresponds to a finite variance fluctuation, while the latter could be considered as an effective diffusion process on the manifold. We analytically compute the fluctuation and the diffusion coefficients for the stimuli representations in the hidden layer as functions of network parameters and input distribution. Further, consistent with experiments, we show that the drift rate is slower for a more frequently presented stimulus. Overall, our analysis yields a theoretical framework for better understanding of the drift phenomenon in biological and artificial neural networks.},
  author = {Farhad Pashakhanloo and Alexei A. Koulakov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pashakhanloo2023stochastic.pdf:pdf},
  mdate = {2025-04-22},
  pages = {27401--27419},
  pdf = {https://proceedings.mlr.press/v202/pashakhanloo23a/pashakhanloo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stochastic Gradient Descent-Induced Drift of Representation in a Two-Layer Neural Network},
  url = {https://proceedings.mlr.press/v202/pashakhanloo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{passaro2023reducing,
  abstract = {Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be SO(3) equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the SO(3) convolutions or tensor products to mathematically equivalent convolutions in SO(2). This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from O(L^6) to O(L^3), where L is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network (eSCN), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale OC-20 and OC-22 datasets.},
  author = {Saro Passaro and C. Lawrence Zitnick},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/passaro2023reducing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27420--27438},
  pdf = {https://proceedings.mlr.press/v202/passaro23a/passaro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs},
  url = {https://proceedings.mlr.press/v202/passaro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{patel2023federated,
  abstract = {We study the problems of distributed online and bandit convex optimization against an adaptive adversary. We aim to minimize the average regret on M machines working in parallel over T rounds with R intermittent communications. Assuming the underlying cost functions are convex and can be generated adaptively, our results show that collaboration is not beneficial when the machines have access to the first-order gradient information at the queried points. This is in contrast to the case for stochastic functions, where each machine samples the cost functions from a fixed distribution. Furthermore, we delve into the more challenging setting of federated online optimization with bandit (zeroth-order) feedback, where the machines can only access values of the cost functions at the queried points. The key finding here is identifying the high-dimensional regime where collaboration is beneficial and may even lead to a linear speedup in the number of machines. We further illustrate our findings through federated adversarial linear bandits by developing novel distributed single and two-point feedback algorithms. Our work is the first attempt towards a systematic understanding of federated online optimization with limited feedback, and it attains tight regret bounds in the intermittent communication setting for both first and zeroth-order feedback. Our results thus bridge the gap between stochastic and adaptive settings in federated online optimization.},
  author = {Kumar Kshitij Patel and Lingxiao Wang and Aadirupa Saha and Nathan Srebro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/patel2023federated.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27439--27460},
  pdf = {https://proceedings.mlr.press/v202/patel23a/patel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Federated Online and Bandit Convex Optimization},
  url = {https://proceedings.mlr.press/v202/patel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pearcecrump2023brauers,
  abstract = {We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$ for three symmetry groups that are missing from the machine learning literature: $O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and $Sp(n)$, the symplectic group. In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$ when the group is $O(n)$ or $SO(n)$, and in the symplectic basis of $\mathbb{R}^{n}$ when the group is $Sp(n)$.},
  author = {Edward Pearce-Crump},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pearcecrump2023brauers.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27461--27482},
  pdf = {https://proceedings.mlr.press/v202/pearce-crump23a/pearce-crump23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Brauer's Group Equivariant Neural Networks},
  url = {https://proceedings.mlr.press/v202/pearce-crump23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pearcecrump2023how,
  abstract = {We provide a full characterisation of all of the possible alternating group ($A_n$) equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a basis of matrices for the learnable, linear, $A_n$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$. We also describe how our approach generalises to the construction of neural networks that are equivariant to local symmetries.},
  author = {Edward Pearce-Crump},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pearcecrump2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27483--27495},
  pdf = {https://proceedings.mlr.press/v202/pearce-crump23b/pearce-crump23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Jellyfish Characterise Alternating Group Equivariant Neural Networks},
  url = {https://proceedings.mlr.press/v202/pearce-crump23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pei2023can,
  abstract = {Identifying invariants is an important program analysis task with applications towards program understanding, bug finding, vulnerability analysis, and formal verification. Existing tools for identifying program invariants rely on dynamic analysis, requiring traces collected from multiple executions in order to produce reliable invariants. We study the application of large language models to invariant prediction, finding that models trained on source code and fine-tuned for invariant generation can perform invariant prediction as static rather than dynamic analysis. Using a scratchpad approach where invariants are predicted sequentially through a program gives the best performance, finding invariants statically of quality comparable to those obtained by a dynamic analysis tool with access to five program traces.},
  author = {Kexin Pei and David Bieber and Kensen Shi and Charles Sutton and Pengcheng Yin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pei2023can.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27496--27520},
  pdf = {https://proceedings.mlr.press/v202/pei23a/pei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Can Large Language Models Reason about Program Invariants?},
  url = {https://proceedings.mlr.press/v202/pei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pei2023dynamicsinspired,
  abstract = {This paper investigates the dynamics-inspired neuromorphic architecture for visual representation learning following Hamilton's principle. Our method converts weight-based neural structure to its dynamics-based form that consists of finite sub-models, whose mutual relations measured by computing path integrals amongst their dynamical states are equivalent to the typical neural weights. Based on the entropy reduction process derived from the Euler-Lagrange equations, the feedback signals interpreted as stress forces amongst sub-models push them to move based on the entropy-reduction process. The method first trains dynamics-based neural models from scratch and shows they outperform traditional feedforward networks on MNIST. Then it converts several pre-trained neural structures (e.g., DenseNet, ResNet, Transformers, etc.) into dynamics-based forms, followed by fine-tuning via entropy-reduction to obtain the stabilized dynamic states. We observe consistent improvements of these transformed models on the ImageNet dataset in terms of computational complexity, the number of trainable units, testing accuracy, and robustness.},
  author = {Zhengqi Pei and Shuhui Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pei2023dynamicsinspired.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27521--27541},
  pdf = {https://proceedings.mlr.press/v202/pei23b/pei23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dynamics-inspired Neuromorphic Visual Representation Learning},
  url = {https://proceedings.mlr.press/v202/pei23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{peltonen2023fair,
  abstract = {We consider fairness in dimensionality reduction. Nonlinear dimensionality reduction yields low dimensional representations that let users visualize and explore high-dimensional data. However, traditional dimensionality reduction may yield biased visualizations overemphasizing relationships of societal phenomena to sensitive attributes or protected groups. We introduce a framework of fair neighbor embedding, the Fair Neighbor Retrieval Visualizer, which formulates fair nonlinear dimensionality reduction as an information retrieval task whose performance and fairness are quantified by information retrieval criteria. The method optimizes low-dimensional embeddings that preserve high-dimensional data neighborhoods without yielding biased association of such neighborhoods to protected groups.},
  author = {Jaakko Peltonen and Wen Xu and Timo Nummenmaa and Jyrki Nummenmaa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/peltonen2023fair.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27564--27584},
  pdf = {https://proceedings.mlr.press/v202/peltonen23a/peltonen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair Neighbor Embedding},
  url = {https://proceedings.mlr.press/v202/peltonen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{peng2023moldiff,
  abstract = {Deep generative models have recently achieved superior performance in 3D molecule generation. Most of them first generate atoms and then add chemical bonds based on the generated atoms in a post-processing manner. However, there might be no corresponding bond solution for the temporally generated atoms as their locations are generated without considering potential bonds. We define this problem as the atom-bond inconsistency problem and claim it is the main reason for current approaches to generating unrealistic 3D molecules. To overcome this problem, we propose a new diffusion model called MolDiff which can generate atoms and bonds simultaneously while still maintaining their consistency by explicitly modeling the dependence between their relationships. We evaluated the generation ability of our proposed model and the quality of the generated molecules using criteria related to both geometry and chemical properties. The empirical studies showed that our model outperforms previous approaches, achieving a three-fold improvement in success rate and generating molecules with significantly better quality.},
  author = {Xingang Peng and Jiaqi Guan and Qiang Liu 0001 and Jianzhu Ma},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/peng2023moldiff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27611--27629},
  pdf = {https://proceedings.mlr.press/v202/peng23b/peng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation},
  url = {https://proceedings.mlr.press/v202/peng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{peng2023ideal,
  abstract = {The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may "forget" how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called "Ideal Continual Learner" (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods.},
  author = {Liangzu Peng and Paris Giampouras and Ren{\'e} Vidal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/peng2023ideal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27585--27610},
  pdf = {https://proceedings.mlr.press/v202/peng23a/peng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Ideal Continual Learner: An Agent That Never Forgets},
  url = {https://proceedings.mlr.press/v202/peng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{peng2023diagnosis,
  abstract = {Policies often fail at test-time due to "distribution shifts"—changes in the state and reward that occur when an end user deploys the policy in environments different from those seen in training. Data augmentation can help models be more robust to such shifts by varying specific concepts in the state, e.g. object color, that are "task-irrelevant" and should not impact desired actions. However, designers training the agent don't often know which concepts are irrelevant "a priori". The authors propose a human-in-the-loop framework to leverage feedback from the end user to quickly identify and augment task-irrelevant visual state concepts. Their framework generates "counterfactual demonstrations" that allow users to quickly isolate shifted state concepts and identify if they should not impact the desired task, and can therefore be augmented using existing actions. They present experiments validating their full pipeline on discrete and continuous control tasks with real human users. Their method better enables users to (1) understand agent failure, (2) improve sample efficiency of demonstrations required for finetuning, and (3) adapt the agent to their desired reward.},
  author = {Andi Peng and Aviv Netanyahu and Mark K. Ho and Tianmin Shu and Andreea Bobu and Julie Shah and Pulkit Agrawal 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/peng2023diagnosis.pdf:pdf},
  mdate = {2024-10-06},
  pages = {27630--27641},
  pdf = {https://proceedings.mlr.press/v202/peng23c/peng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation},
  url = {https://proceedings.mlr.press/v202/peng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{perets2023learning,
  abstract = {The Hidden Markov Model (HMM) is one of the most widely used statistical models for sequential data analysis. One of the key reasons for this versatility is the ability of HMM to deal with missing data. However, standard HMM learning algorithms rely crucially on the assumption that the positions of the missing observations within the observation sequence are known. In the natural sciences, where this assumption is often violated, special variants of HMM, commonly known as Silent-state HMMs (SHMMs), are used. Despite their widespread use, these algorithms strongly rely on specific structural assumptions of the underlying chain, such as acyclicity, thus limiting the applicability of these methods. Moreover, even in the acyclic case, it has been shown that these methods can lead to poor reconstruction. In this paper we consider the general problem of learning an HMM from data with unknown missing observation locations. We provide reconstruction algorithms that do not require any assumptions about the structure of the underlying chain, and can also be used with limited prior knowledge, unlike SHMM. We evaluate and compare the algorithms in a variety of scenarios, measuring their reconstruction precision, and robustness under model miss-specification. Notably, we show that under proper specifications one can reconstruct the process dynamics as well as if the missing observations positions were known.},
  author = {Binyamin Perets and Mark Kozdoba and Shie Mannor},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/perets2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27642--27667},
  pdf = {https://proceedings.mlr.press/v202/perets23a/perets23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Hidden Markov Models When the Locations of Missing Observations are Unknown},
  url = {https://proceedings.mlr.press/v202/perets23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{perini2023estimating,
  abstract = {Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding, so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor of a given unlabeled dataset. We leverage on outputs of several anomaly detectors as a representation that already captures the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the posterior mean improves the anomaly detectors' performance over several alternative methods. All code is publicly available for full reproducibility.},
  author = {Lorenzo Perini and Paul-Christian B{\"u}rkner and Arto Klami},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/perini2023estimating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27668--27679},
  pdf = {https://proceedings.mlr.press/v202/perini23a/perini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection},
  url = {https://proceedings.mlr.press/v202/perini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pesce2023gaussian,
  abstract = {In this manuscript we consider the problem of generalized linear estimation on Gaussian mixture data with labels given by a single-index model. Our first result is a sharp asymptotic expression for the test and training errors in the high-dimensional regime. Motivated by the recent stream of results on the Gaussian universality of the test and training errors in generalized linear estimation, we ask ourselves the question: 'when is a single Gaussian enough to characterize the error?'. Our formula allow us to give sharp answers to this question, both in the positive and negative directions. More precisely, we show that the sufficient conditions for Gaussian universality (or lack of thereof) crucially depend on the alignment between the target weights and the means and covariances of the mixture clusters, which we precisely quantify. In the particular case of least-squares interpolation, we prove a strong universality property of the training error, and show it follows a simple, closed-form expression. Finally, we apply our results to real datasets, clarifying some recent discussion in the literature about Gaussian universality of the errors in this context.},
  author = {Luca Pesce and Florent Krzakala and Bruno Loureiro and Ludovic Stephan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pesce2023gaussian.pdf:pdf},
  mdate = {2024-02-05},
  pages = {27680--27708},
  pdf = {https://proceedings.mlr.press/v202/pesce23a/pesce23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Gaussian Data All You Need? The Extents and Limits of Universality in High-Dimensional Generalized Linear Estimation},
  url = {https://proceedings.mlr.press/v202/pesce23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{petrov2023certifying,
  abstract = {Improving and guaranteeing the robustness of deep learning models has been a topic of intense research. Ensembling, which combines several classifiers to provide a better model, has shown to be beneficial for generalisation, uncertainty estimation, calibration, and mitigating the effects of concept drift. However, the impact of ensembling on certified robustness is less well understood. In this work, we generalise Lipschitz continuity by introducing S-Lipschitz classifiers, which we use to analyse the theoretical robustness of ensembles. Our results are precise conditions when ensembles of robust classifiers are more robust than any constituent classifier, as well as conditions when they are less robust.},
  author = {Aleksandar Petrov and Francisco Eiras and Amartya Sanyal and Philip H. S. Torr and Adel Bibi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/petrov2023certifying.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27709--27736},
  pdf = {https://proceedings.mlr.press/v202/petrov23a/petrov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Certifying Ensembles: A General Certification Theory with S-Lipschitzness},
  url = {https://proceedings.mlr.press/v202/petrov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pfrommer2023power,
  abstract = {A common pipeline in learning-based control is to iteratively estimate a model of system dynamics, and apply a trajectory optimization algorithm - e.g. iLQR - on the learned model to minimize a target cost. This paper conducts a rigorous analysis of a simplified variant of this strategy for general nonlinear systems. We analyze an algorithm which iterates between estimating local linear models of nonlinear system dynamics and performing iLQR-like policy updates. We demonstrate that this algorithm attains sample complexity polynomial in relevant problem parameters, and, by synthesizing locally stabilizing gains, overcomes exponential dependence in problem horizon. Experimental results validate the performance of our algorithm, and compare to natural deep-learning baselines.},
  author = {Daniel Pfrommer and Max Simchowitz and Tyler Westenbroek and Nikolai Matni and Stephen Tu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pfrommer2023power.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27737--27821},
  pdf = {https://proceedings.mlr.press/v202/pfrommer23a/pfrommer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Power of Learned Locally Linear Models for Nonlinear Policy Optimization},
  url = {https://proceedings.mlr.press/v202/pfrommer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pham2023scalable,
  abstract = {We consider the problem of solving large-scale instances of the Max-Cut semidefinite program (SDP), i.e., optimizing a linear function over n×n positive semidefinite (PSD) matrices with unit diagonal. When the cost matrix is PSD, we show how to exactly reformulate the problem as maximizing a smooth concave function over PSD matrices with unit trace. By applying the Frank-Wolfe method, we obtain a simple algorithm that is compatible with recent sampling-based techniques to solve SDPs using low memory. We demonstrate the practical performance of our method on 10^6×10^6 instances of the max-cut SDP with costs having up to 5 × 10^6 non-zero entries. Theoretically, we show that our method solves problems with diagonally dominant costs to relative error ε in O(nε^(-1)) calls to a randomized approximate largest eigenvalue subroutine, each of which succeeds with high probability after O(log(n)ε^(-1/2)) matrix-vector multiplications with the cost matrix.},
  author = {Chi Bach Pham and Wynita M. Griggs and James Saunderson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pham2023scalable.pdf:pdf},
  mdate = {2025-06-23},
  pages = {27822--27839},
  pdf = {https://proceedings.mlr.press/v202/pham23a/pham23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Scalable Frank-Wolfe-Based Algorithm for the Max-Cut SDP},
  url = {https://proceedings.mlr.press/v202/pham23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{phan2023attentionbased,
  abstract = {Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC.},
  author = {Thomy Phan and Fabian Ritz and Philipp Altmann and Maximilian Zorn and Jonas N{\"o}lein and Michael K{\"o}lle 0001 and Thomas Gabor and Claudia Linnhoff-Popien},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/phan2023attentionbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27840--27853},
  pdf = {https://proceedings.mlr.press/v202/phan23a/phan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability},
  url = {https://proceedings.mlr.press/v202/phan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{phang2023hypertuning,
  abstract = {Fine-tuning large language models for different tasks can be costly and inefficient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: first, hyperpretraining with a modified conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task fine-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks.},
  author = {Jason Phang and Yi Mao and Pengcheng He and Weizhu Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/phang2023hypertuning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27854--27875},
  pdf = {https://proceedings.mlr.press/v202/phang23a/phang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {HyperTuning: Toward Adapting Large Language Models without Back-propagation},
  url = {https://proceedings.mlr.press/v202/phang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pinson2023linear,
  abstract = {Our theoretical understanding of the inner workings of general convolutional neural networks (CNN) is limited. We here present a new stepping stone towards such understanding in the form of a theory of learning in linear CNNs. By analyzing the gradient descent equations, we discover that using convolutions leads to a mismatch between the dataset structure and the network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, stage-like transitions, and that the speed of discovery changes depending on this structural mismatch. Moreover, we find that the mismatch lies at the heart of what we call the 'dominant frequency bias', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset.},
  author = {Hannah Pinson and Joeri Lenaerts and Vincent Ginis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pinson2023linear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27876--27906},
  pdf = {https://proceedings.mlr.press/v202/pinson23a/pinson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies},
  url = {https://proceedings.mlr.press/v202/pinson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{andre2023tuning,
  abstract = {Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness to improve generic models pretrained to imitate example outputs across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.},
  author = {Andr{\'{e}} Susano Pinto and Alexander Kolesnikov and Yuge Shi and Lucas Beyer and Xiaohua Zhai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/andre2023tuning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33229--33239},
  pdf = {https://proceedings.mlr.press/v202/susano-pinto23a/susano-pinto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tuning Computer Vision Models With Task Rewards},
  url = {https://proceedings.mlr.press/v202/susano-pinto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{plassier2023conformal,
  abstract = {Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.},
  author = {Vincent Plassier and Mehdi Makni and Aleksandr Rubashevskii and Eric Moulines and Maxim Panov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/plassier2023conformal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27907--27947},
  pdf = {https://proceedings.mlr.press/v202/plassier23a/plassier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conformal Prediction for Federated Uncertainty Quantification Under Label Shift},
  url = {https://proceedings.mlr.press/v202/plassier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{podina2023universal,
  abstract = {In this work we perform symbolic discovery of differential operators in a situation where there is sparse experimental data. This small data regime in machine learning can be made tractable by providing our algorithms with prior information about the underlying dynamics. Physics Informed Neural Networks (PINNs) have been very successful in this regime (reconstructing entire ODE solutions using only a single point or entire PDE solutions with very few measurements of the initial condition). The Universal PINN approach (UPINN) adds a neural network that learns a representation of unknown hidden terms in the differential equation. The algorithm yields both a surrogate solution to the differential equation and a black-box representation of the hidden terms. These hidden term neural networks can then be converted into symbolic equations using symbolic regression techniques like AI Feynman. In order to achieve convergence of the neural networks, we provide our algorithms with (noisy) measurements of both the initial condition as well as (synthetic) experimental data obtained at later times. We demonstrate strong performance of UPINNs even when provided with very few measurements of noisy data in both the ODE and PDE regime.},
  author = {Lena Podina and Brydon Eastman and Mohammad Kohandel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/podina2023universal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27948--27956},
  pdf = {https://proceedings.mlr.press/v202/podina23a/podina23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Universal Physics-Informed Neural Networks: Symbolic Differential Operator Discovery with Sparse Data},
  url = {https://proceedings.mlr.press/v202/podina23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{podkopaev2023sequential,
  abstract = {Independence testing is a fundamental problem in statistics and machine learning. Classical batch tests are not well-suited to streaming data that are collected sequentially and processed online. Sequential approaches can stop data collection early when the test result is already clear, and they can also continuously monitor streaming data while controlling the false discovery rate. We design sequential kernelized independence tests inspired by kernelized dependence measures such as the Hilbert-Schmidt independence criterion and the constrained covariance criterion. Our approach is based on testing by betting, which allows for valid inference even in non-i.i.d. time-varying settings. We demonstrate the power of our approach on both simulated and real data.},
  author = {Aleksandr Podkopaev and Patrick Bl{\"o}baum and Shiva Prasad Kasiviswanathan and Aaditya Ramdas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/podkopaev2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27957--27993},
  pdf = {https://proceedings.mlr.press/v202/podkopaev23a/podkopaev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Kernelized Independence Testing},
  url = {https://proceedings.mlr.press/v202/podkopaev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{poiani2023truncating,
  abstract = {In Reinforcement Learning (RL), an agent acts in an unknown environment to maximize the expected cumulative discounted sum of an external reward signal. In practice, in many tasks of interest, such as policy optimization, the agent usually spends its interaction budget by collecting episodes of fixed length within a simulator. However, given the discounted nature of the RL objective, this data collection strategy might not be the best option.},
  author = {Riccardo Poiani and Alberto Maria Metelli and Marcello Restelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/poiani2023truncating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {27994--28042},
  pdf = {https://proceedings.mlr.press/v202/poiani23a/poiani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Truncating Trajectories in Monte Carlo Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/poiani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{poli2023hyena,
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers at scale, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In challenging reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-space models, transfer functions, and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets WikiText103 and The Pile, reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2k. Hyena operators are 2x faster than highly optimized attention at sequence length 8k, with speedups of 100x at 64k.},
  author = {Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Re},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/poli2023hyena.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28043--28078},
  pdf = {https://proceedings.mlr.press/v202/poli23a/poli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  url = {https://proceedings.mlr.press/v202/poli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pollaci2023spurious,
  abstract = {Neural networks constitute a class of functions that are typically non-surjective, with high-dimensional fibers and complicated image. We prove two main results concerning the geometry of the loss landscape of a neural network.},
  author = {Samuele Pollaci},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pollaci2023spurious.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28079--28099},
  pdf = {https://proceedings.mlr.press/v202/pollaci23a/pollaci23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Spurious Valleys and Clustering Behavior of Neural Networks},
  url = {https://proceedings.mlr.press/v202/pollaci23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pooladian2023multisample,
  abstract = {Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. This method reduces gradient variance, obtains straighter flows, generates high-quality samples with fewer evaluations, and produces low-cost transport maps in high dimensions.},
  author = {Aram-Alexandre Pooladian and Heli Ben-Hamu and Carles Domingo-Enrich and Brandon Amos and Yaron Lipman and Ricky T. Q. Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pooladian2023multisample.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28100--28127},
  pdf = {https://proceedings.mlr.press/v202/pooladian23a/pooladian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  url = {https://proceedings.mlr.press/v202/pooladian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{pooladian2023minimax,
  abstract = {We consider the problem of estimating the optimal transport map between two probability distributions, $P$ and $Q$ in $\mathbb{R}^d$, on the basis of i.i.d. samples. All existing statistical analyses of this problem require the assumption that the transport map is Lipschitz, a strong requirement that, in particular, excludes any examples where the transport map is discontinuous. As a first step towards developing estimation procedures for discontinuous maps, we consider the important special case where the data distribution $Q$ is a discrete measure supported on a finite number of points in $\mathbb{R}^d$. We study a computationally efficient estimator initially proposed by (Pooladian \& Niles-Weed, 2021), based on entropic optimal transport, and show in the semi-discrete setting that it converges at the minimax-optimal rate $n^{-1/2}$, independent of dimension. Other standard map estimation techniques both lack finite-sample guarantees in this setting and provably suffer from the curse of dimensionality. We confirm these results in numerical experiments, and provide experiments for other settings, not covered by our theory, which indicate that the entropic estimator is a promising methodology for other discontinuous transport map estimation problems.},
  author = {Aram-Alexandre Pooladian and Vincent Divol and Jonathan Niles-Weed},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/pooladian2023minimax.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28128--28150},
  pdf = {https://proceedings.mlr.press/v202/pooladian23b/pooladian23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Minimax estimation of discontinuous optimal transport maps: The semi-discrete case},
  url = {https://proceedings.mlr.press/v202/pooladian23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{prabhudesai2023testtime,
  abstract = {Current visual detectors, though impressive within their training distribution, often fail to parse out-of-distribution scenes into their constituent entities. Recent test-time adaptation methods use auxiliary self-supervised losses to adapt the network parameters to each test example independently and have shown promising results towards generalization outside the training distribution for the task of image classification.},
  author = {Mihir Prabhudesai and Anirudh Goyal and Sujoy Paul and Sjoerd van Steenkiste and Mehdi S. M. Sajjadi and Gaurav Aggarwal and Thomas Kipf and Deepak Pathak and Katerina Fragkiadaki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/prabhudesai2023testtime.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28151--28166},
  pdf = {https://proceedings.mlr.press/v202/prabhudesai23a/prabhudesai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Test-time Adaptation with Slot-Centric Models},
  url = {https://proceedings.mlr.press/v202/prabhudesai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{prinster2023jawsx,
  abstract = {We study the efficient estimation of predictive confidence intervals for black-box predictors when the common data exchangeability (e.g., i.i.d.) assumption is violated due to potentially feedback-induced shifts in the input data distribution. That is, we focus on standard and feedback covariate shift ({FCS}), where the latter allows for feedback dependencies between train and test data that occur in many decision-making scenarios like experimental design. Whereas prior conformal prediction methods for this problem are in general either extremely computationally demanding or make inefficient use of labeled data, we propose a collection of methods based on the jackknife+ that achieve a practical balance of computational and statistical efficiency. Theoretically, our proposed {JAW-FCS} method extends the rigorous, finite-sample coverage guarantee of the jackknife+ to {FCS}.},
  author = {Drew Prinster and Suchi Saria and Anqi Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/prinster2023jawsx.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28167--28190},
  pdf = {https://proceedings.mlr.press/v202/prinster23a/prinster23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{JAWS-X}: Addressing Efficiency Bottlenecks of Conformal Prediction Under Standard and Feedback Covariate Shift},
  url = {https://proceedings.mlr.press/v202/prinster23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{puny2023equivariant,
  abstract = {Graph Neural Networks ({GNN}) are inherently limited in their expressive power. Recent seminal works introduced the Weisfeiler-Lehman ({WL}) hierarchy as a measure of expressive power. Although this hierarchy has propelled significant advances in {GNN} analysis and architecture developments, it suffers from several significant limitations. These include a complex definition that lacks direct guidance for model improvement and a {WL} hierarchy that is too coarse to study current {GNN}s. This paper introduces an alternative expressive power hierarchy based on the ability of {GNN}s to calculate equivariant polynomials of a certain degree. As a first step, we provide a full characterization of all equivariant graph polynomials by introducing a concrete basis, significantly generalizing previous results. Each basis element corresponds to a specific multi-graph, and its computation over some graph data input corresponds to a tensor contraction problem. Second, we propose algorithmic tools for evaluating the expressiveness of {GNN}s using tensor contraction sequences, and calculate the expressive power of popular {GNN}s. Finally, we enhance the expressivity of common {GNN} architectures by adding polynomial features or additional operations / aggregations inspired by our theory. These enhanced {GNN}s demonstrate state-of-the-art results in experiments across multiple graph learning benchmarks.},
  author = {Omri Puny and Derek Lim and Bobak Toussi Kiani and Haggai Maron and Yaron Lipman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/puny2023equivariant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28191--28222},
  pdf = {https://proceedings.mlr.press/v202/puny23a/puny23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Equivariant Polynomials for Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/puny23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qi2023contrast,
  abstract = {Mainstream {3D} representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics: (i) contrastive models are data-hungry that suffer from a representation over-fitting issue; (ii) generative models have a data filling issue that shows inferior data scaling capacity compared to contrastive models. This motivates us to learn {3D} representations by sharing the merits of both paradigms, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose Contrast with Reconstruct ({ReCon}) that unifies these two paradigms. {ReCon} is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student. An encoder-decoder style {ReCon}-block is proposed that transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues.},
  author = {Zekun Qi and Runpei Dong and Guofan Fan and Zheng Ge and Xiangyu Zhang and Kaisheng Ma and Li Yi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qi2023contrast.pdf:pdf},
  mdate = {2024-10-28},
  pages = {28223--28243},
  pdf = {https://proceedings.mlr.press/v202/qi23a/qi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contrast with Reconstruct: Contrastive {3D} Representation Learning Guided by Generative Pretraining},
  url = {https://proceedings.mlr.press/v202/qi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qi2023effective,
  abstract = {One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error ({MAE}) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate {MAE} for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric ({MAE} using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true {MAE} -- in particular, is better than several alternative methods.},
  author = {Shiang Qi and Neeraj Kumar and Mahtab Farrokh and Weijie Sun and Li-Hao Kuan and Rajesh Ranganath and Ricardo Henao and Russell Greiner},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qi2023effective.pdf:pdf},
  mdate = {2024-07-26},
  pages = {28244--28276},
  pdf = {https://proceedings.mlr.press/v202/qi23b/qi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Effective Meaningful Way to Evaluate Survival Models},
  url = {https://proceedings.mlr.press/v202/qi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiang2023coarsetofine,
  abstract = {Generating desirable molecular structures in {3D} is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for {3D} non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e., {HierDiff}) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, {HierDiff} first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then, the coarse-grained nodes are decoded into fine-grained fragments by a message-passing process and a newly designed iterative refined sampling module. Finally, the fine-grained fragments are assembled to derive a complete atomic molecular structure.},
  author = {Bo Qiang and Yuxuan Song and Minkai Xu and Jingjing Gong and Bowen Gao and Hao Zhou and Wei-Ying Ma and Yanyan Lan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiang2023coarsetofine.pdf:pdf},
  mdate = {2024-08-04},
  pages = {28277--28299},
  pdf = {https://proceedings.mlr.press/v202/qiang23a/qiang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in {3D}},
  url = {https://proceedings.mlr.press/v202/qiang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiao2023fredis,
  abstract = {To reduce the difficulty of annotation, partial label learning ({PLL}) has been widely studied, where each example is ambiguously annotated with a set of candidate labels instead of the exact correct label. {PLL} assumes that the candidate label set contains the correct label, which induces disambiguation, i.e., identification of the correct label in the candidate label set, adopted in most {PLL} methods. However, this assumption is impractical as no one could guarantee the existence of the correct label in the candidate label set under real-world scenarios. Therefore, Unreliable Partial Label Learning ({UPLL}) is investigated where the correct label of each example may not exist in the candidate label set. In this paper, we propose a fusion framework of refinement and disambiguation named {FREDIS} to handle the {UPLL} problem. Specifically, with theoretical guarantees, not only does disambiguation move incorrect labels from candidate labels to non-candidate labels but also refinement, an opposite procedure, moves correct labels from non-candidate labels to candidate labels. Besides, we prove that the classifier trained by our framework could eventually approximate the Bayes optimal classifier.},
  author = {Congyu Qiao and Ning Xu and Jiaqi Lv and Yi Ren and Xin Geng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiao2023fredis.pdf:pdf},
  mdate = {2023-11-03},
  pages = {28321--28336},
  pdf = {https://proceedings.mlr.press/v202/qiao23b/qiao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FREDIS}: A Fusion Framework of Refinement and Disambiguation for Unreliable Partial Label Learning},
  url = {https://proceedings.mlr.press/v202/qiao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qin2023nugget,
  abstract = {Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. Nugget dynamically chooses which tokens are aggregated into the encoded representation, representing a departure from traditional fixed-size embeddings and providing a more adaptive approach to text representation.},
  author = {Guanghui Qin and Benjamin Van Durme},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qin2023nugget.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28337--28350},
  pdf = {https://proceedings.mlr.press/v202/qin23a/qin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nugget: Neural Agglomerative Embeddings of Text},
  url = {https://proceedings.mlr.press/v202/qin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qin2023bibench,
  abstract = {Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present {BiBench}, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support.},
  author = {Haotong Qin and Mingyuan Zhang and Yifu Ding and Aoyu Li and Zhongang Cai and Ziwei Liu and Fisher Yu and Xianglong Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qin2023bibench.pdf:pdf},
  mdate = {2023-09-18},
  pages = {28351--28388},
  pdf = {https://proceedings.mlr.press/v202/qin23b/qin23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{BiBench}: Benchmarking and Analyzing Network Binarization},
  url = {https://proceedings.mlr.press/v202/qin23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiu2023not,
  abstract = {In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter ignores the fact that ``not all semantics are created equal'', meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization ({DRO}), providing us an intuition about the effect of temperature and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable temperature for each sample. Specifically, samples with frequent semantics use large temperatures to keep local semantic structures, while samples with rare semantics use small temperatures to induce more separable features.},
  author = {Zi-Hao Qiu and Quanqi Hu and Zhuoning Yuan and Denny Zhou and Lijun Zhang and Tianbao Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiu2023not.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28389--28421},
  pdf = {https://proceedings.mlr.press/v202/qiu23a/qiu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization},
  url = {https://proceedings.mlr.press/v202/qiu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiu2023shortest,
  abstract = {Population-based search has recently emerged as a possible alternative to Reinforcement Learning ({RL}) for black-box neural architecture search ({NAS}). It performs well in practice even though it is not theoretically well understood. In particular, whereas traditional population-based search methods such as evolutionary algorithms ({EA}s) draw much power from crossover operations, it is difficult to take advantage of them in {NAS}. The main obstacle is believed to be the permutation problem: The mapping between genotype and phenotype in traditional graph representations is many-to-one, leading to a disruptive effect of standard crossover. This paper presents the first theoretical analysis of the behaviors of mutation, crossover and {RL} in black-box {NAS}, and proposes a new crossover operator based on the shortest edit path ({SEP}) in graph space. The {SEP} crossover is shown theoretically to overcome the permutation problem, and as a result, have a better expected improvement compared to mutation.},
  author = {Xin Qiu and Risto Miikkulainen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiu2023shortest.pdf:pdf},
  mdate = {2025-05-14},
  pages = {28422--28447},
  pdf = {https://proceedings.mlr.press/v202/qiu23b/qiu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Shortest Edit Path Crossover: A Theory-driven Solution to the Permutation Problem in Evolutionary Neural Architecture Search},
  url = {https://proceedings.mlr.press/v202/qiu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{qiu2023simple,
  abstract = {A major challenge to out-of-distribution generalization is reliance on spurious features --- patterns that are predictive of the class label in the training data distribution, but not causally related to the target. Standard methods for reducing the reliance on spurious features typically assume that we know what the spurious feature is, which is rarely true in the real world. Methods that attempt to alleviate this limitation are complex, hard to tune, and lead to a significant computational overhead compared to standard training. In this paper, we propose Automatic Feature Reweighting (AFR), an extremely simple and fast method for updating the model to reduce the reliance on spurious features. AFR retrains the last layer of a standard ERM-trained base model with a weighted loss that emphasizes the examples where the ERM model predicts poorly, automatically upweighting the minority group without group labels. With this simple procedure, we improve upon the best reported results among competing methods trained without spurious attributes on several vision and natural language classification benchmarks, using only a fraction of their compute.},
  author = {Shikai Qiu and Andres Potapczynski and Pavel Izmailov and Andrew Gordon Wilson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/qiu2023simple.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28448--28467},
  pdf = {https://proceedings.mlr.press/v202/qiu23c/qiu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simple and Fast Group Robustness by Automatic Feature Reweighting},
  url = {https://proceedings.mlr.press/v202/qiu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{quinzan2023drcfs,
  abstract = {Knowing the features of a complex system that are highly relevant to a particular target variable is of fundamental interest in many areas of science. Existing approaches are often limited to linear settings, sometimes lack guarantees, and in most cases, do not scale to the problem at hand, in particular to images. We propose DRCFS, a doubly robust feature selection method for identifying the causal features even in nonlinear and high dimensional settings. We provide theoretical guarantees, illustrate necessary conditions for our assumptions, and perform extensive experiments across a wide range of simulated and semi-synthetic datasets. DRCFS significantly outperforms existing state-of-the-art methods, selecting robust features even in challenging highly non-linear and high-dimensional problems.},
  author = {Francesco Quinzan and Ashkan Soleymani and Patrick Jaillet and Cristian R. Rojas and Stefan Bauer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/quinzan2023drcfs.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28468--28491},
  pdf = {https://proceedings.mlr.press/v202/quinzan23a/quinzan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DRCFS: Doubly Robust Causal Feature Selection},
  url = {https://proceedings.mlr.press/v202/quinzan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{radford2023robust,
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. The approach is implemented in Whisper, an Automatic Speech Recognition (ASR) system which approaches human level robustness and accuracy on English speech recognition.},
  author = {Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/radford2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28492--28518},
  pdf = {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  url = {https://proceedings.mlr.press/v202/radford23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{raffel2023shiftable,
  abstract = {Transformer models using segment-based processing create a context mismatch between training and inference environments, hindering potential translation accuracy in simultaneous speech translation. We propose Shiftable Context, a simple yet effective scheme to ensure that consistent segment and context sizes are maintained throughout training and inference, even with the presence of partially filled segments due to the streaming nature of simultaneous translation. This approach is broadly applicable to segment-based transformers for streaming tasks and demonstrates significant improvements in BLEU scores across multiple language pairs when applied to the Augmented Memory Transformer.},
  author = {Matthew Raffel and Drew Penney and Lizhong Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/raffel2023shiftable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28519--28530},
  pdf = {https://proceedings.mlr.press/v202/raffel23a/raffel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation},
  url = {https://proceedings.mlr.press/v202/raffel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{raghu2023sequential,
  abstract = {Most existing self-supervised learning (SSL) methods for clinical time series are limited to unimodal time series, such as sequences of structured features or individual high-dimensional physiological signals. These methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. We propose Sequential Multi-Dimensional SSL, where an SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence to better capture information at both scales. Our method is agnostic to the specific form of loss function used at each level. We evaluate our approach on two real-world clinical datasets where the time series contains sequences of high-frequency electrocardiograms and structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets.},
  author = {Aniruddh Raghu and Payal Chandak and Ridwan Alam and John V. Guttag and Collin M. Stultz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/raghu2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28531--28548},
  pdf = {https://proceedings.mlr.press/v202/raghu23a/raghu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series},
  url = {https://proceedings.mlr.press/v202/raghu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rahbar2023recovery,
  abstract = {We develop a novel theoretical framework for understanding Optimal Transport (OT) schemes respecting a class structure. For this purpose, we propose a convex OT program with a sum-of-norms regularization term, which provably recovers the underlying class structure under geometric assumptions. We derive an accelerated proximal algorithm with a closed-form projection and proximal operator scheme, thereby affording a more scalable algorithm for computing optimal transport plans. We provide a novel argument for the uniqueness of the optimum even in the absence of strong convexity. Our experiments show that the new regularizer not only results in a better preservation of the class structure in the data but also yields additional robustness to the data geometry, compared to previous regularizers.},
  author = {Arman Rahbar and Ashkan Panahi and Morteza Haghir Chehreghani and Devdatt P. Dubhashi and Hamid Krim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rahbar2023recovery.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28549--28577},
  pdf = {https://proceedings.mlr.press/v202/rahbar23a/rahbar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Recovery Bounds on Class-Based Optimal Transport: A Sum-of-Norms Regularization Framework},
  url = {https://proceedings.mlr.press/v202/rahbar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{raj2023algorithmic,
  abstract = {Heavy-tail phenomena in stochastic gradient descent (SGD) have been reported in several empirical studies. Experimental evidence suggests a strong interplay between the heaviness of the tails and generalization behavior of SGD. Very recently, new generalization bounds have been proven, indicating a non-monotonic relationship between the generalization error and heavy tails. While these bounds do not require additional topological assumptions, they can only apply to simple quadratic problems. In this paper, we build on this line of research and develop generalization bounds for a more general class of objective functions, which includes non-convex functions as well. Our approach is based on developing Wasserstein stability bounds for heavy-tailed SDEs and their discretizations, which we then convert to generalization bounds. Our results do not require any nontrivial assumptions; yet, they shed more light on the empirical observations thanks to the generality of the loss functions.},
  author = {Anant Raj and Lingjiong Zhu and Mert G{\"u}rb{\"u}zbalaban and Umut {\c{S}}im{\c{s}}ekli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/raj2023algorithmic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28578--28597},
  pdf = {https://proceedings.mlr.press/v202/raj23a/raj23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Algorithmic Stability of Heavy-Tailed {SGD} with General Loss Functions},
  url = {https://proceedings.mlr.press/v202/raj23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rajeswar2023mastering,
  abstract = {Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as we show in this paper, whether current methods succeed at this goal is unclear, due to the limited coverage of the evaluation. We study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59\% overall normalized performance, surpassing previous baselines by a staggering margin. We also show robust performance on the Real-World RL benchmark, hinting at resiliency to environment perturbations during adaptation.},
  author = {Sai Rajeswar and Pietro Mazzaglia and Tim Verbelen and Alexandre Pich{\'e} and Bart Dhoedt and Aaron C. Courville and Alexandre Lacoste},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rajeswar2023mastering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28598--28617},
  pdf = {https://proceedings.mlr.press/v202/rajeswar23a/rajeswar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels},
  url = {https://proceedings.mlr.press/v202/rajeswar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ramakrishnan2023spotem,
  abstract = {Our goal is to search long egocentric videos to answer natural language queries (e.g., "where did I leave my purse?"). Existing episodic memory (EM) methods exhaustively extract expensive fixed-length clip features to look everywhere in the video for the answer, which is infeasible for long wearable-camera videos that span hours or even days. We propose SpotEM, an approach to achieve efficiency for a given EM method while maintaining good accuracy. SpotEM is a clip-selection approach that spots clips relevant to the query cheaply and selectively processes these clips to serve as inputs to the EM model. SpotEM consists of three key ideas: 1) a novel clip selector that learns to identify promising video regions to search conditioned on the language query; 2) a set of low-cost semantic indexing features that capture the context of rooms, objects, and interactions that suggest where to look; and 3) distillation losses that address the optimization issues arising from end-to-end joint training of the clip selector and EM model. Experiments on 200+ hours of video from the Ego4D EM Natural Language Queries benchmark demonstrate the effectiveness of our approach: computing only 10\%--25\% of the clip features, SpotEM preserves 84\%--97\% of the original EM model's accuracy.},
  author = {Santhosh Kumar Ramakrishnan and Ziad Al-Halah and Kristen Grauman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ramakrishnan2023spotem.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28618--28636},
  pdf = {https://proceedings.mlr.press/v202/ramakrishnan23a/ramakrishnan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SpotEM}: Efficient Video Search for Episodic Memory},
  url = {https://proceedings.mlr.press/v202/ramakrishnan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ramasinghe2023how,
  abstract = {Characterizing the remarkable generalization properties of over-parameterized neural networks remains an open problem. A growing body of recent literature shows that the bias of stochastic gradient descent (SGD) and architecture choice implicitly leads to better generalization. In this paper, we show on the contrary that, independently of architecture, SGD can itself be the cause of poor generalization if one does not ensure good initialization. We prove a theoretical result called the "weak spectral bias law": any differentiably parameterized model, trained under gradient flow, obeys a weak spectral bias law which states that sufficiently high frequencies train arbitrarily slowly, which implies that very high frequencies present at initialization will remain after training, and hamper generalization. Through a Fourier lens, we derive a general result for the spectral bias of neural networks and show that the generalization of neural networks is heavily tied to their initialization. Finally, we contrast our framework with that supplied by the flat-minima conjecture and show that Fourier analysis grants a more reliable framework for understanding the generalization of neural networks.},
  author = {Sameera Ramasinghe and Lachlan Ewen MacDonald and Moshiur R. Farazi and Hemanth Saratchandran and Simon Lucey},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ramasinghe2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28637--28655},
  pdf = {https://proceedings.mlr.press/v202/ramasinghe23a/ramasinghe23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How much does Initialization Affect Generalization?},
  url = {https://proceedings.mlr.press/v202/ramasinghe23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rame2023model,
  abstract = {Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks. Empirically, it improves the state of the art on the reference DomainBed benchmark for out-of-distribution generalization. Looking forward, this work contributes to the emerging paradigm of updatable machine learning where, akin to open-source software development, the community collaborates to reliably update machine learning models.},
  author = {Alexandre Ram{\'e} and Kartik Ahuja and Jianyu Zhang and Matthieu Cord and L{\'e}on Bottou and David Lopez-Paz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rame2023model.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28656--28679},
  pdf = {https://proceedings.mlr.press/v202/rame23a/rame23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization},
  url = {https://proceedings.mlr.press/v202/rame23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ramesh2023picture,
  abstract = {We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on a task is (remarkably) low-dimensional; (2) the dimensionality of this manifold is a function of the number of training examples and the difficulty of the task; (3) the margin of a classifier is directly related to the curvature of this manifold; (4) different tasks trained using the same learning algorithm have similar manifolds, and (5) different learning algorithms trained on the same task have similar manifolds. These observations provide new insights into the nature of task-specific and algorithm-specific representations, the benefits of data augmentation, and the curse of dimensionality in machine learning.},
  author = {Rahul Ramesh and Jialin Mao and Itay Griniasty and Rubing Yang and Han Kheng Teoh and Mark K. Transtrum and James P. Sethna and Pratik Chaudhari},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ramesh2023picture.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28680--28700},
  pdf = {https://proceedings.mlr.press/v202/ramesh23a/ramesh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Picture of the Space of Typical Learnable Tasks},
  url = {https://proceedings.mlr.press/v202/ramesh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ran2023spencnn,
  abstract = {Homomorphic Encryption (HE) is a promising technology to protect clients' data privacy for Machine Learning as a Service (MLaaS) on public clouds. However, HE operations can be orders of magnitude slower than their counterparts for plaintexts and thus result in prohibitively high inference latency, seriously hindering the practicality of HE. In this paper, we propose a HE-based fast neural network (NN) inference framework--SpENCNN built upon the co-design of HE operation-aware model sparsity and the single-instruction-multiple-data (SIMD)-friendly data packing, to improve NN inference latency. In particular, we first develop an encryption-aware HE-group convolution technique that can partition channels among different groups based on the data size and ciphertext size, and then encode them into the same ciphertext by novel group-interleaved encoding, so as to dramatically reduce the number of costly HE operations. Moreover, we propose a HE-friendly sub-block weight pruning to reduce costly HE-based convolution operations while maintaining good model accuracy. Experimental results show that SpENCNN can achieve overall speedups of 8.37×, 12.11×, 19.26×, and 1.87× for LeNet, VGG-5, HEFNet, and ResNet-20 respectively, with negligible accuracy loss.},
  author = {Ran Ran and Xinwei Luo and Wei Wang and Tao Liu and Gang Quan and Xiaolin Xu and Caiwen Ding and Wujie Wen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ran2023spencnn.pdf:pdf},
  mdate = {2024-11-08},
  pages = {28718--28728},
  pdf = {https://proceedings.mlr.press/v202/ran23b/ran23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SpENCNN}: Orchestrating Encoding and Sparsity for Fast Homomorphically Encrypted Neural Network Inference},
  url = {https://proceedings.mlr.press/v202/ran23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ran2023policy,
  abstract = {We consider the problem of learning the best possible policy from a fixed dataset, known as offline Reinforcement Learning (RL). A common taxonomy of existing offline RL works is policy regularization, which typically constrains the learned policy by distribution or support of the behavior policy. However, distribution and support constraints are overly conservative since they both force the policy to choose similar actions as the behavior policy when considering particular states. It will limit the learned policy's performance, especially when the behavior policy is sub-optimal. In this paper, we find that regularizing the policy towards the nearest state-action pair can be more effective and thus propose Policy Regularization with Dataset Constraint (PRDC). When updating the policy in a given state, PRDC searches the entire dataset for the nearest state-action sample and then restricts the policy with the action of this sample. Unlike previous works, PRDC can guide the policy with proper behaviors from the dataset, allowing it to choose actions that do not appear in the dataset along with the given state. It is a softer constraint but still keeps enough conservatism from out-of-distribution actions. Empirical evidence and theoretical analysis show that PRDC can alleviate offline RL's fundamentally challenging value overestimation issue with a bounded performance gap. Moreover, on a set of locomotion and navigation tasks, PRDC achieves state-of-the-art performance compared with existing methods.},
  author = {Yuhang Ran and Yi-Chen Li and Fuxiang Zhang and Zongzhang Zhang and Yang Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ran2023policy.pdf:pdf},
  mdate = {2024-01-09},
  pages = {28701--28717},
  pdf = {https://proceedings.mlr.press/v202/ran23a/ran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Policy Regularization with Dataset Constraint for Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/ran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rangamani2023feature,
  abstract = {In this paper, we conduct an empirical study of the feature learning process in deep classifiers. Recent research has identified a training phenomenon called Neural Collapse (NC), in which the top-layer feature embeddings of samples from the same class tend to concentrate around their means, and the top layer's weights align with those features. Our study aims to investigate if these properties extend to intermediate layers. We empirically study the evolution of the covariance and mean of representations across different layers and show that as we move deeper into a trained neural network, the within-class covariance decreases relative to the between-class covariance. Additionally, we find that in the top layers, where the between-class covariance is dominant, the subspace spanned by the class means aligns with the subspace spanned by the most significant singular vector components of the weight matrix in the corresponding layer.},
  author = {Akshay Rangamani and Marius Lindegaard and Tomer Galanti and Tomaso A. Poggio},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rangamani2023feature.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28729--28745},
  pdf = {https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Feature learning in deep classifiers through Intermediate Neural Collapse},
  url = {https://proceedings.mlr.press/v202/rangamani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rathnam2023unintended,
  abstract = {Discount regularization, using a shorter planning horizon when calculating the optimal policy, is a popular choice to restrict planning to a less complex set of policies when estimating an MDP from sparse or noisy data. It is commonly understood that discount regularization functions by de-emphasizing or ignoring delayed effects. In this paper, we reveal an alternate view of discount regularization that exposes unintended consequences. We demonstrate that planning under a lower discount factor produces an identical optimal policy to planning using any prior on the transition matrix that has the same distribution for all states and actions. This prior view explains the success of discount regularization in many settings, but also exposes problematic assumptions implicit in discount regularization. In particular, this view reveals that discount regularization imposes a stronger penalty on state-action pairs with more transition data, causing systematic bias toward the prior and poor performance when the data is uneven across state-action pairs. Based on our analysis, we propose an improved regularization method and demonstrate its performance across a range of synthetic and real domains.},
  author = {Sarah Rathnam and Sonali Parbhoo and Weiwei Pan and Susan A. Murphy and Finale Doshi-Velez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rathnam2023unintended.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28746--28767},
  pdf = {https://proceedings.mlr.press/v202/rathnam23a/rathnam23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Unintended Consequences of Discount Regularization: Improving Regularization in Certainty Equivalence Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/rathnam23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{razon2023automated,
  abstract = {Formulas involving fundamental mathematical constants had a great impact on various fields of science and mathematics, for example aiding in proofs of irrationality of constants. However, the discovery of such formulas has historically remained scarce, often perceived as an act of mathematical genius by great mathematicians such as Ramanujan, Euler, and Gauss. Recent efforts to automate the discovery of formulas for mathematical constants, such as the Ramanujan Machine project, relied on exhaustive search. Despite several successful discoveries, exhaustive search remains limited by the space of options that can be covered and by the need for vast amounts of computational resources. In this paper, we present a fundamentally different approach for finding conjectures on mathematical constants. Instead of searching for continued fractions, we search for patterns in integer sequences. This approach is motivated by the fact that many mathematical constants can be represented as the limits of infinite series whose partial sums are integer sequences. We then use machine learning to identify these integer sequences and to automatically find the underlying mathematical structures.},
  author = {Ofir Razon and Yoav Harris and Shahar Gottlieb and Dan Carmon and Ofir David and Ido Kaminer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/razon2023automated.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28809--28842},
  pdf = {https://proceedings.mlr.press/v202/razon23a/razon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Automated Search for Conjectures on Mathematical Constants using Analysis of Integer Sequences},
  url = {https://proceedings.mlr.press/v202/razon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{reddi2023efficient,
  abstract = {Large deep learning models have achieved state-of-the-art performance across various natural language processing (NLP) tasks and demonstrated remarkable few-shot learning performance. However, training them is often challenging and resource-intensive. In this paper, we study an efficient approach to train language models using few-shot learners. We show that, by leveraging the fast learning nature of few-shot learners, one can train language models efficiently in a stagewise manner. Our main insight is that stacking a good few-shot learner on a good small language model provides a good initializer for a larger language model. Using this insight and building upon progressive stacking approaches, we develop novel approaches for training such networks in a stagewise manner. Through extensive experiments on multiple datasets, we demonstrate the effectiveness of the proposed approach in terms of both computational efficiency and performance.},
  author = {Sashank J. Reddi and Sobhan Miryoosefi and Stefani Karp and Shankar Krishnan and Satyen Kale and Seungyeon Kim and Sanjiv Kumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/reddi2023efficient.pdf:pdf},
  mdate = {2025-04-24},
  pages = {14553--14568},
  pdf = {https://proceedings.mlr.press/v202/j-reddi23a/j-reddi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Training of Language Models using Few-Shot Learning},
  url = {https://proceedings.mlr.press/v202/j-reddi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{refinetti2023neural,
  abstract = {The uncanny ability of over-parameterised neural networks to generalise well has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first fitting simple, linear classifiers before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a single neuron trained on synthetic data. We then demonstrate DSB empirically in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in networks pre-trained on ImageNet. We discuss the relation of DSB to other simplicity biases and consider its implications for the principle of universality in learning.},
  author = {Maria Refinetti and Alessandro Ingrosso and Sebastian Goldt},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/refinetti2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28843--28863},
  pdf = {https://proceedings.mlr.press/v202/refinetti23a/refinetti23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural networks trained with {SGD} learn distributions of increasing complexity},
  url = {https://proceedings.mlr.press/v202/refinetti23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{reid2023simplex,
  abstract = {We present Simplex Random Features (SimRFs), a new random feature (RF) mechanism for unbiased approximation of the softmax and Gaussian kernels by geometrical correlation of random projection vectors. We prove that SimRFs provide the smallest possible mean square error (MSE) on unbiased estimates of these kernels among the class of weight-independent geometrically-coupled positive random feature (PRF) mechanisms, substantially outperforming the previously most accurate Orthogonal Random Features (ORFs) at no observable extra cost. We present a more computationally expensive SimRFs+ variant, which we prove is asymptotically optimal in the broader family of weight-dependent geometrical coupling schemes (which permit correlations between random vector directions and norms). In extensive empirical studies, we show consistent gains provided by SimRFs in settings including pointwise kernel estimation, nonparametric classification and scalable Transformers.},
  author = {Isaac Reid and Krzysztof Marcin Choromanski and Valerii Likhosherstov and Adrian Weller},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/reid2023simplex.pdf:pdf},
  mdate = {2023-08-28},
  pages = {28864--28888},
  pdf = {https://proceedings.mlr.press/v202/reid23a/reid23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simplex Random Features},
  url = {https://proceedings.mlr.press/v202/reid23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ren2023bayesian,
  abstract = {In this paper, we focus on mean-field variational Bayesian Neural Networks (BNNs) and explore the representation capacity of such BNNs by investigating which types of concepts are less likely to be encoded by the BNN. It has been observed and studied that a relatively small set of interactive concepts usually emerge in the knowledge representation of a sufficiently-trained neural network, and such concepts can faithfully explain the network output. Based on this, our study proves that compared to standard deep neural networks (DNNs), it is less likely for BNNs to encode complex concepts. Experiments verify our theoretical proofs. Note that the tendency to encode less complex concepts does not necessarily imply weak representation power, considering that complex concepts exhibit low generalization power and high adversarial vulnerability.},
  author = {Qihan Ren and Huiqi Deng and Yunuo Chen and Siyu Lou and Quanshi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ren2023bayesian.pdf:pdf},
  mdate = {2025-05-27},
  pages = {28889--28913},
  pdf = {https://proceedings.mlr.press/v202/ren23a/ren23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts},
  url = {https://proceedings.mlr.press/v202/ren23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ren2023escaping,
  abstract = {Two-point zeroth order methods are important in many applications of zeroth-order optimization, such as robotics, wind farms, power systems, online optimization, and adversarial robustness to black-box attacks in deep neural networks, where the problem may be high-dimensional and/or time-varying. Most problems in these applications are nonconvex and contain saddle points. While existing works have shown that zeroth-order methods utilizing Ω(d) function valuations per iteration (with d denoting the problem dimension) can escape saddle points efficiently, it remains an open question if zeroth-order methods based on two-point estimators can escape saddle points. In this paper, we show that by adding an appropriate isotropic perturbation at each iteration, a zeroth-order algorithm based on 2m (for any 1 ≤ m ≤ d) function evaluations per iteration can not only find ε-second order stationary points polynomially fast, but do so using only Õ(d/mε²ψ̄) function evaluations, where ψ̄ ≥ Ω̃(√ε) is a parameter capturing the extent to which the function of interest exhibits the strict saddle property.},
  author = {Zhaolin Ren and Yujie Tang and Na Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ren2023escaping.pdf:pdf},
  mdate = {2024-08-13},
  pages = {28914--28975},
  pdf = {https://proceedings.mlr.press/v202/ren23b/ren23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Escaping saddle points in zeroth-order optimization: the power of two-point estimators},
  url = {https://proceedings.mlr.press/v202/ren23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{reneau2023feature,
  abstract = {We introduce the concept of programmable feature engineering for time series modeling and propose a feature programming framework. This framework generates large amounts of predictive features for noisy multivariate time series while allowing users to incorporate their inductive bias with minimal effort. The key motivation of our framework is to view any multivariate time series as a cumulative sum of fine-grained trajectory increments, with each increment governed by a novel spin-gas dynamical Ising model. This fine-grained perspective motivates the development of a parsimonious set of operators that summarize multivariate time series in an abstract fashion, serving as the foundation for large-scale automated feature engineering. Numerically, we validate the efficacy of our method on several synthetic and real-world noisy time series datasets.},
  author = {Alex Daniel Reneau and Jerry Yao-Chieh Hu and Ammar Gilani and Han Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/reneau2023feature.pdf:pdf},
  mdate = {2024-10-10},
  pages = {29009--29029},
  pdf = {https://proceedings.mlr.press/v202/reneau23a/reneau23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Feature Programming for Multivariate Time Series Prediction},
  url = {https://proceedings.mlr.press/v202/reneau23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rezaei2023runoff,
  abstract = {Data poisoning attacks manipulate training data to induce specific model behaviors during inference. Existing ensemble-based defenses use majority voting across multiple base models, but this approach is wasteful as it does not effectively utilize available information in the logits layers of the base models. We propose Run-Off Election (ROE), a novel aggregation method based on a two-round election across the base models: In the first round, models vote for their preferred class and then a second, Run-Off election is held between the top two classes in the first round. Based on this approach, we propose DPA+ROE and FA+ROE defense methods based on Deep Partition Aggregation (DPA) and Finite Aggregation (FA) approaches. Our evaluation demonstrates that ROE methods obtain improvements in certified accuracy by up to 3\%-4\%, and by applying ROE on a boosted version of DPA, we achieve improvements around 12\%-27\% compared to the current state-of-the-art, establishing a new state-of-the-art in pointwise certified robustness against data poisoning.},
  author = {Keivan Rezaei and Kiarash Banihashem and Atoosa Malemir Chegini and Soheil Feizi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rezaei2023runoff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29030--29050},
  pdf = {https://proceedings.mlr.press/v202/rezaei23a/rezaei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Run-off Election: Improved Provable Defense against Data Poisoning Attacks},
  url = {https://proceedings.mlr.press/v202/rezaei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ribeiro2023high,
  abstract = {We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.},
  author = {Fabio De Sousa Ribeiro and Tian Xia and Miguel Monteiro and Nick Pawlowski and Ben Glocker},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ribeiro2023high.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7390--7425},
  pdf = {https://proceedings.mlr.press/v202/de-sousa-ribeiro23a/de-sousa-ribeiro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {High Fidelity Image Counterfactuals with Probabilistic Causal Models},
  url = {https://proceedings.mlr.press/v202/de-sousa-ribeiro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{richards2023learning,
  abstract = {Even for known nonlinear dynamical systems, feedback controller synthesis is a difficult problem that often requires leveraging the particular structure of the dynamics to induce a stable closed-loop system. For general nonlinear models, including those fit to data, there may not be enough known structure to reliably synthesize a stabilizing feedback controller.},
  author = {Spencer M. Richards and Jean-Jacques E. Slotine and Navid Azizan and Marco Pavone},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/richards2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29051--29062},
  pdf = {https://proceedings.mlr.press/v202/richards23a/richards23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Control-Oriented Dynamical Structure from Data},
  url = {https://proceedings.mlr.press/v202/richards23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{richemond2023edge,
  abstract = {Self-predictive unsupervised learning methods such as BYOL or SimSIAM have shown impressive results, and counter-intuitively, do not collapse to trivial representations. In this work, we aim at exploring the simplest possible mathematical arguments towards explaining the underlying mechanisms behind self-predictive unsupervised learning. We start with the observation that those methods crucially rely on the presence of a predictor network (and stop-gradient). With simple linear algebra, we show that when using a linear predictor, the optimal predictor is close to an orthogonal projection, and propose a general framework based on orthonormalization that enables to interpret and give intuition on why BYOL works. In addition, this framework demonstrates the crucial role of the exponential moving average and stop-gradient operator in BYOL as an efficient orthonormalization mechanism. We use these insights to propose four new closed-form predictor variants of BYOL to support our analysis. Our closed-form predictors outperform standard linear trainable predictor BYOL at 100 and 300 epochs (top-1 linear accuracy on ImageNet).},
  author = {Pierre Harvey Richemond and Allison C. Tam and Yunhao Tang and Florian Strub and Bilal Piot and Felix Hill},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/richemond2023edge.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29063--29081},
  pdf = {https://proceedings.mlr.press/v202/richemond23a/richemond23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Edge of Orthogonality: A Simple View of What Makes {BYOL} Tick},
  url = {https://proceedings.mlr.press/v202/richemond23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rio2023multiagent,
  abstract = {We address multi-agent best arm identification with privacy guarantees. In this setting, agents collaborate by communicating to find the optimal arm. To avoid leaking sensitive data through messages, we consider two notions of privacy withholding different kinds of information: differential privacy and $(\epsilon, \eta)$-privacy. For each privacy definition, we propose an algorithm based on a two-level successive elimination scheme. We provide theoretical guarantees for the privacy level, accuracy and sample complexity of our algorithms. Experiments on various settings support our theoretical findings.},
  author = {Alexandre Rio and Merwan Barlier and Igor Colin and Marta Soare},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rio2023multiagent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29082--29102},
  pdf = {https://proceedings.mlr.press/v202/rio23a/rio23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Agent Best Arm Identification with Private Communications},
  url = {https://proceedings.mlr.press/v202/rio23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rittler2023twostage,
  abstract = {$k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained classifiers converge to the Bayes optimal classifier at a faster asymptotic rate than passively trained $k$-nearest neighbor classifiers.},
  author = {Nicholas Rittler and Kamalika Chaudhuri},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rittler2023twostage.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29103--29129},
  pdf = {https://proceedings.mlr.press/v202/rittler23a/rittler23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors},
  url = {https://proceedings.mlr.press/v202/rittler23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ro2023lowering,
  abstract = {Training data and model sizes are increasing exponentially. One way to reduce training time and resources is to train with a carefully selected subset of the full dataset. Prior work uses the gradient signals obtained during a warm-up or "pre-training" phase over the full dataset, for determining the core subset; if the pre-training phase is too small, the gradients obtained are chaotic and unreliable. As a result, the pre-training phase itself incurs significant time/resource overhead, and prior work has not gone beyond hyperparameter search to reduce pre-training time. We aim to reduce the "pre-training tax" in gradient-based subset training by developing a principled, scalable approach for pre-training in a distributed setup. Our approach is lightweight and minimizes communication between distributed worker nodes, being the first to utilize model-soup based distributed training at initialization.},
  author = {Yeonju Ro and Zhangyang Wang and Vijay Chidambaram and Aditya Akella},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ro2023lowering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29130--29142},
  pdf = {https://proceedings.mlr.press/v202/ro23a/ro23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lowering the Pre-training Tax for Gradient-based Subset Training: A Lightweight Distributed Pre-Training Toolkit},
  url = {https://proceedings.mlr.press/v202/ro23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{roh2023improving,
  abstract = {Model fairness is an essential element for Trustworthy AI. While many techniques for model fairness have been proposed, most of them assume that the training and deployment data distributions are identical, which is often not true in practice. In particular, when the bias between labels and sensitive groups changes, the fairness of the trained model is directly influenced and can worsen. We make two contributions for solving this problem. First, we analytically show that existing in-processing fair algorithms have fundamental limits in accuracy and group fairness. We introduce the notion of correlation shifts, which can explicitly capture the change of the above bias. Second, we propose a novel pre-processing step that samples the input data to reduce correlation shifts and thus enables the in-processing approaches to overcome their limitations. We formulate an optimization problem for adjusting the data ratio among labels and sensitive groups to reflect the shifted correlation. A key benefit of our approach lies in decoupling the roles of pre- and in-processing approaches: correlation adjustment via pre-processing and unfairness mitigation on the processed data via in-processing. Experiments show that our framework effectively improves existing in-processing fair algorithms w.r.t. accuracy and fairness, both on synthetic and real datasets.},
  author = {Yuji Roh and Kangwook Lee and Steven Euijong Whang and Changho Suh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/roh2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29179--29209},
  pdf = {https://proceedings.mlr.press/v202/roh23a/roh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Fair Training under Correlation Shifts},
  url = {https://proceedings.mlr.press/v202/roh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rohekar2023temporal,
  abstract = {We present a constraint-based algorithm for learning causal structures from observational time-series data, in the presence of latent confounders. We assume a discrete-time, stationary structural vector autoregressive process, with both temporal and contemporaneous causal relations. The presented algorithm gradually refines a causal graph by learning long-term temporal relations before short-term ones, where contemporaneous relations are learned last. This ordering of causal relations to be learnt leads to a reduction in the required number of statistical tests. We validate this reduction empirically and demonstrate that it leads to higher accuracy for synthetic data and more plausible causal graphs for real-world data compared to state-of-the-art algorithms.},
  author = {Raanan Y. Rohekar and Shami Nisimov and Yaniv Gurwicz and Gal Novik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rohekar2023temporal.pdf:pdf},
  mdate = {2025-03-11},
  pages = {39939--39950},
  pdf = {https://proceedings.mlr.press/v202/yehezkel-rohekar23a/yehezkel-rohekar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Temporal to Contemporaneous Iterative Causal Discovery in the Presence of Latent Confounders},
  url = {https://proceedings.mlr.press/v202/yehezkel-rohekar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rohrscheidt2023topological,
  abstract = {The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.},
  author = {Julius von Rohrscheidt and Bastian Rieck},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rohrscheidt2023topological.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35175--35197},
  pdf = {https://proceedings.mlr.press/v202/von-rohrscheidt23a/von-rohrscheidt23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Topological Singularity Detection at Multiple Scales},
  url = {https://proceedings.mlr.press/v202/von-rohrscheidt23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rowland2023statistical,
  abstract = {We study the problem of temporal-difference-based policy evaluation in reinforcement learning. In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task. We reach the surprising conclusion that even if a practitioner has no interest in the return distribution beyond the mean, QTD (which learns predictions about the full distribution of returns) may offer performance superior to approaches such as classical TD learning, which predict only the mean return, even in the tabular setting.},
  author = {Mark Rowland and Yunhao Tang and Clare Lyle and Rémi Munos and Marc G. Bellemare and Will Dabney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rowland2023statistical.pdf:pdf},
  mdate = {2024-10-02},
  pages = {29210--29231},
  pdf = {https://proceedings.mlr.press/v202/rowland23a/rowland23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation},
  url = {https://proceedings.mlr.press/v202/rowland23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ruan2023robust,
  abstract = {Despite being a fundamental building block for reinforcement learning, Markov decision processes (MDPs) often suffer from ambiguity in model parameters. Robust MDPs are proposed to overcome this challenge by optimizing the worst-case performance under ambiguity. While robust MDPs can provide reliable policies with limited data, their worst-case performances are often overly conservative, and so they do not offer practical insights into the actual performance of these reliable policies. This paper proposes robust satisficing MDPs (RSMDPs), where the expected returns of feasible policies are softly-constrained to achieve a user-specified target under ambiguity. We derive a tractable reformulation for RSMDPs and develop a first-order method for solving large instances. Experimental results demonstrate that RSMDPs can prescribe policies to achieve their targets, which are much higher than the optimal worst-case returns computed by robust MDPs.},
  author = {Haolin Ruan and Siyu Zhou and Zhi Chen and Chin Pang Ho},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ruan2023robust.pdf:pdf},
  mdate = {2024-06-14},
  pages = {29232--29258},
  pdf = {https://proceedings.mlr.press/v202/ruan23a/ruan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Satisficing {MDPs}},
  url = {https://proceedings.mlr.press/v202/ruan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rucker2023infinite,
  abstract = {For infinite action contextual bandits, smoothed regret and reduction to regression results in state-of-the-art online performance with computational cost independent of the action set: unfortunately, the resulting data exhaust does not have well-defined importance-weights. This frustrates the execution of downstream data science processes such as offline model selection. We describe an online algorithm with an equivalent smoothed regret guarantee, but which generates well-defined importance weights: in exchange, the online computational cost increases, but only to order smoothness (i.e., still independent of the action set). This removes a key obstacle to adoption of smoothed regret in production scenarios.},
  author = {Mark Rucker and Yinglun Zhu and Paul Mineiro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rucker2023infinite.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29259--29274},
  pdf = {https://proceedings.mlr.press/v202/rucker23a/rucker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Infinite Action Contextual Bandits with Reusable Data Exhaust},
  url = {https://proceedings.mlr.press/v202/rucker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rudner2023functionspace,
  abstract = {Parameter-space regularization in neural network optimization is a fundamental tool for improving generalization. However, standard parameter-space regularization methods make it challenging to encode explicit preferences about desired predictive functions into neural network training. We approach regularization in neural networks from a probabilistic perspective and show that by viewing parameter-space regularization as specifying an empirical prior distribution over the model parameters, we can derive a probabilistically well-motivated regularization technique that allows explicitly encoding information about desired predictive functions into neural network training. This method—which we refer to as function-space empirical Bayes (FS-EB)—includes both parameter- and function-space regularization, is mathematically simple, easy to implement, and incurs only minimal computational overhead compared to standard regularization techniques. We evaluate the utility of this regularization technique empirically and demonstrate that the proposed method leads to near-perfect semantic shift detection, highly-calibrated predictive uncertainty estimates, successful task adaption from pre-trained models, and improved generalization under covariate shift.},
  author = {Tim G. J. Rudner and Sanyam Kapoor and Shikai Qiu and Andrew Gordon Wilson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rudner2023functionspace.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29275--29290},
  pdf = {https://proceedings.mlr.press/v202/rudner23a/rudner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Function-Space Regularization in Neural Networks: A Probabilistic Perspective},
  url = {https://proceedings.mlr.press/v202/rudner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{r2023new,
  abstract = {Recent advances to combine structured regression models and deep neural networks for better interpretability, more expressiveness, and statistically valid uncertainty quantification demonstrate the versatility of semi-structured neural networks (SSNs). However, techniques to properly identify the contributions of the different model components in SSNs lead to suboptimal network estimation, slower convergence, and degenerated or erroneous predictions. To solve these problems while preserving favorable model properties, we propose a non-invasive post-hoc orthogonalization (PHO) that guarantees identifiability of model components and provides better estimation and prediction quality. Our theoretical findings are supported by numerical experiments, a benchmark comparison as well as a real-world application to COVID-19 infections.},
  author = {David R{ü}gamer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/r2023new.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29291--29305},
  pdf = {https://proceedings.mlr.press/v202/rugamer23a/rugamer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A New {PHO}-rmula for Improved Performance of Semi-Structured Networks},
  url = {https://proceedings.mlr.press/v202/rugamer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ruhe2023geometric,
  abstract = {We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the Pin(p,q,r) group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable geometric templates that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.},
  author = {David Ruhe and Jayesh K. Gupta and Steven De Keninck and Max Welling and Johannes Brandstetter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ruhe2023geometric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29306--29337},
  pdf = {https://proceedings.mlr.press/v202/ruhe23a/ruhe23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Geometric Clifford Algebra Networks},
  url = {https://proceedings.mlr.press/v202/ruhe23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{runje2023constrained,
  abstract = {Wider adoption of neural networks in many critical domains such as finance and healthcare is being hindered by the need to explain their predictions and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain signs on its weights. Unfortunately, this construction does not work with popular non-saturated activation functions as it can only approximate convex functions. We show that this shortcoming can be fixed by constructing two additional activation functions from a typical unsaturated monotonic activation function and employing each of them on the part of neurons.},
  author = {Davor Runje and Sharath M. Shankaranarayana},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/runje2023constrained.pdf:pdf},
  mdate = {2024-05-07},
  pages = {29338--29353},
  pdf = {https://proceedings.mlr.press/v202/runje23a/runje23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Monotonic Neural Networks},
  url = {https://proceedings.mlr.press/v202/runje23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rust2023differential,
  abstract = {Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.},
  author = {Phillip Rust and Anders S{\o}gaard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rust2023differential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29354--29387},
  pdf = {https://proceedings.mlr.press/v202/rust23a/rust23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models},
  url = {https://proceedings.mlr.press/v202/rust23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rustamov2023intrinsic,
  abstract = {Collections of probability distributions arise in a variety of applications ranging from user activity pattern analysis to brain connectomics. In practice these distributions can be defined over diverse domain types including finite intervals, circles, cylinders, spheres, other manifolds, and graphs. This paper introduces an approach for detecting differences between two collections of distributions over such general domains. To this end, we propose the intrinsic slicing construction that yields a novel class of Wasserstein distances on manifolds and graphs. These distances are Hilbert embeddable, allowing us to reduce the distribution collection comparison problem to a more familiar mean testing problem in a Hilbert space. The authors provide two testing procedures one based on resampling and another on combining p-values from coordinate-wise tests. Their experiments in various synthetic and real data settings show that the resulting tests are powerful and the p-values are well-calibrated.},
  author = {Raif M. Rustamov and Subhabrata Majumdar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rustamov2023intrinsic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29388--29415},
  pdf = {https://proceedings.mlr.press/v202/rustamov23a/rustamov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Intrinsic Sliced Wasserstein Distances for Comparing Collections of Probability Distributions on Manifolds and Graphs},
  url = {https://proceedings.mlr.press/v202/rustamov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ryabinin2023swarm,
  abstract = {Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap "preemptible" instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM Parallelism (Stochastically Wired Adaptively Rebalanced Model Parallelism), a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM Parallelism with existing large-scale training approaches.},
  author = {Max Ryabinin and Tim Dettmers and Michael Diskin and Alexander Borzunov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ryabinin2023swarm.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29416--29440},
  pdf = {https://proceedings.mlr.press/v202/ryabinin23a/ryabinin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SWARM} Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient},
  url = {https://proceedings.mlr.press/v202/ryabinin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ryali2023hiera,
  abstract = {Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. This creates Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training.},
  author = {Chaitanya Ryali and Yuan-Ting Hu and Daniel Bolya and Chen Wei and Haoqi Fan and Po-Yao Huang and Vaibhav Aggarwal and Arkabandhu Chowdhury and Omid Poursaeed and Judy Hoffman and Jitendra Malik and Yanghao Li and Christoph Feichtenhofer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ryali2023hiera.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29441--29454},
  pdf = {https://proceedings.mlr.press/v202/ryali23a/ryali23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles},
  url = {https://proceedings.mlr.press/v202/ryali23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{rychener2023endtoend,
  abstract = {We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes, and we also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.},
  author = {Yves Rychener and Daniel Kuhn and Tobias Sutter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/rychener2023endtoend.pdf:pdf},
  mdate = {2023-12-03},
  pages = {29455--29472},
  pdf = {https://proceedings.mlr.press/v202/rychener23a/rychener23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {End-to-End Learning for Stochastic Optimization: A Bayesian Perspective},
  url = {https://proceedings.mlr.press/v202/rychener23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saad2023sequential,
  abstract = {This paper presents a new approach to automatically discovering accurate models of complex time series data. Working within a Bayesian nonparametric prior over a symbolic space of Gaussian process time series models, we present a novel structure learning algorithm that integrates sequential Monte Carlo (SMC) and involutive MCMC for highly effective posterior inference. Our method can be used both in "online" settings, where new data is incorporated sequentially in time, and in "offline" settings, by using nested subsets of historical data to anneal the posterior. Empirical measurements on real-world time series show that our method can deliver 10x--100x runtime speedups over previous MCMC and greedy-search structure learning algorithms targeting the same model family. We used our method to perform the first large-scale evaluation of Gaussian process time series structure learning on a prominent benchmark of 1,428 econometric datasets.},
  author = {Feras Saad and Brian Patton and Matthew Douglas Hoffman and Rif A. Saurous and Vikash Mansinghka},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saad2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29473--29489},
  pdf = {https://proceedings.mlr.press/v202/saad23a/saad23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Monte Carlo Learning for Time Series Structure Discovery},
  url = {https://proceedings.mlr.press/v202/saad23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saad2023active,
  abstract = {We consider the problem of ranking n experts based on their performances on d tasks. We make a monotonicity assumption stating that for each pair of experts, one outperforms the other on all tasks. We consider the sequential setting where in each round the learner has access to noisy evaluations of actively chosen pair of expert-task, given the information available up to the actual round. Given a confidence parameter $\delta \in (0, 1)$, we provide strategies allowing to recover the correct ranking of experts and develop a bound on the total number of queries made by our algorithm that hold with probability at least $1-\delta$. We show that our strategy is adaptive to the complexity of the problem (our bounds are instance dependent), and develop matching lower bounds up to a ploy-logarithmic factor. Finally, we adapt our strategy to the relaxed problem of best expert identification and provide numerical simulation consistent with our theoretical results.},
  author = {El Mehdi Saad and Nicolas Verzelen and Alexandra Carpentier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saad2023active.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29490--29513},
  pdf = {https://proceedings.mlr.press/v202/saad23b/saad23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Active Ranking of Experts Based on their Performances in Many Tasks},
  url = {https://proceedings.mlr.press/v202/saad23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saberi2023sample,
  abstract = {In this paper, we propose sample complexity bounds for learning a simplex from noisy samples. A dataset of size $n$ is given which includes i.i.d. samples drawn from a uniform distribution over an unknown arbitrary simplex in $\mathbb{R}^K$, where samples are assumed to be corrupted by a multi-variate additive Gaussian noise of an arbitrary magnitude. We prove the existence of an algorithm that with high probability outputs a simplex having a $\ell_2$ distance of at most $\varepsilon$ from the true simplex (for any $\varepsilon>0$). Also, we theoretically show that in order to achieve this bound, it is sufficient to have $n\ge\tilde{\Omega}\left(K^2/\varepsilon^2\right)e^{\Omega\left(K/\mathrm{SNR}^2\right)}$ samples, where $\mathrm{SNR}$ stands for the signal-to-noise ratio and is defined as the ratio of the maximum component-wise standard deviation of the simplex (signal) to that of the noise vector. This result solves an important open problem in this area of research, and shows as long as $\mathrm{SNR}\ge\Omega\left(\sqrt{K}\right)$ the sample complexity of the noisy regime has the same order to that of the noiseless case. Our proofs are a combination of the so-called sample compression technique in (Ashtiani et al., 2018), mathematical tools from high-dimensional geometry, and Fourier analysis. In particular, we have proposed a general Fourier-based technique for recovery of a more general class of distribution families from additive Gaussian noise, which can be further used in a variety of other related problems.},
  author = {Seyed Amir Hossein Saberi and Amir Najafi 0003 and Abolfazl S. Motahari and Babak H. Khalaj},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saberi2023sample.pdf:pdf},
  mdate = {2024-03-27},
  pages = {29514--29541},
  pdf = {https://proceedings.mlr.press/v202/saberi23a/saberi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sample Complexity Bounds for Learning High-dimensional Simplices in Noisy Regimes},
  url = {https://proceedings.mlr.press/v202/saberi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sachidananda2023global,
  abstract = {Contrastive Learning has recently achieved state-of-the-art performance in a wide range of unimodal and multimodal tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbor indices, is more computationally efficient than the most minimal hard negative mining approaches, and makes no changes to the model being trained. Code is available at https://github.com/vinayak1/GCBS.},
  author = {Vin Sachidananda and Ziyi Yang and Chenguang Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sachidananda2023global.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29542--29562},
  pdf = {https://proceedings.mlr.press/v202/sachidananda23a/sachidananda23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Global Selection of Contrastive Batches via Optimization on Sample Permutations},
  url = {https://proceedings.mlr.press/v202/sachidananda23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sadiev2023highprobability,
  abstract = {During the recent years the interest of optimization and machine learning communities in high-probability convergence of stochastic optimization methods has been growing. One of the main reasons for this is that high-probability complexity bounds are more accurate and less studied than in-expectation ones. However, SOTA high-probability non-asymptotic convergence results are derived under strong assumptions such as boundedness of the gradient noise variance or of the objective's gradient itself. In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions. In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central $\alpha$-th moment for $\alpha \in (1,2\]$ in the following setups: (i) smooth non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone / quasi-strongly monotone variational inequalities. These results justify the usage of the considered methods for solving problems that do not fit standard functional classes studied in stochastic optimization.},
  author = {Abdurakhmon Sadiev and Marina Danilova and Eduard Gorbunov and Samuel Horvth and Gauthier Gidel and Pavel E. Dvurechensky and Alexander V. Gasnikov and Peter Richtrik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sadiev2023highprobability.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29563--29648},
  pdf = {https://proceedings.mlr.press/v202/sadiev23a/sadiev23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  url = {https://proceedings.mlr.press/v202/sadiev23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saha2023endtoend,
  abstract = {Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or AMs are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the AM dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with AM, dubbed ClAM. Leveraging the pattern completion ability of AMs, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that ClAM benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60% in terms of the Silhouette Coefficient).},
  author = {Bishwajit Saha and Dmitry Krotov and Mohammed J. Zaki and Parikshit Ram},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saha2023endtoend.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29649--29670},
  pdf = {https://proceedings.mlr.press/v202/saha23a/saha23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {End-to-end Differentiable Clustering with Associative Memories},
  url = {https://proceedings.mlr.press/v202/saha23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saig2023learning,
  abstract = {Optimizing user engagement is a key goal for modern recommendation systems, but blindly pushing users towards increased consumption risks burn-out, churn, or even addictive habits. To promote digital well-being, most platforms now offer a service that periodically prompts users to take breaks. These, however, must be set up manually, and so may be suboptimal for both users and the system. In this paper, we study the role of breaks in recommendation, and propose a framework for learning optimal breaking policies that promote and sustain long-term engagement. Based on the notion that recommendation dynamics are susceptible to both positive and negative feedback, we cast recommendation as a Lotka-Volterra dynamical system, where breaking reduces to a problem of optimal control. We then give an efficient learning algorithm, provide theoretical guarantees, and empirically demonstrate the utility of our approach on semi-synthetic data.},
  author = {Eden Saig and Nir Rosenfeld},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saig2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29671--29696},
  pdf = {https://proceedings.mlr.press/v202/saig23a/saig23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Suggest Breaks: Sustainable Optimization of Long-Term User Engagement},
  url = {https://proceedings.mlr.press/v202/saig23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saito2023multiclass,
  abstract = {This paper develops an approximation to the (effective) $p$-resistance and applies it to multi-class clustering. Spectral methods based on the graph Laplacian and its generalization to the graph $p$-Laplacian have been a backbone of non-euclidean clustering techniques. The advantage of the $p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster structure. The drawback of $p$-Laplacian eigenvector based methods is that the third and higher eigenvectors are difficult to compute. Thus, instead, we are motivated to use the $p$-resistance induced by the $p$-Laplacian for clustering. For $p$-resistance, small $p$ biases towards clusters with high internal connectivity while large $p$ biases towards clusters of small ``extent,'' that is a preference for smaller shortest-path distances between vertices in the cluster. However, the $p$-resistance is expensive to compute. We overcome this by developing an approximation to the $p$-resistance. We prove upper and lower bounds on this approximation and observe that it is exact when the graph is a tree. We also provide theoretical justification for the use of $p$-resistance for clustering. Finally, we provide experiments comparing our approximated $p$-resistance clustering to other $p$-Laplacian based methods.},
  author = {Shota Saito and Mark Herbster},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saito2023multiclass.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29697--29733},
  pdf = {https://proceedings.mlr.press/v202/saito23a/saito23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-class Graph Clustering via Approximated Effective p-Resistance},
  url = {https://proceedings.mlr.press/v202/saito23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saito2023offpolicy,
  abstract = {We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new assumption, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting OffCEM estimator substantially improves bias and variance compared to a range of conventional estimators. Experiments demonstrate that OffCEM provides substantial improvements in OPE especially in the presence of many actions.},
  author = {Yuta Saito and Qingyang Ren and Thorsten Joachims},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saito2023offpolicy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29734--29759},
  pdf = {https://proceedings.mlr.press/v202/saito23b/saito23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling},
  url = {https://proceedings.mlr.press/v202/saito23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sakaue2023rethinking,
  abstract = {An emerging line of work has shown that machine-learned predictions are useful to warm-start algorithms for discrete optimization problems, such as bipartite matching. Previous studies have shown time complexity bounds proportional to some distance between a prediction and an optimal solution, which we can approximately minimize by learning predictions from past optimal solutions. However, such guarantees may not be meaningful when multiple optimal solutions exist. Indeed, the dual problem of bipartite matching and, more generally, L-/L$^{\natural}$-convex function minimization have arbitrarily many optimal solutions, making such prediction-dependent bounds arbitrarily large. To resolve this theoretically critical issue, we present a new warm-start-with-prediction framework for L-/L$^{\natural}$-convex function minimization. Our framework offers time complexity bounds proportional to the distance between a prediction and the set of all optimal solutions. The main technical difficulty lies in learning predictions that are provably close to sets of all optimal solutions, for which we present an online-gradient-descent-based method. We thus give the first polynomial-time learnability of predictions that can provably warm-start algorithms regardless of multiple optimal solutions.},
  author = {Shinsaku Sakaue and Taihei Oki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sakaue2023rethinking.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29760--29776},
  pdf = {https://proceedings.mlr.press/v202/sakaue23a/sakaue23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethinking Warm-Starts with Predictions: Learning Predictions Close to Sets of Optimal Solutions for Faster L-/L$^{\natural}$-Convex Function Minimization},
  url = {https://proceedings.mlr.press/v202/sakaue23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sakhi2023pacbayesian,
  abstract = {This paper introduces a new principled approach for off-policy learning in contextual bandits. Unlike previous work, our approach does not derive learning principles from intractable or loose bounds. We analyse the problem through the PAC-Bayesian lens, interpreting policies as mixtures of decision rules. This allows us to propose novel generalization bounds and provide tractable algorithms to optimize them. We prove that the derived bounds are tighter than their competitors, and can be optimized directly to confidently improve upon the logging policy offline. Our approach learns policies with guarantees, uses all available data and does not require tuning additional hyperparameters on held-out sets. We demonstrate through extensive experiments the effectiveness of our approach in providing performance guarantees in practical scenarios.},
  author = {Otmane Sakhi and Pierre Alquier and Nicolas Chopin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sakhi2023pacbayesian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29777--29799},
  pdf = {https://proceedings.mlr.press/v202/sakhi23a/sakhi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PAC-Bayesian Offline Contextual Bandits With Guarantees},
  url = {https://proceedings.mlr.press/v202/sakhi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{salgia2023distributed,
  abstract = {We consider distributed linear bandits where $M$ agents learn collaboratively to minimize the overall cumulative regret incurred by all agents. Information exchange is facilitated by a central server, and both the uplink and downlink communications are carried over channels with fixed capacity, which limits the amount of information that can be transmitted in each use of the channels. We investigate the regret-communication trade-off by (i) establishing information-theoretic lower bounds on the required communications (in terms of bits) for achieving a sublinear regret order; (ii) developing an efficient algorithm that achieves the minimum sublinear regret order offered by centralized learning using the minimum order of communications dictated by the information-theoretic lower bounds. For sparse linear bandits, we show a variant of the proposed algorithm offers better regret-communication trade-off by leveraging the sparsity of the problem.},
  author = {Sudeep Salgia and Qing Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/salgia2023distributed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29845--29875},
  pdf = {https://proceedings.mlr.press/v202/salgia23b/salgia23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distributed Linear Bandits under Communication Constraints},
  url = {https://proceedings.mlr.press/v202/salgia23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{salgia2023provably,
  abstract = {We consider the neural contextual bandit problem. In contrast to the existing work which primarily focus on ReLU neural nets, we consider a general set of smooth activation functions. Under this more general setting, (i) we derive non-asymptotic error bounds on the difference between an overparameterized neural net and its corresponding neural tangent kernel, (ii) we propose an algorithm with a provable sublinear regret bound that is also efficient in the finite regime as demonstrated by empirical studies. The non-asymptotic error bounds may be of broader interests as a tool to establish the relation between the smoothness of the activation functions in neural contextual bandits and the smoothness of the kernels in kernel bandits.},
  author = {Sudeep Salgia},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/salgia2023provably.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29800--29844},
  pdf = {https://proceedings.mlr.press/v202/salgia23a/salgia23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably and Practically Efficient Neural Contextual Bandits},
  url = {https://proceedings.mlr.press/v202/salgia23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{salinas2023optimizing,
  abstract = {Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty. However, they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.},
  author = {David Salinas and Jacek Golebiowski and Aaron Klein and Matthias Seeger and Cedric Archambeau},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/salinas2023optimizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29876--29893},
  pdf = {https://proceedings.mlr.press/v202/salinas23a/salinas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimizing Hyperparameters with Conformal Quantile Regression},
  url = {https://proceedings.mlr.press/v202/salinas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{salman2023raising,
  abstract = {The paper presents an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. The authors provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, they discuss a policy component necessary to make their approach fully effective and practical -- one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.},
  author = {Hadi Salman and Alaa Khaddaj and Guillaume Leclerc and Andrew Ilyas and Aleksander Madry},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/salman2023raising.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29894--29918},
  pdf = {https://proceedings.mlr.press/v202/salman23a/salman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Raising the Cost of Malicious AI-Powered Image Editing},
  url = {https://proceedings.mlr.press/v202/salman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sander2023fast,
  abstract = {The top-$k$ operator returns a $k$-sparse vector, where the non-zero values correspond to the $k$ largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-$k$ operators.},
  author = {Michael Eli Sander and Joan Puigcerver and Josip Djolonga and Gabriel Peyr{\'e} and Mathieu Blondel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sander2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29919--29936},
  pdf = {https://proceedings.mlr.press/v202/sander23a/sander23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective},
  url = {https://proceedings.mlr.press/v202/sander23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sander2023tan,
  abstract = {Differentially Private methods for training Deep Neural Networks ({DNNs}) have progressed recently, in particular with the use of massive batches and aggregated data augmentations for a large number of training steps. These techniques require much more computing resources than their non-private counterparts, shifting the traditional privacy-accuracy trade-off to a privacy-accuracy-compute trade-off and making hyper-parameter search virtually impossible for realistic scenarios. In this work, we decouple privacy analysis and experimental behavior of noisy training to explore the trade-off with minimal computational requirements. We first use the tools of {Renyi} Differential Privacy ({RDP}) to highlight that the privacy budget, when not overcharged, only depends on the total amount of noise ({TAN}) injected throughout training. We then derive scaling laws for training models with {DP-SGD} to optimize hyper-parameters with more than a $100\times$ reduction in computational budget. We apply the proposed method on {CIFAR-10} and {ImageNet} and, in particular, strongly improve the state-of-the-art on {ImageNet} with a $+9$ points gain in top-1 accuracy for a privacy budget $\varepsilon=8$.},
  author = {Tom Sander and Pierre Stock and Alexandre Sablayrolles},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sander2023tan.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29937--29949},
  pdf = {https://proceedings.mlr.press/v202/sander23b/sander23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TAN} Without a Burn: Scaling Laws of {DP-SGD}},
  url = {https://proceedings.mlr.press/v202/sander23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sangani2023discrete,
  abstract = {We study a new framework of learning mixture models via automatic clustering called {PRESTO}, wherein we optimize a joint objective function on the model parameters and the partitioning, with each model tailored to perform well on its specific cluster. In contrast to prior work, we do not assume any generative model for the data. We convert our training problem to a joint parameter estimation cum a subset selection problem, subject to a matroid span constraint. This allows us to reduce our problem into a constrained set function minimization problem, where the underlying objective is monotone and approximately submodular. We then propose a new joint discrete-continuous optimization algorithm that achieves a bounded approximation guarantee for our problem. We show that {PRESTO} outperforms several alternative methods. Finally, we study {PRESTO} in the context of resource-efficient deep learning, where we train smaller resource-constrained models on each partition and show that it outperforms existing data partitioning and model pruning/knowledge distillation approaches, which in contrast to {PRESTO}, require large initial (teacher) models.},
  author = {Parth Vipul Sangani and Arjun Shashank Kashettiwar and Pritish Chakraborty and Bhuvan Reddy Gangula and Durga Sivasubramanian and Ganesh Ramakrishnan and Rishabh K. Iyer and Abir De},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sangani2023discrete.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29950--29970},
  pdf = {https://proceedings.mlr.press/v202/sangani23a/sangani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discrete Continuous Optimization Framework for Simultaneous Clustering and Training in Mixture Models},
  url = {https://proceedings.mlr.press/v202/sangani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{santos2023blackout,
  abstract = {Typical generative diffusion models rely on a {Gaussian} diffusion process for training the backward transformations, which can then be used to generate samples from {Gaussian} noise. However, real world data often takes place in discrete-state spaces, including many scientific applications. Here, we develop a theoretical formulation for arbitrary discrete-state {Markov} processes in the forward diffusion process using exact (as opposed to variational) analysis.},
  author = {Javier E. Santos and Zachary R. Fox and Nicholas Lubbers and Yen Ting Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/santos2023blackout.pdf:pdf},
  mdate = {2023-08-28},
  pages = {9034--9059},
  pdf = {https://proceedings.mlr.press/v202/santos23a/santos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces},
  url = {https://proceedings.mlr.press/v202/santos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{santurkar2023whose,
  abstract = {Language models ({LMs}) are increasingly being used in open-ended contexts, where the opinions they reflect in response to subjective queries can have a profound impact, both on user satisfaction, and shaping the views of society at large. We put forth a quantitative framework to investigate the opinions reflected by {LMs} -- by leveraging high-quality public opinion polls. Using this framework, we create {OpinionQA}, a dataset for evaluating the alignment of {LM} opinions with those of 60 {US} demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current {LMs} and those of {US} demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the {LMs} towards particular groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned {LMs}, but also surfaces groups whose opinions are poorly reflected by current {LMs} (e.g., 65+ and widowed individuals).},
  author = {Shibani Santurkar and Esin Durmus and Faisal Ladhak and Cinoo Lee and Percy Liang and Tatsunori Hashimoto},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/santurkar2023whose.pdf:pdf},
  mdate = {2023-08-28},
  pages = {29971--30004},
  pdf = {https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Whose Opinions Do Language Models Reflect?},
  url = {https://proceedings.mlr.press/v202/santurkar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saran2023streaming,
  abstract = {Active learning is perhaps most naturally posed as an online learning problem. However, prior active learning approaches with deep neural networks assume offline access to the entire dataset ahead of time. This paper proposes {VeSSAL}, a new algorithm for batch active learning with deep neural networks in streaming settings, which samples groups of points to query for labels at the moment they are encountered. Our approach trades off between uncertainty and diversity of queried samples to match a desired query rate without requiring any hand-tuned hyperparameters. Altogether, we expand the applicability of deep neural networks to realistic active learning scenarios, such as applications relevant to {HCI} and large, fractured datasets.},
  author = {Akanksha Saran and Safoora Yousefi and Akshay Krishnamurthy and John Langford and Jordan T. Ash},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saran2023streaming.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30005--30021},
  pdf = {https://proceedings.mlr.press/v202/saran23a/saran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Streaming Active Learning with Deep Neural Networks},
  url = {https://proceedings.mlr.press/v202/saran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sarnthein2023random,
  abstract = {In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape.},
  author = {Felix Sarnthein and Gregor Bachmann and Sotiris Anagnostidis and Thomas Hofmann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sarnthein2023random.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30022--30041},
  pdf = {https://proceedings.mlr.press/v202/sarnthein23a/sarnthein23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Teachers are Good Teachers},
  url = {https://proceedings.mlr.press/v202/sarnthein23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sasso2023posterior,
  abstract = {Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning ({PSDRL}), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. {PSDRL} combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation.},
  author = {Remo Sasso and Michelangelo Conserva and Paulo E. Rauber},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sasso2023posterior.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30042--30061},
  pdf = {https://proceedings.mlr.press/v202/sasso23a/sasso23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Posterior Sampling for Deep Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/sasso23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sato2023graph,
  abstract = {Graph Neural Networks ({GNNs}) are popular models for graph learning problems. {GNNs} show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether {GNNs} can exploit the graph structure from the perspective of the expressive power of {GNNs}. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is {kNN} graphs constructed from the hidden features. In our main results, we show that {GNNs} can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable.},
  author = {Ryoma Sato},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sato2023graph.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30062--30079},
  pdf = {https://proceedings.mlr.press/v202/sato23a/sato23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure},
  url = {https://proceedings.mlr.press/v202/sato23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sato2023existence,
  abstract = {Previous results have shown that a two time-scale update rule ({TTUR}) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks ({GANs}) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training {GANs} with {TTURs} and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training {GANs} with {TTURs} based on constant learning rates. We theoretically show that, for a {TTUR} with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle ({SFO}) complexity. Then, we use the {Fr{\'e}chet} inception distance ({FID}) as the performance measure for training and provide numerical results indicating that the number of steps needed to achieve a low {FID} score decreases as the batch size increases and that the {SFO} complexity increases once the batch size exceeds the measured critical batch size. Moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results.},
  author = {Naoki Sato and Hideaki Iiduka},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sato2023existence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30080--30104},
  pdf = {https://proceedings.mlr.press/v202/sato23b/sato23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule},
  url = {https://proceedings.mlr.press/v202/sato23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sauer2023stylegant,
  abstract = {Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.},
  author = {Axel Sauer and Tero Karras and Samuli Laine and Andreas Geiger 0001 and Timo Aila},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sauer2023stylegant.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30105-30118},
  pdf = {https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis},
  url = {https://proceedings.mlr.press/v202/sauer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{savchenko2023facial,
  abstract = {We propose a novel sequential procedure with adaptive frame rate selection in short video fragments to speed up video-based facial expression recognition decision-making. The method automatically adjusts the frame rate, processing fewer frames with low frame rates for straightforward videos and more frames for complex ones. To determine when inference is sufficiently reliable, the Benjamini-Hochberg procedure from multiple comparisons theory is employed to control the false discovery rate. The main advantages include improved trustworthiness of decision-making by maintaining only one hyper-parameter (false acceptance rate) and applicability with arbitrary neural network models without retraining.},
  author = {Andrey V. Savchenko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/savchenko2023facial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30119-30129},
  pdf = {https://proceedings.mlr.press/v202/savchenko23a/savchenko23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Facial Expression Recognition with Adaptive Frame Rate based on Multiple Testing Correction},
  url = {https://proceedings.mlr.press/v202/savchenko23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{saxena2023offpolicy,
  abstract = {The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an ε-optimal stationary policy with a sample complexity of Ω(ε^{-2.5}).},
  author = {Naman Saxena and Subhojyoti Khastagir and Shishir Kolathaya and Shalabh Bhatnagar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/saxena2023offpolicy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30130-30203},
  pdf = {https://proceedings.mlr.press/v202/saxena23a/saxena23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Off-Policy Average Reward Actor-Critic with Deterministic Policy Search},
  url = {https://proceedings.mlr.press/v202/saxena23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{scha2023gibbsian,
  abstract = {Polar slice sampling (Roberts \& Rosenthal, 2002) is a Markov chain approach for approximate sampling of distributions that is difficult, if not impossible, to implement efficiently, but behaves provably well with respect to the dimension. By updating the directional and radial components of chain iterates separately, we obtain a family of samplers that mimic polar slice sampling, and yet can be implemented efficiently. Numerical experiments in a variety of settings indicate that our proposed algorithm outperforms the two most closely related approaches, elliptical slice sampling (Murray et al., 2010) and hit-and-run uniform slice sampling (MacKay, 2003). We prove the well-definedness and convergence of our methods under suitable assumptions on the target distribution.},
  author = {Philip Sch{\"{a}}r and Michael Habeck and Daniel Rudolf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/scha2023gibbsian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30204-30223},
  pdf = {https://proceedings.mlr.press/v202/schar23a/schar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gibbsian Polar Slice Sampling},
  url = {https://proceedings.mlr.press/v202/schar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{schlaginhaufen2023identifiability,
  abstract = {Two main challenges in Reinforcement Learning (RL) are designing appropriate reward functions and ensuring the safety of the learned policy. To address these challenges, we present a theoretical framework for Inverse Reinforcement Learning (IRL) in constrained Markov decision processes. From a convex-analytic perspective, we extend prior results on reward identifiability and generalizability to both the constrained setting and a more general class of regularizations. In particular, we show that identifiability up to potential shaping (Cao et al., 2021) is a consequence of entropy regularization and may generally no longer hold for other regularizations or in the presence of safety constraints. We also show that to ensure generalizability to new transition laws and constraints, the true reward must be identified up to a constant. Additionally, we derive a finite sample guarantee for the suboptimality of the learned rewards, and validate our results in a gridworld environment.},
  author = {Andreas Schlaginhaufen and Maryam Kamgarpour},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schlaginhaufen2023identifiability.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30224-30251},
  pdf = {https://proceedings.mlr.press/v202/schlaginhaufen23a/schlaginhaufen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Identifiability and Generalizability in Constrained Inverse Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/schlaginhaufen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{schnaus2023learning,
  abstract = {The paper proposes a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Their learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. They also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. The major enablers are their technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, they exhaustively show the effectiveness of this method for uncertainty estimation and generalization.},
  author = {Dominik Schnaus and Jongseok Lee and Daniel Cremers and Rudolph Triebel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schnaus2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30252-30284},
  pdf = {https://proceedings.mlr.press/v202/schnaus23a/schnaus23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks},
  url = {https://proceedings.mlr.press/v202/schnaus23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{schro2023deterministic,
  abstract = {We consider the problem of learning a random Gaussian network function using a fully connected network with frozen intermediate layers and trainable readout layer. This problem can be seen as a natural generalization of the widely studied random features model to deeper architectures. We prove Gaussian universality of the test error in a ridge regression setting where the learner and target networks share the same intermediate layers, and provide a sharp asymptotic formula for it. Establishing this result requires proving a deterministic equivalent for traces of the deep random features sample covariance matrices which can be of independent interest. We conjecture the asymptotic Gaussian universality of the test error in the more general setting of arbitrary convex losses and generic learner/target architectures. We provide extensive numerical evidence for this conjecture, which requires the derivation of closed-form expressions for the layer-wise post-activation population covariances. In light of our results, we investigate the interplay between architecture design and implicit regularization.},
  author = {Dominik Schr{\"{o}}der and Hugo Cui and Daniil Dmitriev and Bruno Loureiro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schro2023deterministic.pdf:pdf},
  mdate = {2024-10-06},
  pages = {30285-30320},
  pdf = {https://proceedings.mlr.press/v202/schroder23a/schroder23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deterministic equivalent and error universality of deep random features learning},
  url = {https://proceedings.mlr.press/v202/schroder23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{schwarz2023modalityagnostic,
  abstract = {We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases.},
  author = {Jonathan Richard Schwarz and Jihoon Tack and Yee Whye Teh and Jaeho Lee 0001 and Jinwoo Shin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schwarz2023modalityagnostic.pdf:pdf},
  mdate = {2023-09-27},
  pages = {30342-30364},
  pdf = {https://proceedings.mlr.press/v202/schwarz23a/schwarz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Modality-Agnostic Variational Compression of Implicit Neural Representations},
  url = {https://proceedings.mlr.press/v202/schwarz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{schwarzer2023bigger,
  abstract = {We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE.},
  author = {Max Schwarzer and Johan Samir Obando-Ceron and Aaron C. Courville and Marc G. Bellemare and Rishabh Agarwal and Pablo Samuel Castro},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/schwarzer2023bigger.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30365-30380},
  pdf = {https://proceedings.mlr.press/v202/schwarzer23a/schwarzer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bigger, Better, Faster: Human-level Atari with human-level efficiency},
  url = {https://proceedings.mlr.press/v202/schwarzer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sclocchi2023dissecting,
  abstract = {Understanding when the noise in stochastic gradient descent (SGD) affects generalization of deep neural networks remains a challenge, complicated by the fact that networks can operate in distinct training regimes. Here we study how the magnitude of this noise $T$ affects performance as the size of the training set $P$ and the scale of initialization $\alpha$ are varied. For gradient descent, $\alpha$ is a key parameter that controls if the network is `lazy' ($\alpha\gg1$) or instead learns features ($\alpha\ll1$). For classification of MNIST and CIFAR10 images, our central results are: (i) obtaining phase diagrams for performance in the $(\alpha,T)$ plane. They show that SGD noise can be detrimental or instead useful depending on the training regime. Moreover, although increasing $T$ or decreasing $\alpha$ both allow the net to escape the lazy regime, these changes can have opposite effects on performance.},
  author = {Antonio Sclocchi and Mario Geiger and Matthieu Wyart},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sclocchi2023dissecting.pdf:pdf},
  mdate = {2024-10-06},
  pages = {30381-30405},
  pdf = {https://proceedings.mlr.press/v202/sclocchi23a/sclocchi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning},
  url = {https://proceedings.mlr.press/v202/sclocchi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sedlmayer2023fast,
  abstract = {We study monotone variational inequalities that can arise as optimality conditions for constrained convex optimization or convex-concave minimax problems and propose a novel algorithm that uses only one gradient/operator evaluation and one projection onto the constraint set per iteration. The algorithm, which we call fOGDA-VI, achieves a $o(\frac{1}{k})$ rate of convergence in terms of the restricted gap function as well as the natural residual for the last iterate. Moreover, we provide a convergence guarantee for the sequence of iterates to a solution of the variational inequality. These are the best theoretical convergence results for numerical methods for (only) monotone variational inequalities reported in the literature. To empirically validate our algorithm we investigate a two-player matrix game with mixed strategies of the two players. Concluding, we show promising results regarding the application of fOGDA-VI to the training of generative adversarial nets.},
  author = {Michael Sedlmayer and Dang-Khoa Nguyen and Radu Ioan Bot},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sedlmayer2023fast.pdf:pdf},
  mdate = {2024-08-04},
  pages = {30406--30438},
  pdf = {https://proceedings.mlr.press/v202/sedlmayer23a/sedlmayer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Fast Optimistic Method for Monotone Variational Inequalities},
  url = {https://proceedings.mlr.press/v202/sedlmayer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{jose2023doubleweighting,
  abstract = {Supervised learning is often affected by a covariate shift in which the marginal distributions of instances (covariates x) of training and testing samples p_tr(x) and p_te(x) are different but the label conditionals coincide. Existing approaches address such covariate shift by either using the ratio p_te(x)/p_tr(x) to weight training samples (reweighted methods) or using the ratio p_tr(x)/p_te(x) to weight testing samples (robust methods). However, the performance of such approaches can be poor under support mismatch or when the above ratios take large values. We propose a minimax risk classification (MRC) approach for covariate shift adaptation that avoids such limitations by weighting both training and testing samples. In addition, we develop effective techniques that obtain both sets of weights and generalize the conventional kernel mean matching method.},
  author = {Jos\'{e} Ignacio Segovia-Mart\'{i}n and Santiago Mazuelas and Anqi Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/jose2023doubleweighting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30439--30457},
  pdf = {https://proceedings.mlr.press/v202/segovia-martin23a/segovia-martin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Double-Weighting for Covariate Shift Adaptation},
  url = {https://proceedings.mlr.press/v202/segovia-martin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{seidl2023enhancing,
  abstract = {Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pretraining objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery.},
  author = {Philipp Seidl and Andreu Vall and Sepp Hochreiter and G{\"u}nter Klambauer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/seidl2023enhancing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30458--30490},
  pdf = {https://proceedings.mlr.press/v202/seidl23a/seidl23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language},
  url = {https://proceedings.mlr.press/v202/seidl23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{seidman2023variational,
  abstract = {The prevalence of high-dimensional, functional data in fields such as computer vision, climate modeling, and physical systems has sparked interest in unsupervised learning with functional data. A natural way of modeling functional data is by learning operators between infinite dimensional spaces, leading to discretization invariant representations that scale independently of the sample grid resolution. In this work, we present Variational Autoencoding Neural Operators (VANO), a general strategy for making a large class of operator learning architectures act as variational autoencoders. For this purpose, we provide a novel rigorous mathematical formulation of the variational objective in function spaces for training. VANO first maps an input function to a distribution over a latent space using a parametric encoder and then decodes a sample from the latent distribution to reconstruct the input, as in classic variational autoencoders. We test VANO with different model setups and architecture choices for a variety of benchmarks, starting from a simple Gaussian random field where we can analytically track what the model learns, progressively transitioning to more challenging benchmarks including modeling phase separation in Cahn-Hilliard systems and real world satellite data for measuring Earth surface deformation.},
  author = {Jacob H. Seidman and Georgios Kissas and George J. Pappas and Paris Perdikaris},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/seidman2023variational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30491--30522},
  pdf = {https://proceedings.mlr.press/v202/seidman23a/seidman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Variational Autoencoding Neural Operators},
  url = {https://proceedings.mlr.press/v202/seidman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{seifner2023neural,
  abstract = {Markov jump processes are continuous-time stochastic processes with a wide range of applications in both natural and social sciences. Despite their widespread use, inference in these models is highly non-trivial and typically proceeds via either Monte Carlo or expectation-maximization methods. Here, we introduce an alternative, variational inference algorithm for Markov jump processes which relies on neural ordinary differential equations, and is trainable via back-propagation. Our methodology learns neural, continuous-time representations of the observed data, that are used to approximate the initial distribution and time-dependent transition probability rates of the posterior Markov jump process. We test our approach on synthetic data sampled from ground-truth Markov jump processes, experimental switching ion channel data and molecular dynamics simulations.},
  author = {Patrick Seifner and Rams\'{e}s J. S\'{a}nchez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/seifner2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30523--30552},
  pdf = {https://proceedings.mlr.press/v202/seifner23a/seifner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Markov Jump Processes},
  url = {https://proceedings.mlr.press/v202/seifner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sellier2023bayesian,
  abstract = {In this paper, we introduce a variant of Bayesian online change point detection with a reduced-rank Student-t process (TP) and dependent Student-t noise, as a nonparametric time series model. Our method builds and improves upon the state-of-the-art Gaussian process (GP) change point model benchmark of Saatçi et al. (2010). The Student-t process generalizes the concept of a GP and hence yields a more flexible alternative. Unlike a GP, the predictive variance explicitly depends on the training observations, while the use of an entangled Student-t noise model preserves analytical tractability. Our approach also uses a Hilbert space reduced-rank representation of the TP kernel, derived from an eigenfunction expansion of the Laplace operator (Solin \& S\"{a}rkk\"{a}, 2020), to alleviate its computational complexity. Improvements in prediction and training time are demonstrated with real-world data sets.},
  author = {Jeremy Sellier and Petros Dellaportas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sellier2023bayesian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30553--30569},
  pdf = {https://proceedings.mlr.press/v202/sellier23a/sellier23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayesian online change point detection with Hilbert space approximate Student-t process},
  url = {https://proceedings.mlr.press/v202/sellier23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sellke2023incentivizing,
  abstract = {We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.},
  author = {Mark Sellke},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sellke2023incentivizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30570--30583},
  pdf = {https://proceedings.mlr.press/v202/sellke23a/sellke23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Incentivizing Exploration with Linear Contexts and Combinatorial Actions},
  url = {https://proceedings.mlr.press/v202/sellke23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{se2023explainability,
  abstract = {A wide variety of model explanation approaches have been proposed in recent years, all guided by very different rationales and heuristics. In this paper, we take a new route and cast interpretability as a statistical inference problem. We propose a general deep probabilistic model designed to produce interpretable predictions. The model parameters can be learned via maximum likelihood, and the method can be adapted to any predictor network architecture and any type of prediction problem. Our method is a case of amortized interpretability models, where a neural network is used as a selector to allow for fast interpretation at inference time. Several popular interpretability methods are shown to be particular cases of regularised maximum likelihood for our general model. We propose new datasets with ground truth selection which allow for the evaluation of the features importance map. Using these datasets, we show experimentally that using multiple imputation provides more reasonable interpretations.},
  author = {Hugo Henri Joseph S\'{e}n\'{e}taire and Damien Garreau and Jes Frellsen and Pierre-Alexandre Mattei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/se2023explainability.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30584--30612},
  pdf = {https://proceedings.mlr.press/v202/senetaire23a/senetaire23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Explainability as statistical inference},
  url = {https://proceedings.mlr.press/v202/senetaire23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{seo2023multiview,
  abstract = {Visual robotic manipulation research and applications often use multiple cameras, or views, to better perceive the world. How else can we utilize the richness of multi-view data? In this paper, we investigate how to learn good representations with multi-view data and utilize them for visual robotic manipulation. Specifically, we train a multi-view masked autoencoder which reconstructs pixels of randomly masked viewpoints and then learn a world model operating on the representations from the autoencoder. We demonstrate the effectiveness of our method in a range of scenarios, including multi-view control and single-view control with auxiliary cameras for representation learning. We also show that the multi-view masked autoencoder trained with multiple randomized viewpoints enables training a policy with strong viewpoint randomization and transferring the policy to solve real-robot tasks without camera calibration and an adaptation procedure.},
  author = {Younggyo Seo and Junsu Kim and Stephen James and Kimin Lee and Jinwoo Shin and Pieter Abbeel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/seo2023multiview.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30613--30632},
  pdf = {https://proceedings.mlr.press/v202/seo23a/seo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-View Masked World Models for Visual Robotic Manipulation},
  url = {https://proceedings.mlr.press/v202/seo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shah2023modeldiff,
  abstract = {We study the problem of (learning) algorithm comparison, where the goal is to find differences between models trained with two different learning algorithms. We formalize this goal as one of finding distinguishing feature transformations, i.e., input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present ModelDiff, a method that leverages the datamodels framework (Ilyas et al., 2022) to compare learning algorithms based on how they use their training data. ModelDiff can be used to ``explain'' the difference between models trained with two different algorithms and help choose between competing algorithmic choices. We demonstrate ModelDiff through three case studies, comparing models trained with/without data augmentation, with/without pre-training, and with different SGD hyperparameters.},
  author = {Harshay Shah and Sung Min Park and Andrew Ilyas and Aleksander M{\k{a}}dry},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shah2023modeldiff.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30646--30688},
  pdf = {https://proceedings.mlr.press/v202/shah23a/shah23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ModelDiff: A Framework for Comparing Learning Algorithms},
  url = {https://proceedings.mlr.press/v202/shah23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shamsian2023auxiliary,
  abstract = {Auxiliary learning is an effective method for enhancing the generalization capabilities of trained models, particularly when dealing with small datasets. However, this approach may present several difficulties: (i) optimizing multiple objectives can be more challenging, and (ii) how to balance the auxiliary tasks to best assist the main task is unclear. In this work, we propose a novel approach, named AuxiNash, for balancing tasks in auxiliary learning by formalizing the problem as generalized bargaining game with asymmetric task bargaining power. Furthermore, we describe an efficient procedure for learning the bargaining power of tasks based on their contribution to the performance of the main task and derive theoretical guarantees for its convergence.},
  author = {Aviv Shamsian and Aviv Navon and Neta Glazer and Kenji Kawaguchi and Gal Chechik and Ethan Fetaya},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shamsian2023auxiliary.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30689--30705},
  pdf = {https://proceedings.mlr.press/v202/shamsian23a/shamsian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Auxiliary Learning as an Asymmetric Bargaining Game},
  url = {https://proceedings.mlr.press/v202/shamsian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shao2023synthetic,
  abstract = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear.},
  author = {Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shao2023synthetic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30706--30775},
  pdf = {https://proceedings.mlr.press/v202/shao23a/shao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models},
  url = {https://proceedings.mlr.press/v202/shao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shao2023complementary,
  abstract = {In cooperative multi-agent reinforcement learning, centralized training with decentralized execution (CTDE) shows great promise for a trade-off between independent Q-learning and joint action learning. However, vanilla CTDE methods assumed a fixed number of agents could hardly adapt to real-world scenarios where dynamic team compositions typically suffer from dramatically variant partial observability. Specifically, agents with extensive sight ranges are prone to be affected by trivial environmental substrates, dubbed the 'distracted attention' issue; ones with limited observation can hardly sense their teammates, degrading the cooperation quality. In this paper, we propose Complementary Attention for Multi-Agent reinforcement learning (CAMA), which applies a divide-and-conquer strategy on input entities accompanied with the complementary attention of enhancement and replenishment.},
  author = {Jianzhun Shao and Hongchang Zhang and Yun Qu 0002 and Chang Liu 0030 and Shuncheng He and Yuhang Jiang and Xiangyang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shao2023complementary.pdf:pdf},
  mdate = {2024-09-02},
  pages = {30776--30793},
  pdf = {https://proceedings.mlr.press/v202/shao23b/shao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Complementary Attention for Multi-Agent Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/shao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sharifnassab2023toward,
  abstract = {Gradient-based methods for value estimation in reinforcement learning have favorable stability properties, but they are typically much slower than Temporal Difference (TD) learning methods. We study the root causes of this slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned loss function in the sense that its Hessian has large condition-number. To resolve the adverse effect of poor conditioning of MSBE on gradient based methods, we propose a low complexity batch-free proximal method that approximately follows the Gauss-Newton direction and is asymptotically robust to parameterization. Our main algorithm, called RANS, is efficient in the sense that it is significantly faster than the residual gradient methods while having almost the same computational complexity, and is competitive with TD on the classic problems that we tested.},
  author = {Arsalan Sharifnassab and Richard S. Sutton},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sharifnassab2023toward.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30827--30849},
  pdf = {https://proceedings.mlr.press/v202/sharifnassab23a/sharifnassab23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Toward Efficient Gradient-Based Value Estimation},
  url = {https://proceedings.mlr.press/v202/sharifnassab23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sharrock2023coin,
  abstract = {In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.},
  author = {Louis Sharrock and Christopher Nemeth},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sharrock2023coin.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30850--30882},
  pdf = {https://proceedings.mlr.press/v202/sharrock23a/sharrock23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates},
  url = {https://proceedings.mlr.press/v202/sharrock23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shaul2023kinetic,
  abstract = {Recent successful generative models are trained by fitting a neural network to an a-priori defined tractable probability density path taking noise to training examples. In this paper we investigate the space of Gaussian probability paths, which includes diffusion paths as an instance, and look for an optimal member in some useful sense. In particular, minimizing the Kinetic Energy (KE) of a path is known to make particles' trajectories simple, hence easier to sample, and empirically improve performance in terms of likelihood of unseen data and sample generation quality. We investigate Kinetic Optimal (KO) Gaussian paths and offer the following observations: (i) We show the KE takes a simplified form on the space of Gaussian paths, where the data is incorporated only through a single, one dimensional scalar function, called the data separation function. (ii) We characterize the KO solutions with a one dimensional ODE.},
  author = {Neta Shaul and Ricky T. Q. Chen and Maximilian Nickel and Matthew Le 0001 and Yaron Lipman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shaul2023kinetic.pdf:pdf},
  mdate = {2024-08-06},
  pages = {30883--30907},
  pdf = {https://proceedings.mlr.press/v202/shaul23a/shaul23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Kinetic Optimal Probability Paths for Generative Models},
  url = {https://proceedings.mlr.press/v202/shaul23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shekhar2023sequential,
  abstract = {We present a simple reduction from sequential estimation to sequential changepoint detection (SCD). In short, suppose we are interested in detecting changepoints in some parameter or functional θ of the underlying distribution. We demonstrate that if we can construct a confidence sequence (CS) for θ, then we can also successfully perform SCD for θ. This is accomplished by checking if two CSs — one forwards and the other backwards — ever fail to intersect. Since the literature on CSs has been rapidly evolving recently, the reduction provided in this paper immediately solves several old and new change detection problems. Further, our backward CS, constructed by reversing time, is new and potentially of independent interest. We provide strong nonasymptotic guarantees on the frequency of false alarms and detection delay, and demonstrate numerical effectiveness on several problems.},
  author = {Shubhanshu Shekhar and Aaditya Ramdas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shekhar2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30908--30930},
  pdf = {https://proceedings.mlr.press/v202/shekhar23a/shekhar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Changepoint Detection via Backward Confidence Sequences},
  url = {https://proceedings.mlr.press/v202/shekhar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shekhovtsov2023cold,
  abstract = {Many problems in machine learning require an estimate of the gradient of an expectation in discrete random variables with respect to the sampling distribution. This work is motivated by the development of the Gumbel-Softmax family of estimators, which use a temperature-controlled relaxation of discrete variables. The state-of-the art in this family, the Gumbel-Rao estimator uses an extra internal sampling to reduce the variance, which may be costly. We analyze this estimator and show that it possesses a zero temperature limit with a surprisingly simple closed form. The limit estimator, called ZGR, has favorable bias and variance properties, it is easy to implement and computationally inexpensive. It decomposes as the average of the straight through (ST) estimator and DARN estimator — two basic but not very well performing on their own estimators.},
  author = {Alexander Shekhovtsov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shekhovtsov2023cold.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30931--30955},
  pdf = {https://proceedings.mlr.press/v202/shekhovtsov23a/shekhovtsov23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cold Analysis of Rao-Blackwellized Straight-Through Gumbel-Softmax Gradient Estimator},
  url = {https://proceedings.mlr.press/v202/shekhovtsov23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023towards,
  abstract = {Generative flow networks (GFlowNets) are a family of algorithms that learn a generative policy to sample discrete objects x with non-negative reward R(x). Learning objectives guarantee the GFlowNet samples x from the target distribution p*(x) ∝ R(x) when loss is globally minimized over all states or trajectories, but it is unclear how well they perform with practical limits on training resources. We introduce an efficient evaluation strategy to compare the learned sampling distribution to the target reward distribution. As flows can be underdetermined given training data, we clarify the importance of learned flows to generalization and matching p*(x) in practice. We investigate how to learn better flows, and propose (i) prioritized replay training of high-reward x, (ii) relative edge flow policy parametrization, and (iii) a novel guided trajectory balance objective, and show how it can solve a substructure credit assignment problem.},
  author = {Max W. Shen and Emmanuel Bengio and Ehsan Hajiramezanali and Andreas Loukas and Kyunghyun Cho and Tommaso Biancalani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30956--30975},
  pdf = {https://proceedings.mlr.press/v202/shen23a/shen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Understanding and Improving GFlowNet Training},
  url = {https://proceedings.mlr.press/v202/shen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023balancing,
  abstract = {Due to privacy, storage, and other constraints, there is a growing need for unsupervised domain adaptation techniques in machine learning that do not require access to the data used to train a collection of source models. Existing methods for multi-source-free domain adaptation (MSFDA) typically train a target model using pseudo-labeled data produced by the source models, which focus on improving the pseudo-labeling techniques or proposing new training objectives. Instead, we aim to analyze the fundamental limits of MSFDA. In particular, we develop an information-theoretic bound on the generalization error of the resulting target model, which illustrates an inherent bias-variance trade-off. We then provide insights on how to balance this trade-off from three perspectives, including domain aggregation, selective pseudo-labeling, and joint feature alignment, which leads to the design of novel algorithms.},
  author = {Maohao Shen and Yuheng Bu and Gregory W. Wornell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023balancing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30976--30991},
  pdf = {https://proceedings.mlr.press/v202/shen23b/shen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Balancing Bias and Variance in Unsupervised Multi-Source-Free Domain Adaptation},
  url = {https://proceedings.mlr.press/v202/shen23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023penaltybased,
  abstract = {Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel problems are difficult to solve and recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. The experimental results showcase the efficiency of the proposed algorithm.},
  author = {Han Shen and Tianyi Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023penaltybased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30992--31015},
  pdf = {https://proceedings.mlr.press/v202/shen23c/shen23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Penalty-based Bilevel Gradient Descent Method},
  url = {https://proceedings.mlr.press/v202/shen23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023nonautoregressive,
  abstract = {Recently, denoising diffusion models have led to significant breakthroughs in the generation of images, audio and text. However, it is still an open question on how to adapt their strong modeling ability to model time series. In this paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves high-quality time series prediction with the introduction of two novel conditioning mechanisms: future mixup and autoregressive initialization. Similar to teacher forcing, future mixup allows parts of the ground-truth future predictions for conditioning, while autoregressive initialization helps better initialize the model with basic time series patterns such as short-term trends. Extensive experiments are performed on nine real-world datasets. Results show that TimeDiff consistently outperforms existing time series diffusion models, and also achieves the best overall performance across a variety of the existing strong baselines (including transformers and FiLM).},
  author = {Lifeng Shen and James T. Kwok},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023nonautoregressive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31016--31029},
  pdf = {https://proceedings.mlr.press/v202/shen23d/shen23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Non-autoregressive Conditional Diffusion Models for Time Series Prediction},
  url = {https://proceedings.mlr.press/v202/shen23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023crossmodal,
  abstract = {Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities.},
  author = {Junhong Shen and Liam Li and Lucio M. Dery and Corey Staten and Mikhail Khodak and Graham Neubig and Ameet Talwalkar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023crossmodal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31030--31056},
  pdf = {https://proceedings.mlr.press/v202/shen23e/shen23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Cross-Modal Fine-Tuning: Align then Refine},
  url = {https://proceedings.mlr.press/v202/shen23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shen2023auxiliary,
  abstract = {Driven by the need from real-world applications, Auxiliary Modality Learning (AML) offers the possibility to utilize more information from auxiliary data in training, while only requiring data from one or fewer modalities in test, to save the overall computational cost and reduce the amount of input data for inferencing. In this work, we formally define "Auxiliary Modality Learning" (AML), systematically classify types of auxiliary modality (in visual computing) and architectures for AML, and analyze their performance. We also analyze the conditions under which AML works well from the optimization and data distribution perspectives. To guide various choices to achieve optimal performance using AML, we propose a novel method to assist in choosing the best auxiliary modality and estimating an upper bound performance before executing AML. In addition, we propose a new AML method using generalized curriculum distillation to enable more effective curriculum learning.},
  author = {Yu Shen and Xijun Wang and Peng Gao and Ming C. Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shen2023auxiliary.pdf:pdf},
  mdate = {2023-10-31},
  pages = {31057--31076},
  pdf = {https://proceedings.mlr.press/v202/shen23f/shen23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Auxiliary Modality Learning with Generalized Curriculum Distillation},
  url = {https://proceedings.mlr.press/v202/shen23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shenfeld2023tgrl,
  abstract = {We consider solving sequential decision-making problems in the scenario where the agent has access to two supervision sources: reward signal and a teacher that can be queried to obtain a good action for any state encountered by the agent. Learning solely from rewards, or reinforcement learning, is data inefficient and may not learn high-reward policies in challenging scenarios involving sparse rewards or partial observability. On the other hand, learning from a teacher may sometimes be infeasible. For instance, the actions provided by a teacher with privileged information may be unlearnable by an agent with limited information (i.e., partial observability). In other scenarios, the teacher might be sub-optimal, and imitating their actions can limit the agent's performance. To overcome these challenges, prior work proposed to jointly optimize imitation and reinforcement learning objectives but relied on heuristics and problem-specific hyper-parameter tuning to balance the two objectives. We introduce Teacher Guided Reinforcement Learning (TGRL), a principled approach to dynamically balance following the teacher's guidance and leveraging RL. The main idea is to adjust the importance of teacher supervision by comparing the agent's performance to the counterfactual scenario of the agent learning without teacher supervision and only from rewards. If using teacher supervision improves performance, the importance of teacher supervision is increased and otherwise it is decreased. TGRL outperforms strong baselines across diverse domains without hyperparameter tuning.},
  author = {Idan Shenfeld and Zhang-Wei Hong and Aviv Tamar and Pulkit Agrawal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shenfeld2023tgrl.pdf:pdf},
  mdate = {2024-06-02},
  pages = {31077--31093},
  pdf = {https://proceedings.mlr.press/v202/shenfeld23a/shenfeld23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TGRL}: An Algorithm for Teacher Guided Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/shenfeld23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sherman2023improved,
  abstract = {We study reinforcement learning with linear function approximation and adversarially changing cost functions, a setup that has mostly been considered under simplifying assumptions such as full information feedback or exploratory conditions. We present a computationally efficient policy optimization algorithm for the challenging general setting of unknown dynamics and bandit feedback, featuring a combination of mirror-descent and least squares policy evaluation in an auxiliary MDP used to compute exploration bonuses. Our algorithm obtains an $\widetilde{O}(K^{6/7})$ regret bound, improving significantly over previous state-of-the-art of $\widetilde{O}(K^{14/15})$ in this setting. In addition, we present a version of the same algorithm under the assumption a simulator of the environment is available to the learner (but otherwise no exploratory assumptions are made), and prove it obtains state-of-the-art regret of $\widetilde{O}(K^{2/3})$.},
  author = {Uri Sherman and Tomer Koren and Yishay Mansour},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sherman2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31117--31150},
  pdf = {https://proceedings.mlr.press/v202/sherman23a/sherman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Regret for Efficient Online Reinforcement Learning with Linear Function Approximation},
  url = {https://proceedings.mlr.press/v202/sherman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shevchenko2023fundamental,
  abstract = {Autoencoders are a popular model in many branches of machine learning and lossy data compression. However, their fundamental limits, the performance of gradient methods and the features learnt during optimization remain poorly understood, even in the two-layer setting. In fact, earlier work has considered either linear autoencoders or specific training regimes (leading to vanishing or diverging compression rates). Our paper addresses this gap by focusing on non-linear two-layer autoencoders trained in the challenging proportional regime in which the input dimension scales linearly with the size of the representation. Our results characterize the minimizers of the population risk, and show that such minimizers are achieved by gradient methods; their structure is also unveiled, thus leading to a concise description of the features obtained via training. For the special case of a sign activation function, our analysis establishes the fundamental limits for the lossy compression of Gaussian sources via (shallow) autoencoders. Finally, while the results are proved for Gaussian data, numerical simulations on standard datasets display the universality of the theoretical predictions.},
  author = {Aleksandr Shevchenko and Kevin K{\"o}gler and Hamed Hassani and Marco Mondelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shevchenko2023fundamental.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31151--31209},
  pdf = {https://proceedings.mlr.press/v202/shevchenko23a/shevchenko23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient Methods},
  url = {https://proceedings.mlr.press/v202/shevchenko23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023provably,
  abstract = {Existing theoretical studies on offline reinforcement learning (RL) mostly consider a dataset sampled directly from the target task. In practice, however, data often come from several heterogeneous but related sources. Motivated by this gap, this work aims at rigorously understanding offline RL with multiple datasets that are collected from randomly perturbed versions of the target task instead of from itself. An information-theoretic lower bound is derived, which reveals a necessary requirement on the number of involved sources in addition to that on the number of data samples. We then propose HetPEVI, an algorithm that can learn the target task efficiently as long as the perturbed data sources collectively provide good data coverage, which is more practical than the existing individual coverage requirements.},
  author = {Chengshuai Shi and Wei Xiong and Cong Shen and Jing Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023provably.pdf:pdf},
  mdate = {2023-08-29},
  pages = {31353--31388},
  pdf = {https://proceedings.mlr.press/v202/shi23h/shi23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Efficient Offline Reinforcement Learning with Perturbed Data Sources},
  url = {https://proceedings.mlr.press/v202/shi23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023improving,
  abstract = {To mitigate the privacy leakages and communication burdens of Federated Learning (FL), decentralized FL (DFL) discards the central server and each client only communicates with its neighbors in a decentralized communication network. However, existing DFL suffers from high inconsistency among local clients, which results in severe distribution shift and inferior performance compared with centralized FL (CFL), especially on heterogeneous data or sparse communication topologies. To alleviate this issue, we propose two DFL algorithms named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically, DFedSAM leverages gradient perturbation to generate local flat models via Sharpness Aware Minimization (SAM), which searches for models with uniformly low loss values.},
  author = {Yifan Shi and Li Shen and Kang Wei and Yan Sun and Bo Yuan and Xueqian Wang and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31269--31291},
  pdf = {https://proceedings.mlr.press/v202/shi23d/shi23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving the Model Consistency of Decentralized Federated Learning},
  url = {https://proceedings.mlr.press/v202/shi23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023large,
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model prediction can be distracted by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of different prompting techniques for large language models, and find that the model is easily distracted by irrelevant information. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  author = {Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed H. Chi and Nathanael Sch{\"a}rli and Denny Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023large.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31210--31227},
  pdf = {https://proceedings.mlr.press/v202/shi23a/shi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Large Language Models Can Be Easily Distracted by Irrelevant Context},
  url = {https://proceedings.mlr.press/v202/shi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023everyones,
  abstract = {User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference changes with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades at an equal rate with time, thus hurting the performance of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by using the user's sequential engagement more effectively but also automatically learns a set of weights to represent the preference over each embedding so that the candidates can be retrieved from each interest proportionally. Extensive experiments have been done on various industrial-scale datasets to demonstrate the effectiveness of our approach.},
  author = {Hui Shi and Yupeng Gu and Yitong Zhou and Bo Zhao and Sicun Gao and Jishen Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023everyones.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31228--31242},
  pdf = {https://proceedings.mlr.press/v202/shi23b/shi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Everyone's Preference Changes Differently: A Weighted Multi-Interest Model For Retrieval},
  url = {https://proceedings.mlr.press/v202/shi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023nearoptimal,
  abstract = {In many applications of Reinforcement Learning (RL), it is critically important that the algorithm performs safely, such that instantaneous hard constraints are satisfied at each step, and unsafe states and actions are avoided. However, existing algorithms for "safe" RL are often designed under constraints that either require expected cumulative costs to be bounded or assume all states are safe. Thus, such algorithms could violate instantaneous hard constraints and traverse unsafe states (and actions) in practice. Hence, in this paper, we develop the first near-optimal safe RL algorithm for episodic Markov Decision Processes with unsafe states and actions under instantaneous hard constraints and the linear mixture model.},
  author = {Ming Shi and Yingbin Liang and Ness B. Shroff},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023nearoptimal.pdf:pdf},
  mdate = {2025-01-24},
  pages = {31243--31268},
  pdf = {https://proceedings.mlr.press/v202/shi23c/shi23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Near-Optimal Algorithm for Safe Reinforcement Learning Under Instantaneous Hard Constraints},
  url = {https://proceedings.mlr.press/v202/shi23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023upop,
  abstract = {Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, e.g., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the Unified and Progressive Pruning (UPop) as a universal vison-language Transformer compression framework, which incorporates 1) unifiedly searching multimodal subnets in a continuous optimization space from the original model, which enables automatic assignment of pruning ratios among compressible modalities and structures; 2) progressively searching and retraining the subnet, which maintains convergence between the search and retrain to attain higher compression ratios. Experiments on various tasks, datasets, and model architectures demonstrate the effectiveness and versatility of the proposed UPop framework.},
  author = {Dachuan Shi and Chaofan Tao and Ying Jin and Zhendong Yang and Chun Yuan and Jiaqi Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023upop.pdf:pdf},
  mdate = {2023-12-18},
  pages = {31292--31311},
  pdf = {https://proceedings.mlr.press/v202/shi23e/shi23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers},
  url = {https://proceedings.mlr.press/v202/shi23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023sequence,
  abstract = {Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence.},
  author = {Jiaxin Shi and Ke Alexander Wang and Emily B. Fox},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023sequence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31312--31327},
  pdf = {https://proceedings.mlr.press/v202/shi23f/shi23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequence Modeling with Multiresolution Convolutional Memory},
  url = {https://proceedings.mlr.press/v202/shi23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023statistical,
  abstract = {Multi armed bandit (MAB) algorithms have been increasingly used to complement or integrate with A/B tests and randomized clinical trials in e-commerce, healthcare, and policymaking. Recent developments incorporate possible delayed feedback. While existing MAB literature often focuses on maximizing the expected cumulative reward outcomes (or, equivalently, regret minimization), few efforts have been devoted to establish valid statistical inference approaches to quantify the uncertainty of learned policies. We attempt to fill this gap by providing a unified statistical inference framework for policy evaluation where a target policy is allowed to differ from the data collecting policy, and our framework allows delay to be associated with the treatment arms.},
  author = {Lei Shi and Jingshen Wang and Tianhao Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023statistical.pdf:pdf},
  mdate = {2024-03-18},
  pages = {31328--31352},
  pdf = {https://proceedings.mlr.press/v202/shi23g/shi23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Statistical Inference on Multi-armed Bandits with Delayed Feedback},
  url = {https://proceedings.mlr.press/v202/shi23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023complexity,
  abstract = {We examine concept generalization at a large scale in the natural visual spectrum. Established computational modes (i.e., rule-based or similarity-based) are primarily studied isolated, focusing on confined and abstract problem spaces. In this work, we study these two modes when the problem space scales up and when the complexity of concepts becomes diverse. Specifically, at the representational level, we seek to answer how the complexity varies when a visual concept is mapped to the representation space. Prior psychology literature has shown that two types of complexities (i.e., subjective complexity and visual complexity) build an inverted-U relation.},
  author = {Yu-Zhe Shi and Manjie Xu and John E. Hopcroft and Kun He and Joshua B. Tenenbaum and Song-Chun Zhu and Ying Nian Wu and Wenjuan Han and Yixin Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023complexity.pdf:pdf},
  mdate = {2023-12-22},
  pages = {31389--31407},
  pdf = {https://proceedings.mlr.press/v202/shi23i/shi23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Complexity of {Bayesian} Generalization},
  url = {https://proceedings.mlr.press/v202/shi23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shi2023understanding,
  abstract = {Previous research on contrastive learning (CL) has primarily focused on pairwise views to learn representations by attracting positive samples and repelling negative ones. In this work, we aim to understand and generalize CL from a point set matching perspective, instead of the comparison between two points. Specifically, we formulate CL as a form of inverse optimal transport (IOT), which involves a bilevel optimization procedure for learning where the outter minimization aims to learn the representations and the inner is to learn the coupling (i.e. the probability of matching matrix) between the point sets. Specifically, by adjusting the relaxation degree of constraints in the inner minimization, we obtain three contrastive losses and show that the dominant contrastive loss in literature InfoNCE falls into one of these losses.},
  author = {Liangliang Shi and Gu Zhang and Haoyu Zhen and Jintao Fan and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shi2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31408--31421},
  pdf = {https://proceedings.mlr.press/v202/shi23j/shi23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding and Generalizing Contrastive Learning from the Inverse Optimal Transport Perspective},
  url = {https://proceedings.mlr.press/v202/shi23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shih2023long,
  abstract = {Temperature scaling is a popular technique for tuning the sharpness of a model distribution. It is used extensively for sampling likely generations and calibrating model uncertainty, and even features as a controllable parameter to many large language models in deployment. However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token. To address this, we propose Long Horizon Temperature Scaling (LHTS), a novel approach for sampling from temperature-scaled joint distributions. LHTS is compatible with all likelihood-based models, and optimizes for the long horizon likelihood of samples.},
  author = {Andy Shih and Dorsa Sadigh and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shih2023long.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31422--31434},
  pdf = {https://proceedings.mlr.press/v202/shih23a/shih23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Long Horizon Temperature Scaling},
  url = {https://proceedings.mlr.press/v202/shih23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shilton2023gradient,
  abstract = {The study of Neural Tangent Kernels (NTKs) has provided much needed insight into convergence and generalization properties of neural networks in the over-parametrized (wide) limit by approximating the network using a first-order Taylor expansion with respect to its weights in the neighborhood of their initialization values. This allows neural network training to be analyzed from the perspective of reproducing kernel Hilbert spaces (RKHS), which is informative in the over-parametrized regime, but a poor approximation for narrower networks as the weights change more during training. Our goal is to extend beyond the limits of NTK toward a more general theory. We construct an exact power-series representation of the neural network in a finite neighborhood of the initial weights as an inner product of two feature maps, respectively from data and weight-step space, to feature space, allowing neural network training to be analyzed from the perspective of reproducing kernel Banach space (RKBS).},
  author = {Alistair Shilton and Sunil Gupta and Santu Rana and Svetha Venkatesh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shilton2023gradient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31435--31488},
  pdf = {https://proceedings.mlr.press/v202/shilton23a/shilton23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Gradient Descent in Neural Networks as Sequential Learning in Reproducing Kernel {Banach} Space},
  url = {https://proceedings.mlr.press/v202/shilton23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shim2023snerl,
  abstract = {SNeRL (Semantic-aware Neural Radiance Fields for Reinforcement Learning) jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. The method introduces 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.},
  author = {Dongseok Shim and Seungjae Lee and H. Jin Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shim2023snerl.pdf:pdf},
  mdate = {2023-10-10},
  pages = {31489--31503},
  pdf = {https://proceedings.mlr.press/v202/shim23a/shim23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SNeRL}: Semantic-aware Neural Radiance Fields for Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/shim23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shin2023closer,
  abstract = {Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end.},
  author = {Sungbin Shin and Yohan Jo and Sungsoo Ahn and Namhoon Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shin2023closer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31504--31520},
  pdf = {https://proceedings.mlr.press/v202/shin23a/shin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Closer Look at the Intervention Procedure of Concept Bottleneck Models},
  url = {https://proceedings.mlr.press/v202/shin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shin2023metricganokd,
  abstract = {The authors propose an effective multi-metric optimization method in MetricGAN via online knowledge distillation - MetricGAN-OKD. The key innovation is that MetricGAN-OKD consists of multiple generators and target metrics, related by a one-to-one correspondence, enabling generators to learn with respect to a single metric reliably while improving performance with respect to other metrics by mimicking other generators. The approach addresses the challenge in speech enhancement where MetricGAN-based approaches need to reduce discrepancies between different loss functions and evaluation metrics.},
  author = {Wooseok Shin and Byung Hoon Lee and Jin Sob Kim and Hyun Joon Park and Sung Won Han 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shin2023metricganokd.pdf:pdf},
  mdate = {2024-06-12},
  pages = {31521--31538},
  pdf = {https://proceedings.mlr.press/v202/shin23b/shin23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MetricGAN-OKD: Multi-Metric Optimization of MetricGAN via Online Knowledge Distillation for Speech Enhancement},
  url = {https://proceedings.mlr.press/v202/shin23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shin2023improved,
  abstract = {The authors present the first randomized learning-augmented algorithm for the multi-option ski rental problem, surpassing previous performance guarantees given by deterministic algorithms. Their learning-augmented algorithm is based on a new, provably best-possible randomized competitive algorithm for the problem. Learning-augmented algorithms take machine learning predictions as an added part of the input and incorporate these predictions in solving the given problem, combining the power of ML predictions with rigorous performance guarantees.},
  author = {Yongho Shin and Changyeol Lee and Gukryeol Lee and Hyung-Chan An},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shin2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31539--31561},
  pdf = {https://proceedings.mlr.press/v202/shin23c/shin23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Learning-Augmented Algorithms for the Multi-Option Ski Rental Problem via Best-Possible Competitive Analysis},
  url = {https://proceedings.mlr.press/v202/shin23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shin2023oneshot,
  abstract = {One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. The authors present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time.},
  author = {Sangwoo Shin and Daehee Lee 0001 and Minjong Yoo and Woo Kyung Kim and Honguk Woo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shin2023oneshot.pdf:pdf},
  mdate = {2025-05-27},
  pages = {31562--31578},
  pdf = {https://proceedings.mlr.press/v202/shin23d/shin23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill},
  url = {https://proceedings.mlr.press/v202/shin23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shin2023context,
  abstract = {Labels are typically sparse in real-world time series due to the high annotation cost. The paper proposes a novel method of data augmentation called context-attached augmentation, which adds preceding and succeeding instances to a target instance to form its augmented instance. Unlike existing augmentation techniques that modify a target instance by directly perturbing its attributes, the context-attached augmentation generates instances augmented with varying contexts while maintaining the target instance.},
  author = {Yooju Shin and Susik Yoon and Hwanjun Song and Dongmin Park and Byunghyun Kim and Jae-Gil Lee 0001 and Byung Suk Lee 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shin2023context.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31579--31595},
  pdf = {https://proceedings.mlr.press/v202/shin23e/shin23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Context Consistency Regularization for Label Sparsity in Time Series},
  url = {https://proceedings.mlr.press/v202/shin23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shirzad2023exphormer,
  abstract = {The paper introduces Exphormer, a framework for building powerful and scalable graph transformers. The main innovation is a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseudorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph. Exphormer replaces the dense, fully-connected interaction graph of a standard Transformer with edges of a sparse d-regular expander graph.},
  author = {Hamed Shirzad and Ameya Velingker and Balaji Venkatachalam and Danica J. Sutherland and Ali Kemal Sinop},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shirzad2023exphormer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31613--31632},
  pdf = {https://proceedings.mlr.press/v202/shirzad23a/shirzad23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exphormer: Sparse Transformers for Graphs},
  url = {https://proceedings.mlr.press/v202/shirzad23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shoshan2023synthetic,
  abstract = {Recent breakthroughs in synthetic data generation made it possible to produce highly photorealistic images which are hardly distinguishable from real ones. In contrast to using synthetic data for training, this work explores whether synthetic data can be beneficial for model selection. The research demonstrates that when data is scarce, synthetic data can be used to replace the held out validation set, thus allowing to train on a larger dataset for image classification tasks.},
  author = {Alon Shoshan and Nadav Bhonker and Igor Kviatkovsky and Matan Fintz and Gerard G. Medioni},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shoshan2023synthetic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31633--31656},
  pdf = {https://proceedings.mlr.press/v202/shoshan23a/shoshan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Synthetic data for model selection},
  url = {https://proceedings.mlr.press/v202/shoshan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shou2023probabilistic,
  abstract = {The paper addresses the problem of discovering knowledge about which types of events influence others, using datasets of event sequences. The authors provide a novel neural framework - a probabilistic attention-to-influence neural model - which not only captures complex instance-wise interactions between events but also learns influencers for each event type of interest. The method efficiently learns an approximate posterior for type-wise influence by an attention-to-influence transformation using variational inference, then models the conditional likelihood of sequences by sampling the posterior to focus attention on influencing event types.},
  author = {Xiao Shou and Debarun Bhattacharjya and Tian Gao and Dharmashankar Subramanian and Oktie Hassanzadeh and Kristin P. Bennett},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shou2023probabilistic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31657--31674},
  pdf = {https://proceedings.mlr.press/v202/shou23a/shou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Attention-to-Influence Neural Models for Event Sequences},
  url = {https://proceedings.mlr.press/v202/shou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shridharan2023causal,
  abstract = {This paper addresses computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. The research focuses on quasi-Markovian graphs and extends previous work on scalable computation of causal bounds. The authors develop methods for optimization-based approaches to computing causal bounds when existing non-parametric approaches using linear programming formulations become intractable due to exponential growth in the size of the LP with respect to the number of edges in the causal graph.},
  author = {Madhumitha Shridharan and Garud Iyengar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shridharan2023causal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31675--31692},
  pdf = {https://proceedings.mlr.press/v202/shridharan23a/shridharan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Bounds in Quasi-Markovian Graphs},
  url = {https://proceedings.mlr.press/v202/shridharan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shrivastava2023repositorylevel,
  abstract = {The paper addresses the importance of introducing domain-specific knowledge in the prompt design process for large language models of code. The researchers propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files. The technique does not require access to the weights of the LLM, making it applicable when only black-box access to the LLM is available.},
  author = {Disha Shrivastava and Hugo Larochelle and Daniel Tarlow},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shrivastava2023repositorylevel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31693--31715},
  pdf = {https://proceedings.mlr.press/v202/shrivastava23a/shrivastava23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Repository-Level Prompt Generation for Large Language Models of Code},
  url = {https://proceedings.mlr.press/v202/shrivastava23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{shu2023clipood,
  abstract = {Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distribution. Experiments on diverse datasets with different OOD scenarios show that CLIPood consistently outperforms existing generalization techniques.},
  author = {Yang Shu and Xingzhuo Guo and Jialong Wu and Ximei Wang and Jianmin Wang and Mingsheng Long},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/shu2023clipood.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31716--31731},
  pdf = {https://proceedings.mlr.press/v202/shu23a/shu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CLIPood}: Generalizing {CLIP} to Out-of-Distributions},
  url = {https://proceedings.mlr.press/v202/shu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{si2023semiautoregressive,
  abstract = {Training normalizing flow generative models can be challenging due to the need to calculate computationally expensive determinants of Jacobians. This paper studies the likelihood-free training of flows and proposes the energy objective, an alternative sample-based loss based on proper scoring rules. The energy objective is determinant-free and supports flexible model architectures that are not easily compatible with maximum likelihood training, including semi-autoregressive energy flows, a novel model family that interpolates between fully autoregressive and non-autoregressive models. Energy flows feature competitive sample quality, posterior inference, and generation speed relative to likelihood-based flows; this performance is decorrelated from the quality of log-likelihood estimates, which are generally very poor. Our findings question the use of maximum likelihood as an objective or a metric, and contribute to a scientific study of its role in generative modeling.},
  author = {Phillip Si and Zeyi Chen and Subham Sekhar Sahoo and Yair Schiff and Volodymyr Kuleshov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/si2023semiautoregressive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31732--31753},
  pdf = {https://proceedings.mlr.press/v202/si23a/si23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi-Autoregressive Energy Flows: Exploring Likelihood-Free Training of Normalizing Flows},
  url = {https://proceedings.mlr.press/v202/si23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{siahkoohi2023unearthing,
  abstract = {Source separation involves the ill-posed problem of retrieving a set of source signals that have been observed through a mixing operator. Solving this problem requires prior knowledge, which is commonly incorporated by imposing regularity conditions on the source signals, or implicitly learned through supervised or unsupervised methods from existing data. While data-driven methods have shown great promise in source separation, they often require large amounts of data, which rarely exists in planetary space missions. To address this challenge, we propose an unsupervised source separation scheme for domains with limited data access that involves solving an optimization problem in the wavelet scattering covariance representation space---an interpretable, low-dimensional representation of stationary processes. We present a real-data example in which we remove transient, thermally-induced microtilts---known as glitches---from data recorded by a seismometer during {NASA}'s {InSight} mission on {Mars}. Thanks to the wavelet scattering covariances' ability to capture non-{Gaussian} properties of stochastic processes, we are able to separate glitches using only a few glitch-free data snippets.},
  author = {Ali Siahkoohi and Rudy Morel and Maarten V. de Hoop and Erwan Allys and Gr\'{e}gory Sainton and Taichi Kawamura},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/siahkoohi2023unearthing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31754--31772},
  pdf = {https://proceedings.mlr.press/v202/siahkoohi23a/siahkoohi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unearthing {InSights} into {Mars}: Unsupervised Source Separation with Limited Data},
  url = {https://proceedings.mlr.press/v202/siahkoohi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sieber2023quantitative,
  abstract = {We show that deep belief networks with binary hidden units can approximate any multivariate probability density under very mild integrability requirements on the parental density of the visible nodes. The approximation is measured in the $L^q$-norm for $q \in [1,\infty]$ ($q=\infty$ corresponding to the supremum norm) and in Kullback--Leibler divergence. Furthermore, we establish sharp quantitative bounds on the approximation error in terms of the number of hidden units. In particular, our results imply that the approximation error decreases exponentially fast (i.e., at a rate $2^{-cn}$ for some constant $c > 0$) in the number of hidden units in each layer for the class of H\"{o}lder continuous densities. Our proof technique relies on a careful analysis of the Barron space of functions, which allows us to exploit the specific structure of deep belief networks.},
  author = {Julian Sieber and Johann Gehringer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sieber2023quantitative.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31773--31787},
  pdf = {https://proceedings.mlr.press/v202/sieber23a/sieber23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantitative Universal Approximation Bounds for Deep Belief Networks},
  url = {https://proceedings.mlr.press/v202/sieber23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{silva2023value,
  abstract = {A fundamental challenge in machine learning is generalization to out-of-distribution (OOD) data. We show that the generalization error under distribution shift can be a non-monotonic function of the number of OOD training samples. In other words, there is value in training on small amounts of OOD data. This paper builds the first theoretical foundation for understanding this counter-intuitive phenomenon by analyzing the multi-environment setting through the lens of bias-variance tradeoffs. We show that the bias and variance terms have competing effects that can lead to a non-monotonic generalization error as a function of the amount of OOD data used in training. We demonstrate this non-monotonic behavior empirically using Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet. Finally, we present practical guidelines for when to include OOD data in training.},
  author = {Ashwin De Silva and Rahul Ramesh and Carey E. Priebe and Pratik Chaudhari and Joshua T. Vogelstein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/silva2023value.pdf:pdf},
  mdate = {2023-08-28},
  pages = {7366--7389},
  pdf = {https://proceedings.mlr.press/v202/de-silva23a/de-silva23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Value of Out-of-Distribution Data},
  url = {https://proceedings.mlr.press/v202/de-silva23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{simchilevi2023pricing,
  abstract = {When launching a new product, historical sales data is often not available, leaving price as a crucial experimental instrument for sellers to gauge market response. When designing pricing experiments, there are three fundamental objectives: estimating the causal effect of price (i.e., price elasticity), maximizing the expected revenue through the experiment, and controlling the tail risk of suffering from a very huge loss. In this paper, we reveal the relationship among such three objectives. Under a linear structural model, we investigate the trade-offs between causal inference and expected revenue maximization, as well as between expected revenue maximization and tail risk control. Furthermore, we propose an optimal pricing experimental design, which can flexibly adapt to different desired levels of trade-offs. Through the optimal design, we also explore the relationship between causal inference and tail risk control.},
  author = {David Simchi-Levi and Chonghuan Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/simchilevi2023pricing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31788--31799},
  pdf = {https://proceedings.mlr.press/v202/simchi-levi23a/simchi-levi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pricing Experimental Design: Causal Effect, Expected Revenue and Tail Risk},
  url = {https://proceedings.mlr.press/v202/simchi-levi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{simchowitz2023statistical,
  abstract = {We study the prediction of a target $z$ from a pair of random variables $(x,y)$, where the ground-truth predictor is additive $E[z \mid x, y] = f^*(x) + g^*(y)$. We model heterogeneous environments by letting the marginal distributions of $(x,y)$ vary across environments while keeping the structural functions $f^*, g^*$ invariant. We establish a finite-sample excess risk bound for the natural two-step estimator that first estimates $f^*$ and $g^*$ separately, then combines them. Our bound captures how the complexity of the function classes and the heterogeneity across environments affect the excess risk. In particular, we show that when the marginal distributions vary significantly across environments, we can achieve a better excess risk compared to the naive approach of treating $(x,y)$ as a single covariate. We complement this result with a minimax lower bound and provide empirical validation on synthetic and real data.},
  author = {Max Simchowitz and Anurag Ajay and Pulkit Agrawal and Akshay Krishnamurthy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/simchowitz2023statistical.pdf:pdf},
  mdate = {2024-06-02},
  pages = {31800--31851},
  pdf = {https://proceedings.mlr.press/v202/simchowitz23a/simchowitz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Statistical Learning under Heterogenous Distribution Shift},
  url = {https://proceedings.mlr.press/v202/simchowitz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{simon2023stepwise,
  abstract = {We discover that deep networks learn through a process of discrete, sudden transitions between macroscopic network states, rather than smooth, continuous changes. Using methods from statistical physics, we find that these transitions correspond to the network learning distinct, separable representations of the data. Our analysis provides the first comprehensive theory explaining how and when these transitions occur during self-supervised learning. We study a linearized model of Barlow Twins applicable to infinitely wide networks, and solve the training dynamics exactly from small initialization. Surprisingly, we find that the model learns the top eigenmodes of a certain contrastive kernel in a sequential manner, exhibiting discrete phase transitions between the learning of each eigenmode. We observe the same behavior in deep ResNets trained with Barlow Twins, SimCLR, and VICReg losses.},
  author = {James B. Simon and Maksis Knutins and Liu Ziyin and Daniel Geisz and Abraham J. Fetterman and Joshua Albrecht},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/simon2023stepwise.pdf:pdf},
  mdate = {2025-04-10},
  pages = {31852--31876},
  pdf = {https://proceedings.mlr.press/v202/simon23a/simon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Stepwise Nature of Self-Supervised Learning},
  url = {https://proceedings.mlr.press/v202/simon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sinclair2023hindsight,
  abstract = {Many resource management problems require sequential decision-making under uncertainty, where the only uncertainty affecting decision outcomes are exogenous variables outside the control of the decision-maker. We model such problems as Exo-MDPs (Markov Decision Processes with Exogenous Inputs) and design a class of data-efficient algorithms termed Hindsight Learning (HL). The HL algorithms achieve data efficiency by leveraging a key insight: having samples of the exogenous variables, past decisions can be revisited in hindsight to infer counterfactual consequences that can accelerate policy improvements. We guide policy search with informative improvement signals computed by hindsight planning algorithms developed in the operations research literature, which can efficiently solve for the optimal sequence of actions for a given sequence of exogenous inputs. The HL algorithms can quickly learn policies that are ``optimal for most exogenous inputs'', which reduces the sample and computational costs of policy search.},
  author = {Sean R. Sinclair and Felipe Vieira Frujeri and Ching-An Cheng and Luke Marshall and Hugo de Oliveira Barbalho and Jingling Li and Jennifer Neville and Ishai Menache and Adith Swaminathan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sinclair2023hindsight.pdf:pdf},
  mdate = {2024-06-28},
  pages = {31877--31914},
  pdf = {https://proceedings.mlr.press/v202/sinclair23a/sinclair23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hindsight Learning for {MDPs} with Exogenous Inputs},
  url = {https://proceedings.mlr.press/v202/sinclair23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{singer2023textto4d,
  abstract = {We present MAV3D (Make-A-Video3D), a method for generating three-dimensional dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural Radiance Field (NeRF), which is optimized for scene appearance, density, and motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The dynamic video output generated from the provided text can be viewed from any camera location and angle, and can be composited into any 3D environment. MAV3D does not require any 3D or 4D data and the T2V model is trained only on Text-Image pairs and unlabeled videos. To generate 4D scenes, we leverage a procedure that combines careful diffusion model sampling with temporal and spatial stability optimization. We demonstrate that our method can generate complex dynamic 3D scenes from just text descriptions.},
  author = {Uriel Singer and Shelly Sheynin and Adam Polyak and Oron Ashual and Iurii Makarov and Filippos Kokkinos and Naman Goyal and Andrea Vedaldi and Devi Parikh and Justin Johnson and Yaniv Taigman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/singer2023textto4d.pdf:pdf},
  mdate = {2025-02-11},
  pages = {31915--31929},
  pdf = {https://proceedings.mlr.press/v202/singer23a/singer23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Text-To-4{D} Dynamic Scene Generation},
  url = {https://proceedings.mlr.press/v202/singer23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{singh2023the,
  abstract = {While Convolutional Neural Networks ({CNN}s) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their {H}essian maps. The reason is that the loss {H}essian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of {CNN} get manifested in its structure and properties. We develop a framework relying on {T}oeplitz representation of {CNN}s, and then utilize it to reveal the {H}essian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the {H}essian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in {CNN}s, the {H}essian rank grows as the square root of the number of parameters.},
  author = {Sidak Pal Singh and Thomas Hofmann and Bernhard Sch{"o}lkopf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/singh2023the.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31930--31968},
  pdf = {https://proceedings.mlr.press/v202/singh23a/singh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The {H}essian perspective into the {N}ature of {C}onvolutional {N}eural {N}etworks},
  url = {https://proceedings.mlr.press/v202/singh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{singh2023when,
  abstract = {Minimax-fair machine learning minimizes the error for the worst-off group. However, empirical evidence suggests that when sophisticated models are trained with standard empirical risk minimization ({ERM}), they often have the same performance on the worst-off group as a minimax-trained model. Our work makes this counter-intuitive observation concrete. We prove that if the hypothesis class is sufficiently expressive and the group information is recoverable from the features, {ERM} and minimax-fairness learning formulations indeed have the same performance on the worst-off group. We provide additional empirical evidence of how this observation holds on a wide range of datasets and hypothesis classes. Since {ERM} is fundamentally easier than minimax optimization, our findings have implications on the practice of fair machine learning.},
  author = {Harvineet Singh and Matth{"a}us Kleindessner and Volkan Cevher and Rumi Chunara and Chris Russell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/singh2023when.pdf:pdf},
  mdate = {2023-08-28},
  pages = {31969--31989},
  pdf = {https://proceedings.mlr.press/v202/singh23b/singh23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When do {M}inimax-fair {L}earning and {E}mpirical {R}isk {M}inimization {C}oincide?},
  url = {https://proceedings.mlr.press/v202/singh23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sipka2023differentiable,
  abstract = {Simulating rare events, such as the transformation of a reactant into a product in a chemical reaction, requires enhanced sampling techniques that rely on heuristically chosen collective variables ({CV}s). We propose using differentiable simulations ({DiffSim}) for the discovery and enhanced sampling of chemical transformations without a need to resort to preselected {CV}s, using only a distance metric. Our approach merges reaction path discovery and estimation of the biasing potential that enhances the sampling into a single end-to-end problem that is solved by path-integral optimization. We introduce multiple improvements over standard {DiffSim} such as partial backpropagation and graph mini-batching making {DiffSim} training stable and efficient.},
  author = {Martin Sipka and Johannes C. B. Dietschreit and Luk{\'a}\v{s} Grajciar and Rafael G{\'o}mez-Bombarelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sipka2023differentiable.pdf:pdf},
  mdate = {2024-10-06},
  pages = {31990--32007},
  pdf = {https://proceedings.mlr.press/v202/sipka23a/sipka23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentiable {S}imulations for {E}nhanced {S}ampling of {R}are {E}vents},
  url = {https://proceedings.mlr.press/v202/sipka23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sitawarin2023preprocessors,
  abstract = {Decision-based adversarial attacks construct inputs that fool a machine-learning model into making targeted mispredictions by making only hard-label queries. For the most part, these attacks have been applied directly to isolated neural network models. However, in practice, machine learning models are just a component of a much larger system. We find that by adding just a single preprocessor in front of a classifier, state-of-the-art query-based attacks are as much as seven times less effective at attacking a prediction pipeline than attacking the machine learning model alone. Most preprocessors introduce some notion of invariance to the input space. Hence, attacks that are unaware of this invariance inevitably waste a large number of queries to re-discover or overcome it. We develop techniques to first reverse-engineer the preprocessor and then use this extracted information to attack the end-to-end system. Our extraction method requires only a few hundred queries to learn the preprocessors used by most publicly available model pipelines, and our preprocessor-aware attacks recover the same efficacy as just attacking the model alone.},
  author = {Chawin Sitawarin and Florian Tram{\`e}r and Nicholas Carlini},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sitawarin2023preprocessors.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32008--32032},
  pdf = {https://proceedings.mlr.press/v202/sitawarin23a/sitawarin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Preprocessors {M}atter! {R}ealistic {D}ecision-{B}ased {A}ttacks on {M}achine {L}earning {S}ystems},
  url = {https://proceedings.mlr.press/v202/sitawarin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{skalse2023invariance,
  abstract = {It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit, meaning the reward function is only partially identifiable. We formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.},
  author = {Joar Max Viktor Skalse and Matthew Farrugia-Roberts and Stuart Russell and Alessandro Abate and Adam Gleave},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/skalse2023invariance.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32033--32058},
  pdf = {https://proceedings.mlr.press/v202/skalse23a/skalse23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Invariance in {P}olicy {O}ptimisation and {P}artial {I}dentifiability in {R}eward {L}earning},
  url = {https://proceedings.mlr.press/v202/skalse23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{slumbers2023a,
  abstract = {Agents in multi-agent systems ({MAS}) may consider risks posed by other agents' actions. Although there are equilibrium concepts in game theory that account for risk aversion, they either assume agents are risk-neutral with respect to uncertainty caused by other agents' actions, or they are not guaranteed to exist. We introduce a new game theory-based {R}isk-{A}verse {E}quilibrium ({RAE}) that always produces a solution minimizing potential variance in reward while accounting for other agents' strategies. The dominant paradigm in game theory assumes agents are not affected by risk from other agents and only strive to maximize expected utility. In many real-world scenarios, this is not realistic; for example, in hybrid human-{AI} driving systems, it is necessary to limit large deviations in reward resulting from car crashes.},
  author = {Oliver Slumbers and David Henry Mguni and Stefano B. Blumberg and Stephen Marcus McAleer and Yaodong Yang and Jun Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/slumbers2023a.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32059--32087},
  pdf = {https://proceedings.mlr.press/v202/slumbers23a/slumbers23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A {G}ame-{T}heoretic {F}ramework for {M}anaging {R}isk in {M}ulti-{A}gent {S}ystems},
  url = {https://proceedings.mlr.press/v202/slumbers23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sodhi2023on,
  abstract = {Teacher forcing ({TF}) is the dominant paradigm for training large language models for dialogue response generation. {TF} attempts to match human language exactly despite identical meanings being expressible in different ways, thus motivating the use of sequence-level objectives for dialogue response generation. We study the efficacy of various offline reinforcement learning ({RL}) methods to maximize sequence-level objectives and present a comprehensive evaluation across multiple datasets, models, and metrics. Offline {RL} shows a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training budgets.},
  author = {Paloma Sodhi and Felix Wu and Ethan R. Elenberg and Kilian Q. Weinberger and Ryan McDonald},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sodhi2023on.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32088--32104},
  pdf = {https://proceedings.mlr.press/v202/sodhi23a/sodhi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the {E}ffectiveness of {O}ffline {RL} for {D}ialogue {R}esponse {G}eneration},
  url = {https://proceedings.mlr.press/v202/sodhi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{soen2023fair,
  abstract = {We introduce a boosting algorithm to pre-process data for fairness. Starting from an initial fair but inaccurate distribution, our approach shifts towards better data fitting while still ensuring a minimal fairness guarantee. To do so, it learns the sufficient statistics of an exponential family with boosting-compliant convergence. We are able to theoretically prove that the learned distribution will have a representation rate and statistical rate data fairness guarantee. Unlike recent optimization based pre-processing methods, our approach can be easily adapted for continuous domain features. When the weak learners are specified to be decision trees, the sufficient statistics of the learned distribution can be examined to provide clues on sources of (un)fairness.},
  author = {Alexander Soen and Hisham Husain and Richard Nock},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/soen2023fair.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32105--32144},
  pdf = {https://proceedings.mlr.press/v202/soen23a/soen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair {D}ensities via {B}oosting the {S}ufficient {S}tatistics of {E}xponential {F}amilies},
  url = {https://proceedings.mlr.press/v202/soen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sokar2023the,
  abstract = {We identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method called {R}e{D}o that {R}ecycles {D}ormant neurons throughout training. Our experiments demonstrate that {R}e{D}o maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.},
  author = {Ghada Sokar and Rishabh Agarwal and Pablo Samuel Castro and Utku Evci},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sokar2023the.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32145--32168},
  pdf = {https://proceedings.mlr.press/v202/sokar23a/sokar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The {D}ormant {N}euron {P}henomenon in {D}eep {R}einforcement {L}earning},
  url = {https://proceedings.mlr.press/v202/sokar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sokota2023abstracting,
  abstract = {Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because {N}ash equilibria of the game with public policy announcements may not correspond to {N}ash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. We show that a slightly modified version of the insight of Nayyar et al. can be applied to two-player zero-sum games.},
  author = {Samuel Sokota and Ryan D'Orazio and Chun Kai Ling and David J. Wu and J. Zico Kolter and Noam Brown},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sokota2023abstracting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32169--32193},
  pdf = {https://proceedings.mlr.press/v202/sokota23a/sokota23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Abstracting {I}mperfect {I}nformation {A}way from {T}wo-{P}layer {Z}ero-{S}um {G}ames},
  url = {https://proceedings.mlr.press/v202/sokota23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{son2023metasage,
  abstract = {This paper proposes {Meta-SAGE}, a novel approach for improving the scalability of deep reinforcement learning models for combinatorial optimization ({CO}) tasks. Our method adapts pre-trained models to larger-scale problems in test time by suggesting two components: a scale meta-learner ({SML}) and scheduled adaptation with guided exploration ({SAGE}). The scale meta-learner ({SML}) transforms the context embedding for subsequent adaptation of {SAGE} based on scale information. {SAGE} adjusts the model parameters dedicated to the context embedding for a specific instance, introducing locality bias which encourages selecting nearby locations to determine the next location, with the locality bias gradually decaying as the model is adapted to the target instance. Results show that {Meta-SAGE} outperforms previous adaptation methods and significantly improves scalability in representative {CO} tasks.},
  author = {Jiwoo Son and Minsu Kim and Hyeonah Kim and Jinkyoo Park},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/son2023metasage.pdf:pdf},
  mdate = {2025-06-26},
  pages = {32194--32210},
  pdf = {https://proceedings.mlr.press/v202/son23a/son23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Meta-SAGE}: Scale Meta-Learning Scheduled Adaptation with Guided Exploration for Mitigating Scale Shift on Combinatorial Optimization},
  url = {https://proceedings.mlr.press/v202/son23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023importance,
  abstract = {Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose {IsEM-Pro}, an approach to generate protein sequences towards a given fitness criterion. At its core, {IsEM-Pro} is a latent generative model, augmented by combinatorial structure features from a separately learned {Markov} random fields ({MRFs}). We develop a {Monte Carlo} Expectation-Maximization method ({MCEM}) to learn the model. During inference, sampling from its latent space enhances diversity while its {MRFs} features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our {IsEM-Pro} outperforms the previous best methods by at least 55\% on average fitness score and generates more diverse and novel protein sequences.},
  author = {Zhenqiao Song and Lei Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023importance.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32349--32364},
  pdf = {https://proceedings.mlr.press/v202/song23g/song23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Importance Weighted Expectation-Maximization for Protein Sequence Design},
  url = {https://proceedings.mlr.press/v202/song23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023consistency,
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that consistency models outperform existing distillation techniques for diffusion models in one- and few-step sampling.},
  author = {Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023consistency.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32211--32252},
  pdf = {https://proceedings.mlr.press/v202/song23a/song23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Consistency Models},
  url = {https://proceedings.mlr.press/v202/song23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023lipsnet,
  abstract = {Deep reinforcement learning ({RL}) is a powerful approach for solving optimal control problems. However, {RL}-trained policies often suffer from the action fluctuation problem, where the consecutive actions significantly differ despite only slight state variations. This problem results in mechanical components' wear and tear and poses safety hazards. The action fluctuation is caused by the high {Lipschitz} constant of actor networks. To address this problem, we propose a neural network named {LipsNet}. We propose the Multi-dimensional Gradient Normalization ({MGN}) method, to constrain the {Lipschitz} constant of networks with multi-dimensional input and output. Benefiting from {MGN}, {LipsNet} achieves {Lipschitz} continuity, allowing smooth actions while preserving control performance by adjusting {Lipschitz} constant. {LipsNet} addresses the action fluctuation problem at network level rather than algorithm level, which can serve as actor networks in most {RL} algorithms, making it more flexible and user-friendly than previous works. Experiments demonstrate that {LipsNet} has good landscape smoothness and noise robustness, resulting in significantly smoother action compared to the Multilayer Perceptron.},
  author = {Xujie Song and Jingliang Duan and Wenxuan Wang and Shengbo Eben Li and Chen Chen and Bo Cheng and Bo Zhang and Junqing Wei and Xiaoming Simon Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023lipsnet.pdf:pdf},
  mdate = {2024-08-09},
  pages = {32253--32272},
  pdf = {https://proceedings.mlr.press/v202/song23b/song23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LipsNet}: A Smooth and Robust Neural Network with Adaptive {Lipschitz} Constant for High Accuracy Optimal Control},
  url = {https://proceedings.mlr.press/v202/song23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023deep,
  abstract = {Image perturbation technique is widely used to generate adversarial examples to attack networks, greatly decreasing the performance of networks. Unlike the existing works, in this paper, we introduce a novel framework Deep Perturbation Learning ({DPL}), the new insights into understanding image perturbations, to enhance the performance of networks rather than decrease the performance. Specifically, we learn image perturbations to amend the data distribution of training set to improve the performance of networks. This optimization w.r.t data distribution is non-trivial. To approach this, we tactfully construct a differentiable optimization target w.r.t. image perturbations via minimizing the empirical risk. Then we propose an alternating optimization of the network weights and perturbations. {DPL} can easily be adapted to a wide spectrum of downstream tasks and backbone networks. Extensive experiments demonstrate the effectiveness of our {DPL} on 6 datasets ({CIFAR}-10, {CIFAR}100, {ImageNet}, {MS}-{COCO}, {PASCAL} {VOC}, and {SBD}) over 3 popular vision tasks (image classification, object detection, and semantic segmentation) with different backbone architectures (e.g., {ResNet}, {MobileNet}, and {ViT}).},
  author = {Zifan Song and Xiao Gong and Guosheng Hu and Cairong Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32273--32287},
  pdf = {https://proceedings.mlr.press/v202/song23c/song23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Perturbation Learning: Enhancing the Network Performance via Image Perturbations},
  url = {https://proceedings.mlr.press/v202/song23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023latent,
  abstract = {Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in 'disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time.},
  author = {Yue Song and T. Anderson Keller and Nicu Sebe and Max Welling},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023latent.pdf:pdf},
  mdate = {2024-11-14},
  pages = {32288--32303},
  pdf = {https://proceedings.mlr.press/v202/song23d/song23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Latent Traversals in Generative Models as Potential Flows},
  url = {https://proceedings.mlr.press/v202/song23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023rge,
  abstract = {In real-world graphs, noisy connections are inevitable, which makes it difficult to obtain unbiased node representations. Among various attempts to resolve this problem, a method of estimating the counterfactual effects of these connectivities has recently attracted attention, which mainly uses influence functions for single graph elements (i.e., node and edge). However, in this paper, we argue that there is a strongly interacting group effect between the influences of graph elements due to their connectivity. In the same vein, we observe that edge groups connecting to the same train node exhibit significant differences in their influences, hence no matter how negative each is, removing them at once may have a rather negative effect as a group. This work scrutinizes the trend that there exists an influence estimation error of an edge group in graph influence function and this error might decrease the performance in graph rectification. Thus, we propose {RGE}, which eliminates distant edges at each iteration in graph rectification, and demonstrate the effectiveness of {RGE} on various graphs.},
  author = {Jaeyun Song and Sungyub Kim and Eunho Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023rge.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32331--32348},
  pdf = {https://proceedings.mlr.press/v202/song23f/song23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{RGE}: A Repulsive Graph Rectification for Node Classification via Influence},
  url = {https://proceedings.mlr.press/v202/song23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023fedavg,
  abstract = {Federated Learning ({FL}) is a distributed learning paradigm that allows multiple clients to learn a joint model by utilizing privately held data at each client. Significant research efforts have been devoted to develop advanced algorithms that deal with the situation where the data at individual clients have heterogeneous distributions. In this work, we show that data heterogeneity can be dealt from a different perspective. By utilizing a certain overparameterized multi-layer neural network at each client, even the vanilla {FedAvg} (a.k.a. the Local {SGD}) algorithm can accurately optimize the training problem: When each client has a neural network with one wide layer of size N (where N is the number of total training samples), followed by layers of smaller widths, {FedAvg} converges linearly to a solution that achieves (almost) zero training loss, without requiring any assumptions on the clients' data distributions. To our knowledge, this is the first work that demonstrates such resilience to data heterogeneity for {FedAvg} when trained on multi-layer neural networks. Our experiments also confirm that, neural networks of large size can achieve better and more stable performance for {FL} problems.},
  author = {Bingqing Song and Prashant Khanduri and Xinwei Zhang and Jinfeng Yi and Mingyi Hong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023fedavg.pdf:pdf},
  mdate = {2024-08-26},
  pages = {32304--32330},
  pdf = {https://proceedings.mlr.press/v202/song23e/song23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FedAvg} Converges to Zero Training Loss Linearly for Overparameterized Multi-Layer Neural Networks},
  url = {https://proceedings.mlr.press/v202/song23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{song2023lossguided,
  abstract = {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion ({LGD}), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a {Monte Carlo} method that uses multiple samples from a suitable distribution to reduce bias. The method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and control.},
  author = {Jiaming Song and Qinsheng Zhang and Hongxu Yin and Morteza Mardani and Ming-Yu Liu and Jan Kautz and Yongxin Chen and Arash Vahdat},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/song2023lossguided.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32483--32498},
  pdf = {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  url = {https://proceedings.mlr.press/v202/song23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{soulos2023differentiable,
  abstract = {In the context of structure-to-structure transformation tasks, learning sequences of discrete symbolic operations poses significant challenges due to their non-differentiability. To facilitate the learning of these symbolic sequences, we introduce a differentiable tree interpreter that compiles high-level symbolic tree operations into subsymbolic matrix operations on tensors. We present a novel Differentiable Tree Machine ({DTM}) architecture that integrates our interpreter with an external memory and an agent that learns to sequentially select tree operations to execute the target transformation in an end-to-end manner. With respect to out-of-distribution compositional generalization on synthetic semantic parsing and language generation tasks, {DTM} achieves 100\% while existing baselines such as Transformer, Tree Transformer, {LSTM}, and Tree2Tree {LSTM} achieve less than 30\%.},
  author = {Paul Soulos and Edward J. Hu and Kate McCurdy and Yunmo Chen and Roland Fernandez and Paul Smolensky and Jianfeng Gao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/soulos2023differentiable.pdf:pdf},
  mdate = {2024-04-11},
  pages = {32499--32520},
  pdf = {https://proceedings.mlr.press/v202/soulos23a/soulos23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentiable Tree Operations Promote Compositional Generalization},
  url = {https://proceedings.mlr.press/v202/soulos23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sportisse2023labels,
  abstract = {Semi-supervised learning is a powerful technique for leveraging unlabeled data to improve machine learning models, but it can be affected by the presence of 'informative' labels, which occur when some classes are more likely to be labeled than others. This paper addresses the "missing not at random" scenario in semi-supervised learning, proposing an approach to estimate the missing-data mechanism and using inverse propensity weighting to debias SSL algorithms. The method includes a likelihood ratio test to assess label informativeness and demonstrates effectiveness on medical datasets with pseudo-realistic scenarios.},
  author = {Aude Sportisse and Hugo Schmutz and Olivier Humbert and Charles Bouveyron and Pierre-Alexandre Mattei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sportisse2023labels.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32521--32539},
  pdf = {https://proceedings.mlr.press/v202/sportisse23a/sportisse23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are labels informative in semi-supervised learning? {E}stimating and leveraging the missing-data mechanism},
  url = {https://proceedings.mlr.press/v202/sportisse23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{squires2023linear,
  abstract = {Causal disentanglement seeks a representation of data involving latent variables that are related via a causal model. A representation is identifiable if both the latent model and the transformation from latent to observed variables are unique. In this paper, we study observed variables that are a linear transformation of a linear latent causal model. Data from interventions are necessary for identifiability: if one latent variable is missing an intervention, we show that there exist distinct models that cannot be distinguished. Conversely, we show that a single intervention on each latent variable is sufficient for identifiability. Our proof uses a generalization of the RQ decomposition of a matrix that replaces the usual orthogonal and upper triangular conditions with analogues depending on a partial order on the rows of the matrix, with partial order determined by a latent causal model. We corroborate our theoretical results with a method for causal disentanglement. We show that the method accurately recovers a latent causal model on synthetic and semi-synthetic data and we illustrate a use case on a dataset of single-cell RNA sequencing measurements.},
  author = {Chandler Squires and Anna Seigal and Salil S. Bhate and Caroline Uhler},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/squires2023linear.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32540--32560},
  pdf = {https://proceedings.mlr.press/v202/squires23a/squires23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Linear Causal Disentanglement via Interventions},
  url = {https://proceedings.mlr.press/v202/squires23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{srivastava2023generating,
  abstract = {{AI} assistance continues to help advance applications in education, from language learning to intelligent tutoring systems, yet current methods for providing students feedback are still quite limited. Most automatic feedback systems either provide binary correctness feedback, which may not help a student understand how to improve, or require hand-coding feedback templates, which may not generalize to new domains. This can be particularly challenging for physical control tasks, where the rich diversity in student behavior and specialized domains make it challenging to leverage general-purpose assistive tools for providing feedback. We design and build {CORGI}, a model trained to generate language corrections for physical control tasks, such as learning to ride a bike. {CORGI} is designed for three diverse physical control tasks (drawing, steering, and joint movement) and takes in as input a pair of student and expert trajectories, then generates natural language corrections to help the student improve. To train {CORGI}, we collected over 2k crowdsourced corrections for pair-wise (student, expert) trajectories, and further augmented the data with large language-model assistance.},
  author = {Megha Srivastava and Noah D. Goodman and Dorsa Sadigh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/srivastava2023generating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32561--32574},
  pdf = {https://proceedings.mlr.press/v202/srivastava23a/srivastava23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generating Language Corrections for Teaching Physical Control Tasks},
  url = {https://proceedings.mlr.press/v202/srivastava23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{staerman2023fadin,
  abstract = {Temporal point processes ({TPP}) are a natural tool for modeling event-based data. Among all {TPP} models, {H}awkes processes have proven to be the most widely used, mainly due to their adequate modeling for various applications, particularly when considering exponential or non-parametric kernels. Although non-parametric kernels are an option, such models require large datasets. While exponential kernels are more data efficient and relevant for specific applications where events immediately trigger more events, they are ill-suited for applications where latencies need to be estimated, such as in neuroscience. This work aims to offer an efficient solution to {TPP} inference using general parametric kernels with finite support. The developed solution consists of a fast $\ell_2$ gradient-based solver leveraging a discretized version of the events, making it particularly useful for applications like neuroscience where latency estimation is important.},
  author = {Guillaume Staerman and C{\'e}dric Allain and Alexandre Gramfort and Thomas Moreau},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/staerman2023fadin.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32575--32597},
  pdf = {https://proceedings.mlr.press/v202/staerman23a/staerman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FaDIn}: {F}ast {D}iscretized {I}nference for {H}awkes {P}rocesses with {G}eneral {P}arametric {K}ernels},
  url = {https://proceedings.mlr.press/v202/staerman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{stein2023partial,
  abstract = {The higher-order correlation clustering problem is an expressive model, and recently, local search heuristics have been proposed for several applications. Certifying optimality, however, is {NP}-hard and practically hampered already by the complexity of the problem statement. Here, we focus on establishing partial optimality conditions for the special case of complete graphs and cubic objective functions. In addition, we define and implement algorithms for testing these conditions and examine their effect numerically, on two datasets.},
  author = {David Stein and Silvia Di Gregorio and Bj{\"o}rn Andres},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/stein2023partial.pdf:pdf},
  mdate = {2024-05-31},
  pages = {32598--32617},
  pdf = {https://proceedings.mlr.press/v202/stein23a/stein23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Partial Optimality in Cubic Correlation Clustering},
  url = {https://proceedings.mlr.press/v202/stein23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{steiner2023model,
  abstract = {The size of deep neural networks has grown exponentially in recent years. Unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. To cope with this, researchers have proposed various techniques including spilling, rematerialization, reduced precision training, model pruning, and so on. However, these approaches suffer from various limitations, such as increasing training time, affecting model accuracy, or requiring extensive manual modifications to the neural networks. We present {MODeL}, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. Our method automatically reduces the memory usage of existing neural networks without any of the drawbacks of other techniques.},
  author = {Benoit Steiner and Mostafa Elhoushi and Jacob Kahn and James Hegarty},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/steiner2023model.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32618--32632},
  pdf = {https://proceedings.mlr.press/v202/steiner23a/steiner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MODeL}: {M}emory {O}ptimizations for {D}eep {L}earning},
  url = {https://proceedings.mlr.press/v202/steiner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{straitouri2023improving,
  abstract = {Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction---prediction sets---and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently our system will mislead the experts, and the size of the prediction set, which determines the difficulty of the classification task the experts need to solve using our system. We develop an efficient and near-optimal search method to find the conformal predictor under which the experts benefit the most from using our system. Simulation experiments using synthetic and real expert predictions demonstrate that our system may help experts make more accurate predictions and is robust to the accuracy of the classifier the conformal predictor relies on.},
  author = {Eleni Straitouri and Lequn Wang and Nastaran Okati and Manuel G{\'o}mez-Rodr{\'i}guez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/straitouri2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32633--32653},
  pdf = {https://proceedings.mlr.press/v202/straitouri23a/straitouri23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Expert Predictions with Conformal Prediction},
  url = {https://proceedings.mlr.press/v202/straitouri23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{strimel2023lookahead,
  abstract = {Streaming speech recognition architectures are employed for low-latency, real-time applications. Such architectures are often characterized by their causality. Causal architectures emit tokens at each frame, relying only on current and past signal, while non-causal models are exposed to a window of future frames at each step to increase predictive accuracy. This dichotomy amounts to a trade-off for real-time Automatic Speech Recognition ({ASR}) system design: profit from the low-latency benefit of strictly-causal architectures while accepting predictive performance limitations, or realize the modeling benefits of future-context models accompanied by their higher latency penalty. In this work, we relax the constraints of this choice and present the Adaptive Non-Causal Attention Transducer ({ANCAT}). The architecture is non-causal in the traditional sense, but executes in a low-latency, streaming manner by dynamically choosing when to rely on future context and to what degree within the audio stream. The resulting mechanism, when coupled with our novel regularization algorithms, delivers comparable accuracy to non-causal configurations while improving significantly upon latency, closing the gap with their causal counterparts. We showcase our design experimentally by reporting comparative {ASR} task results with measures of accuracy and latency on both publicly accessible and production-scale, voice-assistant datasets.},
  author = {Grant P. Strimel and Yi Xie and Brian John King and Martin Radfar and Ariya Rastrow and Athanasios Mouchtaris},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/strimel2023lookahead.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32654--32676},
  pdf = {https://proceedings.mlr.press/v202/strimel23a/strimel23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Lookahead When It Matters: Adaptive Non-causal Transformers for Streaming Neural Transducers},
  url = {https://proceedings.mlr.press/v202/strimel23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{stucchi2023kernel,
  abstract = {We introduce Kernel {QuantTree}, a method that extends the {QuantTree} algorithm to kernel-based change detection in multivariate data streams. {QuantTree} is a nonparametric approach that adaptively partitions the feature space using binary splitting to detect distribution changes effectively. By incorporating kernel methods, we enable the detection of nonlinear patterns and complex distribution changes that may not be captured by the original linear approach. Our method maintains the computational efficiency and theoretical guarantees of the original {QuantTree} while extending its applicability to more complex data distributions and change patterns.},
  author = {Diego Stucchi and Paolo Rizzo and Nicol{\'o} Folloni and Giacomo Boracchi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/stucchi2023kernel.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32677--32697},
  pdf = {https://proceedings.mlr.press/v202/stucchi23a/stucchi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Kernel {QuantTree}},
  url = {https://proceedings.mlr.press/v202/stucchi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{stucki2023topologically,
  abstract = {Image segmentation is a largely researched field where neural networks find vast applications in many facets of technology. Some of the most popular approaches to train segmentation networks employ loss functions optimizing pixel-overlap, an objective that is insufficient for many segmentation tasks. In recent years, their limitations fueled a growing interest in topology-aware methods, which aim to recover the correct topology of the segmented structures. However, so far, existing methods only consider global topological properties, ignoring the need to preserve topological features spatially, which is crucial for accurate segmentation. We introduce the concept of induced matchings from persistent homology to achieve a spatially correct matching between persistence barcodes in a segmentation setting. Based on this concept, we define the Betti matching error as an interpretable, topologically and feature-wise accurate metric for image segmentations, which resolves the limitations of previous approaches. Our Betti matching error is differentiable and efficient to use as a loss function. We demonstrate that it improves the topological performance of segmentation networks significantly across six diverse datasets while preserving the performance with respect to traditional scores.},
  author = {Nico Stucki and Johannes C. Paetzold and Suprosanna Shit and Bj{\"o}rn H. Menze and Ulrich Bauer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/stucki2023topologically.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32698--32727},
  pdf = {https://proceedings.mlr.press/v202/stucki23a/stucki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Topologically Faithful Image Segmentation via Induced Matching of Persistence Barcodes},
  url = {https://proceedings.mlr.press/v202/stucki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{su2023towards,
  abstract = {Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem. We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting. Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting.},
  author = {Junwei Su and Difan Zou and Zijun Zhang and Chuan Wu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/su2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32728--32748},
  pdf = {https://proceedings.mlr.press/v202/su23a/su23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Robust Graph Incremental Learning on Evolving Graphs},
  url = {https://proceedings.mlr.press/v202/su23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{suau2023duet,
  abstract = {Multiview Self-Supervised Learning (MSSL) is based on learning invariances with respect to a set of input transformations. However, invariance partially or totally removes transformation-related information from the representations, which might harm performance for specific downstream tasks that require such information. We propose 2D strUctured and EquivarianT representations (coined DUET), which are 2d representations organized in a matrix structure, and equivariant with respect to transformations acting on the input data. DUET representations maintain information about an input transformation, while remaining semantically expressive. Compared to SimCLR (Chen et al., 2020) (unstructured and invariant) and ESSL (Dangovski et al., 2022) (unstructured and equivariant), the structured and equivariant nature of DUET representations enables controlled generation with lower reconstruction error, while controllability is not possible with SimCLR or ESSL. DUET also achieves higher accuracy for several discriminative tasks, and improves transfer learning.},
  author = {Xavier Suau and Federico Danieli and T. Anderson Keller and Arno Blaas and Chen Huang 0001 and Jason Ramapuram and Dan Busbridge and Luca Zappella},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/suau2023duet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32749--32769},
  pdf = {https://proceedings.mlr.press/v202/suau23a/suau23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DUET: 2D Structured and Approximately Equivariant Representations},
  url = {https://proceedings.mlr.press/v202/suau23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{suh2023longtailed,
  abstract = {Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The authors propose interpreting long-tailed recognition as mutual information maximization between latent features and ground-truth labels to address class imbalances. Their approach integrates contrastive learning and logit adjustment, demonstrating state-of-the-art performance on long-tailed recognition benchmarks and image segmentation tasks.},
  author = {Min-Kook Suh and Seung-Woo Seo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/suh2023longtailed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32770--32782},
  pdf = {https://proceedings.mlr.press/v202/suh23a/suh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels},
  url = {https://proceedings.mlr.press/v202/suh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sui2023adversarial,
  abstract = {Reinforcement learning (RL) has made significant advancements in artificial intelligence. However, its real-world applications are limited due to differences between simulated environments and the actual world. The authors propose an adversarial learning framework for distributional reinforcement learning that adopts an influence measure concept from statistics, detects performance loss from policy structure or state observations, uses an influence measure based on information geometry, and demonstrates utility in three diagnostic tasks: identifying fragile states in trajectories, determining policy architecture instability, and pinpointing sensitive policy parameters.},
  author = {Yang Sui and Yukun Huang and Hongtu Zhu and Fan Zhou 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sui2023adversarial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32783--32796},
  pdf = {https://proceedings.mlr.press/v202/sui23a/sui23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Learning of Distributional Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/sui23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sumers2023distilling,
  abstract = {Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. The paper explores using vision-language models to help AI agents learn language grounding through innovative techniques like model distillation and hindsight experience replay.},
  author = {Theodore R. Sumers and Kenneth Marino and Arun Ahuja and Rob Fergus and Ishita Dasgupta 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sumers2023distilling.pdf:pdf},
  mdate = {2023-10-16},
  pages = {32797--32818},
  pdf = {https://proceedings.mlr.press/v202/sumers23a/sumers23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distilling Internet-Scale Vision-Language Models into Embodied Agents},
  url = {https://proceedings.mlr.press/v202/sumers23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023vectorvalued,
  abstract = {Control variates are variance reduction tools for Monte Carlo estimators. They can provide significant variance reduction, but usually require a large number of samples, which can be prohibitive when sampling or evaluating the integrand is computationally expensive. Furthermore, there are many scenarios where we need to compute multiple related integrals simultaneously or sequentially, which can further exacerbate computational costs. In this paper, we propose vector-valued control variates, an extension of control variates which can be used to reduce the variance of multiple Monte Carlo estimators jointly.},
  author = {Zhuo Sun and Alessandro Barp and François-Xavier Briol},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023vectorvalued.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32819--32846},
  pdf = {https://proceedings.mlr.press/v202/sun23a/sun23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Vector-Valued Control Variates},
  url = {https://proceedings.mlr.press/v202/sun23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023metamodulation,
  abstract = {Meta-learning algorithms are able to learn a new task using previously learned knowledge, but they often require a large number of meta-training tasks which may not be readily available. To address this issue, the authors propose MetaModulation, a method for few-shot learning with fewer tasks. The key idea is to use a neural network to increase the density of the meta-training tasks by modulating batch normalization parameters during meta-training. Additionally, they modify parameters at various network levels, rather than just a single layer, to increase task diversity. To account for the uncertainty caused by the limited training tasks, they propose a variational MetaModulation where the modulation parameters are treated as latent variables.},
  author = {Wenfang Sun and Yingjun Du and Xiantong Zhen and Fan Wang and Ling Wang and Cees G. M. Snoek},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023metamodulation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32847--32858},
  pdf = {https://proceedings.mlr.press/v202/sun23b/sun23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MetaModulation: Learning Variational Feature Hierarchies for Few-Shot Learning with Fewer Tasks},
  url = {https://proceedings.mlr.press/v202/sun23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023revisiting,
  abstract = {Sampling approaches like Markov chain Monte Carlo were once popular for combinatorial optimization, but the inefficiency of classical methods and the need for problem-specific designs curtailed ongoing development. Recent work has favored data-driven approaches that mitigate the need for hand-craft heuristics, but these are often not usable as out-of-the-box solvers due to dependence on in-distribution training and limited scalability to large instances. In this paper, we revisit the idea of using sampling for combinatorial optimization, motivated by the significant recent advances of gradient-based discrete MCMC and new techniques for parallel neighborhood exploration on accelerators. Remarkably, we find that modern sampling strategies can leverage landscape information to provide general-purpose solvers that require no training and yet are competitive with state of the art combinatorial solvers.},
  author = {Haoran Sun and Katayoon Goshvadi and Azade Nova and Dale Schuurmans and Hanjun Dai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32859--32874},
  pdf = {https://proceedings.mlr.press/v202/sun23c/sun23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Sampling for Combinatorial Optimization},
  url = {https://proceedings.mlr.press/v202/sun23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023what,
  abstract = {Joint representation learning over multi-sourced knowledge graphs (KGs) yields transferable and expressive embeddings that improve downstream tasks. Entity alignment (EA) is a critical step in this process. Despite recent considerable research progress in embedding-based EA, how it works remains to be explored. In this paper, we provide a similarity flooding perspective to explain existing translation-based and aggregation-based EA models. We prove that the embedding learning process of these models actually seeks a fixpoint of pairwise similarities between entities. We also provide experimental evidence to support our theoretical analysis.},
  author = {Zequn Sun and Jiacheng Huang and Xiaozhou Xu and Qijin Chen and Weijun Ren and Wei Hu 0007},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023what.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32875--32885},
  pdf = {https://proceedings.mlr.press/v202/sun23d/sun23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {What Makes Entities Similar? A Similarity Flooding Perspective for Multi-sourced Knowledge Graph Embeddings},
  url = {https://proceedings.mlr.press/v202/sun23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023maximum,
  abstract = {In this paper, we study the predict-then-optimize problem where the output of a machine learning prediction task is used as the input of some downstream optimization problem, say, the objective coefficient vector of a linear program. The problem is also known as predictive analytics or contextual linear programming. The existing approaches largely suffer from either optimization intractability (a non-convex objective function)/statistical inefficiency (a suboptimal generalization bound) or requiring strong condition(s) such as no constraint or loss calibration. We develop a new approach to the problem called maximum optimality margin which designs the machine learning loss function by the optimality condition of the downstream optimization. The max-margin formulation enjoys both computational efficiency and good theoretical properties for the learning procedure. More importantly, our new approach only needs the observations of the optimal solution in the training data rather than the objective function, which makes it a new and natural approach to the inverse linear programming problem under both contextual and context-free settings; we also analyze the proposed method under both offline and online settings, and demonstrate its performance using numerical experiments.},
  author = {Chunlin Sun and Shang Liu and Xiaocheng Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023maximum.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32886--32912},
  pdf = {https://proceedings.mlr.press/v202/sun23e/sun23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Maximum Optimality Margin: A Unified Approach for Contextual Linear Programming and Inverse Linear Programming},
  url = {https://proceedings.mlr.press/v202/sun23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023tensor,
  abstract = {Multi-channel imaging data is prevalent in fields like astronomy and biology. The paper extends a Tensor Gaussian Process model by introducing tensor contraction for dimensionality reduction in scalar-on-tensor regression, motivated by solar flare forecasting.},
  author = {Hu Sun and Ward Manchester and Meng Jin and Yang Liu and Yang Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023tensor.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32913--32935},
  pdf = {https://proceedings.mlr.press/v202/sun23f/sun23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tensor {G}aussian {P}rocess with {C}ontraction for {M}ulti-{C}hannel {I}maging {A}nalysis},
  url = {https://proceedings.mlr.press/v202/sun23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023mabe22,
  abstract = {We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data).},
  author = {Jennifer J. Sun and Markus Marks and Andrew Wesley Ulmer and Dipam Chakraborty and Brian Geuther and Edward Hayes and Heng Jia and Vivek Kumar and Sebastian Oleszko and Zachary Partridge and Milan Peelman and Alice Robie and Catherine E. Schretter and Keith Sheppard and Chao Sun and Param Uttarwar and Julian Morgan Wagner and Erik Werner and Joseph Parker and Pietro Perona and Yisong Yue and Kristin Branson and Ann Kennedy},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023mabe22.pdf:pdf},
  mdate = {2023-08-28},
  pages = {32936--32990},
  pdf = {https://proceedings.mlr.press/v202/sun23g/sun23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MABe22}: A {M}ulti-{S}pecies {M}ulti-{T}ask {B}enchmark for {L}earned {R}epresentations of {B}ehavior},
  url = {https://proceedings.mlr.press/v202/sun23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023dynamic,
  abstract = {In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance.},
  author = {Yan Sun and Li Shen and Shixiang Chen and Liang Ding and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023dynamic.pdf:pdf},
  mdate = {2023-11-10},
  pages = {32991--33013},
  pdf = {https://proceedings.mlr.press/v202/sun23h/sun23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dynamic {R}egularized {S}harpness {A}ware {M}inimization in {F}ederated {L}earning: {A}pproaching {G}lobal {C}onsistency and {S}mooth {L}andscape},
  url = {https://proceedings.mlr.press/v202/sun23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023when,
  abstract = {Novel Class Discovery (NCD) aims at inferring novel classes in an unlabeled set by leveraging prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for NCD.},
  author = {Yiyou Sun and Zhenmei Shi and Yingyu Liang and Yixuan Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023when.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33014--33043},
  pdf = {https://proceedings.mlr.press/v202/sun23i/sun23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis},
  url = {https://proceedings.mlr.press/v202/sun23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023learning,
  abstract = {We study the problem of learning optimal policy from a set of discrete treatment options using observational data. We propose a piecewise linear neural network model that can balance strong prescriptive performance and interpretability, which we refer to as the prescriptive ReLU network, or P-ReLU. We show analytically that this model (i) partitions the input space into disjoint polyhedra, where all instances that belong to the same partition receive the same treatment, and (ii) can be converted into an equivalent prescriptive tree with hyperplane splits for interpretability.},
  author = {Wei Sun and Asterios Tsiourvas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33044--33060},
  pdf = {https://proceedings.mlr.press/v202/sun23j/sun23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning {P}rescriptive {ReLU} {N}etworks},
  url = {https://proceedings.mlr.press/v202/sun23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023momentum,
  abstract = {Sign Stochastic Gradient Descent (signSGD) is a communication-efficient stochastic algorithm that only uses the sign information of the stochastic gradient to update the model's weights. However, the existing convergence theory of signSGD either requires increasing batch sizes during training or assumes the gradient noise is symmetric and unimodal. Error feedback has been used to guarantee the convergence of signSGD under weaker assumptions at the cost of communication overhead. This paper revisits the convergence of signSGD and proves that momentum can remedy signSGD under weaker assumptions than previous techniques; in particular, our convergence theory does not require the assumption of bounded stochastic gradient or increased batch size. Our results resonate with echoes of previous empirical results where, unlike signSGD, signSGD with momentum maintains good performance even with small batch sizes. Another new result is that signSGD with momentum can achieve an improved convergence rate when the objective function is second-order smooth. We further extend our theory to signSGD with major vote and federated learning.},
  author = {Tao Sun 0005 and Qingsong Wang and Dongsheng Li 0001 and Bao Wang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023momentum.pdf:pdf},
  mdate = {2024-09-11},
  pages = {33077--33099},
  pdf = {https://proceedings.mlr.press/v202/sun23l/sun23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Momentum Ensures Convergence of {SIGNSGD} under Weaker Assumptions},
  url = {https://proceedings.mlr.press/v202/sun23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023all,
  abstract = {Compared to Euclidean convolution, existing graph convolution methods generally fail to learn diverse convolution operators under limited parameter scales and depend on additional treatments of multi-scale feature extraction. The challenges of generalizing Euclidean convolution to graphs arise from the irregular structure of graphs. To bridge the gap between Euclidean space and graph space, we propose a differentiable method for regularization on graphs that applies permutations to the input graphs. The permutations constrain all nodes in a row regardless of their input order and therefore enable the flexible generalization of Euclidean convolution. Based on the regularization of graphs, we propose Compressed Convolution Network (CoCN) for hierarchical graph representation learning.},
  author = {Junshu Sun and Shuhui Wang and Xinzhe Han and Zhe Xue and Qingming Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023all.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33061--33076},
  pdf = {https://proceedings.mlr.press/v202/sun23k/sun23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {All in a Row: Compressed Convolution Networks for Graphs},
  url = {https://proceedings.mlr.press/v202/sun23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023critical,
  abstract = {3D point clouds serve as a crucial data representation in numerous real-world applications such as autonomous driving, robotics, and medical imaging. While the advancements in deep learning have spurred the utilization of 3D point clouds, deep models are notoriously vulnerable to adversarial attacks. Various defense solutions have been proposed to build robust models against adversarial attacks. In this work, we pinpoint a major limitation of the leading empirical defense, adversarial training, when applied to 3D point cloud models: gradient obfuscation, which significantly hampers robustness against potent attacks. To bridge the gap, we propose PointDP, a purification strategy that leverages diffusion models to defend against 3D adversarial attacks. Since PointDP does not rely on predefined adversarial examples for training, it can defend against a variety of threats. We conduct a comprehensive evaluation of PointDP across six representative 3D point cloud architectures, employing sixteen strong and adaptive attacks to manifest its foundational robustness. Our evaluation shows that PointDP achieves significantly better (i.e., 12.6%-40.3%) adversarial robustness than state-of-the-art methods under strong attacks bounded by different ℓp norms.},
  author = {Jiachen Sun and Jiongxiao Wang and Weili Nie and Zhiding Yu and Zhuoqing Mao 0001 and Chaowei Xiao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023critical.pdf:pdf},
  mdate = {2024-08-01},
  pages = {33100--33114},
  pdf = {https://proceedings.mlr.press/v202/sun23m/sun23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Critical Revisit of Adversarial Robustness in {3D} Point Cloud Recognition with Diffusion-Driven Purification},
  url = {https://proceedings.mlr.press/v202/sun23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023sddm,
  abstract = {Recent score-based diffusion models (SBDMs) show promising results in unpaired image-to-image translation (I2I). However, existing methods, either energy-based or statistically-based, provide no explicit form of the interfered intermediate generative distributions. This work presents a new score-decomposed diffusion model (SDDM) on manifolds to explicitly optimize the tangled distributions during image generation. SDDM derives manifolds to make the distributions of adjacent time steps separable and decompose the score function or energy guidance into an image "denoising" part and a content "refinement" part. To refine the image in the same noise level, we equalize the refinement parts of the score function and energy guidance, which permits multi-objective optimization on the manifold. We also leverage the block adaptive instance normalization module to construct manifolds with lower dimensions but still concentrated with the perturbed reference image. SDDM outperforms existing SBDM-based methods with much fewer diffusion steps on several I2I benchmarks.},
  author = {Shikun Sun and Longhui Wei and Junliang Xing and Jia Jia 0001 and Qi Tian 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023sddm.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33115--33134},
  pdf = {https://proceedings.mlr.press/v202/sun23n/sun23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SDDM}: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation},
  url = {https://proceedings.mlr.press/v202/sun23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023neural,
  abstract = {Numerical simulation of non-linear partial differential equations plays a crucial role in modeling physical science and engineering phenomena, such as weather, climate, and aerodynamics. Recent Machine Learning (ML) models trained on low-resolution spatio-temporal signals have shown new promises in capturing important dynamics in high-resolution signals, under the condition that the models can effectively recover the missing details. However, this study shows that significant information is often lost in the low-resolution down-sampled features. To address such issues, we propose a new approach, namely Temporal Stencil Modeling (TSM), which combines the strengths of advanced time-series sequence modeling (with the HiPPO features) and state-of-the-art neural PDE solvers (with learnable stencil modeling). TSM aims to recover the lost information from the PDE trajectories and can be regarded as a temporal generalization of classic finite volume methods such as WENO.},
  author = {Zhiqing Sun and Yiming Yang and Shinjae Yoo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33135--33155},
  pdf = {https://proceedings.mlr.press/v202/sun23o/sun23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Neural {PDE} Solver with Temporal Stencil Modeling},
  url = {https://proceedings.mlr.press/v202/sun23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023feature,
  abstract = {Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the effectiveness of our proposed more comprehensive feature space, with comparable inference time to the baseline, and demonstrate its efficient convergence capability.},
  author = {Jiaqi Sun and Lin Zhang and Guangyi Chen and Peng Xu and Kun Zhang and Yujiu Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023feature.pdf:pdf},
  mdate = {2024-05-13},
  pages = {33156--33176},
  pdf = {https://proceedings.mlr.press/v202/sun23p/sun23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Feature Expansion for Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/sun23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sun2023modelb,
  abstract = {For offline reinforcement learning, model-based methods are expected to be data-efficient as they incorporate dynamics models to generate more data. However, due to inevitable model errors, straightforwardly learning a policy in the model typically fails in the offline setting. Previous studies have incorporated conservatism to prevent out-of-distribution exploration. For example, MOPO penalizes rewards through uncertainty measures from predicting the next states, which are loose bounds of the ideal uncertainty, i.e., the Bellman error. In this paper, we propose MOBILE (MOdel-Bellman Inconsistency penalized offLinE Policy Optimization), a novel uncertainty-driven offline RL algorithm. MOBILE conducts uncertainty quantification through the inconsistency of Bellman estimations under an ensemble of learned dynamics models, which can be a better approximator to the true Bellman error, and penalizes the Bellman estimation based on this uncertainty. Empirically, we verify that our proposed uncertainty quantification can be significantly closer to the true Bellman error than the compared methods. Consequently, MOBILE outperforms prior offline RL approaches on most tasks of D4RL and NeoRL benchmarks.},
  author = {Yihao Sun and Jiaji Zhang and Chengxing Jia and Haoxin Lin and Junyin Ye and Yang Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sun2023modelb.pdf:pdf},
  mdate = {2024-05-03},
  pages = {33177--33194},
  pdf = {https://proceedings.mlr.press/v202/sun23q/sun23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-{B}ellman Inconsistency for Model-based Offline Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/sun23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sundararajan2023inflow,
  abstract = {Machine learning is pervasive. It powers recommender systems such as Spotify, Instagram and YouTube, and health-care systems via models that predict sleep patterns, or the risk of disease. Individuals contribute data to these models and benefit from them. Are these contributions (outflows of influence) and benefits (inflows of influence) reciprocal? We propose measures of outflows, inflows and reciprocity building on previously proposed measures of training data influence. Data is pooled across entities (individuals or enterprises) to create machine learning models, and sometimes, the entities that contribute the data also benefit from the models. In this work we propose a framework to study this value exchange, i.e., we model and measure contributions (outflows), benefits (inflows) and the balance between contributions and benefits (the degree of reciprocity). Our initial theoretical and empirical results indicate that under certain distributional assumptions, some classes of models are approximately reciprocal.},
  author = {Mukund Sundararajan and Walid Krichene},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sundararajan2023inflow.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33195--33208},
  pdf = {https://proceedings.mlr.press/v202/sundararajan23a/sundararajan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Inflow, Outflow, and Reciprocity in Machine Learning},
  url = {https://proceedings.mlr.press/v202/sundararajan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{suriyakumar2023when,
  abstract = {Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level -- meaning groups may receive unnecessarily inaccurate predictions by sharing their personal characteristics. We present formal conditions to ensure the ``fair use'' of group attributes in a prediction task, and describe how they can be checked by training one additional model. We call these collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We include a comprehensive empirical study of fair use in clinical prediction tasks, demonstrating the prevalence of fair use violations in practice and illustrating simple interventions to mitigate their harm.},
  author = {Vinith Menon Suriyakumar and Marzyeh Ghassemi and Berk Ustun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/suriyakumar2023when.pdf:pdf},
  mdate = {2023-09-30},
  pages = {33209--33228},
  pdf = {https://proceedings.mlr.press/v202/suriyakumar23a/suriyakumar23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When Personalization Harms Performance: Reconsidering the Use of Group Attributes in Prediction},
  url = {https://proceedings.mlr.press/v202/suriyakumar23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{suttle2023beyond,
  abstract = {Many existing reinforcement learning methods employ stochastic gradient iteration whose stability hinges upon a hypothesis that the data-generating process mixes exponentially fast with a rate parameter that appears in the step-size selection. Unfortunately, this assumption is violated for large state spaces or settings with sparse rewards, and the mixing time is unknown, making the step size inoperable. To solve this problem, we propose an RL methodology attuned to the mixing time by employing a multi-level Monte Carlo estimator for the critic, the actor, and the average reward embedded within an actor-critic algorithm. This method, called Multi-level Actor-Critic (MAC), is developed especially for infinite-horizon average-reward settings and neither relies on oracle knowledge of the mixing time in its parameter selection nor assumes its exponential decay; it, therefore, is readily applicable to applications with slower mixing times. We prove that our approach finds an $\varepsilon$-optimal policy using $\tilde{O}(T_{mix} \varepsilon^{-2})$ episodes, where $T_{mix}$ is the mixing time. This bound is minimax optimal when the mixing time is at most polynomial in the size of the state space.},
  author = {Wesley A. Suttle and Amrit S. Bedi and Bhrij Patel and Brian M. Sadler and Alec Koppel and Dinesh Manocha},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/suttle2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33240--33267},
  pdf = {https://proceedings.mlr.press/v202/suttle23a/suttle23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond Exponentially Fast Mixing in Average-Reward Reinforcement Learning via Multi-Level Monte Carlo Actor-Critic},
  url = {https://proceedings.mlr.press/v202/suttle23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{suzuki2023tight,
  abstract = {Recent studies have experimentally shown that we can achieve in non-Euclidean metric space effective and efficient graph embedding, which aims to obtain the vertices' representations reflecting the graph's structure in the metric space. Specifically, graph embedding in hyperbolic space has experimentally succeeded in embedding graphs with hierarchical-tree structure, e.g., data in natural languages, social networks, and knowledge bases. However, recent theoretical analyses have shown a much higher upper bound on non-Euclidean graph embedding's generalization error than Euclidean one's, where a high generalization error indicates that the incompleteness and noise in the data can significantly damage learning performance. It implies that the existing bound cannot guarantee the success of graph embedding in non-Euclidean metric space in a practical training data size, which can prevent non-Euclidean graph embedding's application in real problems. This paper provides a tight and fast generalization bound for graph embedding in non-Euclidean metric space to fill this gap.},
  author = {Atsushi Suzuki and Atsushi Nitanda and Taiji Suzuki and Jing Wang and Feng Tian and Kenji Yamanishi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/suzuki2023tight.pdf:pdf},
  mdate = {2025-01-27},
  pages = {33268--33284},
  pdf = {https://proceedings.mlr.press/v202/suzuki23a/suzuki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tight and fast generalization error bound of graph embedding in metric space},
  url = {https://proceedings.mlr.press/v202/suzuki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{sverdrup2023proximal,
  abstract = {Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing. A considerable number of promising conditional average treatment effect estimators currently available typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R- and DR-learner, which is a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and where researchers wish to rely on proxy variables for causal inference. The proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well.},
  author = {Erik Sverdrup and Yifan Cui},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/sverdrup2023proximal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33285--33298},
  pdf = {https://proceedings.mlr.press/v202/sverdrup23a/sverdrup23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Proximal Causal Learning of Conditional Average Treatment Effects},
  url = {https://proceedings.mlr.press/v202/sverdrup23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{swamy2023inverse,
  abstract = {Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice, we find that we are able to significantly speed up the prior art on continuous control tasks.},
  author = {Gokul Swamy and David Wu and Sanjiban Choudhury and Drew Bagnell and Zhiwei Steven Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/swamy2023inverse.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33299--33318},
  pdf = {https://proceedings.mlr.press/v202/swamy23a/swamy23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Inverse Reinforcement Learning without Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/swamy23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{swanson2023von,
  abstract = {Molecules are frequently represented as graphs, but the underlying 3D molecular geometry ultimately determines most molecular properties. Most molecules are not static and at room temperature adopt a wide variety of geometries or conformations. The resulting distribution on geometries is known as the Boltzmann distribution, and many molecular properties are expectations computed under this distribution. Generating accurate samples from the Boltzmann distribution is therefore essential for computing these expectations accurately. Traditional sampling-based methods are computationally expensive, and most recent machine learning-based methods have focused on identifying modes in this distribution rather than generating true samples. Generating such samples requires capturing conformational variability, and it has been widely recognized that the majority of conformational variability in molecules arises from rotatable bonds. In this work, we present VonMisesNet, a new graph neural network that captures conformational variability via a variational approximation of rotatable bond torsion angles as a mixture of von Mises distributions. We demonstrate that VonMisesNet can generate conformations for arbitrary molecules in a way that is both physically accurate with respect to the Boltzmann distribution and orders of magnitude faster than existing sampling methods.},
  author = {Kirk Swanson and Jake Lawrence Williams and Eric M. Jonas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/swanson2023von.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33319--33342},
  pdf = {https://proceedings.mlr.press/v202/swanson23a/swanson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Von {M}ises Mixture Distributions for Molecular Conformation Generation},
  url = {https://proceedings.mlr.press/v202/swanson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{syed2023optimal,
  abstract = {The estimation of repeatedly nested expectations is a challenging task that arises in many real-world systems. Existing methods generally suffer from high computational costs when the number of nestings becomes large. Standard Monte Carlo methods typically require at least $O(\varepsilon^{-(2+D)})$ cost to achieve an $\varepsilon$-error estimate, where $D$ is the depth of nesting. In this work, we introduce $\mathsf{READ}$ (Recursive Estimator for Arbitrary Depth), a novel Monte Carlo estimator that leverages the problem's recursive structure and employs randomized multilevel Monte Carlo methods. Under suitable regularity assumptions, $\mathsf{READ}$ achieves the optimal computational cost of $O(\varepsilon^{-2})$ for every fixed depth $D$, representing a significant improvement over existing approaches. Our theoretical analysis demonstrates the optimality of our method, and empirical results validate its effectiveness across various applications involving nested expectation estimation.},
  author = {Yasa Syed and Guanyang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/syed2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33343--33364},
  pdf = {https://proceedings.mlr.press/v202/syed23a/syed23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal randomized multilevel {M}onte {C}arlo for repeatedly nested expectations},
  url = {https://proceedings.mlr.press/v202/syed23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{szot2023adaptive,
  abstract = {We present the task of "Social Rearrangement", consisting of cooperative everyday tasks like setting up the dinner table, tidying a house or unpacking groceries in a simulated multi-agent environment. In Social Rearrangement, two robots coordinate to complete a long-horizon task, using onboard sensing and egocentric observations, and no privileged information about the environment. We study zero-shot coordination (ZSC) in this task, where an agent collaborates with a new partner, emulating a scenario where a robot collaborates with a new human partner. Prior ZSC approaches struggle to generalize in our complex and visually rich setting, and on further analysis, we find that they fail to generate diverse coordination behaviors at training time. To counter this, we propose Behavior Diversity Play (BDP), a novel ZSC approach that encourages diversity through a discriminability objective. Our results demonstrate that BDP learns adaptive agents that can tackle visual coordination, and zero-shot generalize to new partners in unseen environments, achieving 35% higher success and 32% higher efficiency compared to baselines.},
  author = {Andrew Szot and Unnat Jain and Dhruv Batra and Zsolt Kira and Ruta Desai and Akshara Rai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/szot2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33365-33380},
  pdf = {https://proceedings.mlr.press/v202/szot23a/szot23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Coordination in Social Embodied Rearrangement},
  url = {https://proceedings.mlr.press/v202/szot23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{taghibakhshi2023mggnn,
  abstract = {Domain decomposition methods (DDMs) are popular solvers for discretized systems of partial differential equations (PDEs), with one-level and multilevel variants. These solvers rely on several algorithmic and mathematical parameters, prescribing overlap, subdomain boundary conditions, and other properties of the DDM. While some work has been done on optimizing these parameters, it has mostly focused on the one-level setting or special cases such as structured-grid discretizations with regular subdomain construction. In this paper, we propose multigrid graph neural networks (MG-GNN), a novel GNN architecture for learning optimized parameters in two-level DDMs. We train MG-GNN using a new unsupervised loss function, enabling effective training on small problems that yields robust performance on unstructured grids that are orders of magnitude larger than those in the training set. We show that MG-GNN outperforms popular hierarchical graph network architectures for this optimization and that our proposed loss function is critical to achieving this improved performance.},
  author = {Ali Taghibakhshi and Nicolas Nytko and Tareq Uz Zaman and Scott P. MacLachlan and Luke N. Olson and Matthew West 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/taghibakhshi2023mggnn.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33381-33395},
  pdf = {https://proceedings.mlr.press/v202/taghibakhshi23a/taghibakhshi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MG-GNN: Multigrid Graph Neural Networks for Learning Multilevel Domain Decomposition Methods},
  url = {https://proceedings.mlr.press/v202/taghibakhshi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tai2023learning,
  abstract = {We study the problem of learning mixtures of Gaussians with censored data. Statistical learning with censored data is a classical problem, with numerous practical applications, however, finite-sample guarantees for even simple latent variable models such as Gaussian mixtures are missing. Formally, we are given censored data from a mixture of univariate Gaussians $\sum_{i=1}^k w_i N(\mu_i,\sigma^2)$, i.e. the sample is observed only if it lies inside a set $S$. The goal is to learn the weights $w_i$ and the means $\mu_i$. We propose an algorithm that takes only $1/\varepsilon^{O(k)}$ samples to estimate the weights $w_i$ and the means $\mu_i$ within $\varepsilon$ error.},
  author = {Wai Ming Tai and Bryon Aragam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tai2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33396-33415},
  pdf = {https://proceedings.mlr.press/v202/tai23a/tai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Mixtures of Gaussians with Censored Data},
  url = {https://proceedings.mlr.press/v202/tai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{takakura2023approximation,
  abstract = {Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers for high dimensional data.},
  author = {Shokichi Takakura and Taiji Suzuki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/takakura2023approximation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33416-33447},
  pdf = {https://proceedings.mlr.press/v202/takakura23a/takakura23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Approximation and Estimation Ability of Transformers for Sequence-to-Sequence Functions with Infinite Dimensional Input},
  url = {https://proceedings.mlr.press/v202/takakura23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{takamoto2023learning,
  abstract = {Scientific Machine Learning (SciML) is concerned with the development of learned emulators of physical systems governed by partial differential equations (PDE). In application domains such as weather forecasting, molecular dynamics, and inverse design, ML-based surrogate models are increasingly used to augment or replace inefficient and often non-differentiable numerical simulation algorithms. While a number of ML-based methods for approximating the solutions of PDEs have been proposed in recent years, they typically do not adapt to the parameters of the PDEs, making it difficult to generalize to PDE parameters not seen during training. We propose a Channel Attention mechanism guided by PDE Parameter Embeddings (CAPE) component for neural surrogate models and a simple yet effective curriculum learning strategy. CAPE can be combined with any neural PDE solvers allowing them to adapt to unseen PDE parameters. The curriculum learning strategy provides a seamless transition between teacher-forcing and fully auto-regressive training.},
  author = {Makoto Takamoto and Francesco Alesiani and Mathias Niepert},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/takamoto2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33448-33467},
  pdf = {https://proceedings.mlr.press/v202/takamoto23a/takamoto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Neural PDE Solvers with Parameter-Guided Channel Attention},
  url = {https://proceedings.mlr.press/v202/takamoto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{takemura2023contextual,
  abstract = {The performance of a bandit algorithm is usually measured by the cumulative rewards of the actions chosen by the algorithm. However, in many real-world applications, the rewards in each round should be good enough for reasons such as safety and fairness. In this paper, we investigate the contextual conservative interleaving bandit problem, which has a performance constraint that requires the chosen actions to be not much worse than given baseline actions in each round. This work is the first to simultaneously consider the following practical situations: (1) multiple actions are chosen in a round; (2) contexts are observed; and (3) conservative constraints are imposed. We propose the Generalized Conservative Winner (GCW) algorithm, which uses a standard bandit algorithm and achieves minimax optimal regret up to logarithmic factors if the algorithm used is also minimax optimal. We improve the existing analyses for the C²UCB algorithm and the Thompson sampling to combine with GCW.},
  author = {Kei Takemura},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/takemura2023contextual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33468-33489},
  pdf = {https://proceedings.mlr.press/v202/takemura23a/takemura23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Contextual Conservative Interleaving Bandits},
  url = {https://proceedings.mlr.press/v202/takemura23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{takeno2023randomized,
  abstract = {Gaussian process upper confidence bound (GP-UCB) is a theoretically promising approach for black-box optimization; however, the confidence parameter $\beta$ is considerably large in the theorem and chosen heuristically in practice. Then, randomized GP-UCB (RGP-UCB) uses a randomized confidence parameter, which follows the Gamma distribution, to mitigate the impact of manually specifying $\beta$. This study first generalizes the regret analysis of RGP-UCB to a wider class of distributions, including the Gamma distribution. Furthermore, we propose improved RGP-UCB (IRGP-UCB) based on a two-parameter exponential distribution, which achieves tighter Bayesian regret bounds. IRGP-UCB does not require an increase in the confidence parameter in terms of the number of iterations, which avoids over-exploration in the later stages of optimization.},
  author = {Shion Takeno and Yu Inatsu and Masayuki Karasuyama},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/takeno2023randomized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33490-33515},
  pdf = {https://proceedings.mlr.press/v202/takeno23a/takeno23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Randomized Gaussian Process Upper Confidence Bound with Tighter Bayesian Regret Bounds},
  url = {https://proceedings.mlr.press/v202/takeno23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{takeno2023towards,
  abstract = {We study preferential Bayesian optimization (BO) where reliable feedback is limited to pairwise comparison called duels. An important challenge in preferential BO, which uses the preferential Gaussian process (GP) model to represent flexible preference structure, is that the posterior distribution is a computationally intractable skew GP. The most widely used approach for preferential BO is Gaussian approximation, which ignores the skewness of the true posterior. Alternatively, Markov chain Monte Carlo (MCMC) based preferential BO is also proposed. In this work, we first verify the accuracy of Gaussian approximation, from which we reveal the critical problem that the predictive probability of duels can be inaccurate. This motivates improving the MCMC-based estimation for skew GP, and we demonstrate the practical efficiency of Gibbs sampling for preferential BO.},
  author = {Shion Takeno and Masahiro Nomura and Masayuki Karasuyama},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/takeno2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33516-33533},
  pdf = {https://proceedings.mlr.press/v202/takeno23b/takeno23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Practical Preferential Bayesian Optimization with Skew Gaussian Processes},
  url = {https://proceedings.mlr.press/v202/takeno23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tan2023robust,
  abstract = {Explanation methods are vulnerable to adversarial perturbations, which change the explanation but not the prediction. This raises security concerns in high-stakes domains where explanations are used for auditing black-box models. We investigate when robust explanations are necessary and what they cost. We prove that the robustness of explanations is determined by the robustness of the model to be explained, meaning we can have robust explanations for free for a robust model. We introduce and prove the existence of a robustness-faithfulness trade-off: contrary to common expectations, an explanation method may become less faithful when it becomes more robust. We are the first to introduce this trade-off and theoretically prove its existence for SmoothGrad. Our theoretical findings are verified by empirical evidence on six state-of-the-art explanation methods and four backbones.},
  author = {Zeren Tan and Yang Tian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tan2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33534-33562},
  pdf = {https://proceedings.mlr.press/v202/tan23a/tan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Explanation for Free or At the Cost of Faithfulness},
  url = {https://proceedings.mlr.press/v202/tan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tan2023provably,
  abstract = {Typical machine learning applications always assume the data follows independent and identically distributed (IID) assumptions. In contrast, this assumption is frequently violated in real-world circumstances, leading to the Out-of-Distribution (OOD) generalization problem and a major drop in model robustness. To mitigate this issue, the invariant learning technique is leveraged to distinguish between spurious features and invariant features among all input features and to train the model purely on the basis of the invariant features. Numerous invariant learning strategies imply that the training data should contain domain information. Such information includes the environment index or auxiliary information acquired from prior knowledge. However, acquiring this information is typically impossible in practice. This paper proposes a practical invariant learning algorithm that does not require domain information to tackle the OOD generalization problem.},
  author = {Xiaoyu Tan and Lin Yong and Shengyu Zhu 0003 and Chao Qu and Xihe Qiu and Yinghui Xu and Peng Cui 0001 and Yuan Qi 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tan2023provably.pdf:pdf},
  mdate = {2024-08-13},
  pages = {33563-33580},
  pdf = {https://proceedings.mlr.press/v202/tan23b/tan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provably Invariant Learning without Domain Information},
  url = {https://proceedings.mlr.press/v202/tan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023defects,
  abstract = {In this paper, we prove the representation defects of a cascaded convolutional decoder network, considering the capacity of representing different frequency components of an input sample. We conduct the discrete Fourier transform on each channel of the feature map in an intermediate layer of the decoder network. Then, we extend the 2D circular convolution theorem to represent the forward and backward propagations through convolutional layers in the frequency domain. Based on this analysis, we prove three defects in representing feature spectrums. First, the convolutional network tends to weaken high-frequency components. Second, upsampling operations generate repetitive strong signals at certain frequencies. Third, small frequency shifts can prevent effective decoder learning.},
  author = {Ling Tang and Wen Shen and Zhanpeng Zhou and Yuefeng Chen and Quanshi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023defects.pdf:pdf},
  mdate = {2025-06-10},
  pages = {33758--33791},
  pdf = {https://proceedings.mlr.press/v202/tang23i/tang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Defects of Convolutional Decoder Networks in Frequency Representation},
  url = {https://proceedings.mlr.press/v202/tang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023autodifferentiation,
  abstract = {We consider the problem of how to differentiate computations expressed relationally. The relational data model was designed to facilitate large-scale data management and analytics. We demonstrate experimentally that a relational engine running an auto-differentiated relational algorithm can easily scale to very large datasets and is competitive with state-of-the-art, special-purpose systems for large-scale distributed machine learning.},
  author = {Yuxin Tang and Zhimin Ding and Dimitrije Jankov and Binhang Yuan and Daniel Bourgeois and Chris Jermaine},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023autodifferentiation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33581--33598},
  pdf = {https://proceedings.mlr.press/v202/tang23a/tang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Auto-Differentiation of Relational Computations for Very Large Scale Machine Learning},
  url = {https://proceedings.mlr.press/v202/tang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023regretminimizing,
  abstract = {By incorporating regret minimization, double oracle methods have demonstrated rapid convergence to Nash Equilibrium in normal-form games and extensive-form games, through algorithms such as online double oracle and extensive-form double oracle, respectively. In this study, we examine the theoretical convergence rate and sample complexity of such regret minimization-based double oracle methods, utilizing a unified framework called Regret-Minimizing Double Oracle. We extend online double oracle to extensive-form games and determine its sample complexity. We demonstrate that the sample complexity of extensive-form double oracle can be exponential in the number of information sets, owing to the exponentially decaying stopping threshold of restricted games. To solve this problem, we propose the Periodic Double Oracle method, which has the lowest sample complexity among regret minimization-based double oracle methods, being only polynomial in the number of information sets.},
  author = {Xiaohang Tang and Le Cong Dinh and Stephen Marcus McAleer and Yaodong Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023regretminimizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33599--33615},
  pdf = {https://proceedings.mlr.press/v202/tang23b/tang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regret-Minimizing Double Oracle for Extensive-Form Games},
  url = {https://proceedings.mlr.press/v202/tang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023understanding,
  abstract = {We study the learning dynamics of self-predictive learning for reinforcement learning, a family of algorithms that learn representations by minimizing the prediction error of their own future latent representations. Despite recent empirical success, such algorithms have an apparent defect: trivial representations (such as constants) minimize the prediction error, yet it is obviously undesirable to converge to such solutions. Our central insight is that careful designs of the optimization dynamics are critical to learning meaningful representations. We identify that a faster paced optimization of the predictor and semi-gradient updates on the representation are crucial to preventing representation collapse. In an idealized setup, we show self-predictive learning dynamics carries out spectral decomposition on the state transition matrix, effectively capturing information of the transition dynamics. Building on the theoretical insights, we propose bidirectional self-predictive learning, a novel self-predictive algorithm that learns two representations simultaneously.},
  author = {Yunhao Tang and Zhaohan Daniel Guo and Pierre Harvey Richemond and Bernardo {\'{A}}vila Pires and Yash Chandak and R{\'{e}}mi Munos and Mark Rowland and Mohammad Gheshlaghi Azar and Charline Le Lan and Clare Lyle and Andr{\'{a}}s Gy{\"{o}}rgy and Shantanu Thakoor and Will Dabney and Bilal Piot and Daniele Calandriello and Michal Valko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023understanding.pdf:pdf},
  mdate = {2024-10-02},
  pages = {33632--33656},
  pdf = {https://proceedings.mlr.press/v202/tang23d/tang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Self-Predictive Learning for Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/tang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023domoac,
  abstract = {Multi-step learning applies lookahead over multiple time steps and has proved to be beneficial for policy evaluation and control in reinforcement learning. While multi-step policy evaluation is straightforward, multi-step policy improvement is more involved and requires consideration over the interplay of policy evaluation and improvement. We introduce doubly multi-step off-policy VI (DoMo-VI), a novel oracle algorithm that combines multi-step policy improvements and policy evaluations, with guaranteed convergence speed-up to the optimal policy. When combined with function approximation and the IMPALA architecture, DoMo-AC showed improvements over baseline algorithms on Atari-57 game benchmarks.},
  author = {Yunhao Tang and Tadashi Kozuno and Mark Rowland and Anna Harutyunyan and R{\'{e}}mi Munos and Bernardo {\'{A}}vila Pires and Michal Valko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023domoac.pdf:pdf},
  mdate = {2024-10-02},
  pages = {33657--33673},
  pdf = {https://proceedings.mlr.press/v202/tang23e/tang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DoMo-AC}: Doubly Multi-step Off-policy Actor-Critic Algorithm},
  url = {https://proceedings.mlr.press/v202/tang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023towards,
  abstract = {Even though Graph Neural Networks have achieved remarkable success in real-world applications, understanding their working mechanism in theory is still in its primary stage. In this paper, we move towards this goal from the perspective of generalization. With consideration of stochastic optimization, we establish high probability bounds of generalization gap and gradients for transductive learning algorithms. After that, we provide high probability bounds of generalization gap for popular GNNs and analyze the factors affecting their generalization capability. These theoretical results reveal how the network architecture impacts the generalization gap. Experiments on benchmark datasets validate the theoretical findings.},
  author = {Huayi Tang and Yong Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023towards.pdf:pdf},
  mdate = {2023-10-31},
  pages = {33674--33719},
  pdf = {https://proceedings.mlr.press/v202/tang23f/tang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Understanding Generalization of Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/tang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023towards,
  abstract = {TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites.},
  author = {Yunhao Tang and R{\'{e}}mi Munos},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33720--33738},
  pdf = {https://proceedings.mlr.press/v202/tang23g/tang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards a better understanding of representation dynamics under {TD}-learning},
  url = {https://proceedings.mlr.press/v202/tang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023valearning,
  abstract = {In reinforcement learning, the advantage function is critical for policy improvement, but is often extracted from a learned Q-function. A natural question is: Why not learn the advantage function directly? In this work, we introduce VA-learning, which directly learns advantage function and value function using bootstrapping, without explicit reference to Q-functions. VA-learning learns off-policy and enjoys similar theoretical guarantees as Q-learning. Thanks to the direct learning of advantage function and value function, VA-learning improves the sample efficiency over Q-learning both in tabular implementations and deep RL agents on Atari-57 games. We also identify a close connection between VA-learning and the dueling architecture, which partially explains why a simple architectural change to DQN agents tends to improve performance.},
  author = {Yunhao Tang and R{\'e}mi Munos and Mark Rowland and Michal Valko},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023valearning.pdf:pdf},
  mdate = {2024-10-02},
  pages = {33739--33757},
  pdf = {https://proceedings.mlr.press/v202/tang23h/tang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {VA-learning as a more efficient alternative to Q-learning},
  url = {https://proceedings.mlr.press/v202/tang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tang2023differenceindifferences,
  abstract = {This study considers the estimation of conditional causal effects in the presence of unmeasured confounding for a balanced panel with treatment imposed at the last time point. To address this, we combine Difference-in-differences (DiD) and tree-based methods and propose a new identification assumption that allows for the violation of the (conditional) parallel trends assumption adopted by most existing DiD methods. Under this new assumption, we prove partial identifiability of the conditional average treatment effect on the treated group (CATT). Our proposed method estimates CATT through a tree-based causal approach, guided by a novel splitting rule that avoids model misspecification and unnecessary auxiliary parameter estimation. The splitting rule measures both the error of fitting observed data and the violation of conditional parallel trends simultaneously. We also develop an ensemble of multiple trees via gradient boosting to further enhance performance. Experimental results on both synthetic and real-world datasets validate the effectiveness of our proposed method.},
  author = {Caizhi Tang and Huiyuan Wang and Xinyu Li and Qing Cui and Longfei Li and Jun Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tang2023differenceindifferences.pdf:pdf},
  mdate = {2024-10-06},
  pages = {33792--33803},
  pdf = {https://proceedings.mlr.press/v202/tang23j/tang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Difference-in-Differences Meets Tree-based Methods: Heterogeneous Treatment Effects Estimation with Unmeasured Confounding},
  url = {https://proceedings.mlr.press/v202/tang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{taniguchi2023endtoend,
  abstract = {We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.},
  author = {Shohei Taniguchi and Masahiro Suzuki and Yusuke Iwasawa and Yutaka Matsuo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/taniguchi2023endtoend.pdf:pdf},
  mdate = {2025-04-09},
  pages = {33804--33815},
  pdf = {https://proceedings.mlr.press/v202/taniguchi23a/taniguchi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization},
  url = {https://proceedings.mlr.press/v202/taniguchi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tanwisuth2023pouf,
  abstract = {Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models, by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines. PyTorch code is available at https://github.com/korawat-tanwisuth/POUF.},
  author = {Korawat Tanwisuth and Shujian Zhang and Huangjie Zheng and Pengcheng He and Mingyuan Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tanwisuth2023pouf.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33816--33832},
  pdf = {https://proceedings.mlr.press/v202/tanwisuth23a/tanwisuth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {POUF: Prompt-Oriented Unsupervised Fine-tuning for Large Pre-trained Models},
  url = {https://proceedings.mlr.press/v202/tanwisuth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tao2023dual,
  abstract = {The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and demonstrate its effectiveness through evaluations on multiple models and datasets, where it achieves state-of-the-art performance. Code is available at https://github.com/Linwei94/DualFocalLoss},
  author = {Linwei Tao and Minjing Dong and Chang Xu 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tao2023dual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33833--33849},
  pdf = {https://proceedings.mlr.press/v202/tao23a/tao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Dual Focal Loss for Calibration},
  url = {https://proceedings.mlr.press/v202/tao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tao2023abstracttoexecutable,
  abstract = {Training long-horizon robotic policies in complex physical environments is essential for many applications, such as robotic manipulation. However, learning a policy that can generalize to unseen tasks is challenging. In this work, we propose to achieve one-shot task generalization by decoupling plan generation and plan execution. Specifically, our method solves complex long-horizon tasks in three steps: build a paired abstract environment by simplifying geometry and physics, generate abstract trajectories, and solve the original task by an abstract-to-executable trajectory translator. However, this introduces a large domain gap between abstract trajectories and the actual executed trajectories as abstract trajectories lack low-level details and are not aligned frame-to-frame with the executed trajectory. In a manner reminiscent of language translation, our approach leverages a seq-to-seq model to overcome the large domain gap between the abstract and executable trajectories, enabling the low-level policy to follow the abstract trajectory. Experimental results on various unseen long-horizon tasks with different robot embodiments demonstrate the practicability of our methods to achieve one-shot task generalization.},
  author = {Stone Tao and Xiaochen Li and Tongzhou Mu and Zhiao Huang and Yuzhe Qin and Hao Su 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tao2023abstracttoexecutable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33850--33882},
  pdf = {https://proceedings.mlr.press/v202/tao23b/tao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization},
  url = {https://proceedings.mlr.press/v202/tao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{taori2023data,
  abstract = {Datasets scraped from the internet have been critical to large-scale machine learning. Yet, its success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision. In this work, we formalize a system where interactions with one model are recorded as history and scraped as training data in the future. We then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). We find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as uniform faithfulness.},
  author = {Rohan Taori and Tatsunori Hashimoto},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/taori2023data.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33883--33920},
  pdf = {https://proceedings.mlr.press/v202/taori23a/taori23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data Feedback Loops: Model-driven Amplification of Dataset Biases},
  url = {https://proceedings.mlr.press/v202/taori23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tarun2023deep,
  abstract = {With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks.},
  author = {Ayush Kumar Tarun and Vikram Singh Chundawat and Murari Mandal and Mohan S. Kankanhalli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tarun2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33921--33939},
  pdf = {https://proceedings.mlr.press/v202/tarun23a/tarun23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Regression Unlearning},
  url = {https://proceedings.mlr.press/v202/tarun23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{teneggi2023how,
  abstract = {Score-based generative modeling, informally referred to as diffusion models, continue to grow in popularity across several important domains and tasks. While they provide high-quality and diverse samples from empirical distributions, important questions remain on the reliability and trustworthiness of these sampling procedures for their responsible use in critical scenarios. Conformal prediction is a modern tool to construct finite-sample, distribution-free uncertainty guarantees for any black-box predictor. In this work, we focus on image-to-image regression tasks and we present a generalization of the Risk-Controlling Prediction Sets (RCPS) procedure, that we term K-RCPS, which allows to (i) provide entrywise calibrated intervals for future samples of any diffusion model, and (ii) control a certain notion of risk with respect to a ground truth image with minimal mean interval length. Differently from existing conformal risk control procedures, ours relies on a novel convex optimization approach that allows for multidimensional risk control while provably minimizing the mean interval length. We illustrate our approach on two real-world image denoising problems: on natural images of faces as well as on computed tomography (CT) scans of the abdomen, demonstrating state of the art performance.},
  author = {Jacopo Teneggi and Matthew Tivnan and J. Webster Stayman and Jeremias Sulam},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/teneggi2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33940--33960},
  pdf = {https://proceedings.mlr.press/v202/teneggi23a/teneggi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control},
  url = {https://proceedings.mlr.press/v202/teneggi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tenenbaum2023concurrent,
  abstract = {We introduce the concurrent shuffle model of differential privacy. In this model we have multiple concurrent shufflers permuting messages from different, possibly overlapping, batches of users. Similarly to the standard (single) shuffle model, the privacy requirement is that the concatenation of all shuffled messages should be differentially private. We study the private continual summation problem (a.k.a. the counter problem) and show that the concurrent shuffle model allows for significantly improved error compared to a standard (single) shuffle model. Specifically, we give a summation algorithm with error O(n^{1/(2k+1)}) with k concurrent shufflers on a sequence of length n. Furthermore, we prove that this bound is tight for any k, even if the algorithm can choose the sizes of the batches adaptively.},
  author = {Jay Tenenbaum and Haim Kaplan and Yishay Mansour and Uri Stemmer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tenenbaum2023concurrent.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33961--33982},
  pdf = {https://proceedings.mlr.press/v202/tenenbaum23a/tenenbaum23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Concurrent Shuffle Differential Privacy Under Continual Observation},
  url = {https://proceedings.mlr.press/v202/tenenbaum23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{teng2023finding,
  abstract = {Generalization is one of the most fundamental challenges in deep learning, aiming to predict model performances on unseen data. Empirically, such predictions usually rely on a validation set, while recent works showed that an unlabeled validation set also works. Without validation sets, it is extremely difficult to obtain non-vacuous generalization bounds, which leads to a weaker task of finding generalization measures that monotonically relate to generalization error. In this paper, we propose a new generalization measure REF Complexity (RElative Fitting degree between signal and noise), motivated by the intuition that a given model-algorithm pair may generalize well if it fits signal (e.g., true labels) fast while fitting noise (e.g., random labels) slowly.},
  author = {Jiaye Teng and Bohang Zhang and Ruichen Li and Haowei He and Yequan Wang and Yan Tian and Yang Yuan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/teng2023finding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {33983--34010},
  pdf = {https://proceedings.mlr.press/v202/teng23a/teng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Finding Generalization Measures by Contrasting Signal and Noise},
  url = {https://proceedings.mlr.press/v202/teng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tennenholtz2023reinforcement,
  abstract = {We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features.},
  author = {Guy Tennenholtz and Nadav Merlis and Lior Shani and Martin Mladenov and Craig Boutilier},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tennenholtz2023reinforcement.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34011--34053},
  pdf = {https://proceedings.mlr.press/v202/tennenholtz23a/tennenholtz23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reinforcement Learning with History Dependent Dynamic Contexts},
  url = {https://proceedings.mlr.press/v202/tennenholtz23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{terminassian2023pwshap,
  abstract = {Predictive black-box models can exhibit high accuracy but their opaque nature hinders their uptake in safety-critical deployment environments. Explanation methods (XAI) can provide confidence for decision-making through increased transparency. However, existing XAI methods are not tailored towards models in sensitive domains where one predictor is of special interest, such as a treatment effect in a clinical model, or ethnicity in policy models. We introduce Path-Wise Shapley effects (PWSHAP), a framework for assessing the targeted effect of a binary (e.g. treatment) variable from a complex outcome model. Our approach augments the predictive model with a user-defined directed acyclic graph (DAG). The method then uses the graph alongside on-manifold Shapley values to identify effects along causal pathways whilst maintaining robustness to adversarial attacks. We establish error bounds for the identified path-wise Shapley effects and for Shapley values. We show PWSHAP can perform local bias and mediation analyses with faithfulness to the model. Further, if the targeted variable is randomised we can quantify local effect modification. We demonstrate the resolution, interpretability, and true locality of our approach on examples and a real-world experiment.},
  author = {Lucile Ter-Minassian and Oscar Clivio and Karla DiazOrdaz and Robin J. Evans 0002 and Christopher C. Holmes},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/terminassian2023pwshap.pdf:pdf},
  mdate = {2025-06-17},
  pages = {34054--34089},
  pdf = {https://proceedings.mlr.press/v202/ter-minassian23a/ter-minassian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PWSHAP: A Path-Wise Explanation Model for Targeted Variables},
  url = {https://proceedings.mlr.press/v202/ter-minassian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tewari2023estimation,
  abstract = {This paper revisits Gaussian Mixture Copula Model (GMCM), a more expressive alternative to the widely used Gaussian Mixture Model (GMM), with the goal to make its parameter estimation tractable. Both the Expectation Maximization and the direct Likelihood Maximization frameworks for GMCM have to grapple with a likelihood function that lacks a closed form. This has led to a few approximation schemes that alleviate the problem, nonetheless leaving the issue still unresolved. The work addresses key challenges in GMCM parameter estimation, including the fact that the (log-)likelihood function does not admit a closed analytical form and that GMCMs suffer from an inherent issue of parameter non-identifiability.},
  author = {Ashutosh Tewari},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tewari2023estimation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34090--34104},
  pdf = {https://proceedings.mlr.press/v202/tewari23a/tewari23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Estimation of Gaussian Mixture Copula Models},
  url = {https://proceedings.mlr.press/v202/tewari23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{thopalli2023targetaware,
  abstract = {In this paper, we address the problem of adapting models from a source domain to a target domain, a task that has become increasingly important due to the brittle generalization of deep neural networks. While several test-time adaptation techniques have emerged, they typically rely on synthetic toolbox data augmentations in cases of limited target data availability. We consider the challenging setting of single-shot adaptation and explore the design of augmentation strategies. We argue that augmentations utilized by existing methods are insufficient to handle large distribution shifts, and hence propose a new approach SiSTA, which first fine-tunes a generative model from the source domain using a single-shot target, and then employs novel sampling strategies for curating synthetic target data.},
  author = {Kowshik Thopalli and Rakshith Subramanyam and Pavan K. Turaga and Jayaraman J. Thiagarajan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/thopalli2023targetaware.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34105--34119},
  pdf = {https://proceedings.mlr.press/v202/thopalli23a/thopalli23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Target-Aware Generative Augmentations for Single-Shot Adaptation},
  url = {https://proceedings.mlr.press/v202/thopalli23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tian2023elsa,
  abstract = {We study the domain adaptation problem with label shift, where the marginal distribution of the label varies across the training and testing datasets, while the conditional distribution of features given the label is the same. Traditional methods either suffer from large estimation errors or require cumbersome post-prediction calibrations. We propose a moment-matching framework for adapting the label shift based on the geometry of the influence function and introduce a novel method named Efficient Label Shift Adaptation (ELSA), in which the adaptation weights can be estimated by solving linear systems. The ELSA estimator is $\sqrt{n}$-consistent and asymptotically normal. ELSA can achieve state-of-the-art estimation performances without post-prediction calibrations, thus, gaining computational efficiency.},
  author = {Qinglong Tian and Xin Zhang and Jiwei Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tian2023elsa.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34120--34142},
  pdf = {https://proceedings.mlr.press/v202/tian23a/tian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ELSA}: Efficient Label Shift Adaptation through the Lens of Semiparametric Models},
  url = {https://proceedings.mlr.press/v202/tian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tiao2023spherical,
  abstract = {Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.},
  author = {Louis C. Tiao and Vincent Dutordoir and Victor Picheny},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tiao2023spherical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34143--34160},
  pdf = {https://proceedings.mlr.press/v202/tiao23a/tiao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes},
  url = {https://proceedings.mlr.press/v202/tiao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tiapkin2023fast,
  abstract = {We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al. (2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\tilde{O}(H^3S^2A/\varepsilon^2)$ sample complexity thus improving the $\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\tilde{O}(\text{poly}(S,A,H)/\varepsilon)$.},
  author = {Daniil Tiapkin and Denis Belomestny and Daniele Calandriello and Eric Moulines and R\'{e}mi Munos and Alexey Naumov and Pierre Perrault and Yunhao Tang and Michal Valko and Pierre M\'{e}nard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tiapkin2023fast.pdf:pdf},
  mdate = {2024-05-07},
  pages = {34161--34221},
  pdf = {https://proceedings.mlr.press/v202/tiapkin23a/tiapkin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Rates for Maximum Entropy Exploration},
  url = {https://proceedings.mlr.press/v202/tiapkin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tifrea2023marginbased,
  abstract = {We challenge the widely held belief that active learning (AL) algorithms, and in particular margin-based AL, yield better predictive performance than passive learning (PL). Recent empirical evidence suggests this added cost might be in vain, as margin-based AL can sometimes perform even worse than PL. We prove for logistic regression that PL outperforms margin-based AL even for noiseless data and when using the Bayes optimal decision boundary for sampling. Our theoretical insights reveal that the underlying mechanism is entirely different in high dimensions. The high-dimensional phenomenon is exacerbated when the separation between the classes is small. We corroborate our theoretical insights with experiments on 20 high-dimensional datasets spanning diverse applications from finance and histology to chemistry and computer vision.},
  author = {Alexandru Tifrea and Jacob Clarysse and Fanny Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tifrea2023marginbased.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34222--34262},
  pdf = {https://proceedings.mlr.press/v202/tifrea23a/tifrea23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Margin-based sampling in high dimensions: When being active is less efficient than staying passive},
  url = {https://proceedings.mlr.press/v202/tifrea23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tigas2023differentiable,
  abstract = {We introduce a gradient-based approach for the problem of Bayesian optimal experimental design to learn causal models in a batch setting — a critical component for causal discovery from finite data where interventions can be costly or risky. Existing methods rely on greedy approximations to construct a batch of experiments while using black-box methods to optimize over a single target-state pair to intervene with. In this work, we completely dispose of the black-box optimization techniques and greedy heuristics and instead propose a conceptually simple end-to-end gradient-based optimization procedure to acquire a set of optimal intervention target-value pairs.},
  author = {Panagiotis Tigas and Yashas Annadani and Desi R. Ivanova and Andrew Jesson and Yarin Gal and Adam Foster and Stefan Bauer},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tigas2023differentiable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34263--34279},
  pdf = {https://proceedings.mlr.press/v202/tigas23a/tigas23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentiable Multi-Target Causal Bayesian Experimental Design},
  url = {https://proceedings.mlr.press/v202/tigas23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tiomoko2023pcabased,
  author = {Malik Tiomoko and Romain Couillet and Frdric Pascal 0001},
  booktitle = {ICML},
  mdate = {2023-08-28},
  pages = {34280-34300},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PCA-based Multi-Task Learning: a Random Matrix Approach},
  url = {https://proceedings.mlr.press/v202/tiomoko23a.html},
  urltype = {oa},
  year = {2023}
}

@inproceedings{tirer2023perturbation,
  author = {Tom Tirer and Haoxiang Huang and Jonathan Niles-Weed},
  booktitle = {ICML},
  mdate = {2023-08-28},
  pages = {34301-34329},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Perturbation Analysis of Neural Collapse},
  url = {https://proceedings.mlr.press/v202/tirer23a.html},
  urltype = {oa},
  year = {2023}
}

@inproceedings{tiwari2023overcoming,
  author = {Rishabh Tiwari and Pradeep Shenoy},
  booktitle = {ICML},
  mdate = {2023-08-28},
  pages = {34330-34343},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Overcoming Simplicity Bias in Deep Networks using a Feature Sieve},
  url = {https://proceedings.mlr.press/v202/tiwari23a.html},
  urltype = {oa},
  year = {2023}
}

@inproceedings{tomani2023beyond,
  abstract = {Calibrating deep learning models to yield uncertainty-aware predictions is crucial as deep neural networks get increasingly deployed in safety-critical applications. While existing post-hoc calibration methods achieve impressive results on in-domain test datasets, they are limited by their inability to yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD) scenarios.},
  author = {Christian Tomani and Futa Kai Waseda and Yuesong Shen and Daniel Cremers},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tomani2023beyond.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34344--34368},
  pdf = {https://proceedings.mlr.press/v202/tomani23a/tomani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Beyond In-Domain Scenarios: Robust Density-Aware Calibration},
  url = {https://proceedings.mlr.press/v202/tomani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tong2023distribution,
  abstract = {Accurate prediction of the out-of-distribution data is desired for a learning algorithm. In domain generalization, training data from source domains tend to have different distributions from that of the target domain, while the target data are absence in the training process. We propose a Distribution Free Domain Generalization (DFDG) procedure for classification by conducting standardization to avoid the dominance of a few domains in the training process. The essence of the DFDG is its reformulating the cross domain/class discrepancy by pairwise two sample test statistics, and equally weights their importance or the covariance structures to avoid dominant domain/class. A theoretical generalization bound is established for the multi-class classification problem. The DFDG is shown to offer a superior performance in empirical studies with fewer hyperparameters, which means faster and easier implementation.},
  author = {Peifeng Tong and Wu Su and He Li and Jialin Ding and Zhan Haoxiang and Song Xi Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tong2023distribution.pdf:pdf},
  mdate = {2023-09-30},
  pages = {34369--34378},
  pdf = {https://proceedings.mlr.press/v202/tong23a/tong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distribution Free Domain Generalization},
  url = {https://proceedings.mlr.press/v202/tong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tonin2023extending,
  abstract = {The goal of this paper is to revisit Kernel Principal Component Analysis (KPCA) through dualization of a difference of convex functions. This allows to naturally extend KPCA to multiple objective functions and leads to efficient gradient-based algorithms avoiding the expensive SVD of the Gram matrix. Particularly, we consider objective functions that can be written as Moreau envelopes, demonstrating how to promote robustness and sparsity within the same framework. The proposed method is evaluated on synthetic and realworld benchmarks, showing significant speedup in KPCA training time as well as highlighting the benefits in terms of robustness and sparsity.},
  author = {Francesco Tonin and Alex Lambert and Panagiotis Patrinos and Johan A. K. Suykens},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tonin2023extending.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34379--34393},
  pdf = {https://proceedings.mlr.press/v202/tonin23a/tonin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast Algorithms},
  url = {https://proceedings.mlr.press/v202/tonin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tonolini2023robust,
  abstract = {Recent advances in weak supervision (WS) techniques allow to mitigate the enormous cost and effort of human data annotation for supervised machine learning by automating it using simple rule-based labelling functions (LFs). However, LFs need to be carefully designed, often requiring expert domain knowledge and extensive validation for existing WS methods to be effective. To tackle this, we propose the Weak Supervision Variational Auto-Encoder (WS-VAE), a novel framework that combines unsupervised representation learning and weak labelling to reduce the dependence of WS on expert and manual engineering of LFs. Our technique learns from inputs and weak labels jointly to capture the input signals distribution with a latent space. The unsupervised representation component of the WS-VAE regularises the inference of weak labels, while a specifically designed decoder allows the model to learn the relevance of LFs for each input. These unique features lead to considerably improved robustness to the quality of LFs, compared to existing methods. An extensive empirical evaluation on a standard WS benchmark shows that our WS-VAE is competitive to state-of-the-art methods and substantially more robust to LF engineering.},
  author = {Francesco Tonolini and Nikolaos Aletras and Yunlong Jiao and Gabriella Kazai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tonolini2023robust.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34394--34408},
  pdf = {https://proceedings.mlr.press/v202/tonolini23a/tonolini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Weak Supervision with Variational Auto-Encoders},
  url = {https://proceedings.mlr.press/v202/tonolini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tran2023fully,
  abstract = {We present a fully Bayesian autoencoder model that treats both local latent variables and global decoder parameters in a Bayesian fashion. This approach allows for flexible priors and posterior approximations while keeping the inference costs low. To achieve this, we introduce an amortized MCMC approach by utilizing an implicit stochastic network to learn sampling from the posterior over local latent variables. Furthermore, we extend the model by incorporating a Sparse Gaussian Process prior over the latent space, allowing for a fully Bayesian treatment of inducing points and kernel hyperparameters and leading to improved scalability. Additionally, we enable Deep Gaussian Process priors on the latent space and the handling of missing data. We evaluate our model on a range of experiments focusing on dynamic representation learning and generative modeling, demonstrating the strong performance of our approach in comparison to existing methods that combine Gaussian Processes and autoencoders.},
  author = {Ba-Hien Tran and Babak Shahbaba and Stephan Mandt and Maurizio Filippone},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tran2023fully.pdf:pdf},
  mdate = {2023-11-12},
  pages = {34409--34430},
  pdf = {https://proceedings.mlr.press/v202/tran23a/tran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fully Bayesian Autoencoders with Latent Sparse Gaussian Processes},
  url = {https://proceedings.mlr.press/v202/tran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{truble2023discrete,
  abstract = {Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. We propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes.},
  author = {Frederik Truble and Anirudh Goyal and Nasim Rahaman and Michael Curtis Mozer and Kenji Kawaguchi and Yoshua Bengio and Bernhard Schlkopf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/truble2023discrete.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34431--34455},
  pdf = {https://proceedings.mlr.press/v202/trauble23a/trauble23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discrete Key-Value Bottleneck},
  url = {https://proceedings.mlr.press/v202/trauble23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{trockman2023mimetic,
  abstract = {It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they look more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique mimetic initialization.},
  author = {Asher Trockman and J. Zico Kolter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/trockman2023mimetic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34456--34468},
  pdf = {https://proceedings.mlr.press/v202/trockman23a/trockman23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mimetic Initialization of Self-Attention Layers},
  url = {https://proceedings.mlr.press/v202/trockman23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tsai2023representer,
  abstract = {We introduce a novel class of sample-based explanations we term high-dimensional representers, that can be used to explain the predictions of a regularized high-dimensional model in terms of importance weights for each of the training samples. Our workhorse is a novel representer theorem for general regularized high-dimensional models, which decomposes the model prediction in terms of contributions from each of the training samples: with positive (negative) values corresponding to positive (negative) impact training samples to the model's prediction. We derive consequences for the canonical instances of ℓ₁ regularized sparse models and nuclear norm regularized low-rank models. As a case study, we further investigate the application of low-rank models in the context of collaborative filtering, where we instantiate high-dimensional representers for specific popular classes of models. Finally, we study the empirical performance of our proposed methods on three real-world binary classification datasets and two recommender system datasets. We also showcase the utility of high-dimensional representers in explaining model recommendations.},
  author = {Che-Ping Tsai and Jiong Zhang and Hsiang-Fu Yu and Eli Chien and Cho-Jui Hsieh and Pradeep Kumar Ravikumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tsai2023representer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34469--34490},
  pdf = {https://proceedings.mlr.press/v202/tsai23a/tsai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Representer Point Selection for Explaining Regularized High-dimensional Models},
  url = {https://proceedings.mlr.press/v202/tsai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tseran2023expected,
  abstract = {We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks.},
  author = {Hanna Tseran and Guido Montfar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tseran2023expected.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34491--34532},
  pdf = {https://proceedings.mlr.press/v202/tseran23a/tseran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Expected Gradients of Maxout Networks and Consequences to Parameter Initialization},
  url = {https://proceedings.mlr.press/v202/tseran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{tukan2023provable,
  abstract = {The paper focuses on radial basis function neural networks (RBFNN) which are well-known for their capability to approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. The authors introduce the first algorithm to construct coresets for RBFNNs, i.e., small weighted subsets that approximate the loss of the input data on any radial basis function network and thus approximate any function defined by an RBFNN on the larger input data. They use their coresets to obtain a provable data subset selection algorithm for training deep neural networks, and perform empirical evaluations on function approximation and dataset subset selection on popular network architectures and data sets, demonstrating the efficacy and accuracy of their coreset construction.},
  author = {Murad Tukan and Samson Zhou and Alaa Maalouf and Daniela Rus and Vladimir Braverman and Dan Feldman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/tukan2023provable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34533--34555},
  pdf = {https://proceedings.mlr.press/v202/tukan23a/tukan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provable Data Subset Selection For Efficient Neural Networks Training},
  url = {https://proceedings.mlr.press/v202/tukan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{uchendu2023jumpstart,
  abstract = {Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks that present exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that it is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.},
  author = {Ikechukwu Uchendu and Ted Xiao and Yao Lu 0006 and Banghua Zhu and Mengyuan Yan and Josphine Simon and Matthew Bennice and Chuyuan Fu and Cong Ma 0001 and Jiantao Jiao and Sergey Levine and Karol Hausman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/uchendu2023jumpstart.pdf:pdf},
  mdate = {2025-01-31},
  pages = {34556--34583},
  pdf = {https://proceedings.mlr.press/v202/uchendu23a/uchendu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Jump-Start Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/uchendu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{udwani2023submodular,
  abstract = {We define a new class of set functions that in addition to being monotone and subadditive, also admit a very limited form of submodularity defined over a permutation of the ground set. We refer to this permutation as a submodular order. This class of functions includes monotone submodular functions as a sub-family. We give fast algorithms with strong approximation guarantees for maximizing submodular order functions under a variety of constraints and show a nearly tight upper bound on the highest approximation guarantee achievable by algorithms with polynomial query complexity. Applying this new notion to the problem of constrained assortment optimization in fundamental choice models, we obtain new algorithms that are both faster and have stronger approximation guarantees (in some cases, first algorithm with constant factor guarantee). We also show an intriguing connection to the maximization of monotone submodular functions in the streaming model, where we recover best known approximation guarantees as a corollary of our results.},
  author = {Rajan Udwani},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/udwani2023submodular.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34584--34614},
  pdf = {https://proceedings.mlr.press/v202/udwani23a/udwani23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Submodular Order Functions and Assortment Optimization},
  url = {https://proceedings.mlr.press/v202/udwani23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{uehara2023computationally,
  abstract = {We study reinforcement learning with function approximation for large-scale Partially Observable Markov Decision Processes (POMDPs) where the state space and observation space are large or even continuous. Particularly, we consider Hilbert space embeddings of POMDP where the feature of latent states and the feature of observations admit a conditional Hilbert space embedding of the observation emission process, and the latent state transition is deterministic. Under the function approximation setup where the optimal latent state-action Q-function is linear in the state feature, and the optimal Q-function has a gap in actions, we provide a computationally and statistically efficient algorithm for finding the exact optimal policy. The algorithm's computational and statistical complexities scale polynomially with respect to the horizon and the intrinsic dimension of the feature on the observation space. Furthermore, we show both the deterministic latent transitions and gap assumptions are necessary to avoid statistical complexity exponential in horizon or dimension. Since our guarantee does not have an explicit dependence on the size of the state and observation spaces, our algorithm provably scales to large-scale POMDPs.},
  author = {Masatoshi Uehara and Ayush Sekhari and Jason D. Lee and Nathan Kallus and Wen Sun 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/uehara2023computationally.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34615--34641},
  pdf = {https://proceedings.mlr.press/v202/uehara23a/uehara23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Computationally Efficient PAC RL in POMDPs with Latent Determinism and Conditional Embeddings},
  url = {https://proceedings.mlr.press/v202/uehara23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ullah2023adaptive,
  abstract = {We formalize the problem of machine unlearning as the design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We provide efficient unlearning algorithms for linear and prefix-sum query classes and show that unlearning in many problems, particularly stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees. For smooth Lipschitz losses and any ρ>0, our results yield an unlearning algorithm with excess population risk of Õ(1/√n + √d/nρ) with unlearning query (gradient) complexity Õ(ρ · Retraining Complexity), where d is the model dimensionality and n is the initial number of samples. For non-smooth Lipschitz losses, we provide an unlearning algorithm with excess population risk Õ(1/√n + (√d/nρ)^(1/2)) with the same unlearning query (gradient) complexity. In the special case of Generalized Linear Models (GLMs), such as linear and logistic regression, we achieve dimension-independent rates of Õ(1/√n + 1/(nρ)^(2/3)) and Õ(1/√n + 1/(nρ)^(1/3)) for smooth Lipschitz and non-smooth Lipschitz losses respectively. We also provide generalizations from one unlearning request to dynamic streams consisting of insertions and deletions.},
  author = {Enayat Ullah and Raman Arora},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ullah2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34642--34667},
  pdf = {https://proceedings.mlr.press/v202/ullah23a/ullah23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Adaptive Query Release to Machine Unlearning},
  url = {https://proceedings.mlr.press/v202/ullah23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ullah2023private,
  abstract = {We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates. Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy. Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the "hardness of the problem" with minimal interactivity. We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.},
  author = {Enayat Ullah and Christopher A. Choquette-Choo and Peter Kairouz and Sewoong Oh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ullah2023private.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34668--34708},
  pdf = {https://proceedings.mlr.press/v202/ullah23b/ullah23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Private Federated Learning with Autotuned Compression},
  url = {https://proceedings.mlr.press/v202/ullah23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{uscidda2023monge,
  abstract = {Optimal transport (OT) theory has been used in machine learning to study and characterize maps that can push-forward efficiently a probability measure onto another. Recent works have drawn inspiration from Brenier's theorem, which states that when the ground cost is the squared-Euclidean distance, the "best" map to morph a continuous measure in $\mathcal{P}(\mathbb{R}^d)$ into another must be the gradient of a convex function. To exploit that result, Makkuva et. al (2020); Korotin et. al (2020) consider maps $T=\nabla f_\theta$, where $f_\theta$ is an input convex neural network (ICNN), as defined by Amos et. al (2017), and fit $\theta$ with SGD using samples. Despite their mathematical elegance, fitting OT maps with ICNNs raises many challenges, due notably to the many constraints imposed on $\theta$; the need to approximate the conjugate of $f_\theta$; or the limitation that they only work for the squared-Euclidean cost. More generally, we question the relevance of using Brenier's result, which only applies to densities, to constrain the architecture of candidate maps fitted on samples. Motivated by these limitations, we propose a radically different approach to estimating OT maps: Given a cost $c$ and a reference measure $\rho$, we introduce a regularizer, the Monge gap $\mathcal{M}^c_{\rho}(T)$ of a map $T$. That gap quantifies how far a map $T$ deviates from the ideal properties we expect from a $c$-OT map. We drop all architecture requirements for $T$ and simply minimize a distance (e.g., the Sinkhorn divergence) between $T \# \mu$ and $\nu$, regularized by $\mathcal{M}^c_{\rho}(T)$. We show this approach significantly outperforms other baselines in practice.},
  author = {Théo Uscidda and Marco Cuturi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/uscidda2023monge.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34709--34733},
  pdf = {https://proceedings.mlr.press/v202/uscidda23a/uscidda23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Monge Gap: A Regularizer to Learn All Transport Maps},
  url = {https://proceedings.mlr.press/v202/uscidda23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vacher2023semidual,
  abstract = {In this paper, we derive a semi-dual formulation for the problem of unbalanced quadratic optimal transport and we study its stability properties, namely we give upper and lower bounds for the Bregman divergence of the new objective that hold globally. We observe that the new objective gains even more convexity than in the balanced case. We use this formulation to prove the first results on statistical estimation of UOT potentials and we leverage the extra convexity to recover super-parametric rates. Interestingly, unlike in the balanced case, we do not require the potentials to be smooth. Then, use variable metric descent to solve the semi-dual problem for which we prove convergence at a 1/k rate for strongly convex potentials and exponential convergence in the balanced case when potentials are also smooth. We emphasize that our convergence results has an interest on its own as it generalizes previous convergence results to non-equivalent metrics.},
  author = {Adrien Vacher and Fran\c{c}ois-Xavier Vialard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vacher2023semidual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34734--34758},
  pdf = {https://proceedings.mlr.press/v202/vacher23a/vacher23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi-Dual Unbalanced Quadratic Optimal Transport: fast statistical rates and convergent algorithm},
  url = {https://proceedings.mlr.press/v202/vacher23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vadeboncoeur2023random,
  abstract = {We introduce a new class of spatially stochastic physics and data informed deep latent models for parametric partial differential equations (PDEs) which operate through scalable variational neural processes. We achieve this by assigning probability measures to the spatial domain, which allows us to treat collocation grids probabilistically as random variables to be marginalised out. Adapting this spatial statistics view, we solve forward and inverse problems for parametric PDEs in a way that leads to the construction of Gaussian process models of solution fields. The implementation of these random grids poses a unique set of challenges for inverse physics informed deep learning frameworks and we propose a new architecture called Grid Invariant Convolutional Networks (GICNets) to overcome these challenges.},
  author = {Arnaud Vadeboncoeur and Ieva Kazlauskaite and Yanni Papandreou and Fehmi Cirak and Mark Girolami and Ömer Deniz Akyildiz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vadeboncoeur2023random.pdf:pdf},
  mdate = {2024-10-06},
  pages = {34759--34778},
  pdf = {https://proceedings.mlr.press/v202/vadeboncoeur23a/vadeboncoeur23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Random Grid Neural Processes for Parametric Partial Differential Equations},
  url = {https://proceedings.mlr.press/v202/vadeboncoeur23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vakili2023delayed,
  abstract = {Black box optimisation of an unknown function from expensive and noisy evaluations is a ubiquitous problem in machine learning, academic research and industrial production. An abstraction of the problem can be formulated as a kernel based bandit problem (also known as Bayesian optimisation), where a learner aims at optimising a kernelized function through sequential noisy observations. The existing work predominantly assumes feedback is immediately available; an assumption which fails in many real world situations, including recommendation systems, clinical trials and hyperparameter tuning. We consider a kernel bandit problem under stochastically delayed feedback, and propose an algorithm with $\tilde{\mathcal{O}}(\sqrt{\Gamma_k(T)T}+\mathbb{E}[\tau])$ regret, where $T$ is the number of time steps, $\Gamma_k(T)$ is the maximum information gain of the kernel with $T$ observations, and $\tau$ is the delay random variable.},
  author = {Sattar Vakili and Danyal Ahmed and Alberto Bernacchia and Ciara Pike-Burke},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vakili2023delayed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34779--34792},
  pdf = {https://proceedings.mlr.press/v202/vakili23a/vakili23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Delayed Feedback in Kernel Bandits},
  url = {https://proceedings.mlr.press/v202/vakili23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vanderschueren2023accounting,
  abstract = {Machine learning (ML) holds great potential for accurately forecasting treatment outcomes over time, which could ultimately enable the adoption of more individualized treatment strategies in many practical applications. However, a significant challenge that has been largely overlooked by the ML literature on this topic is the presence of informative sampling in observational data. When instances are observed irregularly over time, sampling times are typically not random, but rather informative -- depending on the instance's characteristics, past outcomes, and administered treatments. In this work, we formalize informative sampling as a covariate shift problem and show that it can prohibit accurate estimation of treatment outcomes if not properly accounted for. To overcome this challenge, we present a general framework for learning treatment outcomes in the presence of informative sampling using inverse intensity-weighting, and propose a novel method, TESAR-CDE, that instantiates this framework. Using a simulation environment based on a clinical use case, we demonstrate the effectiveness of our approach in learning under informative sampling.},
  author = {Toon Vanderschueren and Alicia Curth and Wouter Verbeke and Mihaela van der Schaar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vanderschueren2023accounting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34855--34874},
  pdf = {https://proceedings.mlr.press/v202/vanderschueren23a/vanderschueren23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Accounting For Informative Sampling When Learning to Forecast Treatment Outcomes Over Time},
  url = {https://proceedings.mlr.press/v202/vanderschueren23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vannella2023best,
  abstract = {We investigate the problem of best arm identification in Multi-Agent Multi-Armed Bandits (MAMABs) where the rewards are defined through a factor graph. The goal is to find an optimal global action efficiently through collaborative exploration among multiple agents.},
  author = {Filippo Vannella and Alexandre Proutire and Jaeseong Jeong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vannella2023best.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34875--34907},
  pdf = {https://proceedings.mlr.press/v202/vannella23a/vannella23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Best Arm Identification in Multi-Agent Multi-Armed Bandits},
  url = {https://proceedings.mlr.press/v202/vannella23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{varma2023conditional,
  abstract = {We present CTreeOT, a convergent, differentiable algorithm for matching two trees when each tree is conditioned on some input. Such conditional tree matching is useful for light-weight, few-shot adaptation of tree prediction models without parameter fine-tuning, with applications to Text-to-SQL semantic parsing tasks.},
  author = {Harshit Varma and Abhijeet Awasthi and Sunita Sarawagi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/varma2023conditional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34908--34923},
  pdf = {https://proceedings.mlr.press/v202/varma23a/varma23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conditional Tree Matching for Inference-Time Adaptation of Tree Prediction Models},
  url = {https://proceedings.mlr.press/v202/varma23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{veldt2023optimal,
  abstract = {We study the approximability of an existing framework for clustering edge-colored hypergraphs, which is closely related to chromatic correlation clustering and is motivated by machine learning and data mining applications where the goal is to cluster a set of objects based on multiway interactions of different categories or types.},
  author = {Nate Veldt},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/veldt2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34924--34951},
  pdf = {https://proceedings.mlr.press/v202/veldt23a/veldt23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal {LP} Rounding and Linear-Time Approximation Algorithms for Clustering Edge-Colored Hypergraphs},
  url = {https://proceedings.mlr.press/v202/veldt23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{velingker2023fast,
  abstract = {We introduce efficient (1+ε)-approximation algorithms for the binary matrix factorization problem, where the inputs are a matrix A∈{0,1}^{n×d}, a rank parameter k>0, as well as an accuracy parameter ε>0, and the goal is to approximate A as a product of low-rank factors U∈{0,1}^{n×k} and V∈{0,1}^{k×d}. Our techniques generalize to other common variants of the BMF problem, admitting bicriteria (1+ε)-approximation algorithms for Lp loss functions and the setting where matrix operations are performed in F₂.},
  author = {Ameya Velingker and Maximilian V{\"o}tsch and David P. Woodruff and Samson Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/velingker2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34952--34977},
  pdf = {https://proceedings.mlr.press/v202/velingker23a/velingker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast (1+ε)-Approximation Algorithms for Binary Matrix Factorization},
  url = {https://proceedings.mlr.press/v202/velingker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vemula2023virtues,
  abstract = {This paper proposes a novel approach to addressing two fundamental challenges in Model-based Reinforcement Learning (MBRL): the computational expense of repeatedly finding a good policy in the learned model, and the objective mismatch between model fitting and policy computation. The unified objective called "Performance Difference via Advantage in Model" demonstrates that optimizing the expected policy advantage in the learned model under an exploration distribution is sufficient for policy computation, resulting in significant computational efficiency improvements.},
  author = {Anirudh Vemula and Yuda Song and Aarti Singh and Drew Bagnell and Sanjiban Choudhury},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vemula2023virtues.pdf:pdf},
  mdate = {2023-08-28},
  pages = {34978--35005},
  pdf = {https://proceedings.mlr.press/v202/vemula23a/vemula23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Virtues of Laziness in Model-based {RL}: A Unified Objective and Algorithms},
  url = {https://proceedings.mlr.press/v202/vemula23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{venturini2023learning,
  abstract = {The paper addresses clustering and community detection on multilayer graphs, which poses additional complications compared to standard graphs as different layers may be characterized by different structures and types of information. One of the major challenges is to establish the extent to which each layer contributes to the cluster assignment in order to effectively take advantage of the multilayer structure and improve upon the classification obtained using the individual layers or their union.},
  author = {Sara Venturini and Andrea Cristofari and Francesco Rinaldi and Francesco Tudisco},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/venturini2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35006--35023},
  pdf = {https://proceedings.mlr.press/v202/venturini23a/venturini23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning the Right Layers: a Data-Driven Layer-Aggregation Strategy for Semi-Supervised Learning on Multilayer Graphs},
  url = {https://proceedings.mlr.press/v202/venturini23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{venuto2023multienvironment,
  abstract = {Using massive datasets to train large-scale models has emerged as a dominant approach for broad generalization in natural language and vision applications. In reinforcement learning, however, a key challenge is that available data of sequential decision making is often not annotated with actions. We propose to circumvent this challenge by combining large but sparsely-annotated datasets from a target environment of interest with fully-annotated datasets from various other source environments. Our method, Action Limited PreTraining (ALPT), leverages the generalization capabilities of inverse dynamics modelling to label missing action data in the target environment.},
  author = {David Venuto and Sherry Yang and Pieter Abbeel and Doina Precup and Igor Mordatch and Ofir Nachum},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/venuto2023multienvironment.pdf:pdf},
  mdate = {2024-09-06},
  pages = {35024--35036},
  pdf = {https://proceedings.mlr.press/v202/venuto23a/venuto23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multi-Environment Pretraining Enables Transfer to Action Limited Datasets},
  url = {https://proceedings.mlr.press/v202/venuto23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{verma2023abode,
  abstract = {Antibodies are Y-shaped proteins that neutralize pathogens and constitute the core of our adaptive immune system. De novo generation of new antibodies that target specific antigens holds the key to accelerating vaccine discovery. However, this co-design of the amino acid sequence and the 3D structure subsumes and accentuates some central challenges from multiple tasks including protein folding, inverse folding, and docking. We strive to surmount these challenges with a new generative model AbODE that extends graph PDEs to accommodate both contextual information and external interactions, using a system of conjoined ODEs tailored for antibody design.},
  author = {Yogesh Verma and Markus Heinonen and Vikas Garg},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/verma2023abode.pdf:pdf},
  mdate = {2024-07-29},
  pages = {35037--35050},
  pdf = {https://proceedings.mlr.press/v202/verma23a/verma23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{AbODE}: Ab initio antibody design using conjoined {ODEs}},
  url = {https://proceedings.mlr.press/v202/verma23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vero2023tableak,
  abstract = {While federated learning promises to preserve privacy, recent works in the image and text domains have shown that training updates leak private client data. However, most high-stakes applications of FL use tabular data, where the risk of data leakage has not yet been explored. We address this gap and propose TabLeak, the first comprehensive reconstruction attack on tabular data. TabLeak leverages a softmax relaxation and pooled ensembling to solve the optimization problem, and an entropy-based uncertainty quantification scheme to enable human assessment. We show that TabLeak successfully breaks several settings previously deemed safe, extracting large subsets of private data at >90% accuracy even at large batch sizes.},
  author = {Mark Vero and Mislav Balunovi{\'c} and Dimitar Iliev Dimitrov and Martin T. Vechev},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vero2023tableak.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35051--35083},
  pdf = {https://proceedings.mlr.press/v202/vero23a/vero23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{TabLeak}: Tabular Data Leakage in Federated Learning},
  url = {https://proceedings.mlr.press/v202/vero23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vicol2023lowvariance,
  abstract = {We propose an evolution strategies-based algorithm for estimating gradients in unrolled computation graphs, called ES-Single. Similarly to the recently-proposed Persistent Evolution Strategies (PES), ES-Single is unbiased and overcomes chaos arising from recursive function applications by smoothing the meta-loss landscape. ES-Single samples a single perturbation per particle that is kept fixed over the course of an inner problem. Compared to PES, ES-Single is simpler to implement and has lower variance: the variance of ES-Single is constant with respect to the number of truncated unrolls, removing a key barrier in applying ES to long inner problems using short truncations.},
  author = {Paul Vicol},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vicol2023lowvariance.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35084--35119},
  pdf = {https://proceedings.mlr.press/v202/vicol23a/vicol23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Low-Variance Gradient Estimation in Unrolled Computation Graphs with {ES-Single}},
  url = {https://proceedings.mlr.press/v202/vicol23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vilnis2023arithmetic,
  abstract = {Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model.},
  author = {Luke Vilnis and Yury Zemlyanskiy and Patrick Murray and Alexandre Tachard Passos and Sumit Sanghai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vilnis2023arithmetic.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35120--35136},
  pdf = {https://proceedings.mlr.press/v202/vilnis23a/vilnis23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models},
  url = {https://proceedings.mlr.press/v202/vilnis23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{voloshin2023eventual,
  abstract = {Linear temporal logic (LTL) offers a simplified way of specifying tasks for policy optimization that may otherwise be difficult to describe with scalar reward functions. However, the standard RL framework can be too myopic to find maximally LTL satisfying policies. First, we develop a new value-function based proxy, using a technique we call eventual discounting, under which one can find policies that satisfy the LTL specification with highest achievable probability. Second, we develop a new experience replay method for generating off-policy data from on-policy rollouts via counterfactual reasoning on different ways of satisfying the LTL specification.},
  author = {Cameron Voloshin and Abhinav Verma and Yisong Yue},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/voloshin2023eventual.pdf:pdf},
  mdate = {2024-05-07},
  pages = {35137--35150},
  pdf = {https://proceedings.mlr.press/v202/voloshin23a/voloshin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Eventual Discounting Temporal Logic Counterfactual Experience Replay},
  url = {https://proceedings.mlr.press/v202/voloshin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{va2023improving,
  abstract = {Randomized smoothing is a popular method to certify robustness of image classifiers to adversarial input perturbations. It is the only certification technique which scales directly to datasets of higher dimension such as ImageNet. However, current techniques are not able to utilize the fact that any adversarial example has to lie in the image space, that is $[0,1]^d$; otherwise, one can trivially detect it. We derive new certification formulae that lead to improvements in certified $\ell_1$-robustness by leveraging box constraints.},
  author = {V{\'a}clav Vor{\'a}{\v{c}}ek and Matthias Hein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/va2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35198--35222},
  pdf = {https://proceedings.mlr.press/v202/voracek23a/voracek23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving l1-Certified Robustness via Randomized Smoothing by Leveraging Box Constraints},
  url = {https://proceedings.mlr.press/v202/voracek23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vuong2023vector,
  abstract = {Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.},
  author = {Long Tung Vuong and Trung Le and He Zhao and Chuanxia Zheng and Mehrtash Harandi and Jianfei Cai and Dinh Q. Phung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vuong2023vector.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35223--35242},
  pdf = {https://proceedings.mlr.press/v202/vuong23a/vuong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Vector Quantized Wasserstein Auto-Encoder},
  url = {https://proceedings.mlr.press/v202/vuong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{vyas2023provable,
  abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of near access-freeness (NAF) and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $k$-NAF if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that did not access $C$ at all. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content.},
  author = {Nikhil Vyas and Sham M. Kakade and Boaz Barak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/vyas2023provable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35277--35299},
  pdf = {https://proceedings.mlr.press/v202/vyas23b/vyas23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Provable Copyright Protection for Generative Models},
  url = {https://proceedings.mlr.press/v202/vyas23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wagenmaker2023leveraging,
  abstract = {Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\varepsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\varepsilon$-optimal policy. In this work, we consider this setting, which we call the FineTuneRL setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and develop an algorithm, FTPedel, which is provably optimal.},
  author = {Andrew Wagenmaker and Aldo Pacchiano},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wagenmaker2023leveraging.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35300--35338},
  pdf = {https://proceedings.mlr.press/v202/wagenmaker23a/wagenmaker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Leveraging Offline Data in Online Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/wagenmaker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wagner2023fast,
  abstract = {We study efficient mechanisms for differentially private kernel density estimation (DP-KDE). Prior work for the Gaussian kernel described algorithms that run in time exponential in the number of dimensions $d$. This paper breaks the exponential barrier, and shows how the KDE can privately be approximated in time linear in $d$, making it feasible for high-dimensional data. Our approach is based on a locality sensitive quantization scheme that reduces the problem to a simpler one of estimating a discrete distribution.},
  author = {Tal Wagner and Yonatan Naamad and Nina Mishra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wagner2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35339--35367},
  pdf = {https://proceedings.mlr.press/v202/wagner23a/wagner23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Private Kernel Density Estimation via Locality Sensitive Quantization},
  url = {https://proceedings.mlr.press/v202/wagner23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{walker2023investigating,
  abstract = {State of the art reinforcement learning has enabled training agents on tasks of ever increasing complexity. However, the current paradigm tends to favor training agents from scratch on every new task or on collections of tasks with a view towards generalizing to novel task configurations. The former suffers from poor data efficiency while the latter is difficult when test tasks are out-of-distribution. Agents that can effectively transfer their knowledge about the world pose a potential solution to these issues. In this paper, we investigate transfer learning in the context of model-based agents. Specifically, we aim to understand when exactly environment models have an advantage and why. We find that a model-based approach outperforms controlled model-free baselines for transfer learning. Through ablations, we show that both the policy and dynamics model learnt through exploration matter for successful transfer. We demonstrate our results across three domains which vary in their requirements for transfer: in-distribution procedural (Crafter), in-distribution identical (RoboDesk), and out-of-distribution (Meta-World).},
  author = {Jacob C. Walker and Eszter V{\'e}rtes and Yazhe Li and Gabriel Dulac-Arnold and Ankesh Anand and Theophane Weber and Jessica B. Hamrick},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/walker2023investigating.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35368--35383},
  pdf = {https://proceedings.mlr.press/v202/walker23a/walker23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Investigating the Role of Model-Based Learning in Exploration and Transfer},
  url = {https://proceedings.mlr.press/v202/walker23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wan2023upscale,
  abstract = {As neural networks grow in size and complexity, inference speeds decline. To combat this, one of the most effective compression techniques -- channel pruning -- removes channels from weights. However, for multi-branch segments of a model, channel removal can introduce inference-time memory copies. In turn, these copies increase inference latency -- so much so that the pruned model can be slower than the unpruned model. As a workaround, pruners conventionally constrain certain channels to be pruned together. This fully eliminates memory copies but, as we show, significantly impairs accuracy. We now have a dilemma: Remove constraints but increase latency, or add constraints and impair accuracy. In response, our insight is to reorder channels at export time, reducing latency by reducing memory copies and improving accuracy by removing constraints. By removing constraints from existing pruners, we improve ImageNet accuracy for post-training pruned models by 2.1 points on average and improve inference speeds by up to 2x over a baseline export.},
  author = {Alvin Wan and Hanxiang Hao and Kaushik Patnaik and Yueyang Xu and Omer Hadad and David G{\"u}era and Zhile Ren and Qi Shan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wan2023upscale.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35384--35412},
  pdf = {https://proceedings.mlr.press/v202/wan23a/wan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{UPSCALE}: Unconstrained Channel Pruning},
  url = {https://proceedings.mlr.press/v202/wan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wan2023multiplier,
  abstract = {Despite the great interest in the bandit problem, designing efficient algorithms for complex models remains challenging, as there is typically no analytical way to quantify uncertainty. In this paper, we propose Multiplier Bootstrap-based Exploration (MBE), a novel exploration strategy that is applicable to any reward model amenable to weighted loss minimization. We prove both instance-dependent and instance-independent rate-optimal regret bounds for MBE in sub-Gaussian multi-armed bandits. With extensive simulation and real-data experiments, we show the generality and adaptivity of MBE.},
  author = {Runzhe Wan and Haoyu Wei and Branislav Kveton and Rui Song},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wan2023multiplier.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35444--35490},
  pdf = {https://proceedings.mlr.press/v202/wan23d/wan23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Multiplier Bootstrap-based Exploration},
  url = {https://proceedings.mlr.press/v202/wan23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wan2023semail,
  abstract = {Model-based imitation learning (MBIL) is a popular reinforcement learning method that improves sample efficiency on high-dimension input sources, such as images and videos. Following the convention of MBIL research, existing algorithms are highly deceptive by task-irrelevant information, especially moving distractors in videos. To tackle this problem, we propose a new algorithm - named Separated Model-based Adversarial Imitation Learning (SeMAIL) - decoupling the environment dynamics into two parts by task-relevant dependency, which is determined by agent actions, and training separately. In this way, the agent can imagine its trajectories and imitate the expert behavior efficiently in task-relevant state space. Our method achieves near-expert performance on various visual control tasks with complex observations and the more challenging tasks with different backgrounds from expert observations.},
  author = {Shenghua Wan and Yucen Wang and Minghao Shao and Ruying Chen and De-Chuan Zhan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wan2023semail.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35426--35443},
  pdf = {https://proceedings.mlr.press/v202/wan23c/wan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SeMAIL}: Eliminating Distractors in Visual Imitation via Separated Models},
  url = {https://proceedings.mlr.press/v202/wan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wan2023poisoning,
  abstract = {Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions "Joe Biden", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks.},
  author = {Alexander Wan and Eric Wallace and Sheng Shen and Dan Klein},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wan2023poisoning.pdf:pdf},
  mdate = {2025-05-19},
  pages = {35413--35425},
  pdf = {https://proceedings.mlr.press/v202/wan23b/wan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Poisoning Language Models During Instruction Tuning},
  url = {https://proceedings.mlr.press/v202/wan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wan2023bandit,
  abstract = {The paper investigates the online bandit learning of monotone multi-linear DR-submodular functions, designing the algorithm BanditMLSM that attains O(T^{2/3}log T) of (1-1/e)-regret. The authors reduce submodular bandit with partition matroid constraint and bandit sequential monotone maximization to the online bandit learning of the monotone multi-linear DR-submodular functions, attaining O(T^{2/3}log T) of (1-1/e)-regret in both problems, which improve the existing results. To the best of their knowledge, they are the first to give a sublinear regret algorithm for the submodular bandit with partition matroid constraint. A special case of this problem was studied by Streeter et al. (2009), who proved an O(T^{4/5}) (1-1/e)-regret upper bound. For bandit sequential submodular maximization, existing work proves an O(T^{2/3}) regret with a suboptimal 1/2 approximation ratio.},
  author = {Zongqi Wan and Jialin Zhang and Wei Chen and Xiaoming Sun and Zhijie Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wan2023bandit.pdf:pdf},
  mdate = {2025-03-18},
  pages = {35491--35524},
  pdf = {https://proceedings.mlr.press/v202/wan23e/wan23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bandit Multi-linear {DR}-Submodular Maximization and Its Applications on Adversarial Submodular Bandits},
  url = {https://proceedings.mlr.press/v202/wan23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023active,
  abstract = {In this paper, we propose a novel framework, Active Learning based Structural Inference (ALaSI), to infer the existence of directed connections from observed agents' states over a time period in a dynamical system. With the help of deep active learning, ALaSI is competent in learning the representation of connections with a relatively small pool of prior knowledge. Moreover, based on information theory, the proposed inter- and out-of-scope message learning pipelines are remarkably beneficial to structural inference for large dynamical systems.},
  author = {Aoran Wang and Jun Pang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023active.pdf:pdf},
  mdate = {2023-09-30},
  pages = {36224--36245},
  pdf = {https://proceedings.mlr.press/v202/wang23ac/wang23ac.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Active Learning based Structural Inference},
  url = {https://proceedings.mlr.press/v202/wang23ac.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023offline,
  abstract = {Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new tasks.},
  author = {Jianhao Wang and Jin Zhang and Haozhe Jiang and Junyu Zhang and Liwei Wang and Chongjie Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023offline.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36626--36669},
  pdf = {https://proceedings.mlr.press/v202/wang23au/wang23au.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Offline Meta Reinforcement Learning with In-Distribution Online Adaptation},
  url = {https://proceedings.mlr.press/v202/wang23au.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023regularization,
  abstract = {Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, regularization with constraints and constrained inference, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. The paper also explores the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.},
  author = {Kaifu Wang and Hangfeng He and Tin D. Nguyen and Piyush Kumar and Dan Roth},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023regularization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35740--35762},
  pdf = {https://proceedings.mlr.press/v202/wang23h/wang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Regularization and Inference with Label Constraints},
  url = {https://proceedings.mlr.press/v202/wang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023generalized,
  abstract = {In machine learning applications, it is well known that carefully designed learning rate (step size) schedules can significantly improve the convergence of commonly used first-order optimization algorithms. Therefore how to set step size adaptively becomes an important research question. A popular and effective method is the Polyak step size, which sets step size adaptively for gradient descent or stochastic gradient descent without the need to estimate the smoothness parameter of the objective function. However, there has not been a principled way to generalize the Polyak step size for algorithms with momentum accelerations. This paper presents a general framework to set the learning rate adaptively for first-order optimization methods with momentum, motivated by the derivation of Polyak step size. It is shown that the resulting techniques are much less sensitive to the choice of momentum parameter and may avoid the oscillation of the heavy-ball method on ill-conditioned problems. These adaptive step sizes are further extended to the stochastic settings, which are attractive choices for stochastic gradient descent with momentum. Our methods are demonstrated to be more effective for stochastic gradient methods than prior adaptive step size algorithms in large-scale machine learning tasks.},
  author = {Xiaoyu Wang and Mikael Johansson and Tong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023generalized.pdf:pdf},
  mdate = {2024-08-05},
  pages = {35836--35863},
  pdf = {https://proceedings.mlr.press/v202/wang23l/wang23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generalized {P}olyak Step Size for First Order Optimization with Momentum},
  url = {https://proceedings.mlr.press/v202/wang23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023tight,
  abstract = {It is known that for streaming multi-armed bandits (MABs), algorithms with o(K) memory have to incur Omega(T^{2/3}) regret, where K and T are the numbers of arms and trials. However, the previous best regret upper bound is still O(K^{1/3} T^{2/3} log^{1/3}(T)), achieved by the simple uniform exploration algorithm. In this paper, we close this gap and complete the picture of regret minimization in single-pass streaming MABs. First, we improve the regret lower bound to Omega(K^{1/3} T^{2/3}) for algorithms with o(K) memory. Then, we show that the log^{1/3}(T) factor is not necessary by designing algorithms with at most O(log*(K))-arm memory and achieve O(K^{1/3} T^{2/3}) expected regret based on streaming epsilon-best arm algorithms. We tested the empirical performances of our algorithms on simulated MABs instances, where the proposed algorithms outperform the benchmark methods.},
  author = {Chen Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023tight.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35525--35547},
  pdf = {https://proceedings.mlr.press/v202/wang23a/wang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tight Regret Bounds for Single-pass Streaming Multi-armed Bandits},
  url = {https://proceedings.mlr.press/v202/wang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023improved,
  abstract = {To leverage the copious amount of data from source tasks and overcome the scarcity of the target task samples, representation learning based on multi-task pretraining has become a standard approach in many applications. However, up until now, most existing works design a source task selection strategy from a purely empirical perspective. Recently, Chen et al. (2022) gave the first active multi-task representation learning (A-MTRL) algorithm which adaptively samples from source tasks and can provably reduce the total sample complexity using the L2-regularized-target-source-relevance parameter nu^2. But their work is theoretically suboptimal in terms of total source sample complexity and is less practical in some real-world scenarios where sparse training source task selection is desired. In this paper, we address both issues. Specifically, we show the strict dominance of the L1-regularized-relevance-based (nu^1-based) strategy by giving a lower bound for the nu^2-based strategy. When nu^1 is unknown, we propose a practical algorithm that uses the LASSO program to estimate nu^1. Our algorithm successfully recovers the optimal result in the known case. In addition to our sample complexity results, we also characterize the potential of our nu^1-based strategy in sample-cost-sensitive settings.},
  author = {Yiping Wang and Yifang Chen 0001 and Kevin Jamieson 0001 and Simon Shaolei Du},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023improved.pdf:pdf},
  mdate = {2025-02-07},
  pages = {35548--35578},
  pdf = {https://proceedings.mlr.press/v202/wang23b/wang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Active Multi-Task Representation Learning via Lasso},
  url = {https://proceedings.mlr.press/v202/wang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023prenas,
  abstract = {The wide application of pre-trained models is driving the trend of once-for-all training in one-shot neural architecture search (NAS). However, training within a huge sample space damages the performance of individual subnets and requires much computation to search for an optimal model. In this paper, we present PreNAS, a search-free NAS approach that accentuates target models in one-shot training. Specifically, the sample space is dramatically reduced in advance by a zero-cost selector, and weight-sharing one-shot training is performed on the preferred architectures to alleviate update conflicts. Extensive experiments have demonstrated that PreNAS consistently outperforms state-of-the-art one-shot NAS competitors for both Vision Transformer and convolutional architectures, and importantly, enables instant specialization with zero search cost.},
  author = {Haibin Wang and Ce Ge and Hesen Chen and Xiuyu Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023prenas.pdf:pdf},
  mdate = {2024-05-07},
  pages = {35642--35654},
  pdf = {https://proceedings.mlr.press/v202/wang23f/wang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PreNAS}: Preferred One-Shot Learning Towards Efficient Neural Architecture Search},
  url = {https://proceedings.mlr.press/v202/wang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023closer,
  abstract = {Self-supervised learning on large-scale Vision Transformers (ViTs) as pre-training methods has achieved promising downstream performance. Yet, how much these pre-training paradigms promote lightweight ViTs' performance is considerably less studied. In this work, we develop and benchmark several self-supervised pre-training methods on image classification tasks and some downstream dense prediction tasks. We surprisingly find that if proper pre-training is adopted, even vanilla lightweight ViTs show comparable performance to previous SOTA networks with delicate architecture design. It breaks the recently popular conception that vanilla ViTs are not suitable for vision tasks in lightweight regimes. We also point out some defects of such pre-training, e.g., failing to benefit from large-scale pre-training data and showing inferior performance on data-insufficient downstream tasks.},
  author = {Shaoru Wang and Jin Gao and Zeming Li and Xiaoqin Zhang and Weiming Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023closer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35624--35641},
  pdf = {https://proceedings.mlr.press/v202/wang23e/wang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Closer Look at Self-Supervised Lightweight Vision Transformers},
  url = {https://proceedings.mlr.press/v202/wang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023hypergraph,
  abstract = {Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification benchmarks.},
  author = {Yuxin Wang and Quan Gan and Xipeng Qiu and Xuanjing Huang and David Wipf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023hypergraph.pdf:pdf},
  mdate = {2024-10-01},
  pages = {35605--35623},
  pdf = {https://proceedings.mlr.press/v202/wang23d/wang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Hypergraph Energy Functions to Hypergraph Neural Networks},
  url = {https://proceedings.mlr.press/v202/wang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023adversarial,
  abstract = {We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a >97\% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack.},
  author = {Tony Tong Wang and Adam Gleave and Tom Tseng and Kellin Pelrine and Nora Belrose and Joseph Miller and Michael D. Dennis and Yawen Duan and Viktor Pogrebniak and Sergey Levine and Stuart Russell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023adversarial.pdf:pdf},
  mdate = {2024-08-04},
  pages = {35655--35739},
  pdf = {https://proceedings.mlr.press/v202/wang23g/wang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Policies Beat Superhuman Go AIs},
  url = {https://proceedings.mlr.press/v202/wang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023policy,
  abstract = {Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method.},
  author = {Qiuhao Wang and Chin Pang Ho and Marek Petrik},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023policy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35763--35797},
  pdf = {https://proceedings.mlr.press/v202/wang23i/wang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Policy Gradient in Robust MDPs with Global Convergence Guarantee},
  url = {https://proceedings.mlr.press/v202/wang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023adaptive,
  abstract = {Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is prohibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise.},
  author = {Ziming Wang and Runhao Jiang and Shuang Lian and Rui Yan 0005 and Huajin Tang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35798--35816},
  pdf = {https://proceedings.mlr.press/v202/wang23j/wang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Smoothing Gradient Learning for Spiking Neural Networks},
  url = {https://proceedings.mlr.press/v202/wang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023circuitnet,
  abstract = {The successes of artificial neural networks (ANNs) are largely attributed to mimicking the human brain structures. Recent advances in neuroscience revealed that neurons interact with each other through various kinds of connectivity patterns to process information, in which the common connectivity patterns are also called circuit motifs. However, many existing ANNs can only model one or two circuit motifs in their architectures, so that their performance may drastically vary among different types of machine learning tasks. In this paper, we propose a new type of neural network inspired by the architectures of neuronal circuits, namely Circuit Neural Network (CircuitNet). In CircuitNet, a group of densely connected neurons, namely circuit motif unit (CMU), form the basic unit of the network, which is capable of modeling universal circuit motifs by adjusting the weights within the CMUs. Compared with traditional feed-forward networks, CircuitNet has the ability to model more types of neuron connections such as feed-back and lateral motifs. Inspired by the locally dense and globally sparse structure of the human brain, several iterations of signal transmission among different CMUs are achieved by sparse connections through the input ports and output ports of different CMUs. Experiments have demonstrated that CircuitNet can outperform popular neural network architectures in function approximation, reinforcement learning, image classification, and time series forecasting tasks.},
  author = {Yansen Wang and Xinyang Jiang and Kan Ren and Caihua Shan and Xufang Luo and Dongqi Han and Kaitao Song and Yifei Shen and Dongsheng Li 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023circuitnet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35817--35835},
  pdf = {https://proceedings.mlr.press/v202/wang23k/wang23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {CircuitNet: A Generic Neural Network to Realize Universal Circuit Motif Modeling},
  url = {https://proceedings.mlr.press/v202/wang23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023nearminimaxoptimal,
  abstract = {In this paper, we study risk-sensitive Reinforcement Learning (RL), focusing on the objective of Conditional Value at Risk (CVaR) with risk tolerance $\tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regret rate is $\Omega(\sqrt{\tau^{-1}AK})$, where $A$ is the number of actions and $K$ is the number of episodes, and that it is achieved by an Upper Confidence Bound algorithm with a novel Bernstein bonus. For online RL in tabular Markov Decision Processes (MDPs), we show a minimax regret lower bound of $\Omega(\sqrt{\tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$ is the number of states, and we propose a novel bonus-driven Value Iteration procedure. We show that our algorithm achieves the optimal regret of $\widetilde O(\sqrt{\tau^{-1}SAK})$ under a continuity assumption and in general attains a near-optimal regret of $\widetilde O(\tau^{-1}\sqrt{SAK})$, which is minimax-optimal for constant $\tau$. By discretizing rewards appropriately, our algorithms are computationally efficient.},
  author = {Kaiwen Wang and Nathan Kallus and Wen Sun 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023nearminimaxoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35864--35907},
  pdf = {https://proceedings.mlr.press/v202/wang23m/wang23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR},
  url = {https://proceedings.mlr.press/v202/wang23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023fedhpobench,
  abstract = {Research in the field of hyperparameter optimization (HPO) has been greatly accelerated by existing HPO benchmarks. Nonetheless, existing efforts in benchmarking all focus on HPO for traditional learning paradigms while ignoring federated learning (FL), a promising paradigm for collaboratively learning models from dispersed data. In this paper, we first identify some uniqueness of federated hyperparameter optimization (FedHPO) from various aspects, showing that existing HPO benchmarks no longer satisfy the need to study FedHPO methods. To facilitate the research of FedHPO, we propose and implement a benchmark suite FedHPO-Bench that incorporates comprehensive FedHPO problems, enables flexible customization of the function evaluations, and eases continuing extensions. We conduct extensive experiments based on FedHPO-Bench to provide the community with more insights into FedHPO. We open-sourced FedHPO-Bench at https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOBench.},
  author = {Zhen Wang 0036 and Weirui Kuang and Ce Zhang 0001 and Bolin Ding and Yaliang Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023fedhpobench.pdf:pdf},
  mdate = {2025-04-10},
  pages = {35908--35948},
  pdf = {https://proceedings.mlr.press/v202/wang23n/wang23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FedHPO-Bench: A Benchmark Suite for Federated Hyperparameter Optimization},
  url = {https://proceedings.mlr.press/v202/wang23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023slotvae,
  abstract = {Slot attention has shown remarkable object-centric representation learning performance in computer vision tasks without requiring any supervision. Despite its object-centric binding ability brought by compositional modelling, as a deterministic module, slot attention lacks the ability to generate novel scenes. In this paper, we propose the Slot-VAE, a generative model that integrates slot attention with the hierarchical VAE framework for object-centric structured scene generation. For each image, the model simultaneously infers a global scene representation to capture high-level scene structure and object-centric slot representations to embed individual object components. During generation, slot representations are generated from the global scene representation to ensure coherent scene structures.},
  author = {Yanbo Wang and Letao Liu and Justin Dauwels},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023slotvae.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36020--36035},
  pdf = {https://proceedings.mlr.press/v202/wang23r/wang23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Slot-VAE: Object-Centric Scene Generation with Slot Attention},
  url = {https://proceedings.mlr.press/v202/wang23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023testing,
  abstract = {Users linked together through a network often tend to have similar behaviors. This phenomenon is usually known as network interaction. Users' characteristics, the covariates, are often correlated with their outcomes. Therefore, one should incorporate both the covariates and the network information in a carefully designed randomization to improve the estimation of the average treatment effect (ATE) in network A/B testing. In this paper, we propose a new adaptive procedure to balance both the network and the covariates. We show that the imbalance measures with respect to the covariates and the network are $O_p(1)$. We also demonstrate the relationships between the improved balances and the increased efficiency in terms of the mean square error (MSE). Numerical studies demonstrate the advanced performance of the proposed procedure regarding the greater comparability of the treatment groups.},
  author = {Jialu Wang and Ping Li 0001 and Feifang Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023testing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35949--35969},
  pdf = {https://proceedings.mlr.press/v202/wang23o/wang23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A/B Testing in Network Data with Covariate-Adaptive Randomization},
  url = {https://proceedings.mlr.press/v202/wang23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023division,
  abstract = {Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks (DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. The method, called DIVISION, is motivated by observing that: (1) DNN backward propagation mainly uses the low-frequency component (LFC) of activation maps, and (2) most memory is used caching the high-frequency component (HFC). DIVISION preserves a high-precision LFC while compressing HFC into a low-precision copy, significantly reducing memory cost while maintaining model accuracy. Over 10x compression of activation maps with competitive training throughput and no loss of model accuracy.},
  author = {Guanchu Wang and Zirui Liu 0001 and Zhimeng Jiang and Ninghao Liu and Na Zou 0001 and Xia Ben Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023division.pdf:pdf},
  mdate = {2025-02-06},
  pages = {36036--36057},
  pdf = {https://proceedings.mlr.press/v202/wang23s/wang23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{DIVISION}: Memory Efficient Training via Dual Activation Precision},
  url = {https://proceedings.mlr.press/v202/wang23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023learning,
  abstract = {Many important real-world Reinforcement Learning (RL) problems involve partial observability and require policies with memory. Unfortunately, standard deep RL algorithms for partially observable settings typically condition on the full history of interactions and are notoriously difficult to train. The authors propose a novel deep, partially observable RL algorithm that models belief states, decouples belief state modeling from policy optimization, and uses representation learning to capture reward-relevant state features.},
  author = {Andrew Wang and Andrew C. Li and Toryn Q. Klassen and Rodrigo Toro Icarte and Sheila A. McIlraith},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {35970--35988},
  pdf = {https://proceedings.mlr.press/v202/wang23p/wang23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Belief Representations for Partially Observable Deep {RL}},
  url = {https://proceedings.mlr.press/v202/wang23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023cocktailsgd,
  abstract = {Distributed training of foundation models, especially large language models (LLMs), is communication-intensive and so has heavily relied on centralized data centers with fast interconnects. Can we train on slow networks and unlock the potential of decentralized infrastructure for foundation models? The paper proposes CocktailSGD, a communication-efficient training framework that combines random sparsification, top-K sparsification, and quantization. It achieves up to 117× compression in fine-tuning LLMs up to 20 billion parameters and incurs only ~1.2× slowdown on a 500Mbps network compared to data center networks.},
  author = {Jue Wang and Yucheng Lu and Binhang Yuan and Beidi Chen and Percy Liang and Christopher De Sa and Christopher R{\'e} and Ce Zhang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023cocktailsgd.pdf:pdf},
  mdate = {2023-12-20},
  pages = {36058--36076},
  pdf = {https://proceedings.mlr.press/v202/wang23t/wang23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CocktailSGD}: Fine-tuning Foundation Models over 500Mbps Networks},
  url = {https://proceedings.mlr.press/v202/wang23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023warmstart,
  abstract = {Warm-Start reinforcement learning (RL), aided by a prior policy obtained from offline training, is emerging as a promising RL approach for practical applications. Recent empirical studies have demonstrated that the performance of Warm-Start RL can be improved quickly in some cases but become stagnant in other cases, especially when the function approximation is used. To this end, the primary objective of this work is to build a fundamental understanding on "whether and when online learning can be significantly accelerated by a warm-start policy from offline RL?". Specifically, we consider the widely used Actor-Critic (A-C) method with a prior policy. We first quantify the approximation errors in the Actor update and the Critic update, respectively. Next, we cast the Warm-Start A-C algorithm as Newton's method with perturbation, and study the impact of the approximation errors on the finite-time learning performance with inaccurate Actor/Critic updates.},
  author = {Hang Wang and Sen Lin 0001 and Junshan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023warmstart.pdf:pdf},
  mdate = {2024-05-22},
  pages = {35989--36019},
  pdf = {https://proceedings.mlr.press/v202/wang23q/wang23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap},
  url = {https://proceedings.mlr.press/v202/wang23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023npsemiseg,
  abstract = {Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If this predicted probability distribution is incorrect, however, it leads to poor segmentation results which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg.},
  author = {Jianfeng Wang and Daniela Massiceti and Xiaolin Hu 0001 and Vladimir Pavlovic 0001 and Thomas Lukasiewicz},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023npsemiseg.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36138--36156},
  pdf = {https://proceedings.mlr.press/v202/wang23x/wang23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{NP-SemiSeg}: When Neural Processes meet Semi-Supervised Semantic Segmentation},
  url = {https://proceedings.mlr.press/v202/wang23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023gcflow,
  abstract = {Graph convolutional networks (GCNs) are discriminative models that directly model the class posterior p(y|x) for semi-supervised classification of graph data. While being effective, as a representation learning approach, the node representations extracted from a GCN often miss useful information for effective clustering, because the objectives are different. In this work, we design normalizing flows that replace GCN layers, leading to a generative model that models both the class conditional likelihood p(x|y) and the class prior p(y). The resulting neural network, GC-Flow, retains the graph convolution operations while being equipped with a Gaussian mixture representation space. It enjoys two benefits: it not only maintains the predictive power of GCN, but also produces well-separated clusters, due to the structuring of the representation space. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional parameterization, such as that on the adjacency matrix used for graph convolutions, yields additional improvement in clustering.},
  author = {Tianchun Wang and Farzaneh Mirzazadeh and Xiang Zhang 0001 and Jie Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023gcflow.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36157--36173},
  pdf = {https://proceedings.mlr.press/v202/wang23y/wang23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GC-Flow}: A Graph-Based Flow Network for Effective Clustering},
  url = {https://proceedings.mlr.press/v202/wang23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023direct,
  abstract = {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed ℓ² Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP). We provide a "direct" parameterization, i.e., a smooth mapping from ℝᴺ onto the set of weights satisfying the SDP-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the SDP bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the SDP constraint. The new parameterization can equivalently be thought of as either a new layer type (the sandwich layer), or a novel parameterization of standard feedforward networks with parameter sharing.},
  author = {Ruigang Wang and Ian R. Manchester},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023direct.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36093--36110},
  pdf = {https://proceedings.mlr.press/v202/wang23v/wang23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Direct Parameterization of {Lipschitz}-Bounded Deep Networks},
  url = {https://proceedings.mlr.press/v202/wang23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023tighter,
  abstract = {In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke & Zakynthinou (2020)—the setting of the "conditional mutual information" framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting.},
  author = {Ziqiao Wang and Yongyi Mao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023tighter.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36111--36137},
  pdf = {https://proceedings.mlr.press/v202/wang23w/wang23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tighter Information-Theoretic Generalization Bounds from Supersamples},
  url = {https://proceedings.mlr.press/v202/wang23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023magneto,
  abstract = {A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name "Transformers", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).},
  author = {Hongyu Wang 0009 and Shuming Ma and Shaohan Huang and Li Dong 0004 and Wenhui Wang 0003 and Zhiliang Peng and Yu Wu and Payal Bajaj and Saksham Singhal and Alon Benhaim and Barun Patra and Zhun Liu and Vishrav Chaudhary and Xia Song and Furu Wei},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023magneto.pdf:pdf},
  mdate = {2024-04-19},
  pages = {36077--36092},
  pdf = {https://proceedings.mlr.press/v202/wang23u/wang23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Magneto}: A Foundation Transformer},
  url = {https://proceedings.mlr.press/v202/wang23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023better,
  abstract = {It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency (~20 sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the ℓ∞-norm threat model with ε=8/255, our models achieve 70.69% and 42.67% robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by +4.58% and +8.03%. Under the ℓ2-norm threat model with ε=128/255, our models achieve 84.86% on CIFAR-10 (+4.44%). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets.},
  author = {Zekai Wang and Tianyu Pang and Chao Du and Min Lin and Weiwei Liu 0003 and Shuicheng Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023better.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36246--36263},
  pdf = {https://proceedings.mlr.press/v202/wang23ad/wang23ad.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Better Diffusion Models Further Improve Adversarial Training},
  url = {https://proceedings.mlr.press/v202/wang23ad.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023deep,
  abstract = {Machine learning-driven human behavior analysis is gaining attention in behavioral/mental healthcare, due to its potential to identify behavioral patterns that cannot be recognized by traditional assessments. Real-life applications, such as digital behavioral biomarker identification, often require the discovery of complex spatiotemporal patterns in multimodal data, which is largely under-explored. To fill this gap, we propose a novel model that integrates uniquely designed Deep Temporal Sets (DTS) with Evidential Reinforced Attentions (ERA). DTS captures complex temporal relationships in the input and generates a set-based representation, while ERA captures the policy network's uncertainty and conducts evidence-aware exploration to locate attentive regions in behavioral data. Using child-computer interaction data as a testing platform, we demonstrate the effectiveness of DTS-ERA in differentiating children with Autism Spectrum Disorder and typically developing children. Comparisons with baseline methods show that our model achieves superior performance and has the potential to provide objective, quantitative, and precise analysis of complex human behaviors.},
  author = {Dingrong Wang and Deep Shankar Pandey and Krishna Prasad Neupane and Zhiwei Yu and Ervine Zheng and Zhi Zheng and Qi Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023deep.pdf:pdf},
  pages = {36205--36223},
  pdf = {https://proceedings.mlr.press/v202/wang23ab/wang23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Temporal Sets with Evidential Reinforced Attentions for Unique Behavioral Pattern Discovery},
  url = {https://proceedings.mlr.press/v202/wang23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023projected,
  abstract = {This paper investigates the problem of exact community recovery in the symmetric d-uniform (d ≥ 2) hypergraph stochastic block model (d-HSBM). In this model, a d-uniform hypergraph with n nodes is generated by first partitioning the n nodes into K≥2 equal-sized disjoint communities and then generating hyperedges with a probability that depends on the community memberships of d nodes. Despite the non-convex and discrete nature of the maximum likelihood estimation problem, we develop a simple yet efficient iterative method, called the projected tensor power method, to tackle it. As long as the initialization satisfies a partial recovery condition in the logarithmic degree regime of the problem, we show that our proposed method can exactly recover the hidden community structure down to the information-theoretic limit with high probability.},
  author = {Jinxin Wang and Yuen-Man Pun and Xiaolu Wang and Peng Wang and Anthony Man-Cho So},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023projected.pdf:pdf},
  pages = {36285--36307},
  pdf = {https://proceedings.mlr.press/v202/wang23af/wang23af.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Projected Tensor Power Method for Hypergraph Community Recovery},
  url = {https://proceedings.mlr.press/v202/wang23af.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023data,
  abstract = {The number of parameters in large transformers has been observed to grow exponentially. Despite notable performance improvements, concerns have been raised that such a growing model size will run out of data in the near future. As manifested in the neural scaling law, modern learning backbones are not data-efficient. To maintain the utility of the model capacity, training data should be increased proportionally. In this paper, we study the neural scaling law under the previously overlooked data scarcity regime, focusing on the more challenging situation where we need to train a gigantic model with a disproportionately limited supply of available training data. We find that the existing power laws underestimate the data inefficiency of large transformers. Their performance will drop significantly if the training set is insufficient. Fortunately, we discover another blessing - such a data-inefficient scaling law can be restored through a model reusing approach that warm-starts the training of a large model by initializing it using smaller models. Our empirical study shows that model reusing can effectively reproduce the power law under the data scarcity regime. When progressively applying model reusing to expand the model size, we also observe consistent performance improvement in large transformers.},
  author = {Peihao Wang and Rameswar Panda and Zhangyang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023data.pdf:pdf},
  pages = {36193--36204},
  pdf = {https://proceedings.mlr.press/v202/wang23aa/wang23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Data Efficient Neural Scaling Law via Model Reusing},
  url = {https://proceedings.mlr.press/v202/wang23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023curriculum,
  abstract = {Social recommendation aims at predicting users' preferences by leveraging social relations among users. However, in practical scenarios, user preferences are often simultaneously influenced by multiple environments, such as different seasons or different social groups, which leads to challenges for recommender systems. We consider a scenario where multiple environments affect users simultaneously, making the disentanglement of environment-specific representations essential. In this paper, we propose a Curriculum Co-disentangled Representation Learning (CCRL) approach that can disentangle environment-invariant and environment-specific representations for social recommendation. The approach introduces curriculum learning by progressively reducing the sparsity of training data, which helps the model learn better disentangled representations. Extensive experiments on real-world datasets demonstrate that our proposed method outperforms state-of-the-art baselines in terms of recommendation accuracy and representation quality.},
  author = {Xin Wang and Zirui Pan and Yuwei Zhou and Hong Chen and Chendi Ge and Wenwu Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023curriculum.pdf:pdf},
  pages = {36174--36192},
  pdf = {https://proceedings.mlr.press/v202/wang23z/wang23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Curriculum Co-disentangled Representation Learning across Multiple Environments for Social Recommendation},
  url = {https://proceedings.mlr.press/v202/wang23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023estimating,
  abstract = {Causal effect estimation from observational data is a fundamental task in artificial intelligence and has been widely studied given known causal relations. However, in the presence of latent confounders, only a part of causal relations can be identified from observational data, characterized by a partial ancestral graph (PAG), where some causal relations are indeterminate. In such cases, the causal effect is often unidentifiable, as there could be super-exponential number of potential causal graphs consistent with the identified PAG but associated with different causal effects. In this paper, we target set determination within a PAG, i.e., determining the set of possible causal effects of a specified variable X on another variable Y via covariate adjustment. We develop the first set determination method that does not require enumerating any causal graphs. The research tackles the challenge of determining the set of all possible causal effects, where a causal effect is considered possible if there exists a DAG, consistent with observational data, that yields this effect. Although such a set is less informative than identifying a unique causal effect, it still provides valuable information and, more importantly, does not require prior structural knowledge or experimental data, making it broadly applicable in practice.},
  author = {Tian-Zuo Wang and Tian Qin and Zhi-Hua Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023estimating.pdf:pdf},
  pages = {36308--36335},
  pdf = {https://proceedings.mlr.press/v202/wang23ag/wang23ag.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Estimating Possible Causal Effects with Latent Variables via Adjustment},
  url = {https://proceedings.mlr.press/v202/wang23ag.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023infodiffusion,
  abstract = {While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning. Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data. InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders. Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples, e.g., generative design.},
  author = {Yingheng Wang and Yair Schiff and Aaron Gokaslan and Weishen Pan and Fei Wang 0001 and Christopher De Sa and Volodymyr Kuleshov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023infodiffusion.pdf:pdf},
  pages = {36336--36354},
  pdf = {https://proceedings.mlr.press/v202/wang23ah/wang23ah.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models},
  url = {https://proceedings.mlr.press/v202/wang23ah.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023gear,
  abstract = {This paper introduces a distributed, GPU-centric experience replay system, GEAR, designed to perform scalable reinforcement learning (RL) with large sequence models (such as transformers). With such models, existing systems such as Reverb face considerable bottlenecks in memory, computation, and communication. GEAR, however, optimizes memory efficiency by enabling the memory resources on GPU servers (including host memory and device memory) to manage trajectory data. Furthermore, it facilitates decentralized GPU devices to expedite various trajectory selection strategies, circumventing computational bottlenecks. GEAR is equipped with GPU kernels capable of collecting trajectories using zero-copy access to host memory, along with remote-directed-memory access over InfiniBand, improving communication efficiency. Cluster experiments have shown that GEAR can achieve performance levels up to 6x greater than Reverb when training state-of-the-art large RL models.},
  author = {Hanjing Wang and Man-Kit Sit and Congjie He and Ying Wen 0001 and Weinan Zhang 0001 and Jun Wang 0012 and Yaodong Yang 0001 and Luo Mai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023gear.pdf:pdf},
  pages = {36380--36390},
  pdf = {https://proceedings.mlr.press/v202/wang23aj/wang23aj.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models},
  url = {https://proceedings.mlr.press/v202/wang23aj.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023robust,
  abstract = {Reinforcement learning (RL) is a powerful technique that allows an autonomous agent to learn an optimal policy to maximize the expected return. The optimality of various RL algorithms relies on the stationarity assumption, which requires time-invariant state transition and reward functions. However, deviations from stationarity over extended periods often occur in real-world applications like robotics control, health care and digital marketing, resulting in suboptimal policies learned under stationary assumptions. In this paper, we propose a model-based doubly robust procedure for testing the stationarity assumption and detecting change points in offline RL settings with certain degree of homogeneity. Our proposed testing procedure is robust to model misspecifications and can effectively control type-I error while achieving high statistical power, especially in high-dimensional settings.},
  author = {Jitao Wang and Chengchun Shi and Zhenke Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023robust.pdf:pdf},
  pages = {36355--36379},
  pdf = {https://proceedings.mlr.press/v202/wang23ai/wang23ai.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Robust Test for the Stationarity Assumption in Sequential Decision Making},
  url = {https://proceedings.mlr.press/v202/wang23ai.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023effective,
  abstract = {In this paper, we present an effective and efficient structural inference approach by integrating a Reservoir Computing (RC) network into a Variational Auto-encoder-based (VAE-based) structural inference framework. With the help of Bi-level Optimization, the backbone VAE-based method follows the Information Bottleneck principle and infers a general adjacency matrix in its latent space; the RC net substitutes the partial role of the decoder and encourages the whole approach to perform further steps of gradient descent based on limited available data. The experimental results on various datasets including biological networks, simulated fMRI data, and physical simulations show the effectiveness and efficiency of our proposed method for structural inference, either with much fewer trajectories or with much shorter trajectories compared with previous works.},
  author = {Aoran Wang and Tsz Pan Tong and Jun Pang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023effective.pdf:pdf},
  pages = {36391--36410},
  pdf = {https://proceedings.mlr.press/v202/wang23ak/wang23ak.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Effective and Efficient Structural Inference with Reservoir Computing},
  url = {https://proceedings.mlr.press/v202/wang23ak.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023modelfree,
  abstract = {Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence, and Wasserstein distance.},
  author = {Yue Wang 0068 and Alvaro Velasquez and George K. Atia and Ashley Prater-Bennette and Shaofeng Zou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023modelfree.pdf:pdf},
  pages = {36431--36469},
  pdf = {https://proceedings.mlr.press/v202/wang23am/wang23am.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Model-Free Robust Average-Reward Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/wang23am.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023live,
  abstract = {Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for all historical policies does not necessarily benefit model prediction for the current policy since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named Policy-adapted Dynamics Model Learning (PDML). PDML dynamically adjusts the historical policy mixture distribution to ensure the learned model can continually adapt to the state-action visitation distribution of the evolving policy. Experiments on a range of continuous control environments in MuJoCo show that PDML achieves significant improvement in sample efficiency and higher asymptotic performance combined with the state-of-the-art model-based RL methods.},
  author = {Xiyao Wang and Wichayaporn Wongkamjan and Ruonan Jia and Furong Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023live.pdf:pdf},
  mdate = {2024-10-06},
  pages = {36470-36493},
  pdf = {https://proceedings.mlr.press/v202/wang23an/wang23an.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy},
  url = {https://proceedings.mlr.press/v202/wang23an.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023learning,
  abstract = {Budget management strategies in repeated auctions have received growing attention in online advertising markets. However, previous work on budget management in online bidding mainly focused on second-price auctions. The rapid shift from second-price auctions to first-price auctions for online ads in recent years has motivated the challenging question of how to bid in repeated first-price auctions while controlling budgets. In this work, we study the problem of learning in repeated first-price auctions with budgets. We design a dual-based algorithm that can achieve a near-optimal $\widetilde{O}(\sqrt{T})$ regret with full information feedback where the maximum competing bid is always revealed after each auction. We further consider the setting with one-sided information feedback where only the winning bid is revealed after each auction. We show that our modified algorithm can still achieve an $\widetilde{O}(\sqrt{T})$ regret with mild assumptions on the bidder's value distribution. Finally, we complement the theoretical results with numerical experiments to confirm the effectiveness of our budget management policy.},
  author = {Qian Wang 0025 and Zongjun Yang and Xiaotie Deng and Yuqing Kong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023learning.pdf:pdf},
  mdate = {2024-08-16},
  pages = {36494-36513},
  pdf = {https://proceedings.mlr.press/v202/wang23ao/wang23ao.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Bid in Repeated First-Price Auctions with Budgets},
  url = {https://proceedings.mlr.press/v202/wang23ao.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023network,
  abstract = {This paper studies the multi-agent performative prediction (Multi-PP) games over multiplex networks. We consider a distributed learning setting where agents partially cooperate on an agent network, while during learning, the data samples drawn depend on the prediction models of the agent itself and neighboring agents on a population network. The dynamics of Multi-PP games is hence affected by the interplay between both networks. This paper concentrates on this Multi-PP game with the following contributions. Firstly, we analyze sufficient conditions for the existence of the performative stable equilibrium (PSE) and Nash equilibrium (NE) of the Multi-PP games. Secondly, we analyze the changes to the equilibrium induced by perturbed data distributions, and derive the closed-form solutions where the network topologies are explicit. Our results connect the existence of PSE/NE with strengths of agents' cooperation, and the changes of equilibrium solutions across agents with their node centrality, etc. Lastly, we show that a stochastic gradient descent (SGD) based distributed learning procedure finds the PSE under the said sufficient condition. Numerical illustrations on the network effects in Multi-PP games corroborate our findings.},
  author = {Xiaolu Wang and Chung-Yiu Yau and Hoi-To Wai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023network.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36514-36540},
  pdf = {https://proceedings.mlr.press/v202/wang23ap/wang23ap.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Network Effects in Performative Prediction Games},
  url = {https://proceedings.mlr.press/v202/wang23ap.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023robustly,
  abstract = {We study the problem of learning a single neuron with respect to the $L_2^2$-loss in the presence of adversarial label noise. We give an efficient algorithm that, for a broad family of activations including ReLUs, approximates the optimal $L_2^2$-error within a constant factor. Our algorithm applies under much milder distributional assumptions compared to prior work. The key ingredient enabling our results is a novel connection to local error bounds from optimization theory.},
  author = {Puqian Wang and Nikos Zarifis and Ilias Diakonikolas and Jelena Diakonikolas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023robustly.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36541-36577},
  pdf = {https://proceedings.mlr.press/v202/wang23aq/wang23aq.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robustly Learning a Single Neuron via Sharpness},
  url = {https://proceedings.mlr.press/v202/wang23aq.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023reachabilityaware,
  abstract = {In Reinforcement Learning (RL), Laplacian Representation (LapRep) is a task-agnostic state representation that encodes the geometry of the environment. A desirable property of LapRep stated in prior works is that the Euclidean distance in the LapRep space roughly reflects the reachability between states, which motivates the usage of this distance for reward shaping. However, we find that LapRep does not necessarily have this property in general: two states having small distance under LapRep can actually be far away in the environment. Such mismatch would impede the learning process in reward shaping. To fix this issue, we introduce a Reachability-Aware Laplacian Representation (RA-LapRep), by properly scaling each dimension of LapRep. Despite the simplicity, we demonstrate that RA-LapRep can better capture the inter-state reachability as compared to LapRep, through both theoretical explanations and experimental results. Additionally, we show that this improvement yields a significant boost in reward shaping performance and also benefits bottleneck state discovery.},
  author = {Kaixin Wang and Kuangqi Zhou and Jiashi Feng and Bryan Hooi and Xinchao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023reachabilityaware.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36670-36693},
  pdf = {https://proceedings.mlr.press/v202/wang23av/wang23av.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Reachability-Aware Laplacian Representation in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/wang23av.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023ppg,
  abstract = {Model-free reinforcement learning methods based on a phasic policy gradient (PPG) framework have shown impressive improvements in sample efficiency and zero-shot generalization on the challenging Procgen benchmark. In PPG, two design choices were believed to be the key contributing factors to its superior performance over PPO: the high level of value sample reuse and the low frequency of feature distillation. However, through extensive empirical study, we unveil that policy regularization and data diversity are what actually matters. In particular, we find we can achieve the same level of performance with low value sample reuse and frequent feature distillation, as long as the policy regularization strength and data diversity are preserved. Additionally, we can maintain the high performance of PPG while reducing the computational cost to a similar level as PPO. Our comprehensive study covers all 16 Procgen games in both sample efficiency and generalization setups.},
  author = {Kaixin Wang and Daquan Zhou and Jiashi Feng and Shie Mannor},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023ppg.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36694-36713},
  pdf = {https://proceedings.mlr.press/v202/wang23aw/wang23aw.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {PPG Reloaded: An Empirical Study on What Matters in Phasic Policy Gradient},
  url = {https://proceedings.mlr.press/v202/wang23aw.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wang2023linsatnet,
  abstract = {Encoding constraints into neural networks is attractive. This paper studies how to introduce the popular positive linear satisfiability to neural networks. We propose the first differentiable satisfiability layer based on an extension of the classic Sinkhorn algorithm for jointly encoding multiple sets of marginal distributions. We further theoretically characterize the convergence property of the Sinkhorn algorithm for multiple marginals. In contrast to the sequential decision e.g. reinforcement learning-based solvers, we showcase our technique in solving constrained (specifically satisfiability) problems by one-shot neural networks, including i) a neural routing solver learned without supervision of optimal solutions; ii) a partial graph matching network handling graphs with unmatchable outliers on both sides; iii) a predictive network for financial portfolios with continuous constraints. To our knowledge, there exists no one-shot neural solver for these scenarios when they are formulated as satisfiability problems.},
  author = {Runzhong Wang and Yunhao Zhang and Ziao Guo and Tianyi Chen and Xiaokang Yang 0001 and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wang2023linsatnet.pdf:pdf},
  mdate = {2025-03-06},
  pages = {36605-36625},
  pdf = {https://proceedings.mlr.press/v202/wang23at/wang23at.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LinSATNet: The Positive Linear Satisfiability Neural Networks},
  url = {https://proceedings.mlr.press/v202/wang23at.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{watson2023heterogeneous,
  abstract = {Heterogeneity and comorbidity are two interwoven challenges associated with various healthcare problems that greatly hampered research on developing effective treatment and understanding of the underlying neurobiological mechanism. Very few studies have been conducted to investigate heterogeneous causal effects (HCEs) in graphical contexts due to the lack of statistical methods. To characterize this heterogeneity, we first conceptualize heterogeneous causal graphs (HCGs) by generalizing the causal graphical model with confounder-based interactions and multiple mediators. Such confounders with an interaction with the treatment are known as moderators. This allows us to flexibly produce HCGs given different moderators and explicitly characterize HCEs from the treatment or potential mediators on the outcome. An interactive structural learning is developed to estimate the complex HCGs and HCEs with confidence intervals provided. Our method is empirically justified by extensive simulations and its practical usefulness is illustrated by exploring causality among psychiatric disorders for trauma survivors.},
  author = {Richard A. Watson and Hengrui Cai and Xinming An and Samuel A. McLean and Rui Song 0006},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/watson2023heterogeneous.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36714-36747},
  pdf = {https://proceedings.mlr.press/v202/watson23a/watson23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Heterogeneous Treatment Effects in Heterogeneous Causal Graphs},
  url = {https://proceedings.mlr.press/v202/watson23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{waudbysmith2023nonparametric,
  abstract = {This work derives methods for performing nonparametric, nonasymptotic statistical inference for population means under the constraint of local differential privacy (LDP). Given bounded observations $(X_1, \ldots, X_n)$ with mean $\mu^\star$ that are privatized into $(Z_1, \ldots, Z_n)$, we present confidence intervals (CI) and time-uniform confidence sequences (CS) for $\mu^\star$ when only given access to the privatized data. To achieve this, we introduce a nonparametric and sequentially interactive generalization of Warner's famous ``randomized response'' mechanism, satisfying LDP for arbitrary bounded random variables, and then provide CIs and CSs for their means given access to the resulting privatized observations. For example, our results yield private analogues of Hoeffding's inequality in both fixed-time and time-uniform regimes. We extend these methods to capture time-varying (non-stationary) means and demonstrate applications to private online A/B tests.},
  author = {Ian Waudby-Smith and Zhiwei Steven Wu and Aaditya Ramdas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/waudbysmith2023nonparametric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36748--36789},
  pdf = {https://proceedings.mlr.press/v202/waudby-smith23a/waudby-smith23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonparametric Extensions of Randomized Response for Private Confidence Sets},
  url = {https://proceedings.mlr.press/v202/waudby-smith23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{weber2023regularizationfree,
  abstract = {Time-series analysis is confounded by nonlinear time warping of the data. Traditional methods for joint alignment do not generalize: after aligning a given signal ensemble, they lack a mechanism to align previously-unseen signals without solving a new optimization problem. We propose a regularization-free extension of the Diffeomorphic Temporal Alignment Net (DTAN), which learns and applies input-dependent nonlinear time warping to time-series data. This regularization-free approach eliminates the need for hyperparameter tuning required in the original DTAN framework. The method facilitates joint alignment and averaging of time-series ensembles in an unsupervised or weakly-supervised manner, providing a learning-based solution for time-series joint alignment that generalizes well to previously-unseen signals.},
  author = {Ron Shapira Weber and Oren Freifeld},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/weber2023regularizationfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {30794--30826},
  pdf = {https://proceedings.mlr.press/v202/shapira-weber23a/shapira-weber23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regularization-free Diffeomorphic Temporal Alignment Nets},
  url = {https://proceedings.mlr.press/v202/shapira-weber23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wei2023universal,
  abstract = {In conventional supervised classification, true labels are required for individual instances. However, it could be prohibitive to collect the true labels for individual instances, due to privacy concerns or unaffordable annotation costs. This motivates the study on classification from aggregate observations (CFAO), where the supervision is provided to groups of instances, instead of individual instances. CFAO is a generalized learning framework that contains various learning problems, such as multiple-instance learning and learning from label proportions. The goal of this paper is to present a novel universal method of CFAO, which holds an unbiased estimator of the classification risk for arbitrary losses -- previous research failed to achieve this goal. Practically, our method works by weighing the importance of each instance and each label in the group, which provides purified supervision for the classifier to learn. The method is theoretically grounded with guaranteed risk consistency and compatibility with arbitrary loss functions.},
  author = {Zixi Wei and Lei Feng and Bo Han and Tongliang Liu and Gang Niu and Xiaofeng Zhu and Heng Tao Shen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wei2023universal.pdf:pdf},
  mdate = {2023-09-01},
  pages = {36804--36820},
  pdf = {https://proceedings.mlr.press/v202/wei23a/wei23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Universal Unbiased Method for Classification from Aggregate Observations},
  url = {https://proceedings.mlr.press/v202/wei23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wei2023ntkapproximating,
  abstract = {Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion.},
  author = {Tianxin Wei and Zeming Guo and Yifan Chen and Jingrui He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wei2023ntkapproximating.pdf:pdf},
  mdate = {2023-09-14},
  pages = {36821--36838},
  pdf = {https://proceedings.mlr.press/v202/wei23b/wei23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning},
  url = {https://proceedings.mlr.press/v202/wei23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wei2023boosting,
  abstract = {Graph augmentation plays a crucial role in achieving good generalization for contrastive graph self-supervised learning. However, mainstream Graph Contrastive Learning (GCL) often favors random graph augmentations, by relying on random node dropout or edge perturbation on graphs. Random augmentations may inevitably lead to semantic information corruption during the training, and force the network to mistakenly focus on semantically irrelevant environmental background structures. To address these limitations and to improve generalization, we propose a novel self-supervised learning framework for GCL, which can adaptively screen the semantic-related substructure in graphs by capitalizing on the proposed gradient-based Graph Contrastive Saliency (GCS). The goal is to identify the most semantically discriminative structures of a graph via contrastive learning, such that we can generate semantically meaningful augmentations by leveraging on saliency. Empirical evidence on 16 benchmark datasets demonstrates the exclusive merits of the GCS-based framework. We also provide rigorous theoretical justification for GCS's robustness properties.},
  author = {Chunyu Wei and Yu Wang and Bing Bai and Kai Ni and David Brady and Lu Fang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wei2023boosting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36839--36855},
  pdf = {https://proceedings.mlr.press/v202/wei23c/wei23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Boosting Graph Contrastive Learning via Graph Contrastive Saliency},
  url = {https://proceedings.mlr.press/v202/wei23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wei2023setmembership,
  abstract = {Reinforcement learning (RL) has made significant progress in areas such as Atari games and robotic control, where the agents have perfect sensing capabilities. However, in many real-world sequential decision-making tasks, the observation data could be noisy or incomplete due to the intrinsic low quality of the sensors or unexpected malfunctions; that is, the agent's perceptions are rarely perfect. The current POMDP RL methods, such as particle-based and Gaussian-based, can only provide a probability estimate of hidden states rather than certain belief regions, which may lead to inefficient and even wrong decision-making. This paper proposes a novel algorithm called Set-membership Belief state-based Reinforcement Learning (SBRL), which consists of two parts: a Set-membership Belief state learning Model (SBM) for learning bounded belief state sets and an RL controller for making decisions based on SBM. We prove that our belief estimation method can provide a series of belief state sets that always contain the true states under the unknown-but-bounded (UBB) noise. The effectiveness of the proposed method is verified on a collection of benchmark tasks, and the results show that our method outperforms the state-of-the-art methods.},
  author = {Wei Wei and Lijun Zhang and Lin Li and Huizhong Song and Jiye Liang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wei2023setmembership.pdf:pdf},
  mdate = {2024-07-22},
  pages = {36856--36867},
  pdf = {https://proceedings.mlr.press/v202/wei23d/wei23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Set-membership Belief State-based Reinforcement Learning for POMDPs},
  url = {https://proceedings.mlr.press/v202/wei23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wei2023mitigating,
  abstract = {In the presence of noisy labels, designing robust loss functions is critical for securing the generalization performance of deep neural networks. Cross Entropy (CE) loss has been shown to be not robust to noisy labels due to its unboundedness. To alleviate this issue, existing works typically design specialized robust losses with the symmetric condition, which usually lead to the underfitting issue. In this work, we aim to induce a loss bound at the logit level, thus universally enhancing the noise robustiveness of existing losses. Specifically, we propose logit clipping (LogitClip), which clamps the norm of the logit vector to ensure that it is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip method is effectively bounded, mitigating the overfitting to examples with noisy labels. We theoretically analyze LogitClip and demonstrate its noise-tolerant ability. Extensive experiments show that LogitClip not only significantly improves the noise robustness of CE loss, but also broadly enhances the generalization performance of popular robust losses.},
  author = {Hongxin Wei and Huiping Zhuang and Renchunzi Xie and Lei Feng 0006 and Gang Niu 0001 and Bo An 0001 and Yixuan Li 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wei2023mitigating.pdf:pdf},
  mdate = {2024-07-27},
  pages = {36868--36886},
  pdf = {https://proceedings.mlr.press/v202/wei23e/wei23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mitigating Memorization of Noisy Labels by Clipping the Model Prediction},
  url = {https://proceedings.mlr.press/v202/wei23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{weilbach2023graphically,
  abstract = {We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy.},
  author = {Christian Dietrich Weilbach and William Harvey 0002 and Frank Wood},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/weilbach2023graphically.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36887--36909},
  pdf = {https://proceedings.mlr.press/v202/weilbach23a/weilbach23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graphically Structured Diffusion Models},
  url = {https://proceedings.mlr.press/v202/weilbach23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{welke2023expectationcomplete,
  abstract = {We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lov{\'a}sz' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.},
  author = {Pascal Welke and Maximilian Thiessen and Fabian Jogl and Thomas G{\"a}rtner 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/welke2023expectationcomplete.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36910--36925},
  pdf = {https://proceedings.mlr.press/v202/welke23a/welke23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Expectation-Complete Graph Representations with Homomorphisms},
  url = {https://proceedings.mlr.press/v202/welke23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wen2023conditional,
  abstract = {Accelerated magnetic resonance (MR) imaging attempts to reduce acquisition time by collecting data below the Nyquist rate. As an ill-posed inverse problem, many plausible solutions exist, yet the majority of deep learning approaches generate only a single solution. We instead focus on sampling from the posterior distribution, which provides more comprehensive information for downstream inference tasks. To do this, we design a novel conditional normalizing flow (CNF) that infers the signal component in the measurement operator's nullspace, which is later combined with measured data to form complete images. Using fastMRI brain and knee data, we demonstrate fast inference and accuracy that surpasses recent posterior sampling techniques for MRI.},
  author = {Jeffrey Wen and Rizwan Ahmad and Philip Schniter},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wen2023conditional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36926--36939},
  pdf = {https://proceedings.mlr.press/v202/wen23a/wen23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Conditional Normalizing Flow for Accelerated Multi-Coil MR Imaging},
  url = {https://proceedings.mlr.press/v202/wen23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wen2023optimizing,
  abstract = {Class incremental learning (CIL) is one of the most challenging scenarios in continual learning. Existing work mainly focuses on strategies like memory replay, regularization, or dynamic architecture but ignores a crucial aspect: mode connectivity. Recent studies have shown that different minima can be connected by a low-loss valley, and ensembling over the valley shows improved performance and robustness. Motivated by this, we try to investigate the connectivity in CIL and find that the high-loss ridge exists along the linear connection between two adjacent continual minima. To dodge the ridge, we propose parameter-saving OPtimizing Connectivity (OPC) based on Fourier series and gradient projection for finding the low-loss path between minima. The optimized path provides infinite low-loss solutions. We further propose EOPC to ensemble points within a local bent cylinder to improve performance on learned tasks. Our scheme can serve as a plug-in unit, extensive experiments on CIFAR-100, ImageNet-100, and ImageNet-1K show consistent improvements when adapting EOPC to existing representative CIL methods.},
  author = {Haitao Wen and Haoyang Cheng and Heqian Qiu and Lanxiao Wang and Lili Pan 0001 and Hongliang Li 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wen2023optimizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36940--36957},
  pdf = {https://proceedings.mlr.press/v202/wen23b/wen23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimizing Mode Connectivity for Class Incremental Learning},
  url = {https://proceedings.mlr.press/v202/wen23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{weng2023towards,
  abstract = {Some extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks. For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations. However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities from doing tasks. In this work, we investigate and formulate the problem of learning crucial geometric eigen-lengths for 3D shape fitting tasks. We propose a neural module that can be trained to automatically discover crucial geometric quantities and embed them into the representations. Our experiments demonstrate the efficacy of the proposed method on various 3D shape fitting tasks.},
  author = {Yijia Weng and Kaichun Mo and Ruoxi Shi and Yanchao Yang 0001 and Leonidas J. Guibas},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/weng2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36958--36977},
  pdf = {https://proceedings.mlr.press/v202/weng23a/weng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks},
  url = {https://proceedings.mlr.press/v202/weng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{weng2023openvclip,
  abstract = {Contrastive Language-Image Pretraining (CLIP) has demonstrated impressive zero-shot learning abilities for image understanding, yet limited effort has been made to investigate CLIP for zero-shot video recognition. We introduce Open-VCLIP, a simple yet effective approach that transforms CLIP into a strong zero-shot video classifier that can recognize unseen actions and events at test time. Our framework extends CLIP with minimal modifications to model spatial-temporal relationships in videos, making it a specialized video classifier, while striving for generalization. We formally show that training an Open-VCLIP is equivalent to continual learning with zero historical data. To address this problem, we propose Interpolated Weight Optimization, which utilizes the benefit of weight interpolation in both training and test time. We evaluate our method on three popular and challenging action recognition datasets following various zero-shot evaluation protocols and we demonstrate our approach outperforms state-of-the-art methods by clear margins. In particular, we achieve 87.9\%, 58.3\%, 81.1\% zero-shot accuracy on UCF, HMDB and Kinetics-600 respectively, outperforming state-of-the-art methods by 8.3\%, 7.8\% and 12.2\%. Code is released at https://github.com/wengzejia1/Open-VCLIP.},
  author = {Zejia Weng and Xitong Yang and Ang Li and Zuxuan Wu and Yu-Gang Jiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/weng2023openvclip.pdf:pdf},
  mdate = {2025-01-13},
  pages = {36978--36989},
  pdf = {https://proceedings.mlr.press/v202/weng23b/weng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Open-VCLIP}: Transforming {CLIP} to an Open-vocabulary Video Model via Interpolated Weight Optimization},
  url = {https://proceedings.mlr.press/v202/weng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{whitehouse2023fullyadaptive,
  abstract = {Composition is a key feature of differential privacy. Well-known advanced composition theorems allow one to query a private database quadratically more times than basic privacy composition would permit. However, these results require that the privacy parameters of all algorithms be fixed before interacting with the data. To address this, Rogers et al. introduced fully adaptive composition, wherein both algorithms and their privacy parameters can be selected adaptively. They defined two probabilistic objects to measure privacy in adaptive composition: privacy filters, which provide differential privacy guarantees for composed interactions, and privacy odometers, time-uniform bounds on privacy loss. There are substantial gaps between advanced composition and existing filters and odometers. First, existing filters place stronger assumptions on the algorithms being composed. Second, these odometers and filters suffer from large constants, making them impractical. We construct filters that match the rates of advanced composition, including constants, despite allowing for adaptively chosen privacy parameters. En route we also derive a privacy filter for approximate zCDP. We also construct several general families of odometers. These odometers match the tightness of advanced composition at an arbitrary, preselected point in time, or at all points in time simultaneously, up to a doubly-logarithmic factor. We obtain our results by leveraging advances in martingale concentration. In sum, we show that fully adaptive privacy is obtainable at almost no loss.},
  author = {Justin Whitehouse and Aaditya Ramdas and Ryan Rogers and Steven Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/whitehouse2023fullyadaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {36990--37007},
  pdf = {https://proceedings.mlr.press/v202/whitehouse23a/whitehouse23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fully-Adaptive Composition in Differential Privacy},
  url = {https://proceedings.mlr.press/v202/whitehouse23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{willette2023scalable,
  abstract = {Recent work on mini-batch consistency (MBC) for set functions has brought attention to the need for sequentially processing and aggregating chunks of a partitioned set while guaranteeing the same output for all partitions. However, existing constraints on MBC architectures lead to models with limited expressive power. Additionally, prior work has not addressed how to deal with large sets during training when the full set gradient is required. To address these issues, we propose a Universally MBC (UMBC) class of set functions which can be used in conjunction with arbitrary non-MBC components while still satisfying MBC, enabling a wider range of function classes to be used in MBC settings. Furthermore, we propose an efficient MBC training algorithm which gives an unbiased approximation of the full set gradient and has a constant memory overhead for any set size for both train- and test-time. We conduct extensive experiments including image completion, text classification, unsupervised clustering, and cancer detection on high-resolution images to verify the efficiency and efficacy of our scalable set encoding framework. Our code is available at github.com/jeffwillette/umbc},
  author = {Jeffrey Willette and Seanie Lee and Bruno Andreis and Kenji Kawaguchi and Juho Lee and Sung Ju Hwang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/willette2023scalable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37008--37041},
  pdf = {https://proceedings.mlr.press/v202/willette23a/willette23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation},
  url = {https://proceedings.mlr.press/v202/willette23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{williams2023flexible,
  abstract = {Contrastive learning algorithms used as normative models in neuroscience or as candidate approaches for learning on neuromorphic chips are traditionally implemented with rigid, temporally non-local, and periodic learning dynamics that could limit the range of physical systems capable of harnessing contrastive learning. In this work, we show that this form of learning can be made temporally local and can still function even if many of the dynamical requirements of standard training procedures are relaxed. We provide theoretical foundations for the study and development of contrastive learning methods for biological and neuromorphic neural networks through a set of general theorems corroborated by numerical experiments across several CL models.},
  author = {Ezekiel Williams and Colin Bredenberg and Guillaume Lajoie},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/williams2023flexible.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37042--37065},
  pdf = {https://proceedings.mlr.press/v202/williams23a/williams23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Flexible Phase Dynamics for Bio-Plausible Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/williams23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{williams2023approximate,
  abstract = {Truncated density models have intractable normalising constants and hard to satisfy boundary conditions, making them notoriously difficult to estimate. We propose approximate Stein classes, which leads to a relaxed Stein identity for truncated density estimation. Previous methods for dealing with truncated datasets required explicit knowledge of the boundary function, but our method only requires approximate knowledge of the boundary, based on samples on the boundary. Our approach outperforms existing methods on both simulated and real-world datasets, even when the boundary is not well-defined.},
  author = {Daniel J. Williams and Song Liu 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/williams2023approximate.pdf:pdf},
  mdate = {2024-02-03},
  pages = {37066--37090},
  pdf = {https://proceedings.mlr.press/v202/williams23b/williams23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Approximate Stein Classes for Truncated Density Estimation},
  url = {https://proceedings.mlr.press/v202/williams23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wilming2023theoretical,
  abstract = {XAI methods are lacking theoretical and empirical evidence for the 'correctness' of their explanations, limiting their potential use for quality-control and transparency purposes. We investigate how XAI methods behave when dealing with suppressor variables. We derive analytical expressions for the behavior of a variety of popular XAI methods on a simple two-dimensional binary classification problem involving Gaussian class-conditional distributions, showing that the majority of the studied approaches will attribute non-zero importance to a non-class-related suppressor feature in the presence of correlated noise. High importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target.},
  author = {Rick Wilming and Leo Kieslich and Benedict Clark and Stefan Haufe},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wilming2023theoretical.pdf:pdf},
  mdate = {2023-11-12},
  pages = {37091--37107},
  pdf = {https://proceedings.mlr.press/v202/wilming23a/wilming23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables},
  url = {https://proceedings.mlr.press/v202/wilming23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wipf2023marginalization,
  abstract = {The underlying energy function when applied to continuous data remains poorly understood in VAEs. We adopt a deceptively sophisticated single-layer decoder that nonetheless allows the VAE to address the fundamental challenge of learning optimally sparse representations of continuous data originating from popular multiple-response regression models. We prove rigorous conditions which guarantee that any minimum of the VAE energy (local or global) will produce the optimally sparse latent representation, meaning zero reconstruction error using a minimal number of active latent dimensions. This is achieved because VAE marginalization over the latent posterior selectively smooths away bad local minima.},
  author = {David Wipf},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wipf2023marginalization.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37108--37132},
  pdf = {https://proceedings.mlr.press/v202/wipf23a/wipf23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Marginalization is not Marginal: No Bad VAE Local Minima when Learning Optimal Sparse Representations},
  url = {https://proceedings.mlr.press/v202/wipf23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wollschla2023uncertainty,
  abstract = {Graph Neural Networks serve as promising surrogates for quantum mechanical calculations with low errors on molecular dynamics trajectories and fast inference times, but may be unreliable for out-of-distribution samples, where uncertainty estimation could help communicate model certainty. We identify six key desiderata for uncertainty estimation in molecular force fields (three 'physics-informed' and three 'application-focused'), survey existing methods in the field, and conclude that none of the previous works satisfies all criteria. We propose novel methods that address these limitations.},
  author = {Tom Wollschl{\"a}ger and Nicholas Gao and Bertrand Charpentier and Mohamed Amine Ketata and Stephan G{\"u}nnemann},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wollschla2023uncertainty.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37133--37156},
  pdf = {https://proceedings.mlr.press/v202/wollschlager23a/wollschlager23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Uncertainty Estimation for Molecules: Desiderata and Methods},
  url = {https://proceedings.mlr.press/v202/wollschlager23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{woo2023blessing,
  abstract = {We consider federated Q-learning, where multiple agents collect reinforcement learning data in a distributed manner and collaborate without sharing their local data. We provide sample complexity guarantees for both synchronous and asynchronous variants of federated Q-learning. Our bounds exhibit a linear speedup with respect to the number of agents and near-optimal dependencies on other salient problem parameters. We propose a novel federated Q-learning algorithm with importance averaging, giving larger weights to more frequently visited state-action pairs, which achieves a robust linear speedup regardless of the heterogeneity of local behavior policies. The improved sample complexity scales inverse proportionally to the minimum entry of the average stationary state-action occupancy distribution of all agents, thus only requiring the agents to collectively cover the entire state-action space, unveiling the blessing of heterogeneity.},
  author = {Jiin Woo and Gauri Joshi and Yuejie Chi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/woo2023blessing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37157--37216},
  pdf = {https://proceedings.mlr.press/v202/woo23a/woo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Blessing of Heterogeneity in Federated Q-Learning: Linear Speedup and Beyond},
  url = {https://proceedings.mlr.press/v202/woo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{woo2023learning,
  abstract = {Despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. While naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. We propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model.},
  author = {Gerald Woo and Chenghao Liu and Doyen Sahoo and Akshat Kumar and Steven C. H. Hoi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/woo2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37217--37237},
  pdf = {https://proceedings.mlr.press/v202/woo23b/woo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Deep Time-index Models for Time Series Forecasting},
  url = {https://proceedings.mlr.press/v202/woo23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{woodruff2023sharper,
  abstract = {In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\mathfrak{S}$ in remarkably general settings. However, guarantees going beyond this general bound of $\mathfrak{S}d$ are known in perhaps only one setting, for $\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\ell_p$ subspace embeddings for $p \neq 2$ that improve over the general $\mathfrak{S}d$ bound, achieving a bound of roughly $\mathfrak{S}^{2/p}$ for $1 \leq p < 2$ and $\mathfrak{S}^{2-2/p}$ for $2 < p < \infty$. For $1 \leq p < 2$, we show that this bound is tight, in the sense that there exist matrices for which $\mathfrak{S}^{2/p}$ samples is necessary. Furthermore, our techniques yield further new results in the study of sampling algorithms, showing that the root leverage score sampling algorithm achieves a bound of roughly $d$ for $1 \leq p < 2$, and that a combination of leverage score and sensitivity sampling achieves an improved bound of roughly $d^{2/p}\mathfrak{S}^{2-4/p}$ for $2 < p < \infty$. Our sensitivity sampling results yield the best known sample complexity for a wide class of structured matrices that have small $\ell_p$ sensitivity.},
  author = {David P. Woodruff and Taisuke Yasuda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/woodruff2023sharper.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37238--37272},
  pdf = {https://proceedings.mlr.press/v202/woodruff23a/woodruff23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sharper Bounds for $\ell_p$ Sensitivity Sampling},
  url = {https://proceedings.mlr.press/v202/woodruff23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{woodworth2023two,
  abstract = {We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is $\delta$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a $\delta$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.},
  author = {Blake E. Woodworth and Konstantin Mishchenko and Francis R. Bach},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/woodworth2023two.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37273--37292},
  pdf = {https://proceedings.mlr.press/v202/woodworth23a/woodworth23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy},
  url = {https://proceedings.mlr.press/v202/woodworth23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023understanding,
  abstract = {The high computation and memory cost of transformer-based language models limits their deployment and inference efficiency. While INT8 quantization has recently been shown to be effective in reducing both memory cost and latency while preserving model accuracy, it remains unclear whether further quantization to INT4 can yield additional efficiency gain. We systematically study the effect of INT4 weight and activation (W4A4) quantization on language models. Our findings show that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. Our INT4 inference pipeline is 8.5× faster for latency-oriented scenarios and up to 3× for throughput-oriented scenarios compared to FP16 inference, and improves the SOTA BERT INT8 performance from FasterTransformer by up to 1.7×. Additionally, we provide insights into the failure cases when applying W4A4 to decoder-only models, and explore the composability of INT4 quantization with other compression methods.},
  author = {Xiaoxia Wu and Cheng Li and Reza Yazdani Aminabadi and Zhewei Yao and Yuxiong He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023understanding.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37524--37539},
  pdf = {https://proceedings.mlr.press/v202/wu23k/wu23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Int4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases},
  url = {https://proceedings.mlr.press/v202/wu23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023differentially,
  abstract = {In this paper we study the problem of (finite horizon tabular) Markov decision processes ({MDP}s) with heavy-tailed rewards under the constraint of differential privacy ({DP}). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure {DP}, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v\in(0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed {MDP}s, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy ({JDP}) and local differential privacy ({LDP}) models. Based on our frameworks, we provide regret upper bounds for both {JDP} and {LDP} cases, and show that the moment of distributions and privacy budget have significant impact on regrets. Finally, we establish a lower bound of regret minimization for heavy-tailed {MDP}s in {JDP} model by reducing it to the instance-independent lower bound of heavy-tailed multi-armed bandits in {DP} model. We also show the lower bound for the problem in {LDP} by adopting some private minimax methods. Our results reveal that there are fundamental differences between the problem of private {RL} with sub-Gaussian and that with heavy-tailed rewards.},
  author = {Yulian Wu and Xingyu Zhou and Sayak Ray Chowdhury and Di Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023differentially.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37880--37918},
  pdf = {https://proceedings.mlr.press/v202/wu23aa/wu23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards},
  url = {https://proceedings.mlr.press/v202/wu23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023anchor,
  abstract = {Compared with full client participation, partial client participation is a more practical scenario in federated learning, but it may amplify some challenges in federated learning, such as data heterogeneity. The lack of inactive clients' updates in partial client participation makes it more likely for the model aggregation to deviate from the aggregation based on full client participation. Training with large batches on individual clients is proposed to address data heterogeneity in general, but their effectiveness under partial client participation is not clear. Motivated by these challenges, we propose to develop a novel federated learning framework, referred to as {F}ed{AMD}, for partial client participation. The core idea is anchor sampling, which separates partial participants into anchor and miner groups. Each client in the anchor group aims at the local bullseye with the gradient computation using a large batch. Meanwhile, each client in the miner group aims at the global target with the gradient computation using a small batch. Theoretical analysis shows that {F}ed{AMD} can achieve a similar convergence rate as full client participation when the anchor-miner partition is appropriate.},
  author = {Feijie Wu and Song Guo 0001 and Zhihao Qu and Shiqi He and Ziming Liu and Jing Gao 0004},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023anchor.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37379--37416},
  pdf = {https://proceedings.mlr.press/v202/wu23e/wu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Anchor Sampling for Federated Learning with Partial Client Participation},
  url = {https://proceedings.mlr.press/v202/wu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023sega,
  abstract = {In contrastive learning, the choice of "view" controls the information that the representation captures and influences the performance of the model. However, leading graph contrastive learning methods generally produce views via random corruption or learning, which could lead to the loss of essential information and alteration of semantic information. An anchor view that maintains the essential information of input graphs for contrastive learning has been hardly investigated. In this paper, based on the theory of graph information bottleneck, we deduce the definition of this anchor view; put differently, the anchor view with essential information of input graph is supposed to have the minimal structural uncertainty. Furthermore, guided by structural entropy, we implement the anchor view, termed {SEGA}, for graph contrastive learning. We extensively validate the proposed anchor view on various benchmarks regarding graph classification under unsupervised, semi-supervised, and transfer learning and achieve significant performance boosts compared to the state-of-the-art methods.},
  author = {Junran Wu and Xueyuan Chen and Bowen Shi and Shangzhe Li and Ke Xu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023sega.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37293--37312},
  pdf = {https://proceedings.mlr.press/v202/wu23a/wu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SEGA}: Structural Entropy Guided Anchor View for Graph Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/wu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023causal,
  abstract = {Explainability methods for {NLP} systems encounter a version of the fundamental problem of causal inference: for a given ground-truth input text, we never truly observe the counterfactual texts necessary for isolating the causal effects of model representations on outputs. In response, many explainability methods make no use of counterfactual texts, assuming they will be unavailable. In this paper, we show that robust causal explainability methods can be created using approximate counterfactuals, which can be written by humans to approximate a specific counterfactual or simply sampled using metadata-guided heuristics. The core of our proposal is the Causal Proxy Model ({CPM}). A {CPM} explains a black-box model $\mathcal{N}$ because it is trained to have the same actual input/output behavior as $\mathcal{N}$ while creating neural representations that can be intervened upon to simulate the counterfactual input/output behavior of $\mathcal{N}$.},
  author = {Zhengxuan Wu and Karel D'Oosterlinck and Atticus Geiger and Amir Zur and Christopher Potts},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023causal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37313--37334},
  pdf = {https://proceedings.mlr.press/v202/wu23b/wu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Causal Proxy Models for Concept-based Model Explanations},
  url = {https://proceedings.mlr.press/v202/wu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023effective,
  abstract = {Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model ({ECRTM}). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization ({ECR}), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by {ECR}, our {ECRTM} generates diverse and coherent topics together with high-quality topic distributions of documents.},
  author = {Xiaobao Wu and Xinshuai Dong and Thong Thanh Nguyen and Anh Tuan Luu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023effective.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37335--37357},
  pdf = {https://proceedings.mlr.press/v202/wu23c/wu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Effective Neural Topic Modeling with Embedding Clustering Regularization},
  url = {https://proceedings.mlr.press/v202/wu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023adaptive,
  abstract = {This paper focuses on continual meta-learning, where few-shot tasks are heterogeneous and sequentially available. Recent works use a mixture model for meta-knowledge to deal with the heterogeneity. However, these methods suffer from parameter inefficiency caused by two reasons: (1) the underlying assumption of mutual exclusiveness among mixture components hinders sharing meta-knowledge across heterogeneous tasks. (2) they only allow increasing mixture components and cannot adaptively filter out redundant components. In this paper, we propose an Adaptive Compositional Continual Meta-Learning ({ACML}) algorithm, which employs a compositional premise to associate a task with a subset of mixture components, allowing meta-knowledge sharing among heterogeneous tasks. Furthermore, we design an adaptive component selection mechanism to expand or merge mixture components according to task relatedness, enabling the meta-learner to determine whether to learn new meta-knowledge or exploit existing one adaptively.},
  author = {Bin Wu 0025 and Jinyuan Fang and Xiangxiang Zeng and Shangsong Liang and Qiang Zhang 0026},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023adaptive.pdf:pdf},
  mdate = {2025-04-22},
  pages = {37358--37378},
  pdf = {https://proceedings.mlr.press/v202/wu23d/wu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Compositional Continual Meta-Learning},
  url = {https://proceedings.mlr.press/v202/wu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023law,
  abstract = {We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetric distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\Omega(\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li and Nagaraj on two-layer neural networks with polynomial weights. We further identify geometric properties of the training distribution that deteriorate the robustness and provide an algorithm to construct worst-case distributions.},
  author = {Yihan Wu and Heng Huang and Hongyang Zhang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023law.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37439--37455},
  pdf = {https://proceedings.mlr.press/v202/wu23g/wu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Law of Robustness beyond Isoperimetry},
  url = {https://proceedings.mlr.press/v202/wu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023solving,
  abstract = {Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis, we design a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation. Experimentally, LSM achieves consistent state-of-the-art and yields a relative gain of 11.5% averaged on seven benchmarks covering both solid and fluid physics and performs favorable efficiency and transferability.},
  author = {Haixu Wu and Tengge Hu and Huakun Luo and Jianmin Wang 0001 and Mingsheng Long},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023solving.pdf:pdf},
  mdate = {2024-02-06},
  pages = {37417-37438},
  pdf = {https://proceedings.mlr.press/v202/wu23f/wu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Solving High-Dimensional PDEs with Latent Spectral Models},
  url = {https://proceedings.mlr.press/v202/wu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023uncovering,
  abstract = {Recently, test-time adaptation (TTA) has been proposed as a promising solution for addressing distribution shifts. It allows a base model to adapt to an unforseen distribution during inference by leveraging the information from the batch of (unlabeled) test data. However, we uncover a novel security vulnerability of TTA based on the insight that predictions on benign samples can be impacted by malicious samples in the same batch. To exploit this vulnerability, we propose Distribution Invading Attack (DIA), which injects a small fraction of malicious data into the test batch. DIA causes models using TTA to misclassify benign and unperturbed test data, providing an entirely new capability for adversaries that is infeasible in canonical machine learning pipelines. Through comprehensive evaluations, we demonstrate the high effectiveness of our attack on multiple benchmarks across six TTA methods. In response, we investigate two countermeasures to robustify the existing insecure TTA implementations, following the principle of "security by design". Together, we hope our findings can make the community aware of the utility-security tradeoffs in deploying TTA and provide valuable insights for developing robust TTA approaches.},
  author = {Tong Wu and Feiran Jia and Xiangyu Qi and Jiachen T. Wang and Vikash Sehwag and Saeed Mahloujifar and Prateek Mittal},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023uncovering.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37456-37495},
  pdf = {https://proceedings.mlr.press/v202/wu23h/wu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Uncovering Adversarial Risks of Test-Time Adaptation},
  url = {https://proceedings.mlr.press/v202/wu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023stable,
  abstract = {Estimating heterogeneous treatment effects (HTE) is crucial for identifying the variation of treatment effects across individuals or subgroups. Most existing methods estimate HTE by removing the confounding bias from imbalanced treatment assignments. However, these methods may produce unreliable estimates of treatment effects and potentially allocate suboptimal treatment arms for underrepresented populations. To improve the estimation accuracy of HTE for underrepresented populations, we propose a novel Stable CounterFactual Regression (StableCFR) to smooth the population distribution and upsample the underrepresented subpopulations, while balancing confounders between treatment and control groups. Specifically, StableCFR upsamples the underrepresented data using uniform sampling, where each disjoint subpopulation is weighted proportional to the Lebesgue measure of its support.},
  author = {Anpeng Wu and Kun Kuang and Ruoxuan Xiong and Bo Li 0064 and Fei Wu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023stable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37496-37510},
  pdf = {https://proceedings.mlr.press/v202/wu23i/wu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stable Estimation of Heterogeneous Treatment Effects},
  url = {https://proceedings.mlr.press/v202/wu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023quantifying,
  abstract = {Knowledge distillation has emerged as a promising technique for bridging the gap between topology-aware Graph Neural Networks (GNNs) and inference-efficient Multi-Layer Perceptrons (MLPs). However, little work has explored the reliability of different knowledge points (nodes) in GNNs and their roles during distillation. We quantify knowledge reliability in GNNs by measuring the invariance of their information entropy to noise perturbations, observing that different knowledge points show different distillation speeds temporally and are differentially distributed in the graph spatially. To achieve reliable distillation, we propose Knowledge-inspired Reliable Distillation (KRD), which models the probability of each node being an informative and reliable knowledge point, sampling additional reliable knowledge points for training student MLPs. Extensive experiments show that KRD improves over vanilla MLPs by 12.62% and outperforms corresponding teacher GNNs by 2.16% averaged over 7 datasets and 3 GNN architectures.},
  author = {Lirong Wu and Haitao Lin and Yufei Huang 0002 and Stan Z. Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023quantifying.pdf:pdf},
  mdate = {2024-05-27},
  pages = {37571-37581},
  pdf = {https://proceedings.mlr.press/v202/wu23m/wu23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantifying the Knowledge in GNNs for Reliable Distillation into MLPs},
  url = {https://proceedings.mlr.press/v202/wu23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023rethinking,
  abstract = {The success of graph neural networks (GNNs) provokes the question about explainability: "Which fraction of the input graph is the most determinant of the prediction?" Particularly, parametric explainers prevail in existing approaches because of their more robust capability to decipher the black-box (i.e., target GNNs). In this paper, based on the observation that graphs typically share some common motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To alleviate this issue, we designed a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to fix the most informative portion of the graph and merely operates graph augmentations on the rest less informative part. Extensive experiments on synthetic and real-world datasets show the effectiveness of MatchExplainer by outperforming all state-of-the-art parametric baselines with significant margins.},
  author = {Fang Wu 0002 and Siyuan Li 0002 and Xurui Jin and Yinghui Jiang and Dragomir Radev and Zhangming Niu and Stan Z. Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023rethinking.pdf:pdf},
  mdate = {2025-01-28},
  pages = {37511--37523},
  pdf = {https://proceedings.mlr.press/v202/wu23j/wu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching},
  url = {https://proceedings.mlr.press/v202/wu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023delayagnostic,
  abstract = {We propose a delay-agnostic asynchronous coordinate update algorithm (DEGAS) for computing operator fixed points, with applications to asynchronous optimization. DEGAS includes novel asynchronous variants of ADMM and block-coordinate descent as special cases. We prove that DEGAS converges under both bounded and unbounded delays under delay-free parameter conditions. We also validate by theory and experiments that DEGAS adapts well to the actual delays. The effectiveness of DEGAS is demonstrated by numerical experiments on classification problems.},
  author = {Xuyang Wu 0001 and Changxin Liu and Sindri Magnsson and Mikael Johansson 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023delayagnostic.pdf:pdf},
  mdate = {2024-06-18},
  pages = {37582--37606},
  pdf = {https://proceedings.mlr.press/v202/wu23n/wu23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Delay-agnostic Asynchronous Coordinate Update Algorithm},
  url = {https://proceedings.mlr.press/v202/wu23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023towards,
  abstract = {Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: the label-wise class imbalance. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.},
  author = {Guoqiang Wu and Chongxuan Li and Yilong Yin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37540--37570},
  pdf = {https://proceedings.mlr.press/v202/wu23l/wu23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Understanding Generalization of Macro-{AUC} in Multi-label Learning},
  url = {https://proceedings.mlr.press/v202/wu23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023masked,
  abstract = {We introduce Masked Trajectory Models (MTM) as a generic abstraction for sequential decision making. MTM takes a trajectory, such as a state-action sequence, and aims to reconstruct the trajectory conditioned on random subsets of the same trajectory. By training with a highly randomized masking pattern, MTM learns versatile networks that can take on different roles or capabilities, by simply choosing appropriate masks at inference time. For example, the same MTM network can be used as a forward dynamics model, inverse dynamics model, or even an offline RL agent. Through extensive experiments in several continuous control tasks, we show that the same MTM network -- i.e. same weights -- can match or outperform specialized networks trained for the aforementioned capabilities. Additionally, we find that state representations learned by MTM can significantly accelerate the learning speed of traditional RL algorithms.},
  author = {Philipp Wu and Arjun Majumdar and Kevin Stone and Yixin Lin and Igor Mordatch and Pieter Abbeel and Aravind Rajeswaran},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023masked.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37607--37623},
  pdf = {https://proceedings.mlr.press/v202/wu23o/wu23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Masked Trajectory Models for Prediction, Representation, and Control},
  url = {https://proceedings.mlr.press/v202/wu23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023disentangled,
  abstract = {To balance quality and cost, various domain areas of science and engineering run simulations at multiple levels of sophistication. Multi-fidelity active learning aims to learn a direct mapping from input parameters to simulation outputs at the highest fidelity by actively acquiring data from multiple fidelity levels. However, existing approaches based on Gaussian processes are hardly scalable to high-dimensional data. Deep learning-based methods often impose a hierarchical structure in hidden representations, which only supports passing information from low-fidelity to high-fidelity. These approaches can lead to the undesirable propagation of errors from low-fidelity representations to high-fidelity ones. We propose a novel framework called Disentangled Multi-fidelity Deep Bayesian Active Learning (D-MFDAL), which learns the surrogate models conditioned on the distribution of functions at multiple fidelities. On benchmark tasks of learning deep surrogates of partial differential equations including heat equation, Poisson's equation and fluid simulations, our approach significantly outperforms state-of-the-art in prediction accuracy and sample efficiency.},
  author = {Dongxia Wu and Ruijia Niu and Matteo Chinazzi and Yi-An Ma and Rose Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023disentangled.pdf:pdf},
  mdate = {2024-07-24},
  pages = {37624--37634},
  pdf = {https://proceedings.mlr.press/v202/wu23p/wu23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Disentangled Multi-Fidelity Deep Bayesian Active Learning},
  url = {https://proceedings.mlr.press/v202/wu23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023implicit,
  abstract = {In this paper, we study the implicit regularization of stochastic gradient descent (SGD) through the lens of dynamical stability. We start by revising existing stability analyses of SGD, showing how the Frobenius norm and trace of Hessian relate to different notions of stability. Notably, if a global minimum is linearly stable for SGD, then the trace of Hessian must be less than or equal to $2/\eta$, where $\eta$ denotes the learning rate. By contrast, for gradient descent (GD), the stability imposes a similar constraint but only on the largest eigenvalue of Hessian. We then turn to analyze the generalization properties of these stable minima, focusing specifically on two-layer ReLU networks and diagonal linear networks. Notably, we establish the equivalence between these metrics of sharpness and certain parameter norms for the two models, which allows us to show that the stable minima of SGD provably generalize well. By contrast, the stability-induced regularization of GD is provably too weak to ensure satisfactory generalization. This discrepancy provides an explanation of why SGD often generalizes better than GD. Note that the learning rate (LR) plays a pivotal role in the strength of stability-induced regularization. As the LR increases, the regularization effect becomes more pronounced, elucidating why SGD with a larger LR consistently demonstrates superior generalization capabilities. Additionally, numerical experiments are provided to support our theoretical findings.},
  author = {Lei Wu and Weijie J. Su},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023implicit.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37656--37684},
  pdf = {https://proceedings.mlr.press/v202/wu23r/wu23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent},
  url = {https://proceedings.mlr.press/v202/wu23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023distributional,
  abstract = {We study the problem of estimating the distribution of the return of a policy using an offline dataset that is not generated from the policy, i.e., distributional offline policy evaluation ({OPE}). We propose an algorithm called Fitted Likelihood Estimation ({FLE}), which conducts a sequence of Maximum Likelihood Estimation ({MLE}) and has the flexibility of integrating any state-of-the-art probabilistic generative models as long as it can be trained via {MLE}. {FLE} can be used for both finite-horizon and infinite-horizon discounted settings where rewards can be multi-dimensional vectors. The theoretical results show that for both finite-horizon and infinite-horizon discounted settings, {FLE} can learn distributions that are close to the ground truth under total variation distance and {Wasserstein} distance, respectively.},
  author = {Runzhe Wu and Masatoshi Uehara and Wen Sun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023distributional.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37685--37712},
  pdf = {https://proceedings.mlr.press/v202/wu23s/wu23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distributional Offline Policy Evaluation with Predictive Error Guarantees},
  url = {https://proceedings.mlr.press/v202/wu23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023,
  abstract = {Foundation models have achieved great advances in multi-task learning with a unified interface of unimodal and multimodal tasks. However, the potential of such multi-task learners has not been exploited during transfer learning. We propose Predict-Interpolate Tuning ({{$\pi$}}-Tuning), a universal parameter-efficient transfer learning method for vision, language, and vision-language tasks, which aggregates the parameters of lightweight task-specific experts learned from similar tasks to aid the target downstream task, and surpasses fine-tuning and other parameter-efficient transfer learning methods both in full-shot and low-shot regimes.},
  author = {Chengyue Wu and Teng Wang and Yixiao Ge and Zeyu Lu and Ruisong Zhou and Ying Shan and Ping Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023.pdf:pdf},
  mdate = {2024-10-09},
  pages = {37713--37727},
  pdf = {https://proceedings.mlr.press/v202/wu23t/wu23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{$\pi$}-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation},
  url = {https://proceedings.mlr.press/v202/wu23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023learning,
  abstract = {We study the problem of learning functional distributions in the presence of noise. A functional is a map from the space of features to distributions over a set of labels, and is often assumed to belong to a known class of hypotheses. Features are generated by a general random process and labels are sampled independently from feature-dependent distributions. In privacy sensitive applications, labels are passed through a noisy kernel. We consider online learning, where at each time step, a predictor attempts to predict the actual (label) distribution given only the features and noisy labels in prior steps. The performance of the predictor is measured by the expected KL-risk that compares the predicted distributions to the underlying truth. We show that the minimax expected KL-risk is of order $\Theta(\sqrt{T \log|F|})$ for finite hypothesis class $F$ and any non-trivial noise level.},
  author = {Changlong Wu and Yifan Wang and Ananth Grama and Wojciech Szpankowski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37728--37744},
  pdf = {https://proceedings.mlr.press/v202/wu23u/wu23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Functional Distributions with Private Labels},
  url = {https://proceedings.mlr.press/v202/wu23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023quantumdarts,
  abstract = {With the arrival of the Noisy Intermediate-Scale Quantum ({NISQ}) era and the fast development of machine learning, variational quantum algorithms ({VQA}) including Variational Quantum Eigensolver ({VQE}) and quantum neural network ({QNN}) have received increasing attention with wide potential applications in foreseeable near future. We study the problem of quantum architecture search ({QAS}) for {VQA} to automatically design parameterized quantum circuits ({PQC}). We devise a differentiable searching algorithm based on {Gumbel-Softmax} in contrast to peer methods that often require numerous circuit sampling and evaluation. Two versions of our algorithm are provided, namely macro search and micro search, where macro search directly searches for the whole circuit like other literature while the innovative micro search is able to infer the sub-circuit structure from a small-scale and then transfer that to a larger scale problem.},
  author = {Wenjie Wu and Ge Yan and Xudong Lu and Kaisen Pan and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023quantumdarts.pdf:pdf},
  mdate = {2023-08-31},
  pages = {37745--37764},
  pdf = {https://proceedings.mlr.press/v202/wu23v/wu23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{QuantumDARTS}: Differentiable Quantum Architecture Search for Variational Quantum Algorithms},
  url = {https://proceedings.mlr.press/v202/wu23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023training,
  abstract = {We uncover how {SGD} interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle ({SS}) and Random Reshuffle ({RR})---two widely used variants of {SGD}---interact surprisingly differently in the presence of batch normalization: {RR} leads to much more stable evolution of training loss than {SS}. As a concrete example, for regression using a linear network with batch normalization, we prove that {SS} and {RR} converge to distinct global optima that are ``distorted'' away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for {SS} and {RR} can, and cannot occur. We present explicit constructions to show how {SS} leads to distorted optima in regression and divergence for classification, whereas {RR} avoids both distortion and divergence.},
  author = {David Xing Wu and Chulhee Yun and Suvrit Sra},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023training.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37787--37845},
  pdf = {https://proceedings.mlr.press/v202/wu23x/wu23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Training Instability of Shuffling {SGD} with Batch Normalization},
  url = {https://proceedings.mlr.press/v202/wu23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023discover,
  abstract = {Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure ({DISC}), to tackle the issue. With human-interpretable concepts, {DISC} iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, {DISC} provides superior generalization ability and interpretability than the existing approaches.},
  author = {Shirley Wu and Mert Y{\"u}ksekgonul and Linjun Zhang and James Zou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023discover.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37765--37786},
  pdf = {https://proceedings.mlr.press/v202/wu23w/wu23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discover and Cure: Concept-aware Mitigation of Spurious Correlation},
  url = {https://proceedings.mlr.press/v202/wu23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023finitesample,
  abstract = {This paper considers the problem of learning a single {ReLU} neuron with squared loss (a.k.a., {ReLU} regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called {GLM-tron} (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional {ReLU} regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for {GLM-tron}. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional {ReLU} regression problems that can be learned via {GLM-tron}. On the other hand, we provide some negative results for stochastic gradient descent ({SGD}) for {ReLU} regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of {SGD} is provably no better than that of {GLM-tron} ignoring constant factors, for each problem instance; and in the noiseless case, {GLM-tron} can achieve a small risk while {SGD} unavoidably suffers from a constant risk in expectation.},
  author = {Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Sham M. Kakade},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023finitesample.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37919--37951},
  pdf = {https://proceedings.mlr.press/v202/wu23ab/wu23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Finite-Sample Analysis of Learning High-Dimensional Single {ReLU} Neuron},
  url = {https://proceedings.mlr.press/v202/wu23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023dugmatting,
  abstract = {Image matting is the task of cutting out an object and estimating its opacity mask, which is key for image and video editing. Due to the highly ill-posed nature of this problem, additional inputs like user-defined trimaps or scribbles are typically needed to reduce uncertainty, but these approaches are either time consuming or only suitable for experienced users. We propose a decomposed-uncertainty-guided matting (dugMatting) algorithm that explores explicitly decomposed uncertainties to efficiently and effectively improve results. The approach reduces epistemic uncertainty through guided interaction (introducing prior knowledge) and aleatoric uncertainty through modeling data distribution (introducing statistics for both data and possible noise). The proposed matting framework relieves the requirement for users to determine interaction areas by using simple and efficient labeling.},
  author = {Jiawei Wu and Changqing Zhang and Zuoyong Li and Huazhu Fu and Xi Peng and Joey Tianyi Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023dugmatting.pdf:pdf},
  mdate = {2025-02-07},
  pages = {37846--37859},
  pdf = {https://proceedings.mlr.press/v202/wu23y/wu23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {dugMatting: Decomposed-Uncertainty-Guided Matting},
  url = {https://proceedings.mlr.press/v202/wu23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{wu2023personalized,
  abstract = {The recent trend towards Personalized Federated Learning ({PFL}) has garnered significant attention as it allows for the training of models that are tailored to each client while maintaining data privacy. However, current {PFL} techniques primarily focus on modeling the conditional distribution heterogeneity (i.e. concept shift), which can result in suboptimal performance when the distribution of input data across clients diverges (i.e. covariate shift). Additionally, these techniques often lack the ability to adapt to unseen data, further limiting their effectiveness in real-world scenarios. To address these limitations, we propose a novel approach, {FedGMM}, which utilizes {Gaussian} mixture models ({GMM}) to effectively fit the input data distributions across diverse clients.},
  author = {Yue Wu and Shuaicheng Zhang and Wenchao Yu and Yanchi Liu and Quanquan Gu and Dawei Zhou and Haifeng Chen and Wei Cheng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/wu2023personalized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37860--37879},
  pdf = {https://proceedings.mlr.press/v202/wu23z/wu23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Personalized Federated Learning under Mixture of Distributions},
  url = {https://proceedings.mlr.press/v202/wu23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xian2023understanding,
  abstract = {A poisoning backdoor attack is a rising security concern for deep learning. This type of attack can result in the backdoored model functioning normally most of the time but exhibiting abnormal behavior when presented with inputs containing the backdoor trigger, making it difficult to detect and prevent. In this work, we propose the adaptability hypothesis to understand when and why a backdoor attack works for general learning models, including deep neural networks, based on the theoretical investigation of classical kernel-based learning models. The adaptability hypothesis postulates that for an effective attack, the effect of incorporating a new dataset on the predictions of the original data points will be small, provided that the original data points are distant from the new dataset. Experiments on benchmark image datasets and state-of-the-art backdoor attacks for deep neural networks are conducted to corroborate the hypothesis.},
  author = {Xun Xian and Ganghua Wang and Jayanth Srinivasa and Ashish Kundu and Xuan Bi and Mingyi Hong and Jie Ding},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xian2023understanding.pdf:pdf},
  mdate = {2024-08-26},
  pages = {37952--37976},
  pdf = {https://proceedings.mlr.press/v202/xian23a/xian23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Understanding Backdoor Attacks through the Adaptability Hypothesis},
  url = {https://proceedings.mlr.press/v202/xian23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xian2023fair,
  abstract = {To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiveness on benchmark datasets.},
  author = {Ruicheng Xian and Lang Yin and Han Zhao 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xian2023fair.pdf:pdf},
  mdate = {2023-08-28},
  pages = {37977--38012},
  pdf = {https://proceedings.mlr.press/v202/xian23b/xian23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fair and Optimal Classification via Post-Processing},
  url = {https://proceedings.mlr.press/v202/xian23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiang2023umd,
  abstract = {Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based on an aggregation of their reverse-engineered trigger size for detection inference, using a robust and unsupervised anomaly detector we proposed. We conduct comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show that our unsupervised UMD outperforms SOTA detectors (even with supervision) by 17%, 4%, and 8%, respectively, in terms of the detection accuracy against diverse X2X attacks. We also show the strong detection performance of UMD against several strong adaptive attacks.},
  author = {Zhen Xiang and Zidi Xiong and Bo Li 0026},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiang2023umd.pdf:pdf},
  mdate = {2024-08-02},
  pages = {38013--38038},
  pdf = {https://proceedings.mlr.press/v202/xiang23a/xiang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {UMD: Unsupervised Model Detection for X2X Backdoor Attacks},
  url = {https://proceedings.mlr.press/v202/xiang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiao2023communicationefficient,
  abstract = {Federated bilevel optimization has attracted increasing attention due to emerging machine learning and communication applications. The biggest challenge lies in computing the gradient of the upper-level objective function (i.e., hypergradient) in the federated setting due to the nonlinear and distributed construction of a series of global Hessian matrices. In this paper, we propose a novel communication-efficient federated hypergradient estimator via aggregated iterative differentiation (AggITD). AggITD is simple to implement and significantly reduces the communication cost by conducting the federated hypergradient estimation and the lower-level optimization simultaneously. We show that the proposed AggITD-based algorithm achieves the same sample complexity as existing approximate implicit differentiation (AID)-based approaches with much fewer communication rounds in the presence of data heterogeneity. Our results also shed light on the great advantage of ITD over AID in the federated/distributed hypergradient estimation. This differs from the comparison in the non-distributed bilevel optimization, where ITD is less efficient than AID. Our extensive experiments demonstrate the great effectiveness and communication efficiency of the proposed method.},
  author = {Peiyao Xiao and Kaiyi Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiao2023communicationefficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38059--38086},
  pdf = {https://proceedings.mlr.press/v202/xiao23b/xiao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation},
  url = {https://proceedings.mlr.press/v202/xiao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiao2023smoothquant,
  abstract = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.},
  author = {Guangxuan Xiao and Ji Lin 0002 and Micka{\"e}l Seznec and Hao Wu and Julien Demouth and Song Han 0003},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiao2023smoothquant.pdf:pdf},
  mdate = {2023-11-10},
  pages = {38087--38099},
  pdf = {https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  url = {https://proceedings.mlr.press/v202/xiao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiao2023comcat,
  abstract = {Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. The paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.},
  author = {Jinqi Xiao and Miao Yin and Yu Gong and Xiao Zang and Jian Ren 0005 and Bo Yuan 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiao2023comcat.pdf:pdf},
  mdate = {2024-10-31},
  pages = {38125--38136},
  pdf = {https://proceedings.mlr.press/v202/xiao23e/xiao23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models},
  url = {https://proceedings.mlr.press/v202/xiao23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xie2023improving,
  abstract = {In humans' classroom learning, many effective study techniques (e.g., the Feynman technique, peer questioning, etc.) have been developed to improve learning outcomes. We are interested in investigating whether these techniques can inspire the development of ML training strategies to improve bi-level optimization (BLO) based methods. Towards this goal, we develop a general framework, Skillearn, which consists of basic elements such as learners, interaction functions, learning stages, etc. These elements can be flexibly configured to create various training strategies, each emulating a study technique of humans. In case studies, we apply Skillearn to create new training strategies, by emulating the Feynman technique and peer questioning, which are two broadly adopted techniques in humans' classroom learning. These training strategies are used for improving two BLO based applications including neural architecture search and data weighting. Experiments on various datasets demonstrate the effectiveness of our methods.},
  author = {Pengtao Xie},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xie2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38137--38186},
  pdf = {https://proceedings.mlr.press/v202/xie23a/xie23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Bi-level Optimization Based Methods with Inspiration from Humans' Classroom Study Techniques},
  url = {https://proceedings.mlr.press/v202/xie23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023futureconditioned,
  abstract = {Recent research in offline reinforcement learning (RL) has demonstrated that return-conditioned supervised learning is a powerful paradigm for decision-making problems. While promising, return conditioning is limited to training data labeled with rewards and therefore faces challenges in learning from unsupervised data. In this work, we aim to utilize generalized future conditioning to enable efficient unsupervised pretraining from reward-free and sub-optimal offline data. We propose Pretrained Decision Transformer (PDT), a conceptually simple approach for unsupervised RL pretraining. PDT leverages future trajectory information as a privileged context to predict actions during training. The ability to make decisions based on both present and future factors enhances PDT's capability for generalization.},
  author = {Zhihui Xie 0002 and Zichuan Lin and Deheng Ye and Qiang Fu 0016 and Yang Wei and Shuai Li 0010},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023futureconditioned.pdf:pdf},
  mdate = {2024-09-25},
  pages = {38187--38203},
  pdf = {https://proceedings.mlr.press/v202/xie23b/xie23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Future-conditioned Unsupervised Pretraining for Decision Transformer},
  url = {https://proceedings.mlr.press/v202/xie23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xie2023desra,
  abstract = {Image super-resolution (SR) with generative adversarial networks (GAN) has achieved great success in restoring realistic details. However, it is notorious that GAN-based SR models will inevitably produce unpleasant and undesirable artifacts, especially in practical scenarios. Previous works typically suppress artifacts with an extra loss penalty in the training phase. They only work for in-distribution artifact types generated during training. When applied in real-world scenarios, we observe that those improved methods still generate obviously annoying artifacts during inference. In this paper, we analyze the cause and characteristics of the GAN artifacts produced in unseen test data without ground-truths. We then develop a novel method, namely, DeSRA, to Detect and then Delete those SR Artifacts in practice. Specifically, we propose to measure a relative local variance distance from MSE-SR results and GAN-SR results, and locate the problematic areas based on the above distance and semantic-aware thresholds. After detecting the artifact regions, we develop a finetune procedure to improve GAN-based SR models with a few samples, so that they can deal with similar types of artifacts in more unseen real data. Equipped with our DeSRA, we can successfully eliminate artifacts from inference and improve the ability of SR models to be applied in real-world scenarios.},
  author = {Liangbin Xie and Xintao Wang 0002 and Xiangyu Chen 0006 and Gen Li 0011 and Ying Shan and Jiantao Zhou 0001 and Chao Dong 0005},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xie2023desra.pdf:pdf},
  mdate = {2025-05-12},
  pages = {38204--38226},
  pdf = {https://proceedings.mlr.press/v202/xie23c/xie23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {DeSRA: Detect and Delete the Artifacts of GAN-based Real-World Super-Resolution Models},
  url = {https://proceedings.mlr.press/v202/xie23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xie2023semiparametrically,
  abstract = {We study semiparametrically efficient estimation in off-policy evaluation (OPE) where the underlying Markov decision process (MDP) is linear with a known feature map. We characterize the variance lower bound for regular estimators in the linear MDP setting and propose an efficient estimator whose variance achieves that lower bound. Consistency and asymptotic normality of our estimator are established under mild conditions, which merely requires the only infinite-dimensional nuisance parameter to be estimated at a n^{-1/4} convergence rate. We also construct an asymptotically valid confidence interval for statistical inference and conduct simulation studies to validate our results. To our knowledge, this is the first work that concerns efficient estimation in the presence of a known structure of MDPs in the OPE literature.},
  author = {Chuhan Xie and Wenhao Yang and Zhihua Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xie2023semiparametrically.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38227--38257},
  pdf = {https://proceedings.mlr.press/v202/xie23d/xie23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semiparametrically Efficient Off-Policy Evaluation in Linear Markov Decision Processes},
  url = {https://proceedings.mlr.press/v202/xie23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xie2023critical,
  abstract = {Dynamics prediction, which is the problem of predicting future states of scene objects based on current and prior states, is drawing increasing attention as an instance of learning physics. To solve this problem, Region Proposal Convolutional Interaction Network (RPCIN), a vision-based model, was proposed and achieved state-of-the-art performance in long-term prediction. RPCIN only takes raw images and simple object descriptions, such as the bounding box and segmentation mask of each object, as input. However, despite its success, the model's capability can be compromised under conditions of environment misalignment. In this paper, we investigate two challenging conditions for environment misalignment: Cross-Domain and Cross-Context by proposing four datasets that are designed for these challenges: SimB-Border, SimB-Split, BlenB-Border, and BlenB-Split. Using RPCIN as a probe, experiments conducted on the combinations of the proposed datasets reveal potential weaknesses of the vision-based long-term dynamics prediction model. Furthermore, we propose a promising direction to mitigate the Cross-Domain challenge and provide concrete evidence supporting such a direction, which provides dramatic alleviation of the challenge on the proposed datasets.},
  author = {Hanchen Xie and Jiageng Zhu and Mahyar Khayatkhoei and Jiazhi Li 0001 and Mohamed E. Hussein 0001 and Wael AbdAlmageed},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xie2023critical.pdf:pdf},
  mdate = {2024-10-28},
  pages = {38258--38271},
  pdf = {https://proceedings.mlr.press/v202/xie23e/xie23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Critical View of Vision-Based Long-Term Dynamics Prediction Under Environment Misalignment},
  url = {https://proceedings.mlr.press/v202/xie23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xing2023controlling,
  abstract = {Ad hoc teamwork requires an agent to cooperate with unknown teammates without prior coordination. Many works propose to abstract teammate instances into high-level representation of types and then pre-train the best response for each type. However, most of them do not consider the distribution of teammate instances within a type. This could expose the agent to the hidden risk of type confounding. In the worst case, the best response for an abstract teammate type could be the worst response for all specific instances of that type. This work addresses the issue from the lens of causal inference. We first theoretically demonstrate that this phenomenon is due to the spurious correlation brought by uncontrolled teammate distribution. Then, we propose our solution, CTCAT, which disentangles such correlation through an instance-wise teammate feedback rectification. This operation reweights the interaction of teammate instances within a shared type to reduce the influence of type confounding. The effect of CTCAT is evaluated in multiple domains, including classic ad hoc teamwork tasks and real-world scenarios. Results show that CTCAT is robust to the influence of type confounding, a practical issue that directly hazards the robustness of our trained agents but was unnoticed in previous works.},
  author = {Dong Xing and Pengjie Gu and Qian Zheng and Xinrun Wang and Shanqi Liu and Longtao Zheng and Bo An 0001 and Gang Pan 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xing2023controlling.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38272--38285},
  pdf = {https://proceedings.mlr.press/v202/xing23a/xing23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controlling Type Confounding in Ad Hoc Teamwork with Instance-wise Teammate Feedback Rectification},
  url = {https://proceedings.mlr.press/v202/xing23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xiong2023universal,
  abstract = {Learning a universal policy across different robot morphologies can significantly improve learning efficiency and generalization in continuous control. However, it poses a challenging multi-task reinforcement learning problem, as the optimal policy may be quite different across robots and critically depend on the morphology. Existing methods utilize graph neural networks or transformers to handle heterogeneous state and action spaces across different morphologies, but pay little attention to the dependency of a robot's control policy on its morphology context. In this paper, we propose a hierarchical architecture to better model this dependency via contextual modulation, which includes two key submodules: (1) Instead of enforcing hard parameter sharing across robots, we use hypernetworks to generate morphology-dependent control parameters; (2) We propose a fixed attention mechanism that solely depends on the morphology to modulate the interactions between different limbs in a robot. Experimental results show that our method not only improves learning performance on a diverse set of training robots, but also generalizes better to unseen morphologies in a zero-shot fashion.},
  author = {Zheng Xiong and Jacob Beck and Shimon Whiteson},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xiong2023universal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38286--38300},
  pdf = {https://proceedings.mlr.press/v202/xiong23a/xiong23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Universal Morphology Control via Contextual Modulation},
  url = {https://proceedings.mlr.press/v202/xiong23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023relevant,
  abstract = {Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-$K$ relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm---a common tool for finding the maximum likelihood configurations in probabilistic graphical models---and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks.},
  author = {Ping Xiong 0002 and Thomas Schnake and Michael Gastegger and Gr{\'e}goire Montavon and Klaus-Robert M{\"u}ller and Shinichi Nakajima},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023relevant.pdf:pdf},
  mdate = {2023-09-06},
  pages = {38301--38324},
  pdf = {https://proceedings.mlr.press/v202/xiong23b/xiong23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Relevant Walk Search for Explaining Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/xiong23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023why,
  abstract = {Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component.},
  author = {Frank F. Xu and Uri Alon 0002 and Graham Neubig},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023why.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38325--38341},
  pdf = {https://proceedings.mlr.press/v202/xu23a/xu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Why do Nearest Neighbor Language Models Work?},
  url = {https://proceedings.mlr.press/v202/xu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023fascinating,
  abstract = {Due to the unsupervised nature of anomaly detection, the key to fueling deep models is finding supervisory signals. Different from current reconstruction-guided generative models and transformation-based contrastive models, we devise novel data-driven supervision for tabular data by introducing a characteristic -- scale -- as data labels. By representing varied sub-vectors of data instances, we define scale as the relationship between the dimensionality of original sub-vectors and that of representations. Scales serve as labels attached to transformed representations, thus offering ample labeled data for neural network training. Through this proxy task, our approach models inherent regularities and patterns within data, which well describes data normality. Abnormal degrees of testing instances are obtained by measuring whether they fit these learned patterns. Extensive experiments show that our approach leads to significant improvement over state-of-the-art generative/contrastive anomaly detection methods.},
  author = {Hongzuo Xu and Yijie Wang 0001 and Juhui Wei and Songlei Jian and Yizhou Li and Ning Liu 0015},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023fascinating.pdf:pdf},
  mdate = {2025-06-24},
  pages = {38655--38673},
  pdf = {https://proceedings.mlr.press/v202/xu23p/xu23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fascinating Supervisory Signals and Where to Find Them: Deep Anomaly Detection with Scale Learning},
  url = {https://proceedings.mlr.press/v202/xu23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023competing,
  abstract = {This work addresses competitions for shareable and limited resources with strategic agents. In reality, agents often have to learn and maximize the rewards of the resources at the same time. To design an individualized competing policy, we model the competition between agents in a novel multi-player multi-armed bandit (MPMAB) setting where players are selfish and aim to maximize their own rewards. When several players pull the same arm, these players averagely share the arms' rewards by expectation. Under this setting, we first analyze the Nash equilibrium when arms' rewards are known, then propose a novel Selfish MPMAB with Averaging Allocation (SMAA) approach based on the equilibrium. We theoretically demonstrate that SMAA could achieve a good regret guarantee for each player when all players follow the algorithm.},
  author = {Renzhe Xu and Haotian Wang 0001 and Xingxuan Zhang and Bo Li 0064 and Peng Cui 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023competing.pdf:pdf},
  mdate = {2024-07-21},
  pages = {38674--38706},
  pdf = {https://proceedings.mlr.press/v202/xu23q/xu23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Competing for Shareable Arms in Multi-Player Multi-Armed Bandits},
  url = {https://proceedings.mlr.press/v202/xu23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023mixflows,
  abstract = {This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.},
  author = {Zuheng Xu and Naitong Chen and Trevor Campbell},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023mixflows.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38342--38376},
  pdf = {https://proceedings.mlr.press/v202/xu23b/xu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {MixFlows: principled variational inference via mixed flows},
  url = {https://proceedings.mlr.press/v202/xu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023bit,
  abstract = {This paper considers the problem of bit allocation in Neural Video Compression (NVC). We reveal a fundamental relationship between bit allocation in NVC and Semi-Amortized Variational Inference (SAVI). Specifically, we show that SAVI with GoP (Group-of-Picture)-level likelihood is equivalent to pixel-level bit allocation with precise rate and quality dependency model. Based on this equivalence, we establish a new paradigm of bit allocation using SAVI. Different from previous bit allocation methods, our approach requires no empirical model and is thus optimal. We extend the SAVI to multi-level such as NVC by recursively applying back-propagating through gradient ascent, and propose a tractable approximation for practical implementation. Experimental results show that current state-of-the-art bit allocation algorithms still have room of approximately 0.5 dB PSNR to improve compared with ours.},
  author = {Tongda Xu and Han Gao and Chenjian Gao and Yuanyuan Wang and Dailan He and Jinyong Pi and Jixiang Luo and Ziyu Zhu and Mao Ye 0001 and Hongwei Qin and Yan Wang 0080 and Jingjing Liu and Ya-Qin Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023bit.pdf:pdf},
  mdate = {2024-07-16},
  pages = {38377--38399},
  pdf = {https://proceedings.mlr.press/v202/xu23c/xu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bit Allocation using Optimization},
  url = {https://proceedings.mlr.press/v202/xu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023regret,
  abstract = {The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.},
  author = {Wenhao Xu and Xuefeng Gao and Xuedong He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023regret.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38400--38427},
  pdf = {https://proceedings.mlr.press/v202/xu23d/xu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents},
  url = {https://proceedings.mlr.press/v202/xu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023probabilistic,
  abstract = {The studies on adversarial attacks and defenses have greatly improved the robustness of Deep Neural Networks (DNNs). Most advanced approaches have been overwhelmingly designed for continuous data such as images. However, these achievements are still hard to be generalized to categorical data. To bridge this gap, we propose a novel framework, Probabilistic Categorical Adversarial Attack (PCAA). It transfers the discrete optimization problem of finding categorical adversarial examples to a continuous problem that can be solved via gradient-based methods. We also propose a defense method, Probabilistic Categorical Adversarial Training (PCAT), based on PCAA. Extensive experiments on three real-world datasets demonstrate the effectiveness of our methods.},
  author = {Han Xu 0002 and Pengfei He and Jie Ren 0019 and Yuxuan Wan and Zitao Liu 0001 and Hui Liu 0031 and Jiliang Tang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023probabilistic.pdf:pdf},
  mdate = {2024-08-16},
  pages = {38428--38442},
  pdf = {https://proceedings.mlr.press/v202/xu23e/xu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Probabilistic Categorical Adversarial Attack and Adversarial Training},
  url = {https://proceedings.mlr.press/v202/xu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023hierarchical,
  abstract = {This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a three-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target design using a code tree. Concretely, a novel variant of a vector quantized VAE with "masked skip connection" extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the intended design. Extensive experiments demonstrate superior performance on conventional tasks such as random generation while enabling novel interaction capabilities on conditional generation tasks.},
  author = {Xiang Xu and Pradeep Kumar Jayaraman and Joseph George Lambourne and Karl D. D. Willis and Yasutaka Furukawa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023hierarchical.pdf:pdf},
  mdate = {2023-12-02},
  pages = {38443-38461},
  pdf = {https://proceedings.mlr.press/v202/xu23f/xu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Hierarchical Neural Coding for Controllable CAD Model Generation},
  url = {https://proceedings.mlr.press/v202/xu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023efficient,
  abstract = {This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks.},
  author = {Hainan Xu and Fei Jia and Somshubra Majumdar and He Huang and Shinji Watanabe and Boris Ginsburg},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023efficient.pdf:pdf},
  mdate = {2024-12-08},
  pages = {38462-38484},
  pdf = {https://proceedings.mlr.press/v202/xu23g/xu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Sequence Transduction by Jointly Predicting Tokens and Durations},
  url = {https://proceedings.mlr.press/v202/xu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023constrained,
  abstract = {We study the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes. We propose CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve it. Under certain regularity assumptions, we show that our algorithm enjoys the same cumulative regret bound as that in the unconstrained case and similar cumulative constraint violation upper bounds. For commonly used Matern and Squared Exponential kernels, our bounds are sublinear and allow us to derive a convergence rate to the optimal solution of the original constrained problem. In addition, our method naturally provides a scheme to declare infeasibility when the original black-box optimization problem is infeasible.},
  author = {Wenjie Xu and Yuning Jiang and Bratislav Svetozarevic and Colin N. Jones},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023constrained.pdf:pdf},
  mdate = {2025-02-14},
  pages = {38485-38498},
  pdf = {https://proceedings.mlr.press/v202/xu23h/xu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Constrained Efficient Global Optimization of Expensive Black-box Functions},
  url = {https://proceedings.mlr.press/v202/xu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023pareto,
  abstract = {The paper studies Pareto optimality in multi-objective multi-armed bandit by providing a formulation of adversarial multi-objective multi-armed bandit and defining its Pareto regrets that can be applied to both stochastic and adversarial settings. The regrets do not rely on any scalarization functions and reflect Pareto optimality compared to scalarized regrets. The authors also present new algorithms assuming both with and without prior information of the multi-objective multi-armed bandit setting. The algorithms are shown optimal in adversarial settings and nearly optimal up to a logarithmic factor in stochastic settings.},
  author = {Mengfan Xu and Diego Klabjan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023pareto.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38499-38517},
  pdf = {https://proceedings.mlr.press/v202/xu23i/xu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Pareto Regret Analyses in Multi-objective Multi-armed Bandit},
  url = {https://proceedings.mlr.press/v202/xu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023diverse,
  abstract = {The capability to generate responses with diversity and faithfulness using factual knowledge is paramount for creating a human-like, trustworthy dialogue system. Common strategies either adopt a two-step paradigm, which optimizes knowledge selection and response generation separately, and may overlook the inherent correlation between these two tasks, or leverage conditional variational method to jointly optimize knowledge selection and response generation by employing an inference network. In this paper, we present an end-to-end learning framework, termed Sequential Posterior Inference (SPI), capable of selecting knowledge and generating dialogues by approximately sampling from the posterior distribution. Unlike other methods, SPI does not require the inference network or assume a simple geometry of the posterior distribution. This straightforward and intuitive inference procedure of SPI directly queries the response generation model, allowing for accurate knowledge selection and generation of faithful responses. In addition to modeling contributions, our experimental results on two common dialogue datasets (Wizard of Wikipedia and Holl-E) demonstrate that SPI outperforms previous strong baselines according to both automatic and human evaluation metrics.},
  author = {Yan Xu and Deqian Kong and Dehong Xu and Ziwei Ji and Bo Pang and Pascale Fung and Ying Nian Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023diverse.pdf:pdf},
  mdate = {2025-06-24},
  pages = {38518-38534},
  pdf = {https://proceedings.mlr.press/v202/xu23j/xu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference},
  url = {https://proceedings.mlr.press/v202/xu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023quantifying,
  abstract = {Recent studies empirically demonstrate the positive relationship between the transferability of neural networks and the within-class variation of the last layer features. The recently discovered Neural Collapse (NC) phenomenon provides a new perspective of understanding such last layer geometry of neural networks. In this paper, we propose a novel metric, named Variability Collapse Index (VCI), to quantify the variability collapse phenomenon in the NC paradigm. The VCI metric is well-motivated and intrinsically related to the linear probing loss on the last layer features. Moreover, it enjoys desired theoretical and empirical properties, including invariance under invertible linear transformations and numerical stability, that distinguishes it from previous metrics.},
  author = {Jing Xu and Haoxiong Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023quantifying.pdf:pdf},
  mdate = {2023-08-29},
  pages = {38535-38550},
  pdf = {https://proceedings.mlr.press/v202/xu23k/xu23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantifying the Variability Collapse of Neural Networks},
  url = {https://proceedings.mlr.press/v202/xu23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023pfgm,
  abstract = {We introduce a new family of physics-inspired generative models termed PFGM++ that unifies diffusion models and Poisson Flow Generative Models (PFGM). These models realize generative trajectories for N dimensional data by embedding paths in N+D dimensional space while still controlling the progression with a simple scalar norm of the D additional variables. The new models reduce to PFGM when D=1 and to diffusion models when D→∞. The flexibility of choosing D allows us to trade off robustness against rigidity as increasing D results in more concentrated coupling between the data and the additional variable norms. We dispense with the biased large batch field targets used in PFGM and instead provide an unbiased perturbation-based objective similar to diffusion models. To explore different choices of D, we provide a direct alignment method for transferring well-tuned hyperparameters from diffusion models (D→∞) to any finite D values. Our experiments show that models with finite D can be superior to previous state-of-the-art diffusion models on CIFAR-10/FFHQ 64×64 datasets, with FID scores of 1.91/2.43 when D=2048/128. In class-conditional setting, D=2048 yields current state-of-the-art FID of 1.74 on CIFAR-10. In addition, we demonstrate that models with smaller D exhibit improved robustness against modeling errors.},
  author = {Yilun Xu and Ziming Liu and Yonglong Tian and Shangyuan Tong and Max Tegmark and Tommi S. Jaakkola},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023pfgm.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38566-38591},
  pdf = {https://proceedings.mlr.press/v202/xu23m/xu23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{PFGM}++: Unlocking the Potential of Physics-Inspired Generative Models},
  url = {https://proceedings.mlr.press/v202/xu23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023geometric,
  abstract = {Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7% improvement for the valid percentage of large biomolecules. Results also demonstrate GeoLDM's higher capacity for controllable generation thanks to the latent modeling.},
  author = {Minkai Xu and Alexander S. Powers and Ron O. Dror and Stefano Ermon and Jure Leskovec},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023geometric.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38592-38610},
  pdf = {https://proceedings.mlr.press/v202/xu23n/xu23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Geometric Latent Diffusion Models for {3D} Molecule Generation},
  url = {https://proceedings.mlr.press/v202/xu23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023sequential,
  abstract = {We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the sequential predictive conformal inference (SPCI). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empirical coverage.},
  author = {Chen Xu and Yao Xie},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023sequential.pdf:pdf},
  mdate = {2024-01-29},
  pages = {38707-38727},
  pdf = {https://proceedings.mlr.press/v202/xu23r/xu23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Predictive Conformal Inference for Time Series},
  url = {https://proceedings.mlr.press/v202/xu23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023protst,
  abstract = {Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation.},
  author = {Minghao Xu and Xinyu Yuan and Santiago Miret and Jian Tang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023protst.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38749-38767},
  pdf = {https://proceedings.mlr.press/v202/xu23t/xu23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ProtST}: Multi-Modality Learning of Protein Sequences and Biomedical Texts},
  url = {https://proceedings.mlr.press/v202/xu23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023mplug2,
  abstract = {Recent years have witnessed a big convergence of language, vision, and multi-modal pretraining. In this work, we present mPLUG-2, a new unified paradigm with modularized design for multi-modal pretraining, which can benefit from modality collaboration while addressing the problem of modality entanglement. In contrast to predominant paradigms of solely relying on sequence-to-sequence generation or encoder-based instance discrimination, mPLUG-2 introduces a multi-module composition network by sharing common universal modules for modality collaboration and disentangling different modality modules to deal with modality entanglement. It is flexible to select different modules for different understanding and generation tasks across all modalities including text, image, and video. Empirical study shows that mPLUG-2 achieves state-of-the-art or competitive results on a broad range of over 30 downstream tasks, spanning multi-modal tasks of image-text and video-text understanding and generation, and uni-modal tasks of text-only, image-only, and video-only understanding. Notably, mPLUG-2 shows new state-of-the-art results of 48.0 top-1 accuracy and 80.3 CIDEr on the challenging MSRVTT video QA and video caption tasks with a far smaller model size and data scale. It also demonstrates strong zero-shot transferability on vision-language and video-language tasks.},
  author = {Haiyang Xu 0001 and Qinghao Ye and Ming Yan 0008 and Yaya Shi and Jiabo Ye and Yuanhong Xu and Chenliang Li and Bin Bi and Qi Qian 0001 and Wei Wang 0225 and Guohai Xu and Ji Zhang 0011 and Songfang Huang and Fei Huang 0002 and Jingren Zhou 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023mplug2.pdf:pdf},
  mdate = {2025-06-25},
  pages = {38728--38748},
  pdf = {https://proceedings.mlr.press/v202/xu23s/xu23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video},
  url = {https://proceedings.mlr.press/v202/xu23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023bayesian,
  abstract = {We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to create ``algorithmic beliefs'' at each round, and use Bayesian posteriors to make decisions. The optimization objective to create ``algorithmic beliefs,'' which we term ``Algorithmic Information Ratio,'' represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. This is the first approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the ``best-of-all-worlds'' empirical performance in the stochastic, adversarial, and non-stationary environments. And we illustrate how these principles can be used in linear bandits, convex bandits, and reinforcement learning.},
  author = {Yunbei Xu and Assaf Zeevi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023bayesian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38768--38800},
  pdf = {https://proceedings.mlr.press/v202/xu23u/xu23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bayesian Design Principles for Frequentist Sequential Learning},
  url = {https://proceedings.mlr.press/v202/xu23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023slamb,
  abstract = {Distributed training of large deep neural networks requires frequent exchange of massive data between machines, thus communication efficiency is a major concern. Existing compressed communication methods are either not compatible with large batch optimization algorithms, or do not provide sufficient speedup in large scale. In this paper, we combine sparsification-based gradient compression with the layer-wise adaptive moments optimizer for large batch training (LAMB). We propose SLAMB, a novel communication-efficient optimizer that supports large batch sizes and scales to thousands of GPUs. SLAMB employs momentum masking, local error compensation, and element-wise adaptive rescaling to achieve accurate layer-wise weight updates, which translates to fast convergence for very large batches. Our empirical results show that, compared to the state-of-the-art, SLAMB transmits half the amount of data in large-batch BERT pre-training, without sacrificing accuracy.},
  author = {Hang Xu and Wenxuan Zhang 0001 and Jiawei Fei and Yuzhe Wu and Tingwen Xie and Jun Huang and Yuchen Xie and Mohamed Elhoseiny and Panos Kalnis},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023slamb.pdf:pdf},
  mdate = {2025-02-24},
  pages = {38801--38825},
  pdf = {https://proceedings.mlr.press/v202/xu23v/xu23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SLAMB: Accelerated Large Batch Training with Sparse Communication},
  url = {https://proceedings.mlr.press/v202/xu23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023not,
  abstract = {Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to 200× faster and 18.8\% more accurate than existing methods.},
  author = {Peng Xu and Lin Zhang and Xuanzhou Liu and Jiaqi Sun and Yue Zhao 0016 and Haiqin Yang and Bei Yu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023not.pdf:pdf},
  mdate = {2024-01-24},
  pages = {38826--38847},
  pdf = {https://proceedings.mlr.press/v202/xu23w/xu23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/xu23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xu2023instrumental,
  abstract = {Off-policy evaluation (OPE) aims to estimate the return of a target policy using some pre-collected observational data generated by a potentially different behavior policy. In many cases, there exist unmeasured variables that confound the action-reward or action-next-state relationships, rendering many existing OPE approaches ineffective. This paper develops an instrumental variable (IV)-based method for consistent OPE in confounded Markov decision processes (MDPs). Similar to single-stage decision making, we show that IV enables us to correctly identify the target policy's value in infinite horizon settings as well. Furthermore, we propose an efficient and robust value estimator and illustrate its effectiveness through extensive simulations and analysis of real data from a world-leading short-video platform.},
  author = {Yang Xu and Jin Zhu and Chengchun Shi and Shikai Luo and Rui Song 0006},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xu2023instrumental.pdf:pdf},
  mdate = {2024-01-26},
  pages = {38848--38880},
  pdf = {https://proceedings.mlr.press/v202/xu23x/xu23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Instrumental Variable Approach to Confounded Off-Policy Evaluation},
  url = {https://proceedings.mlr.press/v202/xu23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xue2023nearoptimal,
  abstract = {$k$-Clustering in $\mathbb{R}^d$ (e.g., $k$-median and $k$-means) is a fundamental machine learning problem. While near-linear time approximation algorithms were known in the classical setting for a dataset with cardinality $n$, it remains open to find sublinear-time quantum algorithms. We give quantum algorithms that find coresets for $k$-clustering in $\mathbb{R}^d$ with $\tilde{O}(\sqrt{nk}d^{3/2})$ query complexity. Our coreset reduces the input size from $n$ to $\mathrm{poly}(k\varepsilon^{-1}d)$, so that existing $\alpha$-approximation algorithms for clustering can run on top of it and yield $(1 + \varepsilon)\alpha$-approximation. This eventually yields a quadratic speedup for various $k$-clustering approximation algorithms. We complement our algorithm with a nearly matching lower bound, that any quantum algorithm must make $\Omega(\sqrt{nk})$ queries in order to achieve even $O(1)$-approximation for $k$-clustering.},
  author = {Yecheng Xue and Xiaoyu Chen and Tongyang Li and Shaofeng H.-C. Jiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xue2023nearoptimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38881--38912},
  pdf = {https://proceedings.mlr.press/v202/xue23a/xue23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Near-Optimal Quantum Coreset Construction Algorithms for Clustering},
  url = {https://proceedings.mlr.press/v202/xue23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xue2023study,
  abstract = {Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are often adopted. For example, we usually set the base model with hidden size (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations by studying the the relationship between transformer configuration and training objective. We show that the optimal transformer configuration is closely related to the training objective. Specifically, compared with the simple classification objective, the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training.},
  author = {Fuzhao Xue and Jianghai Chen and Aixin Sun and Xiaozhe Ren and Zangwei Zheng and Xiaoxin He and Yongming Chen and Xin Jiang 0002 and Yang You 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xue2023study.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38913--38925},
  pdf = {https://proceedings.mlr.press/v202/xue23b/xue23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Study on Transformer Configuration and Training Objective},
  url = {https://proceedings.mlr.press/v202/xue23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023lazygnn,
  abstract = {Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks.},
  author = {Rui Xue 0006 and Haoyu Han 0001 and MohamadAli Torkamani and Jian Pei 0001 and Xiaorui Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023lazygnn.pdf:pdf},
  mdate = {2025-02-28},
  pages = {38926--38937},
  pdf = {https://proceedings.mlr.press/v202/xue23c/xue23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation},
  url = {https://proceedings.mlr.press/v202/xue23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xue2023which,
  abstract = {Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of class collapse or feature suppression at test time. We provide the first unified theoretically rigorous framework to determine which features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features.},
  author = {Yihao Xue and Siddharth Joshi 0004 and Eric Gan and Pin-Yu Chen and Baharan Mirzasoleiman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xue2023which.pdf:pdf},
  mdate = {2024-10-18},
  pages = {38938--38970},
  pdf = {https://proceedings.mlr.press/v202/xue23d/xue23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression},
  url = {https://proceedings.mlr.press/v202/xue23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{xue2023adaptive,
  abstract = {Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data.},
  author = {Fuzhao Xue and Valerii Likhosherstov and Anurag Arnab and Neil Houlsby and Mostafa Dehghani 0001 and Yang You 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/xue2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38971--38988},
  pdf = {https://proceedings.mlr.press/v202/xue23e/xue23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Computation with Elastic Input Sequence},
  url = {https://proceedings.mlr.press/v202/xue23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yamagata2023qlearning,
  abstract = {Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability -- one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the Dynamic Programming results to relabel the return-to-go in the training data to then train the DT with the relabelled data. Our approach efficiently exploits the benefits of these two approaches and compensates for each other's shortcomings to achieve better performance.},
  author = {Taku Yamagata and Ahmed Khalil and Ra{\'u}l Santos-Rodr{\'i}guez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yamagata2023qlearning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {38989--39007},
  pdf = {https://proceedings.mlr.press/v202/yamagata23a/yamagata23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline {RL}},
  url = {https://proceedings.mlr.press/v202/yamagata23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yamasaki2023quantum,
  abstract = {A significant challenge in the field of quantum machine learning (QML) is to establish applications of quantum computation to accelerate common tasks in machine learning such as those for neural networks. Ridgelet transform has been a fundamental mathematical tool in the theoretical studies of neural networks, but the practical applicability of ridgelet transform to conducting learning tasks was limited since its numerical implementation by conventional classical computation requires an exponential runtime $\exp(O(D))$ as data dimension $D$ increases. To address this problem, we develop a quantum ridgelet transform (QRT), which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation. As an application, we also show that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network. This application discovers an efficient way in this regime to demonstrate the lottery ticket hypothesis on finding such a sparse trainable neural network. These results open an avenue of QML for accelerating learning tasks with commonly used classical neural networks.},
  author = {Hayata Yamasaki and Sathyawageeswar Subramanian and Satoshi Hayakawa and Sho Sonoda},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yamasaki2023quantum.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39008--39034},
  pdf = {https://proceedings.mlr.press/v202/yamasaki23a/yamasaki23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation},
  url = {https://proceedings.mlr.press/v202/yamasaki23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023fast,
  abstract = {Multi-player online games depict the interaction of multiple players with each other over time. Strongly monotone games are of particular interest since they have benign properties and also relate to many classic games that have applications in real life. Existing works mainly focus on the time-invariant case with provable guarantees established. However, the research of the more general time-varying games in changing environments is underexplored and the best-known result cannot match the guarantees in the time-invariant case. In this work, we present a new decentralized online algorithm for time-varying strongly monotone games, which greatly improves existing results and obtains fast rates, matching the best time-invariant guarantee without knowing the environmental non-stationarity. Furthermore, to achieve faster rates, we generalize the RVU property with smoothness and establish a series of problem-dependent bounds that also match the best time-invariant one. To realize all those results, we make a comprehensive use of the techniques in non-stationary and universal online learning.},
  author = {Yu-Hu Yan and Peng Zhao and Zhi-Hua Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39138--39164},
  pdf = {https://proceedings.mlr.press/v202/yan23f/yan23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Rates in Time-Varying Strongly Monotone Games},
  url = {https://proceedings.mlr.press/v202/yan23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023compressed,
  abstract = {We first propose a decentralized proximal stochastic gradient tracking method (DProxSGT) for nonconvex stochastic composite problems, with data heterogeneously distributed on multiple workers in a decentralized connected network. To save communication cost, we then extend DProxSGT to a compressed method by compressing the communicated information. Both methods need only O(1) samples per worker for each proximal update, which is important to achieve good generalization performance on training deep neural networks. With a smoothness condition on the expected loss function (but not on each sample function), the proposed methods can achieve an optimal sample complexity result to produce a near-stationary point. Numerical experiments on training neural networks demonstrate the significantly better generalization performance of our methods over large-batch training methods and momentum variance-reduction methods and also, the ability of handling heterogeneous data by the gradient tracking scheme.},
  author = {Yonggui Yan and Jie Chen and Pin-Yu Chen and Xiaodong Cui and Songtao Lu and Yangyang Xu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023compressed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39035--39061},
  pdf = {https://proceedings.mlr.press/v202/yan23a/yan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Compressed Decentralized Proximal Stochastic Gradient Method for Nonconvex Composite Problems with Heterogeneous Data},
  url = {https://proceedings.mlr.press/v202/yan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023temporally,
  abstract = {To generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks exist for video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer embeddings, applying a temporal transformer, and expanding back using a spatial MaskGit, TECO outperforms existing models across many metrics.},
  author = {Wilson Yan and Danijar Hafner and Stephen James and Pieter Abbeel},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023temporally.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39062--39098},
  pdf = {https://proceedings.mlr.press/v202/yan23b/yan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Temporally Consistent Transformers for Video Generation},
  url = {https://proceedings.mlr.press/v202/yan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023distortion,
  abstract = {Standard MSE or MAE loss function is commonly used in limited field-of-vision depth completion, treating each pixel equally under a basic assumption that all pixels have same contribution during optimization. Recently, with the rapid rise of panoramic photography, panoramic depth completion (PDC) has raised increasing attention in 3D computer vision. However, the assumption is inapplicable to panoramic data due to its latitude-wise distortion and high uncertainty nearby textures and edges. To handle these challenges, we propose distortion and uncertainty aware loss (DUL) that consists of a distortion-aware loss and an uncertainty-aware loss. The distortion-aware loss is designed to tackle the panoramic distortion caused by equirectangular projection, whose coordinate transformation relation is used to adaptively calculate the weight of the latitude-wise distortion, distributing uneven importance instead of the equal treatment for each pixel. The uncertainty-aware loss is presented to handle the inaccuracy in non-smooth regions. Specifically, we characterize uncertainty into PDC solutions under Bayesian deep learning framework, where a novel consistent uncertainty estimation constraint is designed to learn the consistency between multiple uncertainty maps of a single panorama. This consistency constraint allows model to produce more precise uncertainty estimation that is robust to feature deformation. Extensive experiments show the superiority of our method over standard loss functions, reaching the state of the art.},
  author = {Zhiqiang Yan and Xiang Li and Kun Wang and Shuo Chen and Jun Li and Jian Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023distortion.pdf:pdf},
  mdate = {2025-05-22},
  pages = {39099--39109},
  pdf = {https://proceedings.mlr.press/v202/yan23c/yan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Distortion and Uncertainty Aware Loss for Panoramic Depth Completion},
  url = {https://proceedings.mlr.press/v202/yan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023selfinterpretable,
  abstract = {Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.},
  author = {Jingquan Yan and Hao Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023selfinterpretable.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39110--39125},
  pdf = {https://proceedings.mlr.press/v202/yan23d/yan23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Self-Interpretable Time Series Prediction with Counterfactual Explanations},
  url = {https://proceedings.mlr.press/v202/yan23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yan2023quantum,
  abstract = {Learning 3D graph with spatial position as well as node attributes has been recently actively studied, for its utility in different applications e.g. 3D molecules. Quantum computing is known a promising direction for its potential theoretical supremacy for large-scale graph and combinatorial problem as well as the increasing evidence for the availability to physical quantum devices in the near term. In this paper, for the first time to our best knowledge, we propose a quantum 3D embedding ansatz that learns the latent representation of 3D structures from the Hilbert space composed of the Bloch sphere of each qubit. Specifically, the 3D Cartesian coordinates of nodes are converted into rotation and torsion angles and then encode them into the form of qubits. Moreover, Parameterized Quantum Circuit (PQC) is applied to serve as the trainable layers and the output of the PQC is adopted as the final node embedding. Experimental results on two downstream tasks, molecular property prediction and 3D molecular geometries generation, demonstrate the effectiveness of our model. We show the capacity and capability of our model with the evaluation on the QM9 dataset (134k molecules) with very few parameters, and its potential to be executed on a real quantum device.},
  author = {Ge Yan and Huaijin Wu and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yan2023quantum.pdf:pdf},
  mdate = {2023-08-31},
  pages = {39126--39137},
  pdf = {https://proceedings.mlr.press/v202/yan23e/yan23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum {3D} Graph Learning with Applications to Molecule Embedding},
  url = {https://proceedings.mlr.press/v202/yan23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yanagisawa2023proper,
  abstract = {Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.},
  author = {Hiroki Yanagisawa},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yanagisawa2023proper.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39165--39182},
  pdf = {https://proceedings.mlr.press/v202/yanagisawa23a/yanagisawa23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Proper Scoring Rules for Survival Analysis},
  url = {https://proceedings.mlr.press/v202/yanagisawa23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023continual,
  abstract = {How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. CoTASP trains a policy for each task by optimizing the prompts and the sub-network weights alternatively. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing tasks' semantic correlations.},
  author = {Yijun Yang and Tianyi Zhou and Jing Jiang and Guodong Long and Yuhui Shi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023continual.pdf:pdf},
  mdate = {2025-03-31},
  pages = {39623--39638},
  pdf = {https://proceedings.mlr.press/v202/yang23t/yang23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continual Task Allocation in Meta-Policy Network via Sparse Prompting},
  url = {https://proceedings.mlr.press/v202/yang23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023towards,
  abstract = {Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing {GNN} models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of {GNNs} while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. The proposed method is simple to implement but achieves significant improvements in graph learning tasks.},
  author = {Mingqi Yang and Wenjie Feng and Yanming Shen and Bryan Hooi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023towards.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39234--39251},
  pdf = {https://proceedings.mlr.press/v202/yang23c/yang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Better Graph Representation Learning with Parameterized Decomposition \& Filtering},
  url = {https://proceedings.mlr.press/v202/yang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023behavior,
  abstract = {In reinforcement learning, unsupervised skill discovery aims to learn diverse skills without extrinsic rewards. Previous methods discover skills by maximizing the mutual information ({MI}) between states and skills. However, such an {MI} objective tends to learn simple and static skills and may hinder exploration. In this paper, we propose a novel unsupervised skill discovery method through contrastive learning among behaviors, which makes the agent produce similar behaviors for the same skill and diverse behaviors for different skills. Under mild assumptions, our objective maximizes the {MI} between different behaviors based on the same skill, which serves as an upper bound of the previous {MI} objective. Meanwhile, our method implicitly increases the state entropy to obtain better state coverage. We evaluate our method on challenging mazes and continuous control tasks and find it generates diverse and far-reaching skills with competitive performance compared to state-of-the-art methods.},
  author = {Rushuai Yang and Chenjia Bai and Hongyi Guo and Siyuan Li and Bin Zhao and Zhen Wang and Peng Liu and Xuelong Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023behavior.pdf:pdf},
  mdate = {2024-11-25},
  pages = {39183--39204},
  pdf = {https://proceedings.mlr.press/v202/yang23a/yang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Behavior Contrastive Learning for Unsupervised Skill Discovery},
  url = {https://proceedings.mlr.press/v202/yang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023nested,
  abstract = {We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination ({NE}), which is inspired by the nested structure implied by the information-theoretic lower bound. {NE} is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, {NE} utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of {NE}. We also show {NE} achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theoretical findings.},
  author = {Junwen Yang and Yifan Feng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023nested.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39205--39233},
  pdf = {https://proceedings.mlr.press/v202/yang23b/yang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nested Elimination: A Simple Algorithm for Best-Item Identification From Choice-Based Feedback},
  url = {https://proceedings.mlr.press/v202/yang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023chemically,
  abstract = {Coarse-graining ({CG}) accelerates molecular simulations of protein dynamics by simulating sets of atoms as singular beads. Backmapping is the opposite operation of bringing lost atomistic details back from the {CG} representation. While machine learning ({ML}) has produced accurate and efficient {CG} simulations of proteins, fast and reliable backmapping remains a challenge. Rule-based methods produce poor all-atom geometries, needing computationally costly refinement through additional simulations. Recently proposed {ML} approaches outperform traditional baselines but are not transferable between proteins and sometimes generate unphysical atom placements with steric clashes and implausible torsion angles. This work addresses both issues to build a fast, transferable, and reliable generative backmapping tool for {CG} protein representations. We achieve generalization and reliability through a combined set of innovations: representation based on internal coordinates; an equivariant encoder/prior; a custom loss function that helps ensure local structure, global structure, and physical constraints; and expert curation of high-quality out-of-equilibrium protein data for training. Our results pave the way for out-of-the-box backmapping of coarse-grained simulations for arbitrary proteins.},
  author = {Soojung Yang and Rafael G{\'o}mez-Bombarelli},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023chemically.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39277--39298},
  pdf = {https://proceedings.mlr.press/v202/yang23e/yang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Chemically Transferable Generative Backmapping of Coarse-Grained Proteins},
  url = {https://proceedings.mlr.press/v202/yang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023improving,
  abstract = {Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on clean examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.},
  author = {Dongyoon Yang and Insung Kong and Yongdai Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39331--39348},
  pdf = {https://proceedings.mlr.press/v202/yang23h/yang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples},
  url = {https://proceedings.mlr.press/v202/yang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023improving,
  abstract = {Deep equilibrium ({DEQ}) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general {DEQs} becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general {DEQ} models with the widely-used adversarial training ({AT}) framework, but they fail to exploit the structural uniquenesses of {DEQ} models. To this end, we interpret {DEQs} through the lens of neural dynamics and find that {AT} under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During {AT}, we also utilize random intermediate states to compute the loss function. Our methods regulate the neural dynamics of {DEQ} models in this manner. Extensive experiments demonstrate that our methods substantially increase the robustness of {DEQ} models and even outperform the strong deep network baselines.},
  author = {Zonghan Yang and Peng Li and Tianyu Pang and Yang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023improving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39349--39364},
  pdf = {https://proceedings.mlr.press/v202/yang23i/yang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Adversarial Robustness of Deep Equilibrium Models with Explicit Regulations Along the Neural Dynamics},
  url = {https://proceedings.mlr.press/v202/yang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023theory,
  abstract = {The successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally {NNGPs}) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes ({DGPs}) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, which leads to practical algorithmic implications. We call the resulting practical algorithm deep kernel machines ({DKMs}). Unlike most recent work on deep learning theory, our work not only proposes a more accurate theoretical model, but also provides a theoretical basis for a practical algorithm which we show performs well in practice.},
  author = {Adam X. Yang and Maxime Robeyns and Edward Milsom and Ben Anson and Nandi Schoots and Laurence Aitchison},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023theory.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39380--39415},
  pdf = {https://proceedings.mlr.press/v202/yang23k/yang23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A theory of representation learning gives a deep generalisation of kernel methods},
  url = {https://proceedings.mlr.press/v202/yang23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023efficient,
  abstract = {We consider the problem of graph matching, or learning vertex correspondence, between two correlated stochastic block models ({SBMs}). The graph matching problem arises in various fields, including computer vision, natural language processing and bioinformatics, and in particular, matching graphs with inherent community structure has significance related to de-anonymization of correlated social networks. Compared to the correlated Erd{\H o}s-R{\'e}nyi ({ER}) model, where various efficient algorithms have been developed, among which a few algorithms have been proven to achieve the exact matching with constant edge correlation, no low-order polynomial algorithm has been known to achieve exact matching for the correlated {SBMs} with constant correlation. This work proposes an efficient algorithm for matching graphs with community structure, based on the comparison between partition trees rooted from each vertex, by extending the idea of Mao et al. (2021) to graphs with communities. Theoretically, we prove that our algorithm succeeds in exact recovery with high probability when the edge probability is {$\Omega(n^{-1+\epsilon})$} for any {$\epsilon > 0$} under the assumption that the number of communities is bounded. Experimentally, we demonstrate that our algorithm outperforms existing methods in various settings.},
  author = {Joonhyuk Yang and Dongpil Shin and Hye Won Chung},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023efficient.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39416--39452},
  pdf = {https://proceedings.mlr.press/v202/yang23l/yang23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient Algorithms for Exact Graph Matching on Correlated Stochastic Block Models with Constant Correlation},
  url = {https://proceedings.mlr.press/v202/yang23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023neurons,
  abstract = {Recent work has observed an intriguing "Neural Collapse" phenomenon in well-trained neural networks, where the last-layer representations of training samples with the same label collapse into each other. This appears to suggest that the last-layer representations are completely determined by the labels, and do not depend on the intrinsic structure of input distribution. We provide evidence that this is not a complete description, and that the apparent collapse hides important fine-grained structure in the representations. Specifically, even when representations apparently collapse, the small amount of remaining variation can still faithfully and accurately capture the intrinsic structure of input distribution. As an example, if we train on {CIFAR}-10 using only 5 coarse-grained labels (by combining two classes into one super-class) until convergence, we can reconstruct the original 10-class labels from the learned representations via unsupervised clustering. The reconstructed labels achieve 93\% accuracy on the {CIFAR}-10 test set, nearly matching the normal {CIFAR}-10 accuracy for the same architecture. We also provide an initial theoretical result showing the fine-grained representation structure in a simplified synthetic setting. Our results show concretely how the structure of input data can play a significant role in determining the fine-grained structure of neural representations, going beyond what Neural Collapse predicts.},
  author = {Yongyi Yang and Jacob Steinhardt and Wei Hu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023neurons.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39453--39487},
  pdf = {https://proceedings.mlr.press/v202/yang23m/yang23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations},
  url = {https://proceedings.mlr.press/v202/yang23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023generative,
  abstract = {Despite the success of equivariant neural networks in scientific applications, they require knowing the symmetry group a priori. However, it may be difficult to know which symmetry to use as an inductive bias in practice. Enforcing the wrong symmetry could even hurt the performance. In this paper, we propose a framework, {LieGAN}, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. Specifically, a generator learns a group of transformations applied to the data, which preserve the original distribution and fool the discriminator. {LieGAN} represents symmetry as interpretable Lie algebra basis and can discover various symmetries such as the rotation group {SO(n)}, restricted Lorentz group {SO(1,3)+} in trajectory prediction and top-quark tagging tasks. The learned symmetry can also be readily used in several existing equivariant neural networks to improve accuracy and generalization in prediction.},
  author = {Jianke Yang and Robin Walters and Nima Dehmamy and Rose Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023generative.pdf:pdf},
  mdate = {2024-04-29},
  pages = {39488--39508},
  pdf = {https://proceedings.mlr.press/v202/yang23n/yang23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Adversarial Symmetry Discovery},
  url = {https://proceedings.mlr.press/v202/yang23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023boosting,
  abstract = {Training practical agents usually involve offline and online reinforcement learning (RL) to balance the policy's performance and interaction costs. In particular, online fine-tuning has become a commonly used method to correct the erroneous estimates of out-of-distribution data learned in the offline training phase. However, even limited online interactions can be inaccessible or catastrophic for high-stake scenarios like healthcare and autonomous driving. In this work, we introduce an interaction-free training scheme dubbed Offline-with-Action-Preferences (OAP). The main insight is that, compared to online fine-tuning, querying the preferences between pre-collected and learned actions can be equally or even more helpful to the erroneous estimate problem. By adaptively encouraging or suppressing policy constraint according to action preferences, OAP could distinguish overestimation from beneficial policy improvement and thus attains a more accurate evaluation of unseen data. Theoretically, we prove a lower bound of the behavior policy's performance improvement brought by OAP. Moreover, comprehensive experiments on the D4RL benchmark and state-of-the-art algorithms demonstrate that OAP yields higher (29% on average) scores, especially on challenging AntMaze tasks (98% higher).},
  author = {Qisen Yang and Shenzhi Wang and Matthieu Gaetan Lin and Shiji Song and Gao Huang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023boosting.pdf:pdf},
  mdate = {2024-08-20},
  pages = {39509--39523},
  pdf = {https://proceedings.mlr.press/v202/yang23o/yang23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Boosting Offline Reinforcement Learning with Action Preference Query},
  url = {https://proceedings.mlr.press/v202/yang23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023towards,
  abstract = {The mission of active learning is to identify the most valuable data samples, thus attaining decent performance with much fewer samples. The data augmentation techniques seem straightforward yet promising to enhance active learning by extending the exploration of the input space, which helps locate more valuable samples. In this work, we thoroughly study the coupling of data augmentation and active learning, thereby proposing Controllable Augmentation ManiPulator for Active Learning. In contrast to the few prior works that touched on this line, CAMPAL emphasizes a purposeful, tighten, and better-controlled integration of data augmentation into active learning in three folds: (i) carefully designed augmentation policies applied separately on labeled and unlabeled data pools; (ii) controlled and quantifiably optimizable augmentation strengths; (iii) full and flexible coverage for most (if not all) active learning schemes. Theories are proposed and associated with the development of key components in CAMPAL. Through extensive empirical experiments, we bring the performance of active learning methods to a new level: an absolute performance boost of 16.99% on CIFAR-10 and 12.25% on SVHN with 1,000 annotated samples.},
  author = {Jianan Yang and Haobo Wang and Sai Wu and Gang Chen and Junbo Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023towards.pdf:pdf},
  mdate = {2025-05-22},
  pages = {39524--39542},
  pdf = {https://proceedings.mlr.press/v202/yang23p/yang23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Controlled Data Augmentations for Active Learning},
  url = {https://proceedings.mlr.press/v202/yang23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023what,
  abstract = {Offline goal-conditioned RL (GCRL) offers a way to train general-purpose agents from fully offline datasets. In addition to being conservative within the dataset, the generalization ability to achieve unseen goals is another fundamental challenge for offline GCRL. However, to the best of our knowledge, this problem has not been well studied yet. In this paper, we study out-of-distribution (OOD) generalization of offline GCRL both theoretically and empirically to identify factors that are important. In a number of experiments, we observe that weighted imitation learning enjoys better generalization than pessimism-based offline RL method. Based on this insight, we derive a theory for OOD generalization, which characterizes several important design choices. We then propose a new offline GCRL method, Generalizable Offline goAl-condiTioned RL (GOAT), by combining the findings from our theoretical and empirical studies. On a new benchmark containing 9 independent identically distributed (IID) tasks and 17 OOD tasks, GOAT outperforms current state-of-the-art methods by a large margin.},
  author = {Rui Yang and Yong Lin and Xiaoteng Ma and Hao Hu and Chongjie Zhang and Tong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023what.pdf:pdf},
  mdate = {2023-10-18},
  pages = {39543--39571},
  pdf = {https://proceedings.mlr.press/v202/yang23q/yang23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {What is Essential for Unseen Goal Generalization of Offline Goal-conditioned {RL}?},
  url = {https://proceedings.mlr.press/v202/yang23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023neural,
  abstract = {Deep neural networks have long been criticized for lacking the ability to perform analogical visual reasoning. Here, we propose a neural network model to solve Raven's Progressive Matrices (RPM) - one of the standard intelligence tests in human psychology. Specifically, we design a reasoning block based on the well-known concept of prediction error (PE) in neuroscience. Our reasoning block uses convolution to extract abstract rules from high-level visual features of the 8 context images and generates the features of a predicted answer. PEs are then calculated between the predicted features and those of the 8 candidate answers, and are then passed to the next stage. We further integrate our novel reasoning blocks into a residual network and build a new Predictive Reasoning Network (PredRNet). Extensive experiments show that our proposed PredRNet achieves state-of-the-art average performance on several important RPM benchmarks. PredRNet also shows good generalization abilities in a variety of out-of-distribution scenarios and other visual reasoning tasks. Most importantly, our PredRNet forms low-dimensional representations of abstract rules and minimizes hierarchical prediction errors during model training, supporting the critical role of PE minimization in visual reasoning.},
  author = {Lingxiao Yang and Hongzhi You and Zonglei Zhen and Dahui Wang and Xiaohong Wan and Xiaohua Xie and Ru-Yuan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023neural.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39572--39583},
  pdf = {https://proceedings.mlr.press/v202/yang23r/yang23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Neural Prediction Errors enable Analogical Visual Reasoning in Human Standard Intelligence Tests},
  url = {https://proceedings.mlr.press/v202/yang23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yang2023change,
  abstract = {Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information. Finally, unlike existing works that solely aim to improve worst-group accuracy (WGA), we demonstrate the fundamental tradeoff between WGA and other important metrics, highlighting the need to carefully choose testing metrics.},
  author = {Yuzhe Yang and Haoran Zhang and Dina Katabi and Marzyeh Ghassemi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yang2023change.pdf:pdf},
  mdate = {2025-06-25},
  pages = {39584--39622},
  pdf = {https://proceedings.mlr.press/v202/yang23s/yang23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Change is Hard: A Closer Look at Subpopulation Shift},
  url = {https://proceedings.mlr.press/v202/yang23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yao2023which,
  abstract = {In real life, accurately annotating large-scale datasets is sometimes difficult. Datasets used for training deep learning models are likely to contain label noise. To make use of the dataset containing label noise, two typical methods have been proposed. One is to employ the semi-supervised method by exploiting labeled confident examples and unlabeled unconfident examples. The other one is to model label noise and design statistically consistent classifiers. A natural question remains unsolved: which one should be used for a specific real-world application? In this paper, we answer the question from the perspective of causal data generative process. Specifically, the performance of the semi-supervised based method depends heavily on the data generative process while the method modeling label-noise is not influenced by the generation process. For example, for a given dataset, if it has a causal generative structure that the features cause the label, the semi-supervised based method would not be helpful. When the causal structure is unknown, we provide an intuitive method to discover the causal structure for a given dataset containing label noise.},
  author = {Yu Yao and Mingming Gong and Yuxuan Du and Jun Yu and Bo Han and Kun Zhang and Tongliang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yao2023which.pdf:pdf},
  mdate = {2025-02-10},
  pages = {39660--39673},
  pdf = {https://proceedings.mlr.press/v202/yao23a/yao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Which is Better for Learning with Noisy Labels: The Semi-supervised Method or Modeling Label Noise?},
  url = {https://proceedings.mlr.press/v202/yao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yao2023how,
  abstract = {Content creators compete for exposure on recommendation platforms, and such strategic behavior leads to a dynamic shift over the content distribution. However, how the creators' competition impacts user welfare and how the relevance-driven recommendation influences the dynamics in the long run are still largely unknown. This work provides theoretical insights into these research questions. We model the creators' competition under the assumptions that: 1) the platform employs an innocuous top-K recommendation policy; 2) user decisions follow the Random Utility model; 3) content creators compete for user engagement and, without knowing their utility function in hindsight, apply arbitrary no-regret learning algorithms to update their strategies. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on K and randomness in user decisions; we also prove the tightness of this bound. Our result discloses an intrinsic merit of the relevance-driven recommendation policy, as long as users' decisions involve randomness and the platform provides reasonably many alternatives to its users.},
  author = {Fan Yao and Chuanhao Li and Denis Nekipelov and Hongning Wang and Haifeng Xu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yao2023how.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39674--39701},
  pdf = {https://proceedings.mlr.press/v202/yao23b/yao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {How Bad is Top-{K} Recommendation under Competing Content Creators?},
  url = {https://proceedings.mlr.press/v202/yao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yao2023multiadam,
  abstract = {Physics-informed Neural Networks (PINNs) have recently achieved remarkable progress in solving Partial Differential Equations (PDEs) in various fields by minimizing a weighted sum of PDE loss and boundary loss. However, there are several critical challenges in the training of PINNs, including the lack of theoretical frameworks and the imbalance between PDE loss and boundary loss. In this paper, we present an analysis of second-order non-homogeneous PDEs, which are classified into three categories and applicable to various common problems. We also characterize the connections between the training loss and actual error, guaranteeing convergence under mild conditions. The theoretical analysis inspires us to further propose MultiAdam, a scale-invariant optimizer that leverages gradient momentum to parameter-wisely balance the loss terms. Extensive experiment results on multiple problems from different physical domains demonstrate that our MultiAdam solver can improve the predictive accuracy by 1-2 orders of magnitude compared with strong baselines.},
  author = {Jiachen Yao and Chang Su and Zhongkai Hao and Songming Liu and Hang Su and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yao2023multiadam.pdf:pdf},
  mdate = {2023-11-14},
  pages = {39702--39721},
  pdf = {https://proceedings.mlr.press/v202/yao23c/yao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MultiAdam}: Parameter-wise Scale-invariant Optimizer for Multiscale Training of Physics-informed Neural Networks},
  url = {https://proceedings.mlr.press/v202/yao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yardim2023policy,
  abstract = {Mean-field games have been used as a theoretical tool to obtain an approximate Nash equilibrium for symmetric and anonymous $N$-player games. However, limiting applicability, existing theoretical results assume variations of a "population generative model", which allows arbitrary modifications of the population distribution by the learning algorithm. Moreover, learning algorithms typically work on abstract simulators with population instead of the $N$-player game. Instead, we show that $N$ agents running policy mirror ascent converge to the Nash equilibrium of the regularized game within $\widetilde{\mathcal{O}}(\varepsilon^{-2})$ samples from a single sample trajectory without a population generative model, up to a standard $\mathcal{O}(\frac{1}{\sqrt{N}})$ error due to the mean field. Taking a divergent approach from the literature, instead of working with the best-response map we first show that a policy mirror ascent map can be used to construct a contractive operator having the Nash equilibrium as its fixed point. We analyze single-path TD learning for $N$-agent games, proving sample complexity guarantees by only using a sample path from the $N$-agent simulator without a population generative model. Furthermore, we demonstrate that our methodology allows for independent learning by $N$ agents with finite sample guarantees.},
  author = {Batuhan Yardim and Semih Cayci and Matthieu Geist and Niao He},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yardim2023policy.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39722--39754},
  pdf = {https://proceedings.mlr.press/v202/yardim23a/yardim23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games},
  url = {https://proceedings.mlr.press/v202/yardim23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yasunaga2023retrievalaugmented,
  abstract = {Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). We introduce the first multimodal model that can retrieve and generate both text and images, called RA-CM3 (Retrieval-Augmented CM3). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our results show that RA-CM3 significantly outperforms baseline generation models on both text-to-image and image-to-text generation tasks, while being much smaller than models that store all knowledge in parameters.},
  author = {Michihiro Yasunaga and Armen Aghajanyan and Weijia Shi and Richard James and Jure Leskovec and Percy Liang and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yasunaga2023retrievalaugmented.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39755--39769},
  pdf = {https://proceedings.mlr.press/v202/yasunaga23a/yasunaga23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Retrieval-Augmented Multimodal Language Modeling},
  url = {https://proceedings.mlr.press/v202/yasunaga23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023compositional,
  abstract = {The performance of in-context learning (ICL) is highly dominated by the quality of the selected in-context examples, but previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we systematically formulate in-context example selection as a subset selection problem, and optimize it in an end-to-end fashion. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. Comprehensive experiments demonstrate that CEIL substantially outperforms all baseline methods on 12 classification and generation tasks.},
  author = {Jiacheng Ye and Zhiyong Wu and Jiangtao Feng and Tao Yu and Lingpeng Kong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023compositional.pdf:pdf},
  mdate = {2023-10-18},
  pages = {39818--39833},
  pdf = {https://proceedings.mlr.press/v202/ye23c/ye23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Compositional Exemplars for In-context Learning},
  url = {https://proceedings.mlr.press/v202/ye23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023power,
  abstract = {Generalization in reinforcement learning (RL) is fundamentally different from supervised learning generalization, and fine-tuning on the target environment is necessary for good test performance. In this paper, we ask the question of how much pre-training over training environments can be helpful for efficient and effective fine-tuning. We provide two surprising results. First, we show that asymptotically, the improvement from pre-training is at most a constant factor. We give this result by designing a specific target environment where any benefit from pre-training is negligible. Second, in the non-asymptotic regime, we show pre-training can indeed be helpful by designing a policy collection-elimination (PCE) algorithm and proving a distribution-dependent regret bound that is independent of the state-action space. We believe both the asymptotic result and the algorithm contribute to a better understanding of pre-training in RL.},
  author = {Haotian Ye and Xiaoyu Chen and Liwei Wang and Simon Shaolei Du},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023power.pdf:pdf},
  mdate = {2024-08-03},
  pages = {39770--39800},
  pdf = {https://proceedings.mlr.press/v202/ye23a/ye23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Power of Pre-training for Generalization in {RL}: Provable Benefits and Hardness},
  url = {https://proceedings.mlr.press/v202/ye23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023personalized,
  abstract = {Personalized federated learning (FL) aims to collaboratively train a personalized model for each client. Previous methods do not adaptively determine who to collaborate at a fine-grained level, making them difficult to handle diverse data heterogeneity levels and those cases where malicious clients exist. To address this issue, the core idea is to learn a collaboration graph, which models the benefits from each pairwise collaboration and allocates appropriate collaboration strengths. In this paper, we propose a novel personalized FL algorithm, pFedGraph, which consists of two key modules: (1) inferring the collaboration graph based on pairwise model similarity and dataset size at server to promote fine-grained collaboration and (2) optimizing local model with the assistance of aggregated model at client to promote personalization. We theoretically analyze the convergence rate of pFedGraph and show that the learned collaboration weights are optimal. Extensive experiments on four datasets demonstrate the effectiveness of our method.},
  author = {Rui Ye and Zhenyang Ni and Fangzhao Wu and Siheng Chen and Yanfeng Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023personalized.pdf:pdf},
  mdate = {2024-11-14},
  pages = {39801--39817},
  pdf = {https://proceedings.mlr.press/v202/ye23b/ye23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Personalized Federated Learning with Inferred Collaboration Graphs},
  url = {https://proceedings.mlr.press/v202/ye23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023corruptionrobust,
  abstract = {We study contextual bandits and episodic Markov decision processes (MDPs) under adversarial corruption and with general function approximation, where reward functions and transition kernels are subject to adversarial attacks. For contextual bandits with general function approximation, we propose a computationally efficient algorithm to achieve a regret of $\tilde{O}(\sqrt{T} + \zeta)$, where $T$ is the number of rounds and $\zeta$ is the total corruption. This result is based on uncertainty-weighted least-squares regression from linear contextual bandits and a new weighted estimator of uncertainty for the general function class. For episodic MDPs, we generalize this algorithm to achieve the first additive dependence on the corruption level $\zeta$ in the scenario of general function approximation, which improves over existing results that scale with the corruption level multiplicatively.},
  author = {Chenlu Ye and Wei Xiong and Quanquan Gu and Tong Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023corruptionrobust.pdf:pdf},
  mdate = {2024-01-29},
  pages = {39834--39863},
  pdf = {https://proceedings.mlr.press/v202/ye23d/ye23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear Contextual Bandits and Markov Decision Processes},
  url = {https://proceedings.mlr.press/v202/ye23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023gnn,
  abstract = {The latest two-stage optimization framework based on graph neural network (GNN) and large neighborhood search (LNS) is the most popular framework in solving large-scale integer programs (IPs). However, the framework can not effectively use the embedding spatial information in GNN and still highly relies on large-scale solvers in LNS, resulting in the scale of IP being limited by the ability of the current solver and performance bottlenecks. In this paper, we present a GNN\&GBDT-guided fast optimizing framework for large-scale IPs that only uses a small-scale optimizer to solve large-scale IPs efficiently. The framework can be divided into three stages: Multi-task GNN Embedding to generate the embedding space, GBDT Prediction to effectively use the embedding spatial information, and Neighborhood Optimization to solve large-scale problems fast using the small-scale optimizer. Extensive experiments show that our framework can solve IPs with millions of scales and surpass SCIP and Gurobi in the specified wall-clock time using only a small-scale optimizer with 30\% of the problem size. Our framework can save 99\% of running time in achieving the same solution quality as SCIP, which verifies the effectiveness and efficiency of our framework in solving large-scale IPs.},
  author = {Huigen Ye and Hua Xu and Hongyan Wang and Chengming Wang and Yu Jiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023gnn.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39864--39878},
  pdf = {https://proceedings.mlr.press/v202/ye23e/ye23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{GNN}\&{GBDT}-Guided Fast Optimizing Framework for Large-scale Integer Programming},
  url = {https://proceedings.mlr.press/v202/ye23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023feddisco,
  abstract = {Federated learning (FL) is a privacy-preserving machine learning paradigm that enables collaborative training across multiple clients without centralizing their local data. However, data heterogeneity, especially the category distribution heterogeneity, poses significant challenges to FL performance. Most previous works consider either regularizing local models or fine-tuning the global model, while they ignore the adjustment of aggregation weights and simply assign weights based on the dataset size. In this paper, we found through empirical observations and theoretical analysis that dataset size is not optimal and the discrepancy between local and global category distributions could be a beneficial and complementary indicator for determining aggregation weights. Based on this insight, we propose FedDisco, whose aggregation weights not only involve both the dataset size and the discrepancy value, but also contribute to a tighter theoretical upper bound of the optimization error. FedDisco promotes utility and modularity in a communication- and computation-efficient way. Extensive experiments show that FedDisco outperforms several state-of-the-art methods and can be easily incorporated with many existing methods to further enhance the performance.},
  author = {Rui Ye and Mingkai Xu and Jianyu Wang and Chenxin Xu and Siheng Chen and Yanfeng Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023feddisco.pdf:pdf},
  mdate = {2024-11-14},
  pages = {39879--39902},
  pdf = {https://proceedings.mlr.press/v202/ye23f/ye23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{FedDisco}: Federated Learning with Discrepancy-Aware Collaboration},
  url = {https://proceedings.mlr.press/v202/ye23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023towards,
  abstract = {Combinatorial optimization (CO) on the graph is a crucial but challenging research topic. Recent quantum algorithms provide a new perspective for solving CO problems and have the potential to demonstrate quantum advantage. While Quantum Approximate Optimization Algorithm (QAOA) is a well-known quantum heuristic for CO constructed by a parametric quantum circuit, QAOA is originally designed for unconstrained problems and the circuit parameters and solutions are jointly solved with time-consuming iterations. In this paper, we propose a quantum neural network (QNN) for learning CO problems in a supervised manner to achieve better and faster results. We focus on the Quadratic Assignment Problem (QAP) with matching constraints and the node permutation invariance property. A quantum neural network called QAP-QNN is devised to translate the QAP into a constrained vertex classification task. We study two QAP tasks: Graph Matching and Traveling Salesman Problem on TorchQuantum simulators, and empirically show the effectiveness of our approach.},
  author = {Xinyu Ye and Ge Yan and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023towards.pdf:pdf},
  mdate = {2023-08-31},
  pages = {39903--39912},
  pdf = {https://proceedings.mlr.press/v202/ye23g/ye23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Quantum Machine Learning for Constrained Combinatorial Optimization: a Quantum {QAP} Solver},
  url = {https://proceedings.mlr.press/v202/ye23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ye2023temporal,
  abstract = {Models that can predict the occurrence of events ahead of time with low false-alarm rates are critical to the acceptance of decision support systems in the medical community. This challenging task is typically treated as a simple binary classification, ignoring temporal dependencies between samples. We first introduce a common theoretical framework unifying dynamic survival analysis and early event prediction. Following an analysis of objectives from both fields, we propose Temporal Label Smoothing (TLS), a simpler, yet best-performing method that preserves prediction monotonicity over time. TLS is a novel learning strategy that modulates smoothing strength as a function of proximity to the event of interest. This regularization technique reduces model confidence at the class boundary, where the signal is often noisy or uninformative, thus allowing training to focus on clinically informative data points away from this boundary region. By focusing the objective on areas with a stronger predictive signal, TLS improves performance over all baselines on two large-scale benchmark tasks.},
  author = {Hugo Y{\`e}che and Alize{\'e} Pace and Gunnar R{\"a}tsch and Rita Kuznetsova},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ye2023temporal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39913--39938},
  pdf = {https://proceedings.mlr.press/v202/yeche23a/yeche23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Temporal Label Smoothing for Early Event Prediction},
  url = {https://proceedings.mlr.press/v202/yeche23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yi2023online,
  abstract = {Domain adaptation in reinforcement learning (RL) mainly deals with the changes of observation when transferring the policy to a new environment. Many traditional approaches of domain adaptation in RL manage to learn a mapping function between the source and target domain in explicit or implicit ways. However, they typically require access to abundant data from the target domain. Besides, they often rely on visual clues to learn the mapping function and may fail when the source domain looks quite different from the target domain. To address these problems, we propose a novel framework Online Prototype Alignment (OPA) to learn the mapping function based on the functional similarity of elements and is able to achieve the few-shot policy transfer within only several episodes. The key insight of OPA is to introduce an exploration mechanism that can interact with the unseen elements of the target domain in an efficient and purposeful manner, and then connect them with the seen elements in the source domain according to their functionalities instead of visual clues.},
  author = {Qi Yi and Rui Zhang and Shaohui Peng and Jiaming Guo and Yunkai Gao and Kaizhao Yuan and Ruizhi Chen and Siming Lan and Xing Hu and Zidong Du and Xishan Zhang and Qi Guo and Yunji Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yi2023online.pdf:pdf},
  mdate = {2025-04-24},
  pages = {39968--39983},
  pdf = {https://proceedings.mlr.press/v202/yi23b/yi23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Prototype Alignment for Few-shot Policy Transfer},
  url = {https://proceedings.mlr.press/v202/yi23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yi2023doubly,
  abstract = {We study a new non-stochastic federated multi-armed bandit problem with multiple agents collaborating via a communication network. In the doubly adversarial setting we consider, the losses of the arms are assigned by an oblivious adversary that specifies the loss of each arm not only for each time step but also for each agent. In this setting, different agents may choose the same arm in the same time step but observe different feedback. The goal of each agent is to find a globally best arm in hindsight that has the lowest cumulative loss averaged over all agents, which necessitates communication among agents. We provide regret lower bounds for any federated bandit algorithm under different settings, when agents have access to full-information feedback, or bandit feedback. We propose a near-optimal federated bandit algorithm that achieves the lower bound up to logarithmic factors.},
  author = {Jialin Yi and Milan Vojnovi{\'c}},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yi2023doubly.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39951--39967},
  pdf = {https://proceedings.mlr.press/v202/yi23a/yi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Doubly Adversarial Federated Bandits},
  url = {https://proceedings.mlr.press/v202/yi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yi2023monoflow,
  abstract = {The conventional understanding of adversarial training in generative adversarial networks ({GANs}) is that the discriminator is trained to estimate a divergence, and the generator learns to minimize this divergence. We argue that despite the fact that many variants of {GANs} were developed following this paradigm, the current theoretical understanding of {GANs} and their practical algorithms are inconsistent. In this paper, we leverage {Wasserstein} gradient flows which characterize the evolution of particles in the sample space, to gain theoretical insights and algorithmic inspiration of {GANs}. We introduce a unified generative modeling framework - {MonoFlow}: the particle evolution is rescaled via a monotonically increasing mapping of the log density ratio. Under our framework, adversarial training can be viewed as a procedure first obtaining {MonoFlow}'s vector field via training the discriminator and the generator learns to draw the particle flow defined by the corresponding vector field. We also reveal the fundamental difference between variational divergence minimization and adversarial training. This analysis helps us to identify what types of generator loss functions can lead to the successful training of {GANs} and suggest that {GANs} may have more loss designs beyond the literature (e.g., non-saturated loss), as long as they realize {MonoFlow}. Consistent empirical studies are included to validate the effectiveness of our framework.},
  author = {Mingxuan Yi and Zhanxing Zhu and Song Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yi2023monoflow.pdf:pdf},
  mdate = {2023-08-28},
  pages = {39984--40000},
  pdf = {https://proceedings.mlr.press/v202/yi23c/yi23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{MonoFlow}: Rethinking Divergence {GANs} via the Perspective of {Wasserstein} Gradient Flows},
  url = {https://proceedings.mlr.press/v202/yi23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yim2023se,
  abstract = {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in {3D} (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on {SE}(3), the space of orientation preserving rigid motions in {$\mathbb{R}^3$}, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of {SE}(3) invariant diffusion models on multiple frames followed by a novel framework, {FrameDiff}, for learning the {SE}(3) equivariant score over multiple frames. We apply {FrameDiff} on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.},
  author = {Jason Yim and Brian L. Trippe and Valentin De Bortoli and Emile Mathieu and Arnaud Doucet and Regina Barzilay and Tommi S. Jaakkola},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yim2023se.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40001--40039},
  pdf = {https://proceedings.mlr.press/v202/yim23a/yim23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SE}(3) diffusion model with application to protein backbone generation},
  url = {https://proceedings.mlr.press/v202/yim23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yin2023coco,
  abstract = {Although graph neural networks ({GNNs}) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply {GNNs} to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose Coupled Contrastive Graph Representation Learning ({CoCo}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. {CoCo} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. On the one hand, cross-branch contrastive learning encourages the agreement of coupled modules to generate comprehensive graph representations. On the other hand, cross-domain contrastive learning reduces the distances between cross-domain pairs with the same semantics for effective domain alignment. Extensive experiments on popular datasets show that our {CoCo} outperforms these competing baselines in different settings generally.},
  author = {Nan Yin and Li Shen and Mengzhu Wang and Long Lan and Zeyu Ma and Chong Chen and Xian-Sheng Hua and Xiao Luo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yin2023coco.pdf:pdf},
  mdate = {2024-11-18},
  pages = {40040--40053},
  pdf = {https://proceedings.mlr.press/v202/yin23a/yin23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CoCo}: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification},
  url = {https://proceedings.mlr.press/v202/yin23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ying2023adaptive,
  abstract = {We consider the problem of estimating (diagonally dominant) {M}-matrices as precision matrices in {Gaussian} graphical models. Such models have shown interesting properties, e.g., the maximum likelihood estimator exists with as little as two observations in the case of {M}-matrices, and exists even with one observation in the case of diagonally dominant {M}-matrices. We propose an adaptive multiple-stage estimation method, which refines the estimate by solving a weighted $\ell_1$-regularized problem in each stage. We further design a unified framework based on gradient projection method to solve the regularized problem, equipped with different projections to handle the constraints of {M}-matrices and diagonally dominant {M}-matrices.},
  author = {Jiaxi Ying and Jos{\'{e}} Vin{\'{i}}cius de Miranda Cardoso and Daniel P. Palomar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ying2023adaptive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40054--40074},
  pdf = {https://proceedings.mlr.press/v202/ying23a/ying23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Estimation of Graphical Models under Total Positivity},
  url = {https://proceedings.mlr.press/v202/ying23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoo2023improving,
  abstract = {Visual Prompt Tuning ({VPT}) is an effective tuning method for adapting pretrained Vision Transformers ({ViTs}) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained {ViTs}. Although {VPT} has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of {VPT} hinges largely on the {ViT} blocks with which the prompt tokens interact. Specifically, {VPT} shows improved performance on image classification tasks for {MAE} and {MoCo} v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised {ViT} for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method that learns a gate for each {ViT} block to adjust its intervention into the prompt tokens. With our method, prompt tokens are selectively influenced by blocks that require steering for task adaptation. Our method outperforms {VPT} variants in {FGVC} and {VTAB} image classification and {ADE20K} semantic segmentation.},
  author = {Seungryong Yoo and Eunji Kim and Dahuin Jung and Jungbeom Lee and Sungroh Yoon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoo2023improving.pdf:pdf},
  mdate = {2025-05-26},
  pages = {40075--40092},
  pdf = {https://proceedings.mlr.press/v202/yoo23a/yoo23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Visual Prompt Tuning for Self-supervised Vision Transformers},
  url = {https://proceedings.mlr.press/v202/yoo23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoo2023endtoend,
  abstract = {Recent end-to-end multi-object detectors simplify the inference pipeline by removing hand-crafted processes such as non-maximum suppression ({NMS}). However, during training, they still heavily rely on heuristics and hand-crafted processes which deteriorate the reliability of the predicted confidence score. In this paper, we propose a novel framework to train an end-to-end multi-object detector consisting of only two terms: negative log-likelihood ({NLL}) and a regularization term. In doing so, the multi-object detection problem is treated as density estimation of the ground truth bounding boxes utilizing a regularized mixture density model. The proposed end-to-end multi-object Detection with a Regularized Mixture Model ({D-RMM}) is trained by minimizing the {NLL} with the proposed regularization term, maximum component maximization ({MCM}) loss, preventing duplicate predictions. Our method reduces the heuristics of the training process and improves the reliability of the predicted confidence score. Moreover, our {D-RMM} outperforms the previous end-to-end detectors on {MS} {COCO} dataset.},
  author = {Jaeyoung Yoo and Hojun Lee and Seunghyeon Seo and Inseop Chung and Nojun Kwak},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoo2023endtoend.pdf:pdf},
  mdate = {2024-11-22},
  pages = {40093--40110},
  pdf = {https://proceedings.mlr.press/v202/yoo23b/yoo23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {End-to-End Multi-Object Detection with a Regularized Mixture Model},
  url = {https://proceedings.mlr.press/v202/yoo23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoon2023graph,
  abstract = {As the field of Graph Neural Networks ({GNN}) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new {GNN} models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this problem, we introduce a novel graph generative model, Computation Graph Transformer ({CGT}) that learns and reproduces the distribution of real-world graphs in a privacy-controlled way. More specifically, {CGT} (1) generates effective benchmark graphs on which {GNNs} show similar task performance as on the source graphs, (2) scales to large graphs, and (3) achieves strong privacy guarantees. We conduct extensive experiments on seven graph datasets and demonstrate that {CGT} successfully generates synthetic graphs that produce near-identical task performance across various {GNN} models while strongly preserving privacy.},
  author = {Minji Yoon and Yue Wu and John Palowitch and Bryan Perozzi and Russ Salakhutdinov},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoon2023graph.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40175--40198},
  pdf = {https://proceedings.mlr.press/v202/yoon23d/yoon23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Generative Model for Benchmarking Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/yoon23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoon2023em,
  abstract = {We introduce {EM}-Network, a novel self-distillation approach that effectively leverages target information for supervised sequence-to-sequence (seq2seq) learning. In contrast to conventional methods, it is trained with oracle guidance, which is derived from the target sequence. Since the oracle guidance compactly represents the target-side context that can assist the sequence model in solving the task, the {EM}-Network achieves a better prediction compared to using only the source input. To allow the sequence model to inherit the promising capability of the {EM}-Network, we propose a new self-distillation strategy, where the original sequence model can benefit from the knowledge of the {EM}-Network in a one-stage manner. We conduct comprehensive experiments on two types of seq2seq models: connectionist temporal classification ({CTC}) for speech recognition and attention-based encoder-decoder ({AED}) for machine translation. Experimental results demonstrate that the {EM}-Network significantly advances the current state-of-the-art approaches, improving over the best prior work on speech recognition and establishing state-of-the-art performance on {WMT}'14 and {IWSLT}'14.},
  author = {Ji Won Yoon and Sunghwan Ahn and Hyeonseung Lee and Minchan Kim and Seok Min Kim and Nam Soo Kim},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoon2023em.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40111--40128},
  pdf = {https://proceedings.mlr.press/v202/yoon23a/yoon23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{EM}-Network: Oracle Guided Self-distillation for Sequence Learning},
  url = {https://proceedings.mlr.press/v202/yoon23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoon2023continual,
  abstract = {Motivated by the efficiency and rapid convergence of pre-trained models for solving downstream tasks, this paper extensively studies the impact of Continual Learning ({CL}) models as pre-trainers. We find that, in both supervised and unsupervised {CL}, the transfer quality of representations does not show a noticeable degradation of fine-tuning performance but rather increases gradually. This is because {CL} models can learn improved task-general features when easily forgetting task-specific knowledge. Based on this observation, we suggest a new unsupervised {CL} framework with masked modeling, which aims to capture fluent task-generic representation during training. Furthermore, we propose a new fine-tuning scheme, {GLobal} {Attention} {Discretization} ({GLAD}), that preserves rich task-generic representation during solving downstream tasks. The model fine-tuned with {GLAD} achieves competitive performance and can also be used as a good pre-trained model itself. We believe this paper breaks the barriers between pre-training and fine-tuning steps and leads to a sustainable learning framework in which the continual learner incrementally improves model generalization, yielding better transfer to unseen tasks.},
  author = {Jaehong Yoon and Sung Ju Hwang and Yue Cao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoon2023continual.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40129--40146},
  pdf = {https://proceedings.mlr.press/v202/yoon23b/yoon23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Continual Learners are Incremental Model Generalizers},
  url = {https://proceedings.mlr.press/v202/yoon23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yoon2023investigation,
  abstract = {Unsupervised object-centric representation ({OCR}) learning has recently drawn attention as a new paradigm of visual representation. This is because of its potential of being an effective pre-training technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning ({RL}) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in {RL} has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of {OCR} pre-training for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual {RL} benchmark and conduct experiments to answer questions such as {``}Does {OCR} pre-training improve performance on object-centric tasks?{''} and {``}Can {OCR} pre-training help with out-of-distribution generalization?{''}. Our results provide empirical evidence for valuable insights into the effectiveness of {OCR} pre-training for {RL} and the potential limitations of its use in certain scenarios. Additionally, this study also examines the critical aspects of incorporating {OCR} pre-training in {RL}, including performance in a visually complex environment and the appropriate pooling layer to aggregate the object representations.},
  author = {Jaesik Yoon and Yi-Fu Wu and Heechul Bae and Sungjin Ahn},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yoon2023investigation.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40147--40174},
  pdf = {https://proceedings.mlr.press/v202/yoon23c/yoon23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {An Investigation into Pre-Training Object-Centric Representations for Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/yoon23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{you2023analyzing,
  abstract = {A quantum neural network ({QNN}) is a parameterized mapping efficiently implementable on near-term Noisy Intermediate-Scale Quantum ({NISQ}) computers. It can be used for supervised learning when combined with classical gradient-based optimizers. Despite the existing empirical and theoretical investigations, the convergence of {QNN} training is not fully understood. Inspired by the success of the neural tangent kernels ({NTKs}) in probing into the dynamics of classical neural networks, a recent line of works proposes to study over-parameterized {QNNs} by examining a quantum version of tangent kernels. In this work, we study the dynamics of {QNNs} and show that contrary to popular belief it is qualitatively different from that of any kernel regression: due to the unitarity of quantum operations, there is a non-negligible deviation from the tangent kernel regression derived at the random initialization. As a result of the deviation, we prove the at-most sublinear convergence for {QNNs} with Pauli measurements, which is beyond the explanatory power of any kernel regression dynamics. We then present the actual dynamics of {QNNs} in the limit of over-parameterization. The new dynamics capture the change of convergence rate during training and implies that the range of measurements is crucial to the fast {QNN} convergence.},
  author = {Xuchen You and Shouvanik Chakrabarti and Boyang Chen and Xiaodi Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/you2023analyzing.pdf:pdf},
  mdate = {2023-12-17},
  pages = {40199--40224},
  pdf = {https://proceedings.mlr.press/v202/you23a/you23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Analyzing Convergence in Quantum Neural Networks: Deviations from Neural Tangent Kernels},
  url = {https://proceedings.mlr.press/v202/you23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{younes2023entropydriven,
  abstract = {The paper presents a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy ({ISE}) that quantifies the per-pixel information in an image. The authors argue that local entropy of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, they abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations directly from video frames. The method uses two original information-theoretic losses, computed from local entropy, guide their model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a loss that optimizes the keypoints' information transportation over time. The empirical results show superior performance for their information-driven keypoints that resolve challenges like attendance to static and dynamic objects or objects abruptly entering and leaving the scene.},
  author = {Ali Younes and Simone Schaub-Meyer and Georgia Chalvatzaki},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/younes2023entropydriven.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40225--40253},
  pdf = {https://proceedings.mlr.press/v202/younes23a/younes23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Entropy-driven Unsupervised Keypoint Representation Learning in Videos},
  url = {https://proceedings.mlr.press/v202/younes23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{young2023benefits,
  abstract = {Model-Based Reinforcement Learning ({RL}) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay ({ER}) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep {RL}. In principle, a learned parametric model could improve on {ER} by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we provide an illustrative example showing empirically how a similar effect occurs in a more concrete setting with neural network function approximation. Finally, we provide extensive experiments showing the benefit of model-based learning for online {RL} in environments with combinatorial complexity, but factored structure that allows a learned model to generalize.},
  author = {Kenny John Young and Aditya A. Ramesh and Louis Kirsch and J{\"{u}}rgen Schmidhuber},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/young2023benefits.pdf:pdf},
  mdate = {2024-08-30},
  pages = {40254--40276},
  pdf = {https://proceedings.mlr.press/v202/young23a/young23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Benefits of Model-Based Generalization in Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/young23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023cola,
  abstract = {Error correcting output codes ({ECOCs}) have been proposed to improve the robustness of deep neural networks ({DNNs}) against hardware defects of {DNN} hardware accelerators. Unfortunately, existing efforts suffer from drawbacks that would greatly impact their practicality: 1) robust accuracy (with defects) improvement at the cost of degraded clean accuracy (without defects); 2) no guarantee on better robust or clean accuracy using stronger {ECOCs}. In this paper, we first shed light on the connection between these drawbacks and error correlation, and then propose a novel comprehensive error decorrelation framework, namely {COLA}. Specifically, we propose to reduce inner layer feature error correlation by 1) adopting a separated architecture, where the last portions of the paths to all output nodes are separated, and 2) orthogonalizing weights in common {DNN} layers so that the intermediate features are orthogonal with each other. We also propose a regularization technique based on total correlation to mitigate overall error correlation at the outputs. The effectiveness of {COLA} is first analyzed theoretically, and then evaluated experimentally, e.g. up to 6.7\% clean accuracy improvement compared with the original {DNNs} and up to 40\% robust accuracy improvement compared to the state-of-the-art {ECOC}-enhanced {DNNs}.},
  author = {Anlan Yu and Ning Lyu and Jieming Yin and Zhiyuan Yan and Wujie Wen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023cola.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40277--40289},
  pdf = {https://proceedings.mlr.press/v202/yu23a/yu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{COLA}: Orchestrating Error Coding and Learning for Robust Neural Network Inference Against Hardware Defects},
  url = {https://proceedings.mlr.press/v202/yu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023delving,
  abstract = {A critical element of learning with noisy labels is noisy label detection. Notably, numerous previous works assume that no source of labels can be clean in a noisy label detection context. In this work, we relax this assumption and assume that a small subset of the training data is clean, which enables substantial noisy label detection performance gains. We propose a novel framework that leverages clean data by framing the problem of noisy label detection with clean data as a multiple hypothesis testing problem. Moreover, we propose {BHN}, a simple yet effective approach for noisy label detection that integrates the Benjamini-Hochberg ({BH}) procedure into deep neural networks. {BHN} achieves state-of-the-art performance and outperforms baselines by 28.48\% in terms of false discovery rate ({FDR}) and by 18.99\% in terms of {F1} on {CIFAR}-10.},
  author = {Chenglin Yu and Xinsong Ma and Weiwei Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023delving.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40290--40305},
  pdf = {https://proceedings.mlr.press/v202/yu23b/yu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Delving into Noisy Label Detection with Clean Data},
  url = {https://proceedings.mlr.press/v202/yu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023bag,
  abstract = {With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the {GPT}-{Neo} 1.3{B} evaluation results, our proposed tricks outperform the baseline by a large margin in most cases, providing a much stronger baseline for future research.},
  author = {Weichen Yu and Tianyu Pang and Qian Liu and Chao Du and Bingyi Kang and Yan Huang and Min Lin and Shuicheng Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023bag.pdf:pdf},
  mdate = {2025-04-17},
  pages = {40306--40320},
  pdf = {https://proceedings.mlr.press/v202/yu23c/yu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Bag of Tricks for Training Data Extraction from Language Models},
  url = {https://proceedings.mlr.press/v202/yu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023discoverthenrank,
  abstract = {We propose to approach active learning ({AL}) from a novel perspective of discovering and then ranking potential support vectors by leveraging the key properties of the dual space of a sparse kernel max-margin predictor. We theoretically analyze the change of a hinge loss in the dual form and provide both the upper and lower bounds that are deeply connected to the key geometric properties induced by the dual space, which then help us identify various types of important data samples for {AL}. These bounds inform the design of a novel sampling strategy that leverages class-wise evidence as a key vehicle, formed through an affine combination of dual variables and kernel evaluation. We construct two distinct types of sampling functions, including discovery and ranking. The former focuses on samples with low total evidence from all classes, which signifies their potential to support exploration; the latter exploits the current decision boundary to identify the most conflicting regions for sampling.},
  author = {Dayou Yu and Weishi Shi and Qi Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023discoverthenrank.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40321--40338},
  pdf = {https://proceedings.mlr.press/v202/yu23d/yu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Discover-Then-Rank Unlabeled Support Vectors in the Dual Space for Multi-Class Active Learning},
  url = {https://proceedings.mlr.press/v202/yu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023codeipprompt,
  abstract = {Recent advances in large language models ({LMs}) have facilitated their ability to synthesize programming code. However, they have also raised concerns about intellectual property ({IP}) rights violations. Despite the significance of this issue, it has been relatively less explored. In this paper, we aim to bridge the gap by presenting {CodeIPPrompt}, a platform for automatic evaluation of the extent to which code language models may reproduce licensed programs. It comprises two key components: prompts constructed from a licensed code database to elicit {LMs} to generate {IP}-violating code, and a measurement tool to evaluate the extent of {IP} violation of code {LMs}. We conducted an extensive evaluation of existing open-source code {LMs} and commercial products and revealed the prevalence of {IP} violations in all these models.},
  author = {Zhiyuan Yu and Yuhao Wu and Ning Zhang and Chenguang Wang and Yevgeniy Vorobeychik and Chaowei Xiao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023codeipprompt.pdf:pdf},
  mdate = {2024-12-10},
  pages = {40373--40389},
  pdf = {https://proceedings.mlr.press/v202/yu23g/yu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CodeIPPrompt}: Intellectual Property Infringement Assessment of Code Language Models},
  url = {https://proceedings.mlr.press/v202/yu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023longterm,
  abstract = {We consider the problem of generating musical soundtracks in sync with rhythmic visual cues. Most existing works rely on pre-defined music representations, leading to the incompetence of generative flexibility and complexity. Other methods directly generating video-conditioned waveforms suffer from limited scenarios, short lengths, and unstable generation quality. To this end, we present Long-Term Rhythmic Video Soundtracker ({LORIS}), a novel framework to synthesize long-term conditional waveforms. Specifically, our framework consists of a latent conditional diffusion probabilistic model to perform waveform synthesis. Furthermore, a series of context-aware conditioning encoders are proposed to take temporal information into consideration for a long-term generation. Notably, we extend our model's applicability from dances to multiple sports scenarios such as floor exercise and figure skating. To perform comprehensive evaluations, we establish a benchmark for rhythmic video soundtracks including the pre-processed dataset, improved evaluation metrics, and robust generative baselines. Extensive experiments show that our model generates long-term soundtracks with state-of-the-art musical quality and rhythmic correspondence.},
  author = {Jiashuo Yu and Yaohui Wang and Xinyuan Chen and Xiao Sun and Yu Qiao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023longterm.pdf:pdf},
  mdate = {2025-06-23},
  pages = {40339--40353},
  pdf = {https://proceedings.mlr.press/v202/yu23e/yu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Long-Term Rhythmic Video Soundtracker},
  url = {https://proceedings.mlr.press/v202/yu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023adversarial,
  abstract = {The parameter perturbation attack is a safety threat to deep learning, where small parameter perturbations are made such that the attacked network gives wrong or desired labels of the adversary to specified inputs. However, such attacks could be detected by the user, because the accuracy of the attacked network will reduce and the network cannot work normally. To make the attack more stealthy, in this paper, the adversarial parameter attack is proposed, in which small perturbations to the parameters of the network are made such that the accuracy of the attacked network does not decrease much, but its robustness against adversarial example attacks becomes much lower. As a consequence, the attacked network performs normally on standard samples, but is much more vulnerable to adversarial attacks. The existence of nearly perfect adversarial parameters under ${L}_\infty$ norm and ${L}_0$ norm is proved under reasonable conditions. Algorithms are given which can be used to produce high quality adversarial parameters for the commonly used networks trained with various robust training methods, in that the robustness of the attacked networks decreases significantly when they are evaluated using various adversarial attack methods.},
  author = {Lijia Yu and Yihan Wang and Xiao-Shan Gao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023adversarial.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40354--40372},
  pdf = {https://proceedings.mlr.press/v202/yu23f/yu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adversarial Parameter Attack on Deep Neural Networks},
  url = {https://proceedings.mlr.press/v202/yu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023seedgnn,
  abstract = {There is a growing interest in designing Graph Neural Networks ({GNN}s) for seeded graph matching, which aims to match two unlabeled graphs using only topological information and a small set of seed nodes. However, most previous {GNN}s for this task use a semi-supervised approach, which requires a large number of seeds and cannot learn knowledge that is transferable to unseen graphs. In contrast, this paper proposes a new supervised approach that can learn from a training set how to match unseen graphs with only a few seeds. Our {SeedGNN} architecture incorporates several novel designs, inspired by theoretical studies of seeded graph matching: 1) it can learn to compute and use witness-like information from different hops, in a way that can be generalized to graphs of different sizes; 2) it can use easily-matched node-pairs as new seeds to improve the matching in subsequent layers. We evaluate {SeedGNN} on synthetic and real-world graphs and demonstrate significant performance improvements over both non-learning and learning algorithms in the existing literature. Furthermore, our experiments confirm that the knowledge learned by {SeedGNN} from training graphs can be generalized to test graphs of different sizes and categories.},
  author = {Liren Yu and Jiaming Xu and Xiaojun Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023seedgnn.pdf:pdf},
  pages = {40390--40411},
  pdf = {https://proceedings.mlr.press/v202/yu23h/yu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{SeedGNN}: Graph Neural Network for Supervised Seeded Graph Matching},
  url = {https://proceedings.mlr.press/v202/yu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023efficient,
  abstract = {We consider the prediction of the {H}amiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a {SE}(3)-equivariant network, named {QHNet}, that achieves efficiency and equivariance. Our key advance lies at the innovative design of {QHNet} architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\%. In addition, {QHNet} prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on {MD}17 datasets, including four molecular systems. Experimental results show that our {QHNet} can achieve comparable performance to the state of the art methods at a significantly faster speed.},
  author = {Haiyang Yu and Zhao Xu and Xiaofeng Qian and Xiaoning Qian and Shuiwang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023efficient.pdf:pdf},
  pages = {40412--40424},
  pdf = {https://proceedings.mlr.press/v202/yu23i/yu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Efficient and Equivariant Graph Networks for Predicting Quantum {H}amiltonian},
  url = {https://proceedings.mlr.press/v202/yu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023global,
  abstract = {Risk-sensitive reinforcement learning ({RL}) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive {RL}, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures ({ECRM}s), and derive policy gradient updates for {ECRM}-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms.},
  author = {Xian Yu and Lei Ying},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023global.pdf:pdf},
  pages = {40425--40451},
  pdf = {https://proceedings.mlr.press/v202/yu23j/yu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures},
  url = {https://proceedings.mlr.press/v202/yu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023actorcritic,
  abstract = {Deep offline reinforcement learning has recently demonstrated considerable promises in leveraging offline datasets, providing high-quality models that significantly reduce the online interactions required for fine-tuning. However, such a benefit is often diminished due to the marked state-action distribution shift, which causes significant bootstrap error and wipes out the good initial policy. Existing solutions resort to constraining the policy shift or balancing the sample replay based on their online-ness. However, they require online estimation of distribution divergence or density ratio. To avoid such complications, we propose deviating from existing actor-critic approaches that directly transfer the state-action value functions. Instead, we post-process them by aligning with the offline learned policy, so that the {Q}-values for actions outside the offline policy are also tamed.},
  author = {Zishun Yu and Xinhua Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023actorcritic.pdf:pdf},
  pages = {40452--40474},
  pdf = {https://proceedings.mlr.press/v202/yu23k/yu23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Actor-Critic Alignment for Offline-to-Online Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/yu23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yu2023masterasr,
  abstract = {Despite the impressive performance recently achieved by automatic speech recognition ({ASR}), we observe two primary challenges that hinder its broader applications: (1) The difficulty of introducing scalability into the model to support more languages with limited training, inference, and storage overhead; (2) The low-resource adaptation ability that enables effective low-resource adaptation while avoiding over-fitting and catastrophic forgetting issues. Inspired by recent findings, we hypothesize that we can address the above challenges with modules widely shared across languages. To this end, we propose an {ASR} framework, dubbed {Master-ASR}, that, for the first time, simultaneously achieves strong multilingual scalability and low-resource adaptation ability thanks to its modularize-then-assemble strategy. Specifically, {Master-ASR} learns a small set of generalizable sub-modules and adaptively assembles them for different languages to reduce the multilingual overhead and enable effective adaptation.},
  author = {Zhongzhi Yu and Yang Zhang and Kaizhi Qian and Cheng Wan and Yonggan Fu and Yongan Zhang and Yingyan Celine Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yu2023masterasr.pdf:pdf},
  pages = {40475--40487},
  pdf = {https://proceedings.mlr.press/v202/yu23l/yu23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Master-ASR}: Achieving Multilingual Scalability and Low-Resource Adaptation in {ASR} with Modular Learning},
  url = {https://proceedings.mlr.press/v202/yu23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yuan2023coordinate,
  abstract = {We consider a class of structured fractional minimization problems, in which the numerator part of the objective is the sum of a differentiable convex function and a convex nonsmooth function, while the denominator part is a concave or convex function. This problem is difficult to solve since it is nonconvex. By exploiting the structure of the problem, we propose two Coordinate Descent ({CD}) methods for solving this problem. One is applied to the original fractional function, the other is based on the associated parametric problem. The proposed methods iteratively solve a one-dimensional subproblem globally, and they are guaranteed to converge to coordinate-wise stationary points. Under suitable conditions, {CD} methods with an appropriate initialization converge linearly to the optimal point (also the coordinate-wise stationary point). In the case of a concave denominator, we show that the resulting problem is quasi-convex, and any critical point is a global minimum. We prove that the algorithms converge to the global optimal solution with a sublinear convergence rate. We demonstrate the applicability of the proposed methods to some machine learning and signal processing models. Our experiments on real-world data have shown that our method significantly and consistently outperforms existing methods in terms of accuracy.},
  author = {Ganzhao Yuan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yuan2023coordinate.pdf:pdf},
  pages = {40488--40518},
  pdf = {https://proceedings.mlr.press/v202/yuan23a/yuan23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coordinate Descent Methods for Fractional Minimization},
  url = {https://proceedings.mlr.press/v202/yuan23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yuan2023power,
  abstract = {With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We prove that a foundation model with the minimum required power can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources, and provide a categorical framework for supervised and self-supervised learning.},
  author = {Yang Yuan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yuan2023power.pdf:pdf},
  pages = {40519--40530},
  pdf = {https://proceedings.mlr.press/v202/yuan23b/yuan23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Power of Foundation Models},
  url = {https://proceedings.mlr.press/v202/yuan23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yuan2023automatic,
  abstract = {We present {AIRS}: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning ({RL}). More specifically, {AIRS} selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test {AIRS} on various tasks of {MiniGrid}, {Procgen}, and {DeepMind} Control Suite. Extensive simulation demonstrates that {AIRS} can outperform the benchmarking schemes and achieve superior performance with simple architecture.},
  author = {Mingqi Yuan and Bo Li and Xin Jin and Wenjun Zeng},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yuan2023automatic.pdf:pdf},
  pages = {40531--40554},
  pdf = {https://proceedings.mlr.press/v202/yuan23c/yuan23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/yuan23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{yun2023traversing,
  abstract = {Deep ensemble is a simple yet powerful way to improve the performance of deep neural networks. Under this motivation, recent works on mode connectivity have shown that parameters of ensembles are connected by low-loss subspaces, and one can efficiently collect ensemble parameters in those subspaces. While this provides a way to efficiently train ensembles, for inference, multiple forward passes should still be executed using all the ensemble parameters, which often becomes a serious bottleneck for real-world deployment. In this work, we propose a novel framework to reduce such costs. Given a low-loss subspace connecting two modes of a neural network, we build an additional neural network that predicts the output of the original neural network evaluated at a certain point in the low-loss subspace. The additional neural network, which they call a ``bridge,'' is a lightweight network that takes minimal features from the original network and predicts outputs for the low-loss subspace without requiring multiple forward passes.},
  author = {Eunggu Yun and Hyungi Lee and Giung Nam and Juho Lee},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/yun2023traversing.pdf:pdf},
  pages = {40555--40577},
  pdf = {https://proceedings.mlr.press/v202/yun23a/yun23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Traversing Between Modes in Function Space for Fast Ensembling},
  url = {https://proceedings.mlr.press/v202/yun23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zaffran2023conformal,
  abstract = {Conformal prediction is a theoretically grounded framework for constructing predictive intervals. We study conformal prediction with missing values in the covariates -- a setting that brings new challenges to uncertainty quantification. We first show that the marginal coverage guarantee of conformal prediction holds on imputed data for any missingness distribution and almost all imputation functions. However, we emphasize that the average coverage varies depending on the pattern of missing values: conformal methods tend to construct prediction intervals that undercover the response conditionally to some missing patterns. This motivates our novel generalized conformalized quantile regression framework, missing data augmentation, which yields prediction intervals that are valid conditionally to the patterns of missing values, despite their exponential number.},
  author = {Margaux Zaffran and Aymeric Dieuleveut and Julie Josse and Yaniv Romano},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zaffran2023conformal.pdf:pdf},
  pages = {40578--40604},
  pdf = {https://proceedings.mlr.press/v202/zaffran23a/zaffran23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conformal Prediction with Missing Values},
  url = {https://proceedings.mlr.press/v202/zaffran23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zandieh2023kdeformer,
  abstract = {Dot-product attention mechanism plays a crucial role in modern deep architectures (e.g., Transformer) for sequence modeling, however, na{\"\i}ve exact computation of this model incurs quadratic time and memory complexities in sequence length, hindering the training of long-sequence models. Critical bottlenecks are due to the computation of partition functions in the denominator of softmax function as well as the multiplication of the softmax matrix with the matrix of values. Our key observation is that the former can be reduced to a variant of the kernel density estimation ({KDE}) problem, and an efficient {KDE} solver can be further utilized to accelerate the latter via subsampling-based fast matrix products. Our proposed {KDEformer} can approximate the attention in sub-quadratic time with provable spectral norm bounds, while all prior results merely provide entry-wise error bounds. Empirically, we verify that {KDEformer} outperforms other attention approximations in terms of accuracy, memory, and arithmetic operations on various pre-trained models. For instance, on {BigGAN} image generation we achieve better generative scores than the exact computation with over 4$\times$ speedup. For {ImageNet} classification with {T2T-ViT}, {KDEformer} shows over 18$\times$ speedup while the accuracy drop is less than 0.5\%.},
  author = {Amir Zandieh and Insu Han and Majid Daliri and Amin Karbasi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zandieh2023kdeformer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40605--40623},
  pdf = {https://proceedings.mlr.press/v202/zandieh23a/zandieh23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{KDEformer}: Accelerating Transformers via Kernel Density Estimation},
  url = {https://proceedings.mlr.press/v202/zandieh23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zanette2023when,
  abstract = {Model-free algorithms for reinforcement learning typically require a condition called Bellman completeness in order to successfully operate off-policy with function approximation, unless additional conditions are met. However, Bellman completeness is a requirement that is much stronger than realizability and that is deemed to be too strong to hold in practice. In this work, we relax this structural assumption and analyze the statistical complexity of off-policy reinforcement learning when only realizability holds for the prescribed function class. We establish finite-sample guarantees for off-policy reinforcement learning that are free of the approximation error term known as inherent Bellman error, and that depend on the interplay of three factors: the coverage provided by the behavior policy, the approximation power of the function class, and a measure of the complexity of the function class.},
  author = {Andrea Zanette},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zanette2023when.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40637--40668},
  pdf = {https://proceedings.mlr.press/v202/zanette23a/zanette23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When is Realizability Sufficient for Off-Policy Reinforcement Learning?},
  url = {https://proceedings.mlr.press/v202/zanette23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zargarbashi2023conformal,
  abstract = {Despite the widespread use of graph neural networks ({GNNs}) we lack methods to reliably quantify their uncertainty. We propose a conformal procedure to equip {GNNs} with prediction sets that come with distribution-free guarantees -- the output set contains the true label with arbitrarily high probability. Our post-processing procedure can wrap around any (pretrained) {GNN}, and unlike existing methods, results in meaningful sets even when the model provides only the top class. The key idea is to diffuse the node-wise conformity scores to incorporate neighborhood information. By leveraging the network homophily we construct sets with comparable or better efficiency (average size) and significantly improved singleton hit ratio (correct sets of size one). In addition to an extensive empirical evaluation, we investigate the theoretical conditions under which smoothing provably improves efficiency.},
  author = {Soroush H. Zargarbashi and Simone Antonelli and Aleksandar Bojchevski},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zargarbashi2023conformal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {12292--12318},
  pdf = {https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Conformal Prediction Sets for Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/h-zargarbashi23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zeighami2023distribution,
  abstract = {The paper addresses a fundamental problem in data management: finding elements in an array that match a query. Recently, learned indexes have been extensively used to solve this problem, where they learn a model to predict the location of items in the array. They are empirically shown to outperform non-learned methods (e.g., B-trees or binary search that answer queries in O(log n) time) by orders of magnitude. However, success of learned indexes has not been theoretically justified. Only existing attempt shows the same query time of O(log n), but with a constant factor improvement in space complexity over non-learned methods, under some assumptions on data distribution. In this paper, we significantly strengthen this result, showing that under mild assumptions on data distribution, and the same space complexity as non-learned methods, learned indexes can achieve sub-logarithmic query time.},
  author = {Sepanta Zeighami and Cyrus Shahabi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zeighami2023distribution.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40669--40680},
  pdf = {https://proceedings.mlr.press/v202/zeighami23a/zeighami23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing},
  url = {https://proceedings.mlr.press/v202/zeighami23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zenati2023sequential,
  abstract = {Counterfactual Risk Minimization ({CRM}) is a framework for dealing with the logged bandit feedback problem, where the goal is to improve a logging policy using offline data. In this paper, we explore the case where it is possible to deploy learned policies multiple times and acquire new data. We extend the {CRM} principle and its theory to this scenario, which we call ``Sequential Counterfactual Risk Minimization ({SCRM}).'' We introduce a novel counterfactual estimator and identify conditions that can improve the performance of {CRM} in terms of excess risk and regret rates, by using an analysis similar to restart strategies in accelerated optimization methods. We also provide an empirical evaluation of our method in both discrete and continuous action settings, and demonstrate the benefits of multiple deployments of {CRM}.},
  author = {Houssam Zenati and Eustache Diemert and Matthieu Martin and Julien Mairal and Pierre Gaillard},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zenati2023sequential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40681--40706},
  pdf = {https://proceedings.mlr.press/v202/zenati23a/zenati23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sequential Counterfactual Risk Minimization},
  url = {https://proceedings.mlr.press/v202/zenati23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zeng2023attributeefficient,
  abstract = {The concept class of low-degree polynomial threshold functions ({PTFs}) plays a fundamental role in machine learning. In this paper, we study {PAC} learning of K-sparse degree-d {PTFs} on $\mathbb{R}^n$, where any such concept depends only on K out of n attributes of the input. Our main contribution is a new algorithm that runs in time $(nd/\epsilon)^{O(d)}$ and under the Gaussian marginal distribution, {PAC} learns the class up to error rate $\epsilon$ with $O(K^{4d}/\epsilon^{2d} \cdot \log^{5d} n)$ samples even when an $\eta \leq O(\epsilon^d)$ fraction of them are corrupted by the nasty noise of Bshouty et al. (2002), possibly the strongest corruption model. Prior to this work, attribute-efficient robust algorithms are established only for the special case of sparse homogeneous halfspaces.},
  author = {Shiwei Zeng and Jie Shen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zeng2023attributeefficient.pdf:pdf},
  mdate = {2025-06-12},
  pages = {40719--40748},
  pdf = {https://proceedings.mlr.press/v202/zeng23b/zeng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Attribute-Efficient {PAC} Learning of Low-Degree Polynomial Threshold Functions with Nasty Noise},
  url = {https://proceedings.mlr.press/v202/zeng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zeng2023lookupffn,
  abstract = {While {GPU} clusters are the de facto choice for training large deep neural network ({DNN}) models today, several reasons including ease of workflow, security and cost have led to efforts investigating whether {CPUs} may be viable for inference in routine use in many sectors of the industry. But the imbalance between the compute capabilities of {GPUs} and {CPUs} is huge. Motivated by these considerations, we study a module which is a workhorse within modern {DNN} architectures, {GEMM} based Feed Forward Networks ({FFNs}), and assess the extent to which it can be made compute- (or {FLOP}-) lite. Specifically, we propose an alternative formulation (we call it {LookupFFN}) to {GEMM} based {FFNs} inspired by the recent studies of using Locality Sensitive Hashing ({LSH}) to approximate {FFNs}. Our formulation recasts most essential operations as a memory look-up, leveraging the trade-off between the two resources on any platform: compute and memory (since {CPUs} offer it in abundance). For {RoBERTa} language model pretraining, our formulation achieves similar performance compared to {GEMM} based {FFNs}, while dramatically reducing the required {FLOP}.},
  author = {Zhanpeng Zeng and Michael Davies and Pranav Pulijala and Karthikeyan Sankaralingam and Vikas Singh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zeng2023lookupffn.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40707--40718},
  pdf = {https://proceedings.mlr.press/v202/zeng23a/zeng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{LookupFFN}: Making Transformers Compute-lite for {CPU} inference},
  url = {https://proceedings.mlr.press/v202/zeng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zeng2023generative,
  abstract = {Dictionary learning, which approximates data samples by a set of shared atoms, is a fundamental task in representation learning. However, dictionary learning over graphs, namely graph dictionary learning ({GDL}), is much more challenging than vectorial data as graphs lie in disparate metric spaces. The sparse literature on {GDL} formulates the problem from the reconstructive view and often learns linear graph embeddings with a high computational cost. In this paper, we propose a Fused Gromov-Wasserstein ({FGW}) Mixture Model named {FraMe} to address the {GDL} problem from the generative view.},
  author = {Zhichen Zeng and Ruike Zhu and Yinglong Xia and Hanqing Zeng and Hanghang Tong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zeng2023generative.pdf:pdf},
  mdate = {2025-01-22},
  pages = {40749--40769},
  pdf = {https://proceedings.mlr.press/v202/zeng23c/zeng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Generative Graph Dictionary Learning},
  url = {https://proceedings.mlr.press/v202/zeng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhai2023stabilizing,
  abstract = {Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as entropy collapse. As a remedy, we propose $\sigma${Reparam}, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\sigma${Reparam} successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach.},
  author = {Shuangfei Zhai and Tatiana Likhomanenko and Etai Littwin and Dan Busbridge and Jason Ramapuram and Yizhe Zhang and Jiatao Gu and Joshua M. Susskind},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhai2023stabilizing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40770--40803},
  pdf = {https://proceedings.mlr.press/v202/zhai23a/zhai23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Stabilizing Transformer Training by Preventing Attention Entropy Collapse},
  url = {https://proceedings.mlr.press/v202/zhai23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023learning,
  abstract = {Generating molecules with high binding affinities to target proteins (a.k.a. structure-based drug design) is a fundamental and challenging task in drug discovery. Recently, deep generative models have achieved remarkable success in generating 3D molecules conditioned on the protein pocket. However, most existing methods consider molecular generation for protein pockets independently while neglecting the underlying connections such as subpocket-level similarities. Subpockets are the local protein environments of ligand fragments and pockets with similar subpockets may bind the same molecular fragment (motif) even though their overall structures are different. Therefore, the trained models can hardly generalize to unseen protein pockets in real-world applications. In this paper, we propose a novel method {DrugGPS} for generalizable structure-based drug design. With the biochemical priors, we propose to learn subpocket prototypes and construct a global interaction graph to model the interactions between subpocket prototypes and molecular motifs. Moreover, a hierarchical graph transformer encoder and motif-based 3D molecule generation scheme are used to improve the model's performance.},
  author = {Zaixi Zhang and Qi Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41382--41398},
  pdf = {https://proceedings.mlr.press/v202/zhang23z/zhang23z.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Subpocket Prototypes for Generalizable Structure-based Drug Design},
  url = {https://proceedings.mlr.press/v202/zhang23z.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023nonparametric,
  abstract = {The paper explores Nonparametric Iterative Machine Teaching (NIMT), which aims to teach nonparametric target models iteratively. Unlike existing parametric approaches that focus on parameter space convergence, this research treats NIMT as a functional optimization problem in the function space. The authors propose random and greedy functional teaching algorithms, analyzing their iterative teaching dimension (ITD) and verifying findings through experiments.},
  author = {Chen Zhang and Xiaofeng Cao and Weiyang Liu and Ivor W. Tsang and James T. Kwok},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023nonparametric.pdf:pdf},
  mdate = {2024-05-07},
  pages = {40851--40870},
  pdf = {https://proceedings.mlr.press/v202/zhang23c/zhang23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nonparametric Iterative Machine Teaching},
  url = {https://proceedings.mlr.press/v202/zhang23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023fedcbs,
  abstract = {Due to limited communication bandwidth of edge devices, most existing federated learning (FL) methods randomly select only a subset of devices to participate in training at each communication round. Compared with engaging all available clients, such random-selection mechanism could lead to significant performance degradation on non-IID data. The essential reason resulting in such performance degradation is the class-imbalance of the grouped data from randomly selected clients. Based on this observation, we design an efficient heterogeneity-aware client sampling mechanism, namely, Federated Class-balanced Sampling (Fed-CBS), which can effectively reduce class-imbalance of the grouped dataset from the intentionally selected clients. We employ homomorphic encryption to derive this measure in a privacy-preserving way and propose a computation-efficient client sampling strategy such that the actively selected clients will generate a more class-balanced grouped dataset with theoretical guarantees.},
  author = {Jianyi Zhang and Ang Li and Minxue Tang and Jingwei Sun and Xiang Chen and Fan Zhang and Changyou Chen and Yiran Chen and Hai Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023fedcbs.pdf:pdf},
  mdate = {2024-11-14},
  pages = {41354--41381},
  pdf = {https://proceedings.mlr.press/v202/zhang23y/zhang23y.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{Fed-CBS}: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction},
  url = {https://proceedings.mlr.press/v202/zhang23y.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023enhancing,
  abstract = {This paper explores the expressive power of deep neural networks through the framework of function compositions. We prove that repeated compositions of a single fixed-size ReLU network exhibit surprising expressive power, despite the limited expressive capabilities of the individual network. Specifically, we show that $\mathcal{L}_2\circ \boldsymbol{g}^{\circ r}\circ \boldsymbol{\mathcal{L}}_1$ can approximate $1$-Lipschitz continuous functions on $[0,1]^d$ with an error $\mathcal{O}(r^{-1/d})$, where $\boldsymbol{g}$ is realized by a fixed-size ReLU network, and $\boldsymbol{g}^{\circ r}$ denotes the $r$-times composition. We extend such results to generic continuous functions characterized by the modulus of continuity. The results reveal that a continuous-depth network generated via a dynamical system has immense approximation power even if its dynamics function is time-independent and realized by a fixed-size ReLU network.},
  author = {Shijun Zhang and Jianfeng Lu and Hongkai Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023enhancing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41452--41487},
  pdf = {https://proceedings.mlr.press/v202/zhang23ad/zhang23ad.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Enhancing Expressive Power via Compositions of Single Fixed-Size {ReLU} Network},
  url = {https://proceedings.mlr.press/v202/zhang23ad.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023learning,
  abstract = {We investigate whether the dominant approach to learn representations (as a side effect of optimizing an expected cost for a single training distribution) remains a good approach when dealing with multiple distributions. We argue that such scenarios are better served by representations that are richer than those obtained with a single optimization episode. We support this thesis with experiments utilizing an apparently naive ensembling technique: concatenating the representations obtained from multiple training episodes using the same data, model, algorithm, and hyper-parameters, but different random seeds. We find that merely training with different random seeds provides sufficient diversity to achieve excellent performances for a variety of problems involving changes in data distribution, revealing the limitations of representations constructed with a single training run.},
  author = {Jianyu Zhang and L{\'e}on Bottou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40830--40850},
  pdf = {https://proceedings.mlr.press/v202/zhang23b/zhang23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning useful representations for shifting tasks and distributions},
  url = {https://proceedings.mlr.press/v202/zhang23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023offline,
  abstract = {We study offline multi-agent reinforcement learning in Markov games, where the goal is to learn an approximate equilibrium from an offline dataset pre-collected from the game. We provide the first framework for sample-efficient offline learning in Markov games under general function approximation, handling all three equilibria (Nash equilibrium, Correlated Equilibrium, and Coarse Correlated Equilibrium) in a unified manner. Our approach uses Bellman-consistent pessimism to obtain interval estimation for policies' returns, using both upper and lower bounds to obtain a relaxation on the gap of a candidate policy. We introduce an improved data coverage condition that enhances over the recently proposed unilateral concentrability, providing theoretical guarantees for offline learning in complex environments requiring advanced function-approximation techniques.},
  author = {Yuheng Zhang and Yu Bai and Nan Jiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023offline.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40804--40829},
  pdf = {https://proceedings.mlr.press/v202/zhang23a/zhang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Offline Learning in {Markov} Games with General Function Approximation},
  url = {https://proceedings.mlr.press/v202/zhang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023convergence,
  abstract = {SARSA, a classical on-policy control algorithm for reinforcement learning, is known to chatter when combined with linear function approximation: SARSA does not diverge but oscillates in a bounded region. However, little is known about how fast SARSA converges to that region and how large the region is. We make progress towards this open problem by showing the convergence rate of projected SARSA to a bounded region. The region is much smaller than the region that they project into, provided that the magnitude of the reward is not too large. Existing works regarding the convergence of linear SARSA to a fixed point all require the Lipschitz constant of SARSA's policy improvement operator to be sufficiently small; our analysis instead applies to arbitrary Lipschitz constants and thus characterizes the behavior of linear SARSA for a new regime.},
  author = {Shangtong Zhang and R{\'e}mi Tachet des Combes and Romain Laroche},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023convergence.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41613--41646},
  pdf = {https://proceedings.mlr.press/v202/zhang23al/zhang23al.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Convergence of {SARSA} with Linear Function Approximation},
  url = {https://proceedings.mlr.press/v202/zhang23al.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023graph,
  abstract = {Graph Contrastive Learning (GCL) has attracted considerable interest due to its impressive node representation learning capability. Despite the wide application of GCL techniques, little attention has been paid to the security of GCL. In this paper, we systematically study the vulnerability of GCL in the presence of malicious backdoor adversaries. We introduce GCBA, the first backdoor attack for graph contrastive learning. GCBA incorporates three attacks: poisoning, crafting, and natural backdoor, each targeting one stage of the GCL pipeline. We formulate our attacks as optimization problems and solve them with a novel discrete optimization technique to overcome the discrete nature of graph-structured data. By extensively evaluating GCBA on multiple datasets and GCL methods, we show that our attack can achieve high attack success rates while preserving stealthiness.},
  author = {Hangfan Zhang and Jinghui Chen and Lu Lin and Jinyuan Jia and Dinghao Wu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023graph.pdf:pdf},
  mdate = {2024-08-07},
  pages = {40888--40910},
  pdf = {https://proceedings.mlr.press/v202/zhang23e/zhang23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Graph Contrastive Backdoor Attacks},
  url = {https://proceedings.mlr.press/v202/zhang23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023matrix,
  abstract = {We connect two important concepts: individual fairness (IF), which requires that individuals who are similar receive similar treatment, and matrix estimation (ME), which has emerged as a natural paradigm for handling noisy data with missing values. Our main finding is that pre-processing data using matrix estimation can improve an algorithm's individual fairness without sacrificing performance. Specifically, we show that using singular value thresholding (SVT) to pre-process the data provides a strong individual fairness guarantee under appropriate conditions. We also demonstrate that under analogous conditions, SVT pre-processing yields estimates that are consistent and approximately minimax optimal.},
  author = {Cindy Y. Zhang and Sarah Huiyi Cen and Devavrat Shah},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023matrix.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40871--40887},
  pdf = {https://proceedings.mlr.press/v202/zhang23d/zhang23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Matrix Estimation for Individual Fairness},
  url = {https://proceedings.mlr.press/v202/zhang23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023effective,
  abstract = {Existing theories on deep nonparametric regression have shown that when the input data lie on a low-dimensional manifold, deep neural networks can adapt to the intrinsic data structures. However, in real world applications, the assumption of data lying exactly on a low dimensional manifold is stringent. We introduce a relaxed assumption that the input data are concentrated around a subset of $\mathbb{R}^d$ denoted by $\mathcal{S}$, and the intrinsic dimension of $\mathcal{S}$ can be characterized by a new complexity notation -- effective Minkowski dimension. We show that the sample complexity of deep nonparametric regression only depends on the effective Minkowski dimension of $\mathcal{S}$. When the eigenvalues of $\Sigma$ have an exponential or polynomial decay, the effective Minkowski dimension is $p=\mathcal{O}(\sqrt{\log n})$ or $p=\mathcal{O}(n^\gamma)$ respectively, where $n$ is the sample size and $\gamma\in(0,1)$ is a small constant. Our theory shows that when the manifold assumption does not hold, deep neural networks can still adapt to the effective Minkowski dimension of the data, and circumvent the curse of the ambient dimensionality for moderate sample sizes.},
  author = {Zixuan Zhang and Minshuo Chen and Mengdi Wang and Wenjing Liao and Tuo Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023effective.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40911--40931},
  pdf = {https://proceedings.mlr.press/v202/zhang23f/zhang23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Effective {Minkowski} Dimension of Deep Nonparametric Regression: Function Approximation and Statistical Theories},
  url = {https://proceedings.mlr.press/v202/zhang23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023catabeem,
  abstract = {We develop CataBEEM, a novel approach for integrating latent interaction categories in node-wise community detection models for network data. This method extends block edge exchangeable models (BEEM) to incorporate categorical latent variables that capture different types of interactions between nodes. CataBEEM allows for more flexible modeling of complex network structures by accounting for heterogeneous interaction patterns within and between communities. The approach provides both theoretical advantages in terms of model flexibility and practical benefits in capturing diverse network relationships that traditional vertex-centric methods may miss.},
  author = {Yuhua Zhang and Walter H. Dempsey},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023catabeem.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40946--40975},
  pdf = {https://proceedings.mlr.press/v202/zhang23h/zhang23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CataBEEM}: Integrating Latent Interaction Categories in Node-wise Community Detection Models for Network Data},
  url = {https://proceedings.mlr.press/v202/zhang23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023rethink,
  abstract = {{DARTS} search space ({DSS}) has become a canonical benchmark for {NAS} whereas some emerging works pointed out the issue of narrow accuracy range and claimed it would hurt the method ranking. The authors propose improvements to the {DARTS} search space, creating a new benchmark called {LHD} that aims to provide a more comprehensive evaluation of neural architecture search methods while maintaining high efficiency in search.},
  author = {Jiuling Zhang and Zhiming Ding},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023rethink.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40976--40995},
  pdf = {https://proceedings.mlr.press/v202/zhang23i/zhang23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rethink {DARTS} Search Space and Renovate a New Benchmark},
  url = {https://proceedings.mlr.press/v202/zhang23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023tractable,
  abstract = {Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution is intractable for even the simplest lexical constraints. The paper proposes {GeLaTo} (Generating Language with Tractable Constraints) to address this challenge by using tractable probabilistic models to impose lexical constraints in text generation. They demonstrate their approach using distilled hidden Markov models to guide {GPT}2 generation, achieving state-of-the-art performance on benchmarks like {CommonGen}.},
  author = {Honghua Zhang and Meihua Dang and Nanyun Peng and Guy Van den Broeck},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023tractable.pdf:pdf},
  mdate = {2024-12-14},
  pages = {40932--40945},
  pdf = {https://proceedings.mlr.press/v202/zhang23g/zhang23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Tractable Control for Autoregressive Language Generation},
  url = {https://proceedings.mlr.press/v202/zhang23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023complete,
  abstract = {Recently, subgraph {GNN}s have emerged as an important direction for developing expressive graph neural networks. While numerous architectures have been proposed, there is still limited understanding of how various design paradigms differ in terms of expressive power. The paper explores the expressiveness hierarchy of subgraph {GNN}s using Subgraph {Weisfeiler-Lehman} Tests ({SWL}), identifying six equivalence classes with varying levels of expressive power and showing that {SSWL} achieves maximal expressiveness.},
  author = {Bohang Zhang and Guhao Feng and Yiheng Du and Di He and Liwei Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023complete.pdf:pdf},
  mdate = {2024-02-03},
  pages = {41019--41077},
  pdf = {https://proceedings.mlr.press/v202/zhang23k/zhang23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Complete Expressiveness Hierarchy for Subgraph {GNN}s via Subgraph {Weisfeiler-Lehman} Tests},
  url = {https://proceedings.mlr.press/v202/zhang23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023team,
  abstract = {A classic result in the theory of extensive-form games asserts that the set of strategies available to any perfect-recall player is strategically equivalent to a low-dimensional convex polytope, called the sequence-form polytope. However, when optimizing over the joint strategy space of a team of players, one cannot use the sequence form to obtain a strategically-equivalent convex description of the strategy set of the team. The paper proposes a new representation, coined team belief {DAG} ({TB-DAG}), that describes team strategies as a convex set and enjoys state-of-the-art parameterized complexity bounds while enabling efficient regret minimization techniques.},
  author = {Brian Hu Zhang and Gabriele Farina and Tuomas Sandholm},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023team.pdf:pdf},
  mdate = {2023-08-28},
  pages = {40996--41018},
  pdf = {https://proceedings.mlr.press/v202/zhang23j/zhang23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Team Belief {DAG}: Generalizing the Sequence Form to Team Games for Fast Computation of Correlated Team Max-Min Equilibria via Regret Minimization},
  url = {https://proceedings.mlr.press/v202/zhang23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023crafting,
  abstract = {Super-resolution ({SR}) techniques designed for real-world applications commonly encounter two primary challenges: generalization performance and restoration accuracy. When methods are trained using complex, large-range degradations to enhance generalization, a decline in accuracy is inevitable. The authors introduce a novel approach to craft training degradation distributions using a small set of reference images, founded upon the binned representation of the degradation space and the {Fr\'echet} distance between degradation distributions, significantly improving performance while preserving generalization capabilities.},
  author = {Ruofan Zhang and Jinjin Gu and Haoyu Chen and Chao Dong and Yulun Zhang and Wenming Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023crafting.pdf:pdf},
  mdate = {2024-10-25},
  pages = {41078--41091},
  pdf = {https://proceedings.mlr.press/v202/zhang23l/zhang23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution},
  url = {https://proceedings.mlr.press/v202/zhang23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023when,
  abstract = {Graph Neural Networks ({GNN}s) are powerful models for non-{Euclidean} data, but their training is often accentuated by massive unnecessary computation and class imbalance properties. The paper develops a unified data model dynamic sparsity framework called {DataDec} to address these challenges by identifying informative subsets dynamically during training through sparse graph contrastive learning, achieving better class-balanced representations with less graph data.},
  author = {Chunhui Zhang and Chao Huang and Yijun Tian and Qianlong Wen and Zhongyu Ouyang and Youhuan Li and Yanfang Ye and Chuxu Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023when.pdf:pdf},
  mdate = {2024-07-16},
  pages = {41133--41150},
  pdf = {https://proceedings.mlr.press/v202/zhang23o/zhang23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {When Sparsity Meets Contrastive Models: Less Graph Data Can Bring Better Class-Balanced Representations},
  url = {https://proceedings.mlr.press/v202/zhang23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023interplay,
  abstract = {The paper studies linear contextual bandits in the misspecified setting, where the expected reward function can be approximated by a linear function class up to a bounded misspecification level. The authors propose an algorithm based on a novel data selection scheme and show that when the misspecification level is dominated by the minimal sub-optimality gap, their algorithm enjoys the same gap-dependent regret bound as in the well-specified setting, revealing an important interplay between misspecification and sub-optimality gap.},
  author = {Weitong Zhang and Jiafan He and Zhiyuan Fan and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023interplay.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41111--41132},
  pdf = {https://proceedings.mlr.press/v202/zhang23n/zhang23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits},
  url = {https://proceedings.mlr.press/v202/zhang23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023spatialtemporal,
  abstract = {Spatial-temporal graph learning has emerged as the state-of-the-art solution for modeling structured spatial-temporal data in learning region representations for various urban sensing tasks. However, most existing models are vulnerable to the quality of the generated region graph due to ubiquitous spatial-temporal data noise and incompleteness. The paper proposes {STAG} (Spatial-Temporal Adversarial Graph contrastive learning model) for adaptive self-supervised graph augmentation with learnable contrastive learning function and adversarial contrastive learning mechanism to enhance representation discrimination ability and robustness.},
  author = {Qianru Zhang and Chao Huang and Lianghao Xia and Zheng Wang and Siu Ming Yiu and Ruihua Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023spatialtemporal.pdf:pdf},
  mdate = {2023-10-31},
  pages = {41151--41163},
  pdf = {https://proceedings.mlr.press/v202/zhang23p/zhang23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Spatial-Temporal Graph Learning with Adversarial Contrastive Adaptation},
  url = {https://proceedings.mlr.press/v202/zhang23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023cab,
  abstract = {Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. However, {Long Range Arena} ({LRA}) only focuses on the standard bidirectional self attention, and completely ignores cross attentions and unidirectional attentions. The paper proposes Comprehensive Attention Benchmark ({CAB}) under a fine-grained attention taxonomy with four distinguishable attention patterns: noncausal self, causal self, noncausal cross, and causal cross attentions.},
  author = {Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023cab.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41194--41218},
  pdf = {https://proceedings.mlr.press/v202/zhang23r/zhang23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{CAB}: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  url = {https://proceedings.mlr.press/v202/zhang23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023adaptive,
  abstract = {Differentiable physics-based simulators have witnessed remarkable success in robot learning involving contact dynamics, benefiting from their improved accuracy and efficiency in solving the underlying complementarity problem. However, when utilizing the First-Order Policy Gradient ({FOPG}) method, the complementarity-based systems suffer from stiffness, leading to an explosion in the gradient variance. The paper proposes Adaptive Barrier Smoothing ({ABS}), which introduces a class of softened complementarity systems that correspond to barrier-smoothed objectives to tackle the non-smooth optimization landscape.},
  author = {Shenao Zhang and Wanxin Jin and Zhaoran Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023adaptive.pdf:pdf},
  mdate = {2023-12-27},
  pages = {41219--41243},
  pdf = {https://proceedings.mlr.press/v202/zhang23s/zhang23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Adaptive Barrier Smoothing for First-Order Policy Gradient with Contact Dynamics},
  url = {https://proceedings.mlr.press/v202/zhang23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023towards,
  abstract = {Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose CoPaint, which can coherently inpaint the whole image without introducing mismatches. CoPaint also uses the Bayesian framework to jointly modify both revealed and unrevealed regions but approximates the posterior distribution in a way that allows the errors to gradually drop to zero throughout the denoising steps, thus strongly penalizing any mismatches with the reference image. Our experiments verify that CoPaint can outperform the existing diffusion-based methods under both objective and subjective metrics.},
  author = {Guanhua Zhang and Jiabao Ji and Yang Zhang 0001 and Mo Yu and Tommi S. Jaakkola and Shiyu Chang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023towards.pdf:pdf},
  mdate = {2024-01-29},
  pages = {41164--41193},
  pdf = {https://proceedings.mlr.press/v202/zhang23q/zhang23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models},
  url = {https://proceedings.mlr.press/v202/zhang23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023onestep,
  abstract = {This paper considers the unlabeled sparse recovery under multiple measurements, i.e., $\mathbf{Y} = \mathbf{\Pi}^{\natural} \mathbf{X} \mathbf{B}^{\natural} + \mathbf{W}$, where $\mathbf{Y} \in \mathbb{R}^{n\times m}, \mathbf{\Pi}^{\natural}\in \mathbb{R}^{n\times n}, \mathbf{X} \in \mathbb{R}^{n\times p}, \mathbf{B}^{\natural}\in \mathbb{R}^{p\times m}, \mathbf{W}\in \mathbb{R}^{n\times m}$ represents the observations, missing (or incomplete) correspondence information, sensing matrix, sparse signals, and additive sensing noise, respectively. Different from the previous works on multiple measurements ($m > 1$) which all focus on the sufficient samples regime, namely, $n > p$, we consider a sparse matrix $\mathbf{B}^{\natural}$ and investigate the insufficient samples regime (i.e., $n \ll p$) for the first time. To begin with, we establish the lower bound on the sample number and signal-to-noise ratio (SNR) for the correct permutation recovery. Moreover, we present a simple yet effective estimator. Under mild conditions, we show that our estimator can restore the correct correspondence information with high probability. Numerical experiments are presented to corroborate our theoretical claims.},
  author = {Hang Zhang and Ping Li 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023onestep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41244--41267},
  pdf = {https://proceedings.mlr.press/v202/zhang23t/zhang23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {One-Step Estimator for Permuted Sparse Recovery},
  url = {https://proceedings.mlr.press/v202/zhang23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023quantum,
  abstract = {Quantum computing is an emerging technology that has been rapidly advancing in the past decades. In this paper, we conduct a systematic study of quantum lower bounds on finding $\epsilon$-approximate stationary points of nonconvex functions, and we consider the following two important settings: 1) having access to $p$-th order derivatives; or 2) having access to stochastic gradients. The classical query lower bounds are $\Omega\big(\epsilon^{-\frac{1+p}{p}}\big)$ regarding the first setting and $\Omega(\epsilon^{-4})$ regarding the second setting (or $\Omega(\epsilon^{-3})$ if the stochastic gradient function is mean-squared smooth). In this paper, we extend all these classical lower bounds to the quantum setting. They match the classical algorithmic results respectively, demonstrating that there is no quantum speedup for finding $\epsilon$-stationary points of nonconvex functions with $p$-th order derivative inputs or stochastic gradient inputs. Technically, we prove our quantum lower bounds by showing that the sequential nature of classical hard instances in all these settings also applies to quantum queries, preventing any quantum speedup other than revealing information of the stationary points sequentially.},
  author = {Chenyi Zhang and Tongyang Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023quantum.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41268--41299},
  pdf = {https://proceedings.mlr.press/v202/zhang23u/zhang23u.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Quantum Lower Bounds for Finding Stationary Points of Nonconvex Functions},
  url = {https://proceedings.mlr.press/v202/zhang23u.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023improving,
  abstract = {Health conditions among patients in intensive care units (ICUs) are monitored via electronic health records (EHRs), composed of numerical time series and lengthy clinical note sequences, both taken at irregular time intervals. Dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging problem. Our method first addresses irregularity in each single modality by (1) modeling irregular time series by dynamically incorporating hand-crafted imputation embeddings into learned interpolation embeddings via a gating mechanism, and (2) casting a series of clinical note representations as multivariate irregular time series and tackling irregularity via a time attention mechanism. We further integrate irregularity in multimodal fusion with an interleaved attention mechanism across temporal steps. To the best of our knowledge, this is the first work to thoroughly model irregularity in multimodalities for improving medical predictions. Our proposed methods for two medical prediction tasks consistently outperforms state-of-the-art (SOTA) baselines in each single modality and multimodal fusion scenarios. Specifically, we observe relative improvements of 6.5\%, 3.6\%, and 4.3\% in F1 for time series, clinical notes, and multimodal fusion, respectively.},
  author = {Xinlu Zhang and Shiyang Li and Zhiyu Chen 0002 and Xifeng Yan and Linda Ruth Petzold},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023improving.pdf:pdf},
  mdate = {2024-08-04},
  pages = {41300--41313},
  pdf = {https://proceedings.mlr.press/v202/zhang23v/zhang23v.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improving Medical Predictions by Irregular Multimodal Electronic Health Records Modeling},
  url = {https://proceedings.mlr.press/v202/zhang23v.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023fedcr,
  abstract = {In personalized federated learning (PFL), multiple clients train customized models to fulfill their personal objectives, which, however, are prone to overfitting to local data due to the heterogeneity and scarcity of local data. To address this, we propose from the information-theoretic perspective a personalized federated learning framework based on the common representation learned across clients, named FedCR. Specifically, we introduce to the local client update a regularizer that aims at minimizing the discrepancy between local and global conditional mutual information (CMI), such that clients are encouraged to learn and exploit the common representation. Upon this, each client learns individually a customized predictor (head), while the extractor (body) remains to be aggregated by the server. Our CMI regularizer leads to a theoretically sound alignment between the local and global stochastic feature distributions in terms of their Kullback-Leibler (KL) divergence. More importantly, by modeling the global joint feature distribution as a product of multiple local feature distributions, clients can efficiently extract diverse information from the global data but without need of the raw data from other clients. We further show that noise injection via feature alignment and ensemble of local predictors in FedCR would help enhance its generalization capability. Experiments on benchmark datasets demonstrate a consistent performance gain and better generalization behavior of FedCR.},
  author = {Hao Zhang and Chenglin Li and Wenrui Dai and Junni Zou and Hongkai Xiong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023fedcr.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41314--41330},
  pdf = {https://proceedings.mlr.press/v202/zhang23w/zhang23w.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {FedCR: Personalized Federated Learning Based on Across-Client Common Representation with Conditional Mutual Information Regularization},
  url = {https://proceedings.mlr.press/v202/zhang23w.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023optimality,
  abstract = {In the misspecified kernel ridge regression problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}<\infty$ which implicitly requires $s > \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the KRR is optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that KRR is minimax optimal for any $s\in (0,1)$ when the $\mathcal{H}$ is a Sobolev RKHS.},
  author = {Haobo Zhang 0004 and Yicheng Li and Weihao Lu 0002 and Qian Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023optimality.pdf:pdf},
  mdate = {2025-01-28},
  pages = {41331--41353},
  pdf = {https://proceedings.mlr.press/v202/zhang23x/zhang23x.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Optimality of Misspecified Kernel Ridge Regression},
  url = {https://proceedings.mlr.press/v202/zhang23x.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023one,
  abstract = {Federated learning suffers from a latency bottleneck induced by network stragglers, which hampers the training efficiency significantly. In addition, due to the heterogeneous data distribution and security requirements, simple and fast averaging aggregation is not feasible anymore. Instead, complicated aggregation operations, such as knowledge distillation, are required. The time cost for complicated aggregation becomes a new bottleneck that limits the computational efficiency of FL. In this work, we claim that the root cause of training latency actually lies in the aggregation-then-broadcasting workflow of the server. By swapping the computational order of aggregation and broadcasting, we propose a novel and efficient parallel federated learning (PFL) framework that unlocks the edge nodes during global computation and the central server during local computation.},
  author = {Feilong Zhang 0002 and Xianming Liu and Shiyi Lin and Gang Wu 0010 and Xiong Zhou and Junjun Jiang and Xiangyang Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023one.pdf:pdf},
  mdate = {2024-10-06},
  pages = {41399--41413},
  pdf = {https://proceedings.mlr.press/v202/zhang23aa/zhang23aa.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {No One Idles: Efficient Heterogeneous Federated Learning with Parallel Edge and Server Computation},
  url = {https://proceedings.mlr.press/v202/zhang23aa.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023wisdom,
  abstract = {Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying reinforcement learning algorithm is complex and requires additional training for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised fine-tuning.},
  author = {Tianjun Zhang and Fangchen Liu and Justin Wong and Pieter Abbeel and Joseph E. Gonzalez},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023wisdom.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41414--41428},
  pdf = {https://proceedings.mlr.press/v202/zhang23ab/zhang23ab.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
  url = {https://proceedings.mlr.press/v202/zhang23ab.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023detecting,
  abstract = {Adversarial detection aims to determine whether a given sample is an adversarial one based on the discrepancy between natural and adversarial distributions. Unfortunately, estimating or comparing two data distributions is extremely difficult, especially in high-dimension spaces. Recently, the gradient of log probability density (a.k.a., score) w.r.t. the sample is used as an alternative statistic to compute. However, we find that the score is sensitive in identifying adversarial samples due to insufficient information with one sample only. In this paper, we propose a new statistic called expected perturbation score (EPS), which is essentially the expected score of a sample after various perturbations. Specifically, to obtain adequate information regarding one sample, we perturb it by adding various noises to capture its multi-view observations.},
  author = {Shuhai Zhang and Feng Liu 0003 and Jiahao Yang and Yifan Yang and Changsheng Li and Bo Han 0003 and Mingkui Tan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023detecting.pdf:pdf},
  mdate = {2024-02-14},
  pages = {41429--41451},
  pdf = {https://proceedings.mlr.press/v202/zhang23ac/zhang23ac.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Detecting Adversarial Data by Probing Multiple Perturbations Using Expected Perturbation Score},
  url = {https://proceedings.mlr.press/v202/zhang23ac.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023towards,
  abstract = {Federated Semi-supervised Learning (FedSSL) has emerged as a new paradigm for allowing distributed clients to collaboratively train a machine learning model over scarce labeled data and abundant unlabeled data. However, existing works for FedSSL rely on a closed-world assumption that all local training data and global testing data are from seen classes observed in the labeled dataset. It is crucial to go one step further: adapting FL models to an open-world setting, where unseen classes exist in the unlabeled data. In this paper, we propose a novel Federated open-world Semi-Supervised Learning (FedoSSL) framework, which can solve the key challenge in distributed and open-world settings, i.e., the biased training process for heterogeneously distributed unseen classes.},
  author = {Jie Zhang 0076 and Xiaosong Ma and Song Guo 0001 and Wenchao Xu 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023towards.pdf:pdf},
  mdate = {2024-07-20},
  pages = {41498--41509},
  pdf = {https://proceedings.mlr.press/v202/zhang23af/zhang23af.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Unbiased Training in Federated Open-world Semi-supervised Learning},
  url = {https://proceedings.mlr.press/v202/zhang23af.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023interactive,
  abstract = {Object placement aims to insert a foreground object into a background image with a suitable location and size to create a natural composition. To predict a diverse distribution of placements, existing methods usually establish a one-to-one mapping from random vectors to the placements. However, these random vectors are not interpretable, which prevents users from interacting with the object placement process. To address this problem, we propose an Interactive Object Placement method with Reinforcement Learning, dubbed IOPRE, to make sequential decisions for producing a reasonable placement given an initial location and size of the foreground. We first design a novel action space to flexibly and stably adjust the location and size of the foreground while preserving its aspect ratio.},
  author = {Shengping Zhang and Quanling Meng and Qinglin Liu and Liqiang Nie and Bineng Zhong and Xiaopeng Fan and Rongrong Ji},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023interactive.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41510--41522},
  pdf = {https://proceedings.mlr.press/v202/zhang23ag/zhang23ag.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Interactive Object Placement with Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/zhang23ag.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023optimal,
  abstract = {In this work, we address the problem of Hessian inversion bias in distributed second-order optimization algorithms. We introduce a novel shrinkage-based estimator for the resolvent of gram matrices which is asymptotically unbiased, and characterize its non-asymptotic convergence rate in the isotropic case. We apply this estimator to bias correction of Newton steps in distributed second-order optimization algorithms, as well as randomized sketching based methods. The work examines the bias present in the naive averaging-based distributed Newton's method using analytical expressions and contrasts it with their proposed bias-free approach. Their approach leads to significant improvements in convergence rate compared to standard baselines and recent proposals, as shown through experiments on both real and synthetic datasets.},
  author = {Fangzhao Zhang and Mert Pilanci},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41523--41549},
  pdf = {https://proceedings.mlr.press/v202/zhang23ah/zhang23ah.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Shrinkage for Distributed Second-Order Optimization},
  url = {https://proceedings.mlr.press/v202/zhang23ah.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023categorytheoretical,
  abstract = {Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate one for their specific context.},
  author = {Yivan Zhang and Masashi Sugiyama},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023categorytheoretical.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41596--41612},
  pdf = {https://proceedings.mlr.press/v202/zhang23ak/zhang23ak.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Category-theoretical Meta-analysis of Definitions of Disentanglement},
  url = {https://proceedings.mlr.press/v202/zhang23ak.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023learning,
  abstract = {This paper studies Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. The authors propose a framework called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Their approach is easy to tune and is able to focus on local regions of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. They show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. They demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.},
  author = {Fengxue Zhang and Jialin Song and James C. Bowden and Alexander Ladd and Yisong Yue and Thomas Desautels and Yuxin Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41579--41595},
  pdf = {https://proceedings.mlr.press/v202/zhang23aj/zhang23aj.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation},
  url = {https://proceedings.mlr.press/v202/zhang23aj.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023why,
  abstract = {The paper addresses a critical problem in machine learning deployment. Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions.},
  author = {Haoran Zhang and Harvineet Singh and Marzyeh Ghassemi and Shalmali Joshi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023why.pdf:pdf},
  mdate = {2024-04-26},
  pages = {41550--41578},
  pdf = {https://proceedings.mlr.press/v202/zhang23ai/zhang23ai.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {"Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts},
  url = {https://proceedings.mlr.press/v202/zhang23ai.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023generalization,
  abstract = {Multi-modal contrastive learning (MMCL) has recently garnered considerable interest due to its superior performance in visual tasks, achieved by embedding multi-modal data, such as visual-language pairs. However, there still lack theoretical understandings of how MMCL extracts useful visual representation from multi-modal pairs, and particularly, how MMCL outperforms previous approaches like self-supervised contrastive learning (SSCL). In this paper, by drawing an intrinsic connection between MMCL and asymmetric matrix factorization, we establish the first generalization guarantees of MMCL for visual downstream tasks. Based on this framework, we further unify MMCL and SSCL by showing that MMCL implicitly performs SSCL with (pseudo) positive pairs induced by text pairs. Through this unified perspective, we characterize the advantage of MMCL by showing that text pairs induce more semantically consistent and diverse positive pairs, which, according to our analysis, provably benefit downstream generalization.},
  author = {Qi Zhang and Yifei Wang and Yisen Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023generalization.pdf:pdf},
  mdate = {2025-05-21},
  pages = {41677--41693},
  pdf = {https://proceedings.mlr.press/v202/zhang23an/zhang23an.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On the Generalization of Multi-modal Contrastive Learning},
  url = {https://proceedings.mlr.press/v202/zhang23an.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023concernet,
  abstract = {Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin.},
  author = {Wang Zhang and Tsui-Wei Weng and Subhro Das and Alexandre Megretski and Luca Daniel and Lam M. Nguyen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023concernet.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41694--41714},
  pdf = {https://proceedings.mlr.press/v202/zhang23ao/zhang23ao.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction},
  url = {https://proceedings.mlr.press/v202/zhang23ao.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023adanpc,
  abstract = {Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls k closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distribution with very little extra computation cost.},
  author = {Yifan Zhang and Xue Wang and Kexin Jin and Kun Yuan and Zhang Zhang and Liang Wang and Rong Jin and Tieniu Tan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023adanpc.pdf:pdf},
  mdate = {2025-05-30},
  pages = {41647--41676},
  pdf = {https://proceedings.mlr.press/v202/zhang23am/zhang23am.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation},
  url = {https://proceedings.mlr.press/v202/zhang23am.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023towards,
  abstract = {With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales.},
  author = {Wenbo Zhang and Tong Wu and Yunlong Wang and Yong Cai and Hengrui Cai},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023towards.pdf:pdf},
  mdate = {2025-05-14},
  pages = {41715--41736},
  pdf = {https://proceedings.mlr.press/v202/zhang23ap/zhang23ap.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards Trustworthy Explanation: On Causal Rationalization},
  url = {https://proceedings.mlr.press/v202/zhang23ap.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023demystifying,
  abstract = {While graph neural networks (GNNs) dominate the state-of-the-art for exploring graphs in real-world applications, they have been shown to be vulnerable to a growing number of privacy attacks. For instance, link stealing is a well-known membership inference attack (MIA) on edges that infers the presence of an edge in a GNN's training graph. Recent studies on independent and identically distributed data (e.g., images) have empirically demonstrated that individuals from different groups suffer from different levels of privacy risks to MIAs, i.e., uneven vulnerability. However, theoretical evidence of such uneven vulnerability is missing. In this paper, we first present theoretical evidence of the uneven vulnerability of GNNs to link stealing attacks, which lays the foundation for demystifying such uneven risks among different groups of edges.},
  author = {He Zhang and Bang Wu and Shuo Wang and Xiangwen Yang and Minhui Xue and Shirui Pan and Xingliang Yuan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023demystifying.pdf:pdf},
  mdate = {2024-10-16},
  pages = {41737--41752},
  pdf = {https://proceedings.mlr.press/v202/zhang23aq/zhang23aq.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Demystifying Uneven Vulnerability of Link Stealing Attacks against Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/zhang23aq.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023provable,
  abstract = {The inherent challenge of multimodal fusion is to precisely capture the cross-modal correlation and flexibly conduct cross-modal interaction. To fully release the value of each modality and mitigate the influence of low-quality multimodal data, dynamic multimodal fusion emerges as a promising learning paradigm. Despite its widespread use, theoretical justifications in this field are still notably lacking. Can we design a provably robust multimodal fusion method? This paper provides theoretical understandings to answer this question under a most popular multimodal fusion framework from the generalization perspective. We proceed to reveal that several uncertainty estimation solutions are naturally available to achieve robust multimodal fusion. Then a novel multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is proposed, which can improve the performance in terms of classification accuracy and model robustness. Extensive experimental results on multiple benchmarks can support our findings.},
  author = {Qingyang Zhang and Haitao Wu and Changqing Zhang and Qinghua Hu and Huazhu Fu and Joey Tianyi Zhou and Xi Peng 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023provable.pdf:pdf},
  mdate = {2024-02-08},
  pages = {41753--41769},
  pdf = {https://proceedings.mlr.press/v202/zhang23ar/zhang23ar.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provable Dynamic Fusion for Low-Quality Multimodal Data},
  url = {https://proceedings.mlr.press/v202/zhang23ar.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023nearly,
  abstract = {In this paper, we investigate the online allocation problem of maximizing the overall revenue subject to both lower and upper bound constraints. Compared to the extensively studied online problems with only resource upper bounds, the two-sided constraints affect the prospects of resource consumption more severely. We consider a general online allocation problem with finite types of requests and two-sided resource constraints, and design an efficient online algorithm that achieves a nearly optimal competitive ratio. Our algorithm combines the primal-dual technique and the online learning framework, where we maintain dual variables to guide the allocation decisions. We prove that our algorithm achieves a competitive ratio of $1-O(\sqrt{(\log m \log T)/T})$ where $m$ is the number of resource types and $T$ is the number of time steps. We also provide a lower bound showing that no online algorithm can achieve a competitive ratio better than $1-\Omega(1/\sqrt{T})$, thus our result is nearly optimal. Numerical experiments demonstrate the effectiveness of our proposed algorithm.},
  author = {Qixin Zhang 0001 and Wenbing Ye and Zaiyi Chen and Haoyuan Hu and Enhong Chen and Yu Yang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023nearly.pdf:pdf},
  mdate = {2025-05-27},
  pages = {41786--41818},
  pdf = {https://proceedings.mlr.press/v202/zhang23at/zhang23at.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly Optimal Competitive Ratio for Online Allocation Problems with Two-sided Resource Constraints and Finite Requests},
  url = {https://proceedings.mlr.press/v202/zhang23at.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023coder,
  abstract = {Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17\% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.},
  author = {Tianyi Zhang and Tao Yu 0009 and Tatsunori Hashimoto and Mike Lewis and Wen-Tau Yih and Daniel Fried and Sida Wang 0001},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023coder.pdf:pdf},
  mdate = {2024-01-28},
  pages = {41832--41846},
  pdf = {https://proceedings.mlr.press/v202/zhang23av/zhang23av.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Coder Reviewer Reranking for Code Generation},
  url = {https://proceedings.mlr.press/v202/zhang23av.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023you,
  abstract = {Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. We also exploit the genuine audio similarity to guide the fine-tuning procedure. Our experiments demonstrate that RAWM can effectively overcome catastrophic forgetting and significantly improve the performance of fake audio detection in continual learning scenarios.},
  author = {Xiaohui Zhang 0006 and Jiangyan Yi and Jianhua Tao 0001 and Chenglong Wang and Chu Yuan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023you.pdf:pdf},
  mdate = {2024-04-04},
  pages = {41819--41831},
  pdf = {https://proceedings.mlr.press/v202/zhang23au/zhang23au.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection},
  url = {https://proceedings.mlr.press/v202/zhang23au.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023redi,
  abstract = {Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.},
  author = {Kexun Zhang and Xianjun Yang and William Yang Wang and Lei Li 0005},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023redi.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41770--41785},
  pdf = {https://proceedings.mlr.press/v202/zhang23as/zhang23as.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval},
  url = {https://proceedings.mlr.press/v202/zhang23as.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023nearlytight,
  author = {Yifan Zhang and Min-Ling Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023nearlytight.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41861--41879},
  pdf = {https://proceedings.mlr.press/v202/zhang23ax/zhang23ax.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Nearly-tight Bounds for Deep Kernel Learning},
  url = {https://proceedings.mlr.press/v202/zhang23ax.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023openfe,
  abstract = {The goal of automated feature generation is to liberate machine learning experts from the laborious task of manual feature generation, which is crucial for improving the learning performance of tabular data. The major challenge in automated feature generation is to efficiently and accurately identify effective features from a vast pool of candidate features. In this paper, we present OpenFE, an automated feature generation tool that provides competitive results against machine learning experts. OpenFE achieves high efficiency and accuracy with two components: 1) a novel feature boosting method for accurately evaluating the incremental performance of candidate features and 2) a two-stage pruning algorithm that performs feature pruning in a coarse-to-fine manner. Extensive experiments on ten benchmark datasets show that OpenFE outperforms existing baseline methods by a large margin. We further evaluate OpenFE in two Kaggle competitions with thousands of data science teams participating. In the two competitions, features generated by OpenFE with a simple baseline model can beat 99.3\% and 99.6\% data science teams respectively. In addition to the empirical results, we provide a theoretical perspective to show that feature generation can be beneficial in a simple yet representative setting.},
  author = {Tianping Zhang and Zheyu Aqa Zhang and Zhiyuan Fan and Haoyan Luo and Fengyuan Liu and Qian Liu and Wei Cao and Li Jian},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023openfe.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41880--41901},
  pdf = {https://proceedings.mlr.press/v202/zhang23ay/zhang23ay.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {OpenFE: Automated Feature Generation with Expert-level Performance},
  url = {https://proceedings.mlr.press/v202/zhang23ay.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023optimal,
  abstract = {We study reward-free reinforcement learning (RL) with linear function approximation, where the agent works in two phases: (1) in the exploration phase, the agent interacts with the environment but cannot access the reward; and (2) in the planning phase, the agent is given a reward function and is expected to find a near-optimal policy based on samples collected in the exploration phase. The sample complexities of existing reward-free algorithms have a polynomial dependence on the planning horizon, which makes them intractable for long planning horizon RL problems. In this paper, we propose a new reward-free algorithm for learning linear mixture Markov decision processes (MDPs), where the transition probability can be parameterized as a linear combination of known feature mappings. Our algorithm achieves a sample complexity that is independent of the planning horizon for both the exploration and planning phases. To our knowledge, this is the first horizon-free sample complexity bound for reward-free RL with function approximation.},
  author = {Junkai Zhang and Weitong Zhang and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023optimal.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41902--41930},
  pdf = {https://proceedings.mlr.press/v202/zhang23az/zhang23az.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs},
  url = {https://proceedings.mlr.press/v202/zhang23az.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023unlocking,
  abstract = {Slot attention is a powerful method for object-centric modeling in images and videos. However, its set-equivariance limits its ability to handle videos with a dynamic number of objects because it cannot break ties. To overcome this limitation, we first establish a connection between slot attention and optimal transport. Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn): a cross-attention module that combines the tiebreaking properties of unregularized optimal transport with the speed of regularized optimal transport. We evaluate slot attention using MESH on multiple object-centric learning benchmarks and find significant improvements over slot attention in every setting.},
  author = {Yan Zhang and David W. Zhang and Simon Lacoste-Julien and Gertjan J. Burghouts and Cees G. M. Snoek},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023unlocking.pdf:pdf},
  mdate = {2023-08-28},
  pages = {41931--41951},
  pdf = {https://proceedings.mlr.press/v202/zhang23ba/zhang23ba.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unlocking Slot Attention by Changing Optimal Transport Costs},
  url = {https://proceedings.mlr.press/v202/zhang23ba.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{2023towards,
  abstract = {Recent works have identified that existing methods, which construct persistence diagrams in Topological Data Analysis (TDA), are not robust to noise and varied densities in a point cloud. We analyze the necessary properties of an approach that can address these two issues, and propose a new filter function for TDA based on a new data-dependent kernel which possesses these properties. Our empirical evaluation reveals that the proposed filter function provides a better means for t-SNE visualization and SVM classification than three existing methods of TDA.},
  author = {Hang Zhang 0003 and Kaifeng Zhang and Kai Ming Ting and Ye Zhu 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/2023towards.pdf:pdf},
  mdate = {2025-05-08},
  pages = {41952--41972},
  pdf = {https://proceedings.mlr.press/v202/zhang23bb/zhang23bb.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Towards a Persistence Diagram that is Robust to Noise and Varied Densities},
  url = {https://proceedings.mlr.press/v202/zhang23bb.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023patchlevel,
  abstract = {Dense contrastive learning (DCL) has been recently explored for learning localized information for dense prediction tasks (e.g., detection and segmentation). It still suffers the difficulty of mining pixels/patches correspondence between two views. A simple way is inputting the same view twice and aligning the pixel/patch representation. However, it would reduce the variance of inputs, and hurts the performance. We propose a plug-in method PQCL (Positional Query for patch-level Contrastive Learning), which allows performing patch-level contrasts between two views with exact patch correspondence. Besides, by using positional queries, PQCL increases the variance of inputs, to enhance training. We apply PQCL to popular transformer-based CL frameworks (DINO and iBOT), and evaluate them on classification, detection and segmentation tasks, where our method obtains stable improvements, especially for dense tasks. It achieves new state-of-the-art in most settings.},
  author = {Shaofeng Zhang and Qiang Zhou and Zhibin Wang and Fan Wang 0019 and Junchi Yan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023patchlevel.pdf:pdf},
  mdate = {2024-08-06},
  pages = {41990--41999},
  pdf = {https://proceedings.mlr.press/v202/zhang23bd/zhang23bd.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Patch-level Contrastive Learning via Positional Query for Visual Pre-training},
  url = {https://proceedings.mlr.press/v202/zhang23bd.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhang2023robust,
  abstract = {The paper addresses a challenge in reinforcement learning where some state features, called contexts, are independent of action signals (such as customer demand in inventory control or speed of lead car in autonomous driving). The key problem is that true context transitions can be exposed to unknown sources of contamination, leading to shifts between source and target domains that cause performance degradation for RL algorithms. While existing robust RL methods aim to learn policies robust against deviations of entire system dynamics, this paper proposes the framework of robust situational Markov decision process (RS-MDP) which explicitly captures possible deviations of context transitions.},
  author = {Jinpeng Zhang and Yufeng Zheng and Chuheng Zhang and Li Zhao 0007 and Lei Song 0001 and Yuan Zhou 0007 and Jiang Bian 0002},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhang2023robust.pdf:pdf},
  mdate = {2025-06-25},
  pages = {41973--41989},
  pdf = {https://proceedings.mlr.press/v202/zhang23bc/zhang23bc.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Robust Situational Reinforcement Learning in Face of Context Disturbances},
  url = {https://proceedings.mlr.press/v202/zhang23bc.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023does,
  abstract = {Distribution shift (e.g., task or domain shift) in continual learning (CL) usually results in catastrophic forgetting of previously learned knowledge. Although it can be alleviated by repeatedly replaying buffered data, the every-step replay is time-consuming. In this paper, we study which modules in neural networks are more prone to forgetting by investigating their training dynamics during CL. Our proposed metrics show that only a few modules are more task-specific and sensitive to task change, while others can be shared across tasks as common knowledge. Hence, we attribute forgetting mainly to the former and find that finetuning them only on a small buffer at the end of any CL method can bring non-trivial improvement.},
  author = {Haiyan Zhao and Tianyi Zhou 0001 and Guodong Long and Jing Jiang 0002 and Chengqi Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023does.pdf:pdf},
  mdate = {2023-09-19},
  pages = {42280--42303},
  pdf = {https://proceedings.mlr.press/v202/zhao23n/zhao23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Does Continual Learning Equally Forget All Parameters?},
  url = {https://proceedings.mlr.press/v202/zhao23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023men,
  abstract = {The phenomenon of bias amplification occurs when models amplify training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., computer). However, large-scale datasets typically consist of instances with multiple attribute annotations (e.g., {computer, keyboard}). The authors demonstrate that models can learn to exploit correlations with respect to multiple attributes, which are not accounted for by current metrics. Moreover, they show that current metrics can give the erroneous impression that little to no bias amplification has occurred as they aggregate positive and negative bias scores. Further, these metrics lack an ideal value, making them difficult to interpret.},
  author = {Dora Zhao and Jerone Theodore Alexander Andrews and Alice Xiang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023men.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42000--42017},
  pdf = {https://proceedings.mlr.press/v202/zhao23a/zhao23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Men Also Do Laundry: Multi-Attribute Bias Amplification},
  url = {https://proceedings.mlr.press/v202/zhao23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023rockmate,
  abstract = {We propose Rockmate to control the memory requirements when training PyTorch DNN models. Rockmate is an automatic tool that starts from the model code and generates an equivalent model, using a predefined amount of memory for activations, at the cost of a few re-computations. Rockmate automatically detects the structure of computational and data dependencies and rewrites the initial model as a sequence of complex blocks. We show that such a structure is widespread and can be found in many models in the literature (Transformer based models, ResNet, RegNets, etc.). This structure allows us to solve the problem in a fast and efficient way, using an adaptation of Checkmate at the level of individual blocks and an adaptation of Rotor at the level of the sequence itself.},
  author = {Xunyi Zhao and Thotime Le Hellard and Lionel Eyraud-Dubois and Julia Gusak and Olivier Beaumont},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023rockmate.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42018--42045},
  pdf = {https://proceedings.mlr.press/v202/zhao23b/zhao23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch},
  url = {https://proceedings.mlr.press/v202/zhao23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023revisiting,
  abstract = {Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency.},
  author = {Yixiu Zhao and Scott W. Linderman},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42046--42057},
  pdf = {https://proceedings.mlr.press/v202/zhao23c/zhao23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Structured Variational Autoencoders},
  url = {https://proceedings.mlr.press/v202/zhao23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023pitfalls,
  abstract = {Test-Time Adaptation (TTA) has recently emerged as a promising approach for tackling the robustness challenge under distribution shifts. However, the lack of consistent settings and systematic studies in prior literature hinders thorough assessments of existing methods. To address this issue, we present TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art algorithms, a diverse array of distribution shifts, and two evaluation protocols. Through extensive experiments, our benchmark reveals three common pitfalls in prior efforts. First, selecting appropriate hyper-parameters, especially for model selection, is exceedingly difficult due to online batch dependency. Second, the effectiveness of TTA varies greatly depending on the quality and properties of the model being adapted. Third, even under optimal algorithmic conditions, none of the existing methods are capable of addressing all common types of distribution shifts.},
  author = {Hao Zhao and Yuejiang Liu and Alexandre Alahi and Tao Lin},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023pitfalls.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42058--42080},
  pdf = {https://proceedings.mlr.press/v202/zhao23d/zhao23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Pitfalls of Test-Time Adaptation},
  url = {https://proceedings.mlr.press/v202/zhao23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023addressing,
  abstract = {High-quality machine learning models are dependent on access to high-quality training data. When the data are not already available, it is tedious and costly to obtain them. Data markets help with identifying valuable training data: model consumers pay to train a model, the market uses that budget to identify data and train the model (the budget allocation problem), and finally the market compensates data providers according to their data contribution (revenue allocation problem). For example, a bank could pay the data market to access data from other financial institutions to train a fraud detection model. Compensating data contributors requires understanding data's contribution to the model; recent efforts to solve this revenue allocation problem based on the Shapley value are inefficient to lead to practical data markets.},
  author = {Boxin Zhao and Boxiang Lyu and Raul Castro Fernandez and Mladen Kolar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023addressing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42081--42097},
  pdf = {https://proceedings.mlr.press/v202/zhao23e/zhao23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Addressing Budget Allocation and Revenue Allocation in Data Market Environments Using an Adaptive Sampling Algorithm},
  url = {https://proceedings.mlr.press/v202/zhao23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023xpaste,
  abstract = {Copy-Paste is a simple and effective data augmentation strategy for instance segmentation. By randomly pasting object instances onto new background images, it creates new training data for free and significantly boosts the segmentation performance, especially for rare object categories. Although diverse, high-quality object instances used in Copy-Paste result in more performance gain, previous works utilize object instances either from human-annotated instance segmentation datasets or rendered from 3D object models, and both approaches are too expensive to scale up to obtain good diversity. In this paper, we revisit Copy-Paste at scale with the power of newly emerged zero-shot recognition models (e.g., CLIP) and text2image models (e.g., StableDiffusion). We demonstrate for the first time that using a text2image model to generate images or zero-shot recognition model to filter noisily crawled images for different object categories is a feasible way to make Copy-Paste truly scalable.},
  author = {Hanqing Zhao and Dianmo Sheng and Jianmin Bao and Dongdong Chen 0001 and Dong Chen 0003 and Fang Wen 0001 and Lu Yuan and Ce Liu 0001 and Wenbo Zhou and Qi Chu 0001 and Weiming Zhang 0001 and Nenghai Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023xpaste.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42098--42109},
  pdf = {https://proceedings.mlr.press/v202/zhao23f/zhao23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion},
  url = {https://proceedings.mlr.press/v202/zhao23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023revisiting,
  abstract = {Simple regret is a natural and parameter-free performance criterion for pure exploration in multi-armed bandits yet is less popular than the probability of missing the best arm or an $\varepsilon$-good arm, perhaps due to lack of easy ways to characterize it. In this paper, we make significant progress on minimizing simple regret in both data-rich ($T \geq n$) and data-poor regime ($T \leq n$) where $n$ is the number of arms, and $T$ is the number of samples. At its heart is our improved instance-dependent analysis of the well-known Sequential Halving (SH) algorithm, where we bound the probability of returning an arm whose mean reward is not within $\varepsilon$ from the best (i.e., not $\varepsilon$-good) for any choice of $\varepsilon > 0$, although $\varepsilon$ is not an input to SH. Our bound not only leads to an optimal worst-case simple regret bound of $\sqrt{n/T}$ up to logarithmic factors but also essentially matches the instance-dependent lower bound for returning an $\varepsilon$-good arm.},
  author = {Yao Zhao and Connor Stephens and Csaba Szepesv{\'a}ri and Kwang-Sung Jun},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42110--42158},
  pdf = {https://proceedings.mlr.press/v202/zhao23g/zhao23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Simple Regret: Fast Rates for Returning a Good Arm},
  url = {https://proceedings.mlr.press/v202/zhao23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023protecting,
  abstract = {Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and make fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs.},
  author = {Xuandong Zhao and Yu-Xiang Wang and Lei Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023protecting.pdf:pdf},
  mdate = {2023-11-20},
  pages = {42187--42199},
  pdf = {https://proceedings.mlr.press/v202/zhao23i/zhao23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Protecting Language Generation Models via Invisible Watermarking},
  url = {https://proceedings.mlr.press/v202/zhao23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023simplified,
  abstract = {Reinforcement learning (RL) is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but also when utilizing the representation as policy and value function features in model-free RL.},
  author = {Yi Zhao and Wenshuai Zhao and Rinu Boney and Juho Kannala and Joni Pajarinen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023simplified.pdf:pdf},
  mdate = {2025-05-19},
  pages = {42227--42246},
  pdf = {https://proceedings.mlr.press/v202/zhao23k/zhao23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Simplified Temporal Consistency Reinforcement Learning},
  url = {https://proceedings.mlr.press/v202/zhao23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023optimal,
  abstract = {We study the problem of online generalized linear regression in the stochastic setting, where the label is generated from a generalized linear model with possibly unbounded additive noise. We provide a sharp analysis of the classical follow-the-regularized-leader (FTRL) algorithm to cope with the label noise. More specifically, for σ-sub-Gaussian label noise, our analysis provides a regret upper bound of O(σ²d log T) + o(log T), where d is the dimension of the input vector, T is the total number of rounds. We also prove an Ω(σ²d log(T/d)) lower bound for stochastic online linear regression, which indicates that our upper bound is nearly optimal. In addition, we extend our analysis to a more refined Bernstein noise condition.},
  author = {Heyang Zhao and Dongruo Zhou and Jiafan He and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023optimal.pdf:pdf},
  mdate = {2024-12-24},
  pages = {42259--42279},
  pdf = {https://proceedings.mlr.press/v202/zhao23m/zhao23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Optimal Online Generalized Linear Regression with Stochastic Noise and Its Application to Heteroscedastic Bandits},
  url = {https://proceedings.mlr.press/v202/zhao23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023online,
  abstract = {We study the problem of online learning in a two-player decentralized cooperative Stackelberg game. In each round, the leader first takes an action, followed by the follower who takes their action after observing the leader's move. The goal of the leader is to learn to minimize the cumulative regret based on the history of interactions. Differing from the traditional formulation of repeated Stackelberg games, we assume the follower is omniscient, with full knowledge of the true reward, and that they always best-respond to the leader's actions. We analyze the sample complexity of regret minimization in this repeated Stackelberg game. We show that depending on the reward structure, the existence of the omniscient follower may change the sample complexity drastically, from constant to exponential, even for linear cooperative Stackelberg games. This poses unique challenges for the learning process of the leader and the subsequent regret analysis.},
  author = {Geng Zhao and Banghua Zhu and Jiantao Jiao and Michael I. Jordan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023online.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42304--42316},
  pdf = {https://proceedings.mlr.press/v202/zhao23o/zhao23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Online Learning in Stackelberg Games with an Omniscient Follower},
  url = {https://proceedings.mlr.press/v202/zhao23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhao2023rleg,
  abstract = {Vision-language representation learning models (e.g., CLIP) have achieved state-of-the-art performance on various downstream tasks, which usually need large-scale training data to learn discriminative representation. Recent progress on generative diffusion models (e.g., DALL-E 2) has demonstrated that diverse high-quality samples can be synthesized by randomly sampling from generative distribution. By virtue of generative capability in this paper, we propose a novel vision-language Representation Learning method with diffusion-based Embedding Generation (RLEG), which exploits diffusion models to generate feature embedding online for learning effective vision-language representation.},
  author = {Liming Zhao and Kecheng Zheng and Yun Zheng and Deli Zhao and Jingren Zhou},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhao2023rleg.pdf:pdf},
  mdate = {2025-03-19},
  pages = {42247--42258},
  pdf = {https://proceedings.mlr.press/v202/zhao23l/zhao23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation},
  url = {https://proceedings.mlr.press/v202/zhao23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023improved,
  abstract = {Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly existing in diffusion ODEs. Building upon these techniques, we achieve state-of-the-art likelihood estimation results on image datasets (2.56 on CIFAR-10, 3.43/3.69 on ImageNet-32) without variational dequantization or data augmentation.},
  author = {Kaiwen Zheng and Cheng Lu and Jianfei Chen and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023improved.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42363--42389},
  pdf = {https://proceedings.mlr.press/v202/zheng23c/zheng23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs},
  url = {https://proceedings.mlr.press/v202/zheng23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023evidential,
  abstract = {Medical image captioning alleviates the burden of physicians and possibly reduces medical errors by automatically generating text descriptions to describe image contents and convey findings. It is more challenging than conventional image captioning due to the complexity of medical images and the difficulty of aligning image regions with medical terms. In this paper, we propose an evidential interactive learning framework that leverages evidence-based uncertainty estimation and interactive machine learning to improve image captioning with limited labeled data. The interactive learning process involves three stages: keyword prediction, caption generation, and model retraining. First, the model predicts a list of keywords with evidence-based uncertainty and selects the most informative keywords to seek user feedback. Second, user-approved keywords are used as model input to guide the model to generate satisfactory captions.},
  author = {Ervine Zheng and Qi Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023evidential.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42478--42491},
  pdf = {https://proceedings.mlr.press/v202/zheng23g/zheng23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Evidential Interactive Learning for Medical Image Captioning},
  url = {https://proceedings.mlr.press/v202/zheng23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023structureinformed,
  abstract = {This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes).},
  author = {Zaixiang Zheng and Yifan Deng and Dongyu Xue and Yi Zhou and Fei Ye and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023structureinformed.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42317--42338},
  pdf = {https://proceedings.mlr.press/v202/zheng23a/zheng23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Structure-informed Language Models Are Protein Designers},
  url = {https://proceedings.mlr.press/v202/zheng23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023semisupervised,
  abstract = {Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories.},
  author = {Qinqing Zheng and Mikael Henaff and Brandon Amos and Aditya Grover},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023semisupervised.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42339--42362},
  pdf = {https://proceedings.mlr.press/v202/zheng23b/zheng23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories},
  url = {https://proceedings.mlr.press/v202/zheng23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023fast,
  abstract = {Diffusion models have found widespread adoption in various areas. However, their sampling process is slow because it requires hundreds to thousands of network evaluations to emulate a continuous process defined by differential equations. In this work, we use neural operators, an efficient method to solve the probability flow differential equations, to accelerate the sampling process of diffusion models. Compared to other fast sampling methods that have a sequential nature, we are the first to propose a parallel decoding method that generates images with only one model forward pass. We propose diffusion model sampling with neural operator (DSNO) that maps the initial condition, i.e., Gaussian distribution, to the continuous-time solution trajectory of the reverse diffusion process.},
  author = {Hongkai Zheng and Weili Nie and Arash Vahdat and Kamyar Azizzadenesheli and Anima Anandkumar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42390--42402},
  pdf = {https://proceedings.mlr.press/v202/zheng23d/zheng23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Sampling of Diffusion Models via Operator Learning},
  url = {https://proceedings.mlr.press/v202/zheng23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023outline,
  abstract = {For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. Inspired by this coarse-to-fine process, we propose ChainCoder, a program synthesis model that generates Python code progressively through multiple passes, using abstract syntax tree parsing to guide the generation process. ChainCoder leverages a tailored transformer architecture that jointly encodes natural language descriptions and input-output data samples, easing the reasoning procedure by breaking down complex code generation into manageable stages. Our experiments demonstrate that ChainCoder outperforms state-of-the-art code generation models across various benchmarks.},
  author = {Wenqing Zheng and S. P. Sharan and Ajay Kumar Jaiswal and Kevin Wang and Yihan Xi and Dejia Xu and Zhangyang Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023outline.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42403--42419},
  pdf = {https://proceedings.mlr.press/v202/zheng23e/zheng23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation},
  url = {https://proceedings.mlr.press/v202/zheng23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023revisiting,
  abstract = {A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, this paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, we consider the surrogate loss instead of the zero-one loss in analyses and generalize the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires O(log n) samples to approach its asymptotic error while the corresponding multiclass logistic regression requires O(n) samples, where n is the feature dimension. This provides theoretical justification for using generative classifiers in transfer learning scenarios with limited labeled data.},
  author = {Chenyu Zheng and Guoqiang Wu and Fan Bao and Yue Cao and Chongxuan Li and Jun Zhu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023revisiting.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42420--42477},
  pdf = {https://proceedings.mlr.press/v202/zheng23f/zheng23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting Discriminative vs. Generative Classifiers: Theory and Implications},
  url = {https://proceedings.mlr.press/v202/zheng23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zheng2023finding,
  abstract = {Real-world graphs generally have only one kind of tendency in their connections. These connections are either homophily-prone or heterophily-prone. While graphs with homophily-prone edges tend to connect nodes with the same class (i.e., intra-class nodes), heterophily-prone edges tend to build relationships between nodes with different classes (i.e., inter-class nodes). Existing GNNs only take the original graph during training. The problem with this approach is that it forgets to take into consideration the \"missing-half\" structural information, that is, heterophily-prone topology for homophily-prone graphs and homophily-prone topology for heterophily-prone graphs. In our paper, we introduce Graph cOmplementAry Learning, namely GOAL, which consists of two components: graph complementation and complemented graph convolution. The first component finds the missing-half structural information for a given graph to complement it. The complemented graph has two sets of graphs including both homophily- and heterophily-prone topology. In the latter component, to handle complemented graphs, we design a new graph convolution from the perspective of optimization. The experiment results show that GOAL consistently outperforms all baselines in eight real-world datasets.},
  author = {Yizhen Zheng and He Zhang and Vincent Cheng-Siong Lee and Yu Zheng and Xiao Wang and Shirui Pan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zheng2023finding.pdf:pdf},
  mdate = {2024-05-07},
  pages = {42492--42505},
  pdf = {https://proceedings.mlr.press/v202/zheng23h/zheng23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Finding the Missing-half: Graph Complementary Learning for Homophily-prone and Heterophily-prone Graphs},
  url = {https://proceedings.mlr.press/v202/zheng23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023implicit,
  abstract = {In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem y = β*ᵀx + ξ with sparse β*, neither minimum ℓ₁ or ℓ₂ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of ℓ₁ and ℓ₂ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. The result is based on careful analysis of the training dynamics and provides another example of implicit regularization effect that goes beyond norm minimization.},
  author = {Mo Zhou and Rong Ge},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023implicit.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42543--42573},
  pdf = {https://proceedings.mlr.press/v202/zhou23d/zhou23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression},
  url = {https://proceedings.mlr.press/v202/zhou23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023fourmer,
  abstract = {Global modeling-based image restoration frameworks have become popular. However, they often require a high memory footprint and do not consider task-specific degradation. Our work presents an alternative approach to global modeling that is more efficient for image restoration. The key insights which motivate our study are two-fold: 1) Fourier transform is capable of disentangling image degradation and content component to a certain extent, serving as the image degradation prior, and 2) Fourier domain innately embraces global properties, where each pixel in the Fourier space is involved with all spatial pixels. While adhering to the \"spatial interaction + channel evolution\" rule of previous studies, we customize the core designs with Fourier spatial interaction modeling and Fourier channel evolution. Our paradigm, Fourmer, achieves competitive performance on common image restoration tasks such as image de-raining, image enhancement, image dehazing, and guided image super-resolution, while requiring fewer computational resources.},
  author = {Man Zhou and Jie Huang and Chun-Le Guo and Chongyi Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023fourmer.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42589--42601},
  pdf = {https://proceedings.mlr.press/v202/zhou23f/zhou23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fourmer: An Efficient Global Modeling Paradigm for Image Restoration},
  url = {https://proceedings.mlr.press/v202/zhou23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023phaseaware,
  abstract = {Deep neural networks have been found to be vulnerable to adversarial noise. Recent works show that exploring the impact of adversarial noise on intrinsic components of data can help improve adversarial robustness. However, the pattern closely related to human perception has not been deeply studied. In this paper, inspired by the cognitive science, we investigate the interference of adversarial noise from the perspective of image phase, and find ordinarily-trained models lack enough robustness against phase-level perturbations. Motivated by this, we propose a joint adversarial defense method: a phase-level adversarial training mechanism to enhance the adversarial robustness on the phase pattern; an amplitude-based pre-processing operation to mitigate the adversarial perturbation in the amplitude pattern. Experimental results show that the proposed method can significantly improve the robust accuracy against multiple attacks and even adaptive attacks. In addition, ablation studies demonstrate the effectiveness of our defense strategy.},
  author = {Dawei Zhou and Nannan Wang and Heng Yang and Xinbo Gao and Tongliang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023phaseaware.pdf:pdf},
  mdate = {2024-05-07},
  pages = {42724--42741},
  pdf = {https://proceedings.mlr.press/v202/zhou23m/zhou23m.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Phase-aware Adversarial Defense for Improving Adversarial Robustness},
  url = {https://proceedings.mlr.press/v202/zhou23m.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023eliminating,
  abstract = {Deep neural networks (DNNs) are vulnerable to adversarial noise. Denoising model-based defense is a major protection strategy. However, denoising models may fail and induce negative effects in fully white-box scenarios. In this work, we start from the latent inherent properties of adversarial samples to break the limitations. Unlike solely learning a mapping from adversarial samples to natural samples, we aim to achieve denoising by destroying the spatial characteristics of adversarial noise and preserving the robust features of natural information. Motivated by this, we propose a defense based on information discard and robust representation restoration. Our method utilizes complementary masks to disrupt adversarial noise and guided denoising models to restore robust-predictive representations from masked samples. Extensive experiments demonstrate the effectiveness and robustness of our approach against various adversarial attacks.},
  author = {Dawei Zhou and Yukun Chen and Nannan Wang and Decheng Liu and Xinbo Gao and Tongliang Liu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023eliminating.pdf:pdf},
  mdate = {2024-05-07},
  pages = {42517--42530},
  pdf = {https://proceedings.mlr.press/v202/zhou23b/zhou23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Eliminating Adversarial Noise via Information Discard and Robust Representation Restoration},
  url = {https://proceedings.mlr.press/v202/zhou23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023brainformers,
  abstract = {Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse set of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also demonstrates a 3% higher SuperGLUE score with fine-tuning compared to GLaM with a similar number of activated parameters. Finally, Brainformer largely outperforms a Primer dense model derived with NAS with similar computation per token on fewshot evaluations.},
  author = {Yanqi Zhou and Nan Du and Yanping Huang and Daiyi Peng and Chang Lan and Da Huang and Siamak Shakeri and David R. So and Andrew M. Dai and Yifeng Lu and Zhifeng Chen and Quoc V. Le and Claire Cui and James Laudon and Jeff Dean},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023brainformers.pdf:pdf},
  mdate = {2023-12-01},
  pages = {42531--42542},
  pdf = {https://proceedings.mlr.press/v202/zhou23c/zhou23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Brainformers: Trading Simplicity for Efficiency},
  url = {https://proceedings.mlr.press/v202/zhou23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023controlled,
  abstract = {Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Extensive experiments on multiple datasets show that InstructCTG achieves state-of-the-art performance on various controlled text generation tasks while maintaining competitive performance on fluency and coherence metrics.},
  author = {Wangchunshu Zhou and Yuchen Eleanor Jiang and Ethan Wilcox and Ryan Cotterell and Mrinmaya Sachan},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023controlled.pdf:pdf},
  mdate = {2023-11-12},
  pages = {42602--42613},
  pdf = {https://proceedings.mlr.press/v202/zhou23g/zhou23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Controlled Text Generation with Natural Language Instructions},
  url = {https://proceedings.mlr.press/v202/zhou23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023nnsplitter,
  abstract = {As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users with the support of the trusted execution environment. Experimental results demonstrate the effectiveness of NNSplitter, e.g., by only modifying 275 out of over 11 million (i.e., 0.002%) weights, the accuracy of the obfuscated ResNet-18 model on CIFAR-10 can drop to 10%. Moreover, NNSplitter is stealthy and resilient against norm clipping and fine-tuning attacks, making it an appealing solution for DNN model protection.},
  author = {Tong Zhou and Yukui Luo and Shaolei Ren and Xiaolin Xu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023nnsplitter.pdf:pdf},
  mdate = {2024-11-08},
  pages = {42614--42624},
  pdf = {https://proceedings.mlr.press/v202/zhou23h/zhou23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation},
  url = {https://proceedings.mlr.press/v202/zhou23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023deep,
  abstract = {Methods based on ordinary differential equations (ODEs) are widely used to build generative models of time-series. In addition to high computational overhead due to explicitly computing hidden states recurrence, existing ODE-based models fall short in learning sequence data with sharp transitions - common in many real-world systems - due to numerical challenges during optimization. In this work, we propose LS4, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity. Inspired by recent deep state space models (S4), we achieve speedups by leveraging a convolutional representation of LS4 which bypasses the explicit evaluation of hidden states. We show that LS4 significantly outperforms previous continuous-time generative models in terms of marginal distribution, classification, and prediction scores on real-world datasets in the Monash Forecasting Repository, and is capable of modeling highly stochastic data with sharp transitions.},
  author = {Linqi Zhou and Michael Poli and Winnie Xu and Stefano Massaroli and Stefano Ermon},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023deep.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42625--42643},
  pdf = {https://proceedings.mlr.press/v202/zhou23i/zhou23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Deep Latent State Space Models for Time-Series Generation},
  url = {https://proceedings.mlr.press/v202/zhou23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023fast,
  abstract = {This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with $\mathcal{O}(n^3)$ runtime and $\mathcal{O}(n^2)$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the online relaxation technique introduced by a series of works (Rakhlin et al., 2012; Rakhlin \& Sridharan, 2015; 2017). We first prove an effective regret $\mathcal{O}(\sqrt{n^{1+\gamma}})$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm FastONL enjoying $\mathcal{O}(k\sqrt{n^{1+\gamma}})$ regret based on this relaxation. The key of FastONL is a generalized local push method that effectively approximates inverse matrix columns and applies to a series of popular kernels.},
  author = {Baojian Zhou and Yifan Sun and Reza Babanezhad Harikandeh},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023fast.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42658--42697},
  pdf = {https://proceedings.mlr.press/v202/zhou23k/zhou23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Fast Online Node Labeling for Very Large Graphs},
  url = {https://proceedings.mlr.press/v202/zhou23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023slotgat,
  abstract = {Heterogeneous graphs are ubiquitous to model complex data. There are urgent needs on powerful heterogeneous graph neural networks to effectively support important applications. We identify a potential semantic mixing issue in existing message passing processes, where the representations of the neighbors of a node v are forced to be transformed to the feature space of v for aggregation, though the neighbors are in different types. That is, the semantics in different node types are entangled together into node v's representation. To address the issue, we propose SlotGAT with separate message passing processes in slots, one for each node type, to maintain the representations in their own node-type feature spaces. Moreover, in a slot-based message passing layer, we design an attention mechanism for effective slot-wise message aggregation. Further, we develop a slot attention technique after the last layer of SlotGAT, to learn the importance of different slots in downstream tasks. Our analysis indicates that the slots in SlotGAT can preserve different semantics in various feature spaces. The superiority of SlotGAT is evaluated against 13 baselines on 6 datasets for node classification and link prediction.},
  author = {Ziang Zhou and Jieming Shi and Renchi Yang and Yuanhang Zou and Qing Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023slotgat.pdf:pdf},
  mdate = {2025-02-09},
  pages = {42644--42657},
  pdf = {https://proceedings.mlr.press/v202/zhou23j/zhou23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {SlotGAT: Slot-based Message Passing for Heterogeneous Graphs},
  url = {https://proceedings.mlr.press/v202/zhou23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023horizonfree,
  abstract = {We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\tilde{O}(\sqrt{\mathsf{Var}^\star M \Gamma S A K})$ regret bound where $\tilde{O}$ hides logarithm factors, $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, $\Gamma\le S$ is the maximum transition degree of any state-action pair, and $\mathsf{Var}^\star$ is a variance quantity describing the determinism of the LMDP. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. This is also the first problem-dependent regret bound for LMDP.},
  author = {Runlong Zhou and Ruosong Wang and Simon Shaolei Du},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023horizonfree.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42698--42723},
  pdf = {https://proceedings.mlr.press/v202/zhou23l/zhou23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Horizon-Free and Variance-Dependent Reinforcement Learning for Latent {M}arkov Decision Processes},
  url = {https://proceedings.mlr.press/v202/zhou23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023relational,
  abstract = {Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler-Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve graph isomorphism distinguishing power of message passing neural networks. The method is then extended to higher-dimensional WL, leading to a novel k,l-WL algorithm, a more general framework than k-WL. We further introduce the subgraph concept into our hierarchy and propose a localized k,l-WL framework, incorporating a wide range of existing work, including many subgraph GNNs. Theoretically, we analyze the expressivity of k,l-WL w.r.t. k and l and compare it with the traditional k-WL. Complexity reduction methods are also systematically discussed to build powerful and practical k,l-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our k,l-GNNs achieve superior performance on many synthetic and real-world datasets, which verifies the effectiveness of our framework.},
  author = {Cai Zhou and Xiyuan Wang and Muhan Zhang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023relational.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42742--42768},
  pdf = {https://proceedings.mlr.press/v202/zhou23n/zhou23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {From Relational Pooling to Subgraph {GNN}s: A Universal Framework for More Expressive Graph Neural Networks},
  url = {https://proceedings.mlr.press/v202/zhou23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023learning,
  abstract = {A complex system with cluttered observations may be a coupled mixture of multiple simple sub-systems corresponding to latent entities. Such sub-systems may hold distinct dynamics in the continuous-time domain; therein, complicated interactions between sub-systems also evolve over time. This setting is fairly common in the real world but has been less considered. In this paper, we propose a sequential learning approach under this setting by decoupling a complex system for handling irregularly sampled and cluttered sequential observations. Such decoupling brings about not only subsystems describing the dynamics of each latent entity but also a meta-system capturing the interaction between entities over time. Specifically, we argue that the meta-system evolving within a simplex is governed by projected differential equations (ProjDEs).},
  author = {Zihan Zhou and Tianshu Yu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023learning.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42810--42828},
  pdf = {https://proceedings.mlr.press/v202/zhou23q/zhou23q.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning to Decouple Complex Systems},
  url = {https://proceedings.mlr.press/v202/zhou23q.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023threeregime,
  abstract = {Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. We build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals that the dichotomous effect of high temperature is associated with transitions between distinct types of global structures in the post-pruned model. Our new insights lead to new practical approaches of hyperparameter tuning and model selection to improve pruning.},
  author = {Yefan Zhou and Yaoqing Yang and Arin Chang and Michael W. Mahoney},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023threeregime.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42790--42809},
  pdf = {https://proceedings.mlr.press/v202/zhou23p/zhou23p.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {A Three-regime Model of Network Pruning},
  url = {https://proceedings.mlr.press/v202/zhou23p.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023sharp,
  abstract = {We study variance-dependent regret bounds for Markov decision processes (MDPs). Algorithms with variance-dependent regret guarantees can automatically exploit environments with low variance (e.g., enjoying constant regret on deterministic MDPs). The existing algorithms are either variance-independent or suboptimal. We first propose two new environment norms to characterize the fine-grained variance properties of the environment. For model-based methods, we design a variant of the MVP algorithm (Zhang et al., 2021a). We apply new analysis techniques to demonstrate that this algorithm enjoys variance-dependent bounds with respect to the norms we propose. In particular, this bound is simultaneously minimax optimal for both stochastic and deterministic MDPs, the first result of its kind.},
  author = {Runlong Zhou and Zihan Zhang and Simon Shaolei Du},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023sharp.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42878--42914},
  pdf = {https://proceedings.mlr.press/v202/zhou23t/zhou23t.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Sharp Variance-Dependent Bounds in Reinforcement Learning: Best of Both Worlds in Stochastic and Deterministic Environments},
  url = {https://proceedings.mlr.press/v202/zhou23t.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023strengthening,
  abstract = {Although powerful graph neural networks (GNNs) have boosted numerous real-world applications, the potential privacy risk is still underexplored. To close this gap, we perform the first comprehensive study of graph reconstruction attack that aims to reconstruct the adjacency of nodes. We show that a range of factors in GNNs can lead to the surprising leakage of private links. Especially by taking GNNs as a Markov chain and attacking GNNs via a flexible chain approximation, we systematically explore the underneath principles of graph reconstruction attack, and propose two information theory-guided mechanisms: (1) the chain-based attack method with adaptive designs for extracting more private information; (2) the chain-based defense method that sharply reduces the attack fidelity with moderate accuracy loss. Such two objectives disclose a critical belief that to recover better in attack, you must extract more multi-aspect knowledge from the trained GNN; while to learn safer for defense, you must forget more link-sensitive information in training GNNs. Empirically, we achieve state-of-the-art results on six datasets and three common GNNs.},
  author = {Zhanke Zhou and Chenyu Zhou and Xuan Li and Jiangchao Yao and Quanming Yao and Bo Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023strengthening.pdf:pdf},
  mdate = {2025-05-16},
  pages = {42843--42877},
  pdf = {https://proceedings.mlr.press/v202/zhou23s/zhou23s.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {On Strengthening and Defending Graph Reconstruction Attack with {M}arkov Chain Approximation},
  url = {https://proceedings.mlr.press/v202/zhou23s.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhou2023esc,
  abstract = {The ability to accurately locate and navigate to a specific object is a crucial capability for embodied agents that operate in the real world and interact with objects to complete tasks. Such object navigation tasks usually require large-scale training in visual environments with labeled objects, which generalizes poorly to novel objects in unknown environments. In this work, we present a novel zero-shot object navigation method, Exploration with Soft Commonsense constraints (ESC), that transfers commonsense knowledge in pre-trained models to open-world object navigation without any navigation experience nor any other training on the visual environments. First, ESC leverages a pre-trained vision and language model for open-world prompt-based grounding and a pre-trained commonsense language model for room and object reasoning. Then ESC converts commonsense knowledge into navigation actions by modeling it as soft logic predicates for efficient exploration. Extensive experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method improves significantly over baselines, and achieves new state-of-the-art results for zero-shot object navigation (e.g., 288\% relative Success Rate improvement than CoW on MP3D).},
  author = {Kaiwen Zhou and Kaizhi Zheng and Connor Pryor and Yilin Shen and Hongxia Jin and Lise Getoor and Xin Eric Wang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhou2023esc.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42829--42842},
  pdf = {https://proceedings.mlr.press/v202/zhou23r/zhou23r.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{ESC}: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation},
  url = {https://proceedings.mlr.press/v202/zhou23r.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023benign,
  abstract = {This paper focuses on over-parameterized deep neural networks (DNNs) with ReLU activation functions and proves that when the data distribution is well-separated, DNNs can achieve Bayes-optimal test error for classification while obtaining (nearly) zero-training error under the lazy training regime. For this purpose, we unify three interrelated concepts of overparameterization, benign overfitting, and the Lipschitz constant of DNNs. Our results indicate that interpolating with smoother functions leads to better generalization. Furthermore, we investigate the special case where interpolating smooth ground-truth functions is performed by DNNs under the Neural Tangent Kernel (NTK) regime for generalization.},
  author = {Zhenyu Zhu and Fanghui Liu and Grigorios Chrysos and Francesco Locatello and Volkan Cevher},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023benign.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43105--43128},
  pdf = {https://proceedings.mlr.press/v202/zhu23h/zhu23h.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Benign Overfitting in Deep Neural Networks under Lazy Training},
  url = {https://proceedings.mlr.press/v202/zhu23h.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023learning,
  abstract = {Current approaches for training robust models are typically tailored to scenarios where data variations are accessible in the training set. While shown effective in achieving robustness to these foreseen variations, these approaches are ineffective in learning unforeseen robustness, i.e., robustness to data variations without known characterization or training examples reflecting them. In this work, we learn unforeseen robustness by harnessing the variations in the abundant out-of-distribution data. To overcome the main challenge of using such data, the domain gap, we use a domain translator to bridge it and bound the unforeseen robustness on the target distribution. As implied by our analysis, we propose a two-step algorithm that first trains an equivariant domain translator to map out-of-distribution data to the target distribution while preserving the considered variation, and then regularizes a model's output consistency on the domain-translated data to improve its robustness. We empirically show the effectiveness of our approach in improving unforeseen and foreseen robustness compared to existing approaches. Additionally, we show that training the equivariant domain translator serves as an effective criterion for source data selection.},
  author = {Sicheng Zhu and Bang An and Furong Huang and Sanghyun Hong},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023learning.pdf:pdf},
  mdate = {2025-01-21},
  pages = {42915--42937},
  pdf = {https://proceedings.mlr.press/v202/zhu23a/zhu23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Learning Unforeseen Robustness from Out-of-distribution Data Using Equivariant Domain Translator},
  url = {https://proceedings.mlr.press/v202/zhu23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023mixture,
  abstract = {The task of mixture proportion estimation (MPE) is to estimate the weight of a component distribution in a mixture, given observations from both the component and mixture. Previous work on MPE adopts the irreducibility assumption, which ensures identifiablity of the mixture proportion. In this paper, we propose a more general sufficient condition that accommodates several settings of interest where irreducibility does not hold. We further present a resampling-based meta-algorithm that takes any existing MPE algorithm designed to work under irreducibility and adapts it to work under our more general condition. Our approach empirically exhibits improved estimation performance relative to baseline methods and to a recently proposed regrouping-based algorithm.},
  author = {Yilun Zhu and Aaron Fjeldsted and Darren Holland and George Landon and Azaree Lintereur and Clayton Scott},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023mixture.pdf:pdf},
  mdate = {2025-05-13},
  pages = {42962--42982},
  pdf = {https://proceedings.mlr.press/v202/zhu23c/zhu23c.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Mixture Proportion Estimation Beyond Irreducibility},
  url = {https://proceedings.mlr.press/v202/zhu23c.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023exploring,
  abstract = {Adversarial poisoning attacks pose huge threats to various machine learning applications. Especially, the recent accumulative poisoning attacks show that it is possible to achieve irreparable harm on models via a sequence of imperceptible attacks followed by a trigger batch. Due to the limited data-level discrepancy in real-time data streaming, current defensive methods are indiscriminate in handling the poison and clean samples. In this paper, we dive into the perspective of model dynamics and propose a novel information measure, namely, Memorization Discrepancy, to explore the defense via the model-level information. By implicitly transferring the changes in the data manipulation to that in the model outputs, Memorization Discrepancy can discover the imperceptible poison samples based on their distinct dynamics from the clean samples. We thoroughly explore its properties and propose Discrepancy-aware Sample Correction (DSC) to defend against accumulative poisoning attacks.},
  author = {Jianing Zhu and Xiawei Guo and Jiangchao Yao and Chao Du and Li He and Shuo Yuan and Tongliang Liu and Liang Wang and Bo Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023exploring.pdf:pdf},
  mdate = {2024-08-13},
  pages = {42983--43004},
  pdf = {https://proceedings.mlr.press/v202/zhu23d/zhu23d.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Exploring Model Dynamics for Accumulative Poisoning Discovery},
  url = {https://proceedings.mlr.press/v202/zhu23d.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023decentralized,
  abstract = {Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-β-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch scenarios.},
  author = {Tongtian Zhu and Fengxiang He and Kaixuan Chen and Mingli Song and Dacheng Tao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023decentralized.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43005--43036},
  pdf = {https://proceedings.mlr.press/v202/zhu23e/zhu23e.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Decentralized SGD and Average-direction SAM are Asymptotically Equivalent},
  url = {https://proceedings.mlr.press/v202/zhu23e.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023principled,
  abstract = {We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). We show that when the underlying true reward is linear, under both Bradley-Terry-Luce (BTL) model (pairwise comparison) and Plackett-Luce (PL) model (K-wise comparison), MLE converges under certain semi-norm for the family of linear reward. On the other hand, when training a policy based on the learned reward model, we show that MLE fails while a pessimistic MLE provides policies with good performance under certain coverage assumption. We also show that under the PL model, both the true MLE and a different MLE which splits the K-wise comparison into pairwise comparisons converge, while the true MLE is asymptotically more efficient. Our results validate the empirical success of the existing RLHF algorithms, and provide new insights for algorithm design.},
  author = {Banghua Zhu and Michael I. Jordan and Jiantao Jiao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023principled.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43037--43067},
  pdf = {https://proceedings.mlr.press/v202/zhu23f/zhu23f.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons},
  url = {https://proceedings.mlr.press/v202/zhu23f.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023unleashing,
  abstract = {Out-of-distribution (OOD) detection is an indispensable aspect of secure AI when deploying machine learning models in real-world applications. Previous paradigms either explore better scoring functions or utilize the knowledge of outliers to equip the models with the ability of OOD detection. However, few of them pay attention to the intrinsic OOD detection capability of the given model. In this work, we generally discover the existence of an intermediate stage of a model trained on in-distribution (ID) data having higher OOD detection performance than that of its final stage across different settings, and further identify one critical data-level attribution to be learning with the atypical samples. Based on such insights, we propose a novel method, Unleashing Mask, which aims to restore the OOD discriminative capabilities of the well-trained model with ID data. Our method utilizes a mask to figure out the memorized atypical samples, and then finetune the model or prune it with the introduced mask to forget them. Extensive experiments and analysis demonstrate the effectiveness of our method.},
  author = {Jianing Zhu and Hengzhuang Li and Jiangchao Yao and Tongliang Liu and Jianliang Xu and Bo Han},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023unleashing.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43068--43104},
  pdf = {https://proceedings.mlr.press/v202/zhu23g/zhu23g.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability},
  url = {https://proceedings.mlr.press/v202/zhu23g.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023interpolation,
  abstract = {We propose to study and promote the robustness of a model as per its performance on a continuous geodesic interpolation of subpopulations, e.g., a class of samples in a classification problem. Specifically, (1) we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions. (2) we regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on four datasets including CIFAR-100 and ImageNet, establish the efficacy of our method, e.g., our method improves the baselines' certifiable robustness on CIFAR10 upto 7.7%, with 16.8% on empirical robustness on CIFAR-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpolation with a practical off-the-shelf strategy that can be combined with existing robust training methods.},
  author = {Jiacheng Zhu and Jielin Qiu and Aritra Guha and Zhuolin Yang and XuanLong Nguyen and Bo Li and Ding Zhao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023interpolation.pdf:pdf},
  mdate = {2024-08-13},
  pages = {43129--43157},
  pdf = {https://proceedings.mlr.press/v202/zhu23i/zhu23i.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics},
  url = {https://proceedings.mlr.press/v202/zhu23i.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023leadfl,
  abstract = {We propose LeadFL, a client-self defense that is combined with existing server-side defenses to thwart backdoor and targeted attacks. The core idea of LeadFL is a novel regularization term in local model training such that the Hessian matrix of local gradients is nullified. We provide the convergence analysis of LeadFL and its robustness guarantee in terms of certified radius. Our empirical evaluation shows that LeadFL is able to mitigate bursty adversarial patterns for both iid and non-iid data distributions. It frequently reduces the backdoor accuracy from more than 75% for state-of-the-art defenses to less than 10% while its impact on the main task accuracy is always less than for other client-side defenses.},
  author = {Chaoyi Zhu and Stefanie Roos and Lydia Y. Chen},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023leadfl.pdf:pdf},
  mdate = {2023-08-28},
  pages = {43158--43180},
  pdf = {https://proceedings.mlr.press/v202/zhu23j/zhu23j.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {LeadFL: Client Self-Defense against Model Poisoning in Federated Learning},
  url = {https://proceedings.mlr.press/v202/zhu23j.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023markovian,
  abstract = {Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practitioners. We leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. This method performs favourably compared to existing approaches whilst being computationally highly scalable.},
  author = {Harrison Zhu and Carles Balsells Rodas and Yingzhen Li},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023markovian.pdf:pdf},
  mdate = {2023-08-28},
  pages = {42938--42961},
  pdf = {https://proceedings.mlr.press/v202/zhu23b/zhu23b.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Markovian Gaussian Process Variational Autoencoders},
  url = {https://proceedings.mlr.press/v202/zhu23b.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023xtab,
  abstract = {The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce {XTab}, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the {OpenML}-{AutoML} Benchmark ({AMLB}), we show that (1) {XTab} consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining {FT}-Transformer via {XTab}, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, and multiclass classification.},
  author = {Bingzhao Zhu and Xingjian Shi and Nick Erickson and Mu Li 0003 and George Karypis and Mahsa Shoaran},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023xtab.pdf:pdf},
  pages = {43181--43204},
  pdf = {https://proceedings.mlr.press/v202/zhu23k/zhu23k.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {{XTab}: Cross-table Pretraining for Tabular Transformers},
  url = {https://proceedings.mlr.press/v202/zhu23k.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023provable,
  abstract = {This paper considers a novel application of deep {AUC} maximization ({DAM}) for multi-instance learning ({MIL}), in which a single class label is assigned to a bag of instances (e.g., multiple {2D} slices of a {CT} scan for a patient). The paper addresses a neglected yet non-negligible computational challenge of {MIL} in the context of {DAM}, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of {MIL}. To tackle this challenge, they propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, they propose a unified and provable multi-instance {DAM} ({MIDAM}) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a stochastic gradient estimator and to update the model parameter. They establish a similar convergence rate of the proposed {MIDAM} algorithm as the state-of-the-art {DAM} algorithms. Their extensive experiments on conventional {MIL} datasets and medical datasets demonstrate the superiority of their {MIDAM} algorithm.},
  author = {Dixian Zhu and Bokun Wang and Zhi Chen 0025 and Yaxing Wang and Milan Sonka and Xiaodong Wu 0001 and Tianbao Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023provable.pdf:pdf},
  pages = {43205--43227},
  pdf = {https://proceedings.mlr.press/v202/zhu23l/zhu23l.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Provable Multi-instance Deep {AUC} Maximization with Stochastic Pooling},
  url = {https://proceedings.mlr.press/v202/zhu23l.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023weak,
  abstract = {Evaluating fairness can be challenging in practice because the sensitive attributes of data are often inaccessible due to privacy constraints. The go-to approach that the industry frequently adopts is using off-the-shelf proxy models to predict the missing sensitive attributes, e.g. Meta [Alao et al., 2021] and Twitter [Belli et al., 2022]. Despite its popularity, there are three important questions unanswered: (1) Is directly using proxies efficacious in measuring fairness? (2) If not, is it possible to accurately evaluate fairness using proxies only? (3) Given the ethical controversy over inferring user private information, is it possible to only use weak (i.e. inaccurate) proxies in order to protect privacy? Our theoretical analyses show that directly using proxy models can give a false sense of (un)fairness. Second, we develop an algorithm that is able to measure fairness (provably) accurately with only three properly identified proxies. Third, we show that our algorithm allows the use of only weak proxies (e.g. with only 68.85\% accuracy on {COMPAS}), adding an extra layer of protection on user privacy. Experiments validate our theoretical analyses and show our algorithm can effectively measure and mitigate bias.},
  author = {Zhaowei Zhu and Yuanshun Yao and Jiankai Sun and Hang Li 0001 and Yang Liu 0018},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023weak.pdf:pdf},
  pages = {43258--43288},
  pdf = {https://proceedings.mlr.press/v202/zhu23n/zhu23n.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Weak Proxies are Sufficient and Preferable for Fairness with Missing Sensitive Attributes},
  url = {https://proceedings.mlr.press/v202/zhu23n.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhu2023label,
  abstract = {We study a family of loss functions named label-distributionally robust ({LDR}) losses for multi-class classification that are formulated from distributionally robust optimization ({DRO}) perspective, where the uncertainty in the given label information are modeled and captured by taking the worse case of distributional weights. The benefits of this perspective are several fold: (i) it provides a unified framework to explain the classical cross-entropy ({CE}) loss and {SVM} loss and their variants, (ii) it includes a special family corresponding to the temperature-scaled {CE} loss, which is widely adopted but poorly understood; (iii) it allows us to achieve adaptivity to the uncertainty degree of label information at an instance level. Our contributions include: (1) we study both consistency and robustness by establishing top-k (∀ k≥ 1) consistency of {LDR} losses for multi-class classification, and a negative result that a top-1 consistent and symmetric robust loss cannot achieve top-k consistency simultaneously for all k≥ 2; (2) we propose a new adaptive {LDR} loss that automatically adapts the individualized temperature parameter to the noise degree of class label of each instance; (3) we demonstrate stable and competitive performance for the proposed adaptive {LDR} loss on 7 benchmark datasets under 6 noisy label and 1 clean settings against 13 loss functions, and on one real-world noisy dataset.},
  author = {Dixian Zhu and Yiming Ying and Tianbao Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhu2023label.pdf:pdf},
  pages = {43289--43325},
  pdf = {https://proceedings.mlr.press/v202/zhu23o/zhu23o.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Label Distributionally Robust Losses for Multi-class Classification: Consistency, Robustness and Adaptivity},
  url = {https://proceedings.mlr.press/v202/zhu23o.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zhuang2023likelihood,
  abstract = {Clustering is a widely deployed unsupervised learning tool. Model-based clustering is a flexible framework to tackle data heterogeneity when the clusters have different shapes. Likelihood-based inference for mixture distributions often involves non-convex and high-dimensional objective functions, imposing difficult computational and statistical challenges. The classic expectation-maximization ({EM}) algorithm is a computationally thrifty iterative method that maximizes a surrogate function minorizing the log-likelihood of observed data in each iteration, which however suffers from bad local maxima even in the special case of the standard Gaussian mixture model with common isotropic covariance matrices. In this work, we develop likelihood adjusted semidefinite programs ({LSDPs}) for clustering, which integrate likelihood-based objectives with convex semidefinite relaxation. Through a careful analysis of the semidefinite relaxation, we establish sufficient conditions under which the proposed method exactly recovers the ground truth cluster assignment. Our approach enjoys global optimality and can handle general non-convex clustering objectives. Extensive experiments on simulated and real data show that the proposed method significantly outperforms state-of-the-art methods in challenging settings.},
  author = {Yubo Zhuang and Xiaohui Chen and Yun Yang},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zhuang2023likelihood.pdf:pdf},
  pages = {43326--43346},
  pdf = {https://proceedings.mlr.press/v202/zhuang23a/zhuang23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data},
  url = {https://proceedings.mlr.press/v202/zhuang23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{ziomek2023random,
  abstract = {Learning decompositions of expensive-to-evaluate black-box functions promises to scale {Bayesian} optimisation ({BO}) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. We propose the random decomposition upper-confidence bound algorithm ({RDUCB}) that is straightforward to implement - (almost) plug-and-play - and, surprisingly, yields significant empirical gains compared to the previous state-of-the-art on a comprehensive set of benchmarks.},
  author = {Juliusz Krysztof Ziomek and Haitham Bou-Ammar},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/ziomek2023random.pdf:pdf},
  pages = {43347--43368},
  pdf = {https://proceedings.mlr.press/v202/ziomek23a/ziomek23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Are Random Decompositions all we need in High Dimensional {Bayesian} Optimisation?},
  url = {https://proceedings.mlr.press/v202/ziomek23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zitovsky2023revisiting,
  abstract = {Offline model selection ({OMS}), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline {RL} in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared {Bellman} error ({MSBE}) of the associated Q-functions. However, previous work has struggled to obtain adequate {OMS} performance with {Bellman} errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with {Bellman} errors and identify conditions under which {OMS} algorithms based on {Bellman} errors will perform well. Moreover, we develop a new estimator of the {MSBE} that is more accurate than prior methods. Our estimator obtains impressive {OMS} performance on diverse discrete control tasks, including Atari games.},
  author = {Joshua P. Zitovsky and Daniel de Marchi and Rishabh Agarwal and Michael Rene Kosorok},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zitovsky2023revisiting.pdf:pdf},
  pages = {43369--43406},
  pdf = {https://proceedings.mlr.press/v202/zitovsky23a/zitovsky23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {Revisiting {Bellman} Errors for Offline Model Selection},
  url = {https://proceedings.mlr.press/v202/zitovsky23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}

@inproceedings{zou2023benefits,
  abstract = {{Mixup}, a simple data augmentation method that randomly mixes two data points via linear interpolation, has been extensively applied in various deep learning applications to gain better generalization. However, the theoretical underpinnings of its efficacy are not yet fully understood. In this paper, we aim to seek a fundamental understanding of the benefits of {Mixup}. We first show that {Mixup} using different linear interpolation parameters for features and labels can still achieve similar performance to the standard {Mixup}. This indicates that the intuitive linearity explanation in Zhang et al., (2018) may not fully explain the success of {Mixup}. Then we perform a theoretical study of {Mixup} from the feature learning perspective. We consider a feature-noise data model and show that {Mixup} training can effectively learn the rare features (appearing in a small fraction of data) from its mixture with the common features (appearing in a large fraction of data). In contrast, standard training can only learn the common features but fails to learn the rare features, thus suffering from bad generalization performance. Moreover, our theoretical analysis also shows that the benefits of {Mixup} for feature learning are mostly gained in the early training phase, based on which we propose to apply early stopping in {Mixup}. Experimental results verify our theoretical findings and demonstrate the effectiveness of the early-stopped {Mixup} training.},
  author = {Difan Zou and Yuan Cao 0006 and Yuanzhi Li and Quanquan Gu},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  file = {:/home/b/documents/inproceedings/zou2023benefits.pdf:pdf},
  pages = {43423--43479},
  pdf = {https://proceedings.mlr.press/v202/zou23a/zou23a.pdf},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  title = {The Benefits of {Mixup} for Feature Learning},
  url = {https://proceedings.mlr.press/v202/zou23a.html},
  urltype = {oa},
  volume = {202},
  year = {2023}
}
